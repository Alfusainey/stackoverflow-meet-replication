@article{10.5555/3503984.3503990,
author = {Crandall, Kalee},
title = {Knowledge sharing technology in school counseling: a literature review},
year = {2021},
issue_date = {October 2021},
publisher = {Consortium for Computing Sciences in Colleges},
address = {Evansville, IN, USA},
volume = {37},
number = {2},
issn = {1937-4771},
abstract = {School counselors are expected to perform a wide range of tasks to improve student outcomes but are oftentimes limited in the resources needed to perform these tasks. The lack of resources, including time and the knowledge needed to complete tasks effectively, may contribute to unnecessary stress and possible burnout. This research uses a modified systematic literature review process to explore knowledge sharing technology in school counseling and presents a proposed model for future research adapted from Alavi's Model of Knowledge Transfer among Individuals in a Group. The findings of this study outline the current state of research and are relevant to research and practice.},
journal = {J. Comput. Sci. Coll.},
month = oct,
pages = {61–69},
numpages = {9}
}

@article{10.1145/2629445,
author = {Wang, G. Alan and Wang, Harry Jiannan and Li, Jiexun and Abrahams, Alan S. and Fan, Weiguo},
title = {An Analytical Framework for Understanding Knowledge-Sharing Processes in Online Q&amp;A Communities},
year = {2014},
issue_date = {March 2015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {5},
number = {4},
issn = {2158-656X},
url = {https://doi.org/10.1145/2629445},
doi = {10.1145/2629445},
abstract = {Online communities have become popular knowledge sources for both individuals and organizations. Computer-mediated communication research shows that communication patterns play an important role in the collaborative efforts of online knowledge-sharing activities. Existing research is mainly focused on either user egocentric positions in communication networks or communication patterns at the community level. Very few studies examine thread-level communication and process patterns and their impacts on the effectiveness of knowledge sharing. In this study, we fill this research gap by proposing an innovative analytical framework for understanding thread-level knowledge sharing in online Q&amp;A communities based on dialogue act theory, network analysis, and process mining. More specifically, we assign a dialogue act tag for each post in a discussion thread to capture its conversation purpose and then apply graph and process mining algorithms to examine knowledge-sharing processes. Our results, which are based on a real support forum dataset, show that the proposed analytical framework is effective in identifying important communication, conversation, and process patterns that lead to helpful knowledge sharing in online Q&amp;A communities.},
journal = {ACM Trans. Manage. Inf. Syst.},
month = dec,
articleno = {18},
numpages = {31},
keywords = {process mining, online community, knowledge sharing, dialogue act, communication network, Computer-mediated communication}
}

@article{10.1145/3441302,
author = {Ghasemi, Negin and Fatourechi, Ramin and Momtazi, Saeedeh},
title = {User Embedding for Expert Finding in Community Question Answering},
year = {2021},
issue_date = {August 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {15},
number = {4},
issn = {1556-4681},
url = {https://doi.org/10.1145/3441302},
doi = {10.1145/3441302},
abstract = {The number of users who have the appropriate knowledge to answer asked questions in community question answering is lower than those who ask questions. Therefore, finding expert users who can answer the questions is very crucial and useful. In this article, we propose a framework to find experts for given questions and assign them the related questions. The proposed model benefits from users’ relations in a community along with the lexical and semantic similarities between new question and existing answers. Node embedding is applied to the community graph to find similar users. Our experiments on four different Stack Exchange datasets show that adding community relations improves the performance of expert finding models.},
journal = {ACM Trans. Knowl. Discov. Data},
month = mar,
articleno = {70},
numpages = {16},
keywords = {semantic text similarity, graph embedding, community question answering, Expert finding}
}

@article{10.1145/3603398,
author = {Ahmed, Muzamil and Khan, Hikmat Ullah and Khan, Muhammad Attique and Tariq, Usman and Kadry, Seifedine},
title = {Context-aware Answer Selection in Community Question Answering Exploiting Spatial Temporal Bidirectional Long Short-Term Memory},
year = {2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
issn = {2375-4699},
url = {https://doi.org/10.1145/3603398},
doi = {10.1145/3603398},
abstract = {Community Question Answering (CQA) sites provide knowledge sharing facility as the users can post questions and other users can share their answers. The selection of top-quality answers from the set of answers in a thread is a significant and challenging task in Natural Language Processing (NLP). To address this issue, we propose a deep learning based spatial temporal Bidirectional Long Short-Term Memory (Bi-LSTM) algorithm. The existing studies mainly focus only computing semantic similarity between questions and answers using votes given by the users. The proposed hybrid approach, based on both forward and backward, consider question to answer and answer to answer similarity. The forward LSTM captures the spatial impact of the answer to estimate the relevancy, whereas the backward LSTM learns temporal features with the answer to predict the best quality answer. Moreover, spatial Bi-LSTM captures past and future dependencies for a better understanding of context and to improve the effectiveness of answer selection. For extracting meaningful information from noisy text data, data is preprocessed following standard steps such as tokenization, parsing, lemmatization, stop words removal, part of speech tagging and entities extraction. Word embeddings-based Paragraph to vector (par2vec) has additional input nodes to represent paragraph information in vector for context understanding. The empirical analysis carried out on the SemEval CQA dataset shows that the proposed model outperforms state-of-art answer selection approaches.},
note = {Just Accepted},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = jun,
keywords = {Word Embeddings, Spatial Temporal Bi-LSTM, Deep Learning, Community Question Answering, Natural Language Processing}
}

@article{10.1145/3565799,
author = {Wu, Di and Jing, Xiao-Yuan and Zhang, Hongyu and Feng, Yang and Chen, Haowen and Zhou, Yuming and Xu, Baowen},
title = {Retrieving API Knowledge from Tutorials and Stack Overflow Based on Natural Language Queries},
year = {2023},
issue_date = {September 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {32},
number = {5},
issn = {1049-331X},
url = {https://doi.org/10.1145/3565799},
doi = {10.1145/3565799},
abstract = {When encountering unfamiliar APIs, developers tend to seek help from API tutorials and Stack Overflow (SO). API tutorials help developers understand the API knowledge in a general context, while SO often explains the API knowledge in a specific programming task. Thus, tutorials and SO posts together can provide more API knowledge. However, it is non-trivial to retrieve API knowledge from both API tutorials and SO posts based on natural language queries. Two major problems are irrelevant API knowledge in two different resources and the lexical gap between the queries and documents. In this article, we regard a fragment in tutorials and a Question and Answering (Q&amp;A) pair in SO as a knowledge item (KI). We generate ⟨ API, FRA⟩ pairs (FRA stands for fragment) from tutorial fragments and APIs and build ⟨ API, QA⟩ pairs based on heuristic rules of SO posts. We fuse ⟨ API, FRA⟩ pairs and ⟨ API, QA⟩ pairs to generate API knowledge (AK for short) datasets, where each data item is an ⟨ API, KI⟩ pair. We propose a novel approach, called PLAN, to automatically retrieve API knowledge from both API tutorials and SO posts based on natural language queries. PLAN contains three main stages: (1) API knowledge modeling, (2) query mapping, and (3) API knowledge retrieving. It first utilizes a deep-transfer-metric-learning-based relevance identification (DTML) model to effectively find relevant ⟨ API, KI⟩ pairs containing two different knowledge items (⟨ API, QA⟩ pairs and ⟨ API, FRA⟩ pairs) simultaneously. Then, PLAN generates several potential APIs as a way to reduce the lexical gap between the query and ⟨ API, KI⟩ pairs. According to potential APIs, we can select relevant ⟨ API, KI⟩ pairs to generate potential results. Finally, PLAN returns a list of ranked ⟨ API, KI⟩ pairs that are related to the query. We evaluate the effectiveness of PLAN with 270 queries on Java and Android AK datasets containing 10,072 ⟨ API, KI⟩ pairs. Our experimental results show that PLAN is effective and outperforms the state-of-the-art approaches. Our user study further confirms the effectiveness of PLAN in locating useful API knowledge.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = jul,
articleno = {109},
numpages = {36},
keywords = {natural language queries, deep transfer metric learning, Stack Overflow, API tutorial}
}

@article{10.1145/3550150,
author = {Gao, Zhipeng and Xia, Xin and Lo, David and Grundy, John and Zhang, Xindong and Xing, Zhenchang},
title = {I Know What You Are Searching for: Code Snippet Recommendation from Stack Overflow Posts},
year = {2023},
issue_date = {May 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {32},
number = {3},
issn = {1049-331X},
url = {https://doi.org/10.1145/3550150},
doi = {10.1145/3550150},
abstract = {Stack Overflow has been heavily used by software developers to seek programming-related information. More and more developers use Community Question and Answer forums, such as Stack Overflow, to search for code examples of how to accomplish a certain coding task. This is often considered to be more efficient than working from source documentation, tutorials, or full worked examples. However, due to the complexity of these online Question and Answer forums and the very large volume of information they contain, developers can be overwhelmed by the sheer volume of available information. This makes it hard to find and/or even be aware of the most relevant code examples to meet their needs. To alleviate this issue, in this work, we present a query-driven code recommendation tool, named Que2Code, that identifies the best code snippets for a user query from Stack Overflow posts. Our approach has two main stages: (i) semantically equivalent question retrieval and (ii) best code snippet recommendation. During the first stage, for a given query question formulated by a developer, we first generate paraphrase questions for the input query as a way of query boosting and then retrieve the relevant Stack Overflow posted questions based on these generated questions. In the second stage, we collect all of the code snippets within questions retrieved in the first stage and develop a novel scheme to rank code snippet candidates from Stack Overflow posts via pairwise comparisons. To evaluate the performance of our proposed model, we conduct a large-scale experiment to evaluate the effectiveness of the semantically equivalent question retrieval task and best code snippet recommendation task separately on Python and Java datasets in Stack Overflow. We also perform a human study to measure how real-world developers perceive the results generated by our model. Both the automatic and human evaluation results demonstrate the promising performance of our model, and we have released our code and data to assist other researchers.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = apr,
articleno = {80},
numpages = {42},
keywords = {Duplicate questions, paraphrase mining, Stack Overflow, Code Search}
}

@article{10.1145/2983645,
author = {Park, Sangkeun and Ackerman, Mark S. and Lee, Uichin},
title = {Localness of Location-based Knowledge Sharing: A Study of Naver KiN “Here”},
year = {2018},
issue_date = {August 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {12},
number = {3},
issn = {1559-1131},
url = {https://doi.org/10.1145/2983645},
doi = {10.1145/2983645},
abstract = {In location-based social Q8A services, people ask a question with a high expectation that local residents who have local knowledge will answer the question. However, little is known about the locality of user activities in location-based social Q8A services. This study aims to deepen our understanding of location-based knowledge sharing by investigating the following: general behavioral characteristics of users, the topical and typological patterns related to geographic characteristics, geographic locality of user activities, and motivations of local knowledge sharing. To this end, we analyzed a 12-month period Q8A dataset from Naver KiN “Here,” a location-based social Q8A mobile app, in addition to a supplementary survey dataset obtained from 285 mobile users. Our results reveal several unique characteristics of location-based social Q8A. When compared with conventional social Q8A sites, users ask and answer different topical/typological questions. In addition, those who answer have a strong spatial locality wherein they primarily have local knowledge in a few regions, in areas such as their home and work. We also find unique motivators such as ownership of local knowledge and a sense of local community. The findings reported in the article have significant implications for the design of Q8A systems, especially location-based social Q8A systems.},
journal = {ACM Trans. Web},
month = jul,
articleno = {16},
numpages = {33},
keywords = {mobile applications, Knowledge sharing}
}

@article{10.1145/3449215,
author = {Guo, Cheng and Caine, Kelly},
title = {Anonymity, User Engagement, Quality, and Trolling on Q&amp;A Sites},
year = {2021},
issue_date = {April 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {5},
number = {CSCW1},
url = {https://doi.org/10.1145/3449215},
doi = {10.1145/3449215},
abstract = {In online question and answer (Q&amp;A) communities, people ask questions and share answers at all levels of topic sensitivity. Identity options within these communities range from anonymity to real name. The amount of engagement, and the quality of engagement on Q&amp;A sites may differ depending on the identity options available. In this paper, we investigate the relationship between the amount of engagement, the quality of engagement, and different types of identity by analyzing three Q&amp;A sites with different identity policies. We find that highly sensitive questions are more likely to be asked anonymously. Furthermore, allowing anonymity does not affect answer quality and only has a weak, negative indirect effect on engagement. On the other hand, anonymity leads to more trolling. We suggest online communities provide a way for users to ask highly sensitive questions anonymously and pair this with moderation mechanisms to reduce trolling},
journal = {Proc. ACM Hum.-Comput. Interact.},
month = apr,
articleno = {141},
numpages = {27},
keywords = {anonymity, identity, online communities, privacy, q&amp;a sites, trolling, user engagement}
}

@article{10.1145/3439769,
author = {Uddin, Gias and Khomh, Foutse and Roy, Chanchal K.},
title = {Automatic API Usage Scenario Documentation from Technical Q&amp;A Sites},
year = {2021},
issue_date = {July 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {30},
number = {3},
issn = {1049-331X},
url = {https://doi.org/10.1145/3439769},
doi = {10.1145/3439769},
abstract = {The online technical Q&amp;A site Stack Overflow (SO) is popular among developers to support their coding and diverse development needs. To address shortcomings in API official documentation resources, several research works have thus focused on augmenting official API documentation with insights (e.g., code examples) from SO. The techniques propose to add code examples/insights about APIs into its official documentation. Recently, surveys of software developers find that developers in SO consider the combination of code examples and reviews about APIs as a form of API documentation, and that they consider such a combination to be more useful than official API documentation when the official resources can be incomplete, ambiguous, incorrect, and outdated. Reviews are opinionated sentences with positive/negative sentiments. However, we are aware of no previous research that attempts to automatically produce API documentation from SO by considering both API code examples and reviews. In this article, we present two novel algorithms that can be used to automatically produce API documentation from SO by combining code examples and reviews towards those examples. The first algorithm is called statistical documentation, which shows the distribution of positivity and negativity around the code examples of an API using different metrics (e.g., star ratings). The second algorithm is called concept-based documentation, which clusters similar and conceptually relevant usage scenarios. An API usage scenario contains a code example, a textual description of the underlying task addressed by the code example, and the reviews (i.e., opinions with positive and negative sentiments) from other developers towards the code example. We deployed the algorithms in Opiner, a web-based platform to aggregate information about APIs from online forums. We evaluated the algorithms by mining all Java JSON-based posts in SO and by conducting three user studies based on produced documentation from the posts. The first study is a survey, where we asked the participants to compare our proposed algorithms against a Javadoc-syle documentation format (called as Type-based documentation in Opiner). The participants were asked to compare along four development scenarios (e.g., selection, documentation). The participants preferred our proposed two algorithms over type-based documentation. In our second user study, we asked the participants to complete four coding tasks using Opiner and the API official and informal documentation resources. The participants were more effective and accurate while using Opiner. In a subsequent survey, more than 80\% of participants asked the Opiner documentation platform to be integrated into the formal API documentation to complement and improve the API official documentation.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = apr,
articleno = {31},
numpages = {45},
keywords = {usage scenario, documentation, crowd-sourced developer forum, API}
}

@article{10.1145/3401026,
author = {Gao, Zhipeng and Xia, Xin and Grundy, John and Lo, David and Li, Yuan-Fang},
title = {Generating Question Titles for Stack Overflow from Mined Code Snippets},
year = {2020},
issue_date = {October 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {29},
number = {4},
issn = {1049-331X},
url = {https://doi.org/10.1145/3401026},
doi = {10.1145/3401026},
abstract = {Stack Overflow has been heavily used by software developers as a popular way to seek programming-related information from peers via the internet. The Stack Overflow community recommends users to provide the related code snippet when they are creating a question to help others better understand it and offer their help. Previous studies have shown that a significant number of these questions are of low-quality and not attractive to other potential experts in Stack Overflow. These poorly asked questions are less likely to receive useful answers and hinder the overall knowledge generation and sharing process. Considering one of the reasons for introducing low-quality questions in SO is that many developers may not be able to clarify and summarize the key problems behind their presented code snippets due to their lack of knowledge and terminology related to the problem, and/or their poor writing skills, in this study we propose an approach to assist developers in writing high-quality questions by automatically generating question titles for a code snippet using a deep sequence-to-sequence learning approach. Our approach is fully data-driven and uses an attention mechanism to perform better content selection, a copy mechanism to handle the rare-words problem and a coverage mechanism to eliminate word repetition problem. We evaluate our approach on Stack Overflow datasets over a variety of programming languages (e.g., Python, Java, Javascript, C# and SQL) and our experimental results show that our approach significantly outperforms several state-of-the-art baselines in both automatic and human evaluation. We have released our code and datasets to facilitate other researchers to verify their ideas and inspire the follow up work.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = sep,
articleno = {26},
numpages = {37},
keywords = {sequence-to-sequence, question quality, question generation, Stack overflow}
}

@article{10.1145/3187011,
author = {Liu, Zhenguang and Xia, Yingjie and Liu, Qi and He, Qinming and Zhang, Chao and Zimmermann, Roger},
title = {Toward Personalized Activity Level Prediction in Community Question Answering Websites},
year = {2018},
issue_date = {April 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {14},
number = {2s},
issn = {1551-6857},
url = {https://doi.org/10.1145/3187011},
doi = {10.1145/3187011},
abstract = {Community Question Answering (CQA) websites have become valuable knowledge repositories. Millions of internet users resort to CQA websites to seek answers to their encountered questions. CQA websites provide information far beyond a search on a site such as Google due to (1) the plethora of high-quality answers, and (2) the capabilities to post new questions toward the communities of domain experts. While most research efforts have been made to identify experts or to preliminarily detect potential experts of CQA websites, there has been a remarkable shift toward investigating how to keep the engagement of experts. Experts are usually the major contributors of high-quality answers and questions of CQA websites. Consequently, keeping the expert communities active is vital to improving the lifespan of these websites. In this article, we present an algorithm termed PALP to predict the activity level of expert users of CQA websites. To the best of our knowledge, PALP is the first approach to address a personalized activity level prediction model for CQA websites. Furthermore, it takes into consideration user behavior change over time and focuses specifically on expert users. Extensive experiments on the Stack Overflow website demonstrate the competitiveness of PALP over existing methods.},
journal = {ACM Trans. Multimedia Comput. Commun. Appl.},
month = apr,
articleno = {41},
numpages = {15},
keywords = {personalized model, logistic regression, activity level prediction, Question answering website}
}

@article{10.1145/3434279,
author = {Zhang, Haoxiang and Wang, Shaowei and Chen, Tse-Hsun (Peter) and Hassan, Ahmed E.},
title = {Are Comments on Stack Overflow Well Organized for Easy Retrieval by Developers?},
year = {2021},
issue_date = {April 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {30},
number = {2},
issn = {1049-331X},
url = {https://doi.org/10.1145/3434279},
doi = {10.1145/3434279},
abstract = {Many Stack Overflow answers have associated informative comments that can strengthen them and assist developers. A prior study found that comments can provide additional information to point out issues in their associated answer, such as the obsolescence of an answer. By showing more informative comments (e.g., the ones with higher scores) and hiding less informative ones, developers can more effectively retrieve information from the comments that are associated with an answer. Currently, Stack Overflow prioritizes the display of comments, and, as a result, 4.4 million comments (possibly including informative comments) are hidden by default from developers. In this study, we investigate whether this mechanism effectively organizes informative comments. We find that (1) the current comment organization mechanism does not work well due to the large amount of tie-scored comments (e.g., 87\% of the comments have 0-score) and (2) in 97.3\% of answers with hidden comments, at least one comment that is possibly informative is hidden while another comment with the same score is shown (i.e., unfairly hidden comments). The longest unfairly hidden comment is more likely to be informative than the shortest one. Our findings highlight that Stack Overflow should consider adjusting the comment organization mechanism to help developers effectively retrieve informative comments. Furthermore, we build a classifier that can effectively distinguish informative comments from uninformative comments. We also evaluate two alternative comment organization mechanisms (i.e., the Length mechanism and the Random mechanism) based on text similarity and the prediction of our classifier.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = feb,
articleno = {22},
numpages = {31},
keywords = {stack overflow, crowdsourced knowledge sharing, commenting, Q8A website, Empirical software engineering}
}

@article{10.1145/3494518,
author = {Yang, Wenhua and Zhang, Chong and Pan, Minxue and Xu, Chang and Zhou, Yu and Huang, Zhiqiu},
title = {Do Developers Really Know How to Use Git Commands? A Large-scale Study Using Stack Overflow},
year = {2022},
issue_date = {July 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {31},
number = {3},
issn = {1049-331X},
url = {https://doi.org/10.1145/3494518},
doi = {10.1145/3494518},
abstract = {Git, a cross-platform and open source distributed version control tool, provides strong support for non-linear development and is capable of handling everything from small to large projects with speed and efficiency. It has become an indispensable tool for millions of software developers and is the de facto standard of version control in software development nowadays. However, despite its widespread use, developers still frequently face difficulties when using various Git commands to manage projects and collaborate. To better help developers use Git, it is necessary to understand the issues and difficulties that they may encounter when using Git. Unfortunately, this problem has not yet been comprehensively studied. To fill this knowledge gap, in this article, we conduct a large-scale study on Stack Overflow, a popular Q&amp;A forum for developers. We extracted and analyzed 80,370 relevant questions from Stack Overflow, and reported the increasing popularity of the Git command questions. By analyzing the questions, we identified the Git commands that are frequently asked and those that are associated with difficult questions on Stack Overflow to help understand the difficulties developers may encounter when using Git commands. In addition, we conducted a survey to understand how developers learn Git commands in practice, showing that self-learning is the primary learning approach. These findings provide a range of actionable implications for researchers, educators, and developers.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = apr,
articleno = {44},
numpages = {29},
keywords = {user survey, Stack Overflow, Git commands}
}

@article{10.1145/3479547,
author = {Ma, Suyu and Chen, Chunyang and Khalajzadeh, Hourieh and Grundy, John},
title = {Latexify Math: Mathematical Formula Markup Revision to Assist Collaborative Editing in Math Q&amp;A Sites},
year = {2021},
issue_date = {October 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {5},
number = {CSCW2},
url = {https://doi.org/10.1145/3479547},
doi = {10.1145/3479547},
abstract = {Collaborative editing questions and answers plays an important role in quality control of Mathematics StackExchange which is a math Q&amp;A Site. Our study of post edits in Mathematics Stack Exchange shows that there is a large number of math-related edits about latexifying formulas, revising LaTeX and converting the blurred math formula screenshots to LaTeX sequence. Despite its importance, manually editing one math-related post especially those with complex mathematical formulas is time-consuming and error-prone even for experienced users. To assist post owners and editors to do this editing, we have developed an edit-assistance tool, MathLatexEdit for formula latexification, LaTeX revision and screenshot transcription. We formulate this formula editing task as a translation problem, in which an original post is translated to a revised post. MathLatexEdit implements a deep learning based approach including two encoder-decoder models for textual and visual LaTeX edit recommendation with math-specific inference. The two models are trained on large-scale historical original-edited post pairs and synthesized screenshot-formula pairs. Our evaluation of MathLatexEdit not only demonstrates the accuracy of our model, but also the usefulness of MathLatexEdit in editing real-world posts which are accepted in Mathematics Stack Exchange.},
journal = {Proc. ACM Hum.-Comput. Interact.},
month = oct,
articleno = {403},
numpages = {24},
keywords = {Q&amp;A sites, collaborative editing, deep learning, latex, math}
}

@article{10.1145/2934687,
author = {Srba, Ivan and Bielikova, Maria},
title = {A Comprehensive Survey and Classification of Approaches for Community Question Answering},
year = {2016},
issue_date = {August 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {10},
number = {3},
issn = {1559-1131},
url = {https://doi.org/10.1145/2934687},
doi = {10.1145/2934687},
abstract = {Community question-answering (CQA) systems, such as Yahoo! Answers or Stack Overflow, belong to a prominent group of successful and popular Web 2.0 applications, which are used every day by millions of users to find an answer on complex, subjective, or context-dependent questions. In order to obtain answers effectively, CQA systems should optimally harness collective intelligence of the whole online community, which will be impossible without appropriate collaboration support provided by information technologies. Therefore, CQA became an interesting and promising subject of research in computer science and now we can gather the results of 10 years of research. Nevertheless, in spite of the increasing number of publications emerging each year, so far the research on CQA systems has missed a comprehensive state-of-the-art survey. We attempt to fill this gap by a review of 265 articles published between 2005 and 2014, which were selected from major conferences and journals. According to this evaluation, at first we propose a framework that defines descriptive attributes of CQA approaches. Second, we introduce a classification of all approaches with respect to problems they are aimed to solve. The classification is consequently employed in a review of a significant number of representative approaches, which are described by means of attributes from the descriptive framework. As a part of the survey, we also depict the current trends as well as highlight the areas that require further attention from the research community.},
journal = {ACM Trans. Web},
month = aug,
articleno = {18},
numpages = {63},
keywords = {user modeling, online communities, knowledge sharing, exploratory studies, content modeling, adaptive collaboration support, Community question answering}
}

@article{10.1145/3274302,
author = {Chen, Chunyang and Chen, Xi and Sun, Jiamou and Xing, Zhenchang and Li, Guoqiang},
title = {Data-Driven Proactive Policy Assurance of Post Quality in Community q&amp;a Sites},
year = {2018},
issue_date = {November 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {2},
number = {CSCW},
url = {https://doi.org/10.1145/3274302},
doi = {10.1145/3274302},
abstract = {To ensure the post quality, Q&amp;A sites usually develop a list of quality assurance guidelines for "dos and don'ts", and adopt collaborative editing mechanism to fix quality violations. Quality guidelines are mostly high-level principles, and many tacit and context-sensitive aspects of the expected quality cannot be easily enforced by a set of explicit rules. Collaborative editing is a reactive mechanism after low-quality posts have been posted. Our study of collaborative editing data on Stack Overflow suggests that tacit and context-sensitive quality-assurance knowledge is manifested in the editing patterns of large numbers of collaborative edits. Inspired by this observation, we develop and evaluate a Convolutional Neural Network based approach to learn editing patterns from historical post edits for predicting the need of editing a post. Our approach provides a proactive policy assurance mechanism that warns users potential quality issues in a post before it is posted.},
journal = {Proc. ACM Hum.-Comput. Interact.},
month = nov,
articleno = {33},
numpages = {22},
keywords = {quality assurance, deep learning, collaborative editing, Q&amp;A sites}
}

@article{10.1145/2990497,
author = {Azad, Shams and Rigby, Peter C. and Guerrouj, Latifa},
title = {Generating API Call Rules from Version History and Stack Overflow Posts},
year = {2017},
issue_date = {May 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {25},
number = {4},
issn = {1049-331X},
url = {https://doi.org/10.1145/2990497},
doi = {10.1145/2990497},
abstract = {Researchers have shown that related functions can be mined from groupings of functions found in the version history of a system. Our first contribution is to expand this approach to a community of applications and set of similar applications. Android developers use a set of application programming interface (API) calls when creating apps. These API calls are used in similar ways across multiple applications. By clustering co-changing API calls used by 230 Android apps across 12k versions, we are able to predict the API calls that individual app developers will use with an average precision of 75\% and recall of 22\%. When we make predictions from the same category of app, such as Finance, we attain precision and recall of 81\% and 28\%, respectively.Our second contribution can be characterized as “programmers who discussed these functions were also interested in these functions.” Informal discussions on Stack Overflow provide a rich source of information about related API calls as developers provide solutions to common problems. By grouping API calls contained in each positively voted answer posts, we are able to create rules that predict the calls that app developers will use in their own apps with an average precision of 66\% and recall of 13\%.For comparison purposes, we developed a baseline by clustering co-changing API calls for each individual app and generated association rules from them. The baseline predicts API calls used by app developers with a precision and recall of 36\% and 23\%, respectively.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = jan,
articleno = {29},
numpages = {22},
keywords = {version history, informal documentation, community of applications, association rule mining, Stack Overflow, API method calls}
}

@article{10.1145/1870096.1870099,
author = {Bouguessa, Mohamed and Wang, Shengrui and Dumoulin, Benoit},
title = {Discovering Knowledge-Sharing Communities in Question-Answering Forums},
year = {2010},
issue_date = {December 2010},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {5},
number = {1},
issn = {1556-4681},
url = {https://doi.org/10.1145/1870096.1870099},
doi = {10.1145/1870096.1870099},
abstract = {In this article, we define a knowledge-sharing community in a question-answering forum as a set of askers and authoritative users such that, within each community, askers exhibit more homogeneous behavior in terms of their interactions with authoritative users than elsewhere. A procedure for discovering members of such a community is devised. As a case study, we focus on Yahoo! Answers, a large and diverse online question-answering service. Our contribution is twofold. First, we propose a method for automatic identification of authoritative actors in Yahoo! Answers. To this end, we estimate and then model the authority scores of participants as a mixture of gamma distributions. The number of components in the mixture is determined using the Bayesian Information Criterion (BIC), while the parameters of each component are estimated using the Expectation-Maximization (EM) algorithm. This method allows us to automatically discriminate between authoritative and nonauthoritative users. Second, we represent the forum environment as a type of transactional data such that each transaction summarizes the interaction of an asker with a specific set of authoritative users. Then, to group askers on the basis of their interactions with authoritative users, we propose a parameter-free transaction data clustering algorithm which is based on a novel criterion function. The identified clusters correspond to the communities that we aim to discover. To evaluate the suitability of our clustering algorithm, we conduct a series of experiments on both synthetic data and public real-life data. Finally, we put our approach to work using data from Yahoo! Answers which represent users’ activities over one full year.},
journal = {ACM Trans. Knowl. Discov. Data},
month = dec,
articleno = {3},
numpages = {49},
keywords = {transaction data, mixture models, Clustering}
}

@article{10.1109/TASLP.2016.2544661,
author = {Zhou, Guangyou and Xie, Zhiwen and He, Tingting and Zhao, Jun and Hu, Xiaohua Tony},
title = {Learning the multilingual translation representations for question retrieval in community question answering via non-negative matrix factorization},
year = {2016},
issue_date = {July 2016},
publisher = {IEEE Press},
volume = {24},
number = {7},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2016.2544661},
doi = {10.1109/TASLP.2016.2544661},
abstract = {Community question answering (CQA) has become an increasingly popular research topic. In this paper, we focus on the problem of question retrieval. Question retrieval in CQA can automatically find the most relevant and recent questions that have been solved by other users. However, the word ambiguity and word mismatch problems bring about new challenges for question retrieval in CQA. State-of-the-art approaches address these issues by implicitly expanding the queried questions with additional words or phrases using monolingual translation models. While useful, the effectiveness of these models is highly dependent on the availability of quality parallel monolingual corpora (e.g., question--answer pairs) in the absence of which they are troubled by noise issues. In this work, we propose an alternative way to address the word ambiguity and word mismatch problems by taking advantage of potentially rich semantic information drawn from other languages. Our proposed method employs statistical machine translation to improve question retrieval and enriches the question representation with the translated words from other languages via non-negative matrix factorization. Experiments conducted on real CQA data sets show that our proposed approach is promising.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = jul,
pages = {1305–1314},
numpages = {10},
keywords = {text mining, question retrieval, natural language processing, information retrieval, community question answering}
}

@article{10.1145/1514888.1514893,
author = {Agichtein, Eugene and Liu, Yandong and Bian, Jiang},
title = {Modeling information-seeker satisfaction in community question answering},
year = {2009},
issue_date = {April 2009},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {3},
number = {2},
issn = {1556-4681},
url = {https://doi.org/10.1145/1514888.1514893},
doi = {10.1145/1514888.1514893},
abstract = {Question Answering Communities such as Naver, Baidu Knows, and Yahoo! Answers have emerged as popular, and often effective, means of information seeking on the web. By posting questions for other participants to answer, information seekers can obtain specific answers to their questions. Users of CQA portals have already contributed millions of questions, and received hundreds of millions of answers from other participants. However, CQA is not always effective: in some cases, a user may obtain a perfect answer within minutes, and in others it may require hours—and sometimes days—until a satisfactory answer is contributed. We investigate the problem of predicting information seeker satisfaction in collaborative question answering communities, where we attempt to predict whether a question author will be satisfied with the answers submitted by the community participants. We present a general prediction model, and develop a variety of content, structure, and community-focused features for this task. Our experimental results, obtained from a large-scale evaluation over thousands of real questions and user ratings, demonstrate the feasibility of modeling and predicting asker satisfaction. We complement our results with a thorough investigation of the interactions and information seeking patterns in question answering communities that correlate with information seeker satisfaction. We also explore personalized models of asker satisfaction, and show that when sufficient interaction history exists, personalization can significantly improve prediction accuracy over a “one-size-fits-all” model. Our models and predictions could be useful for a variety of applications, such as user intent inference, answer ranking, interface design, and query suggestion and routing.},
journal = {ACM Trans. Knowl. Discov. Data},
month = apr,
articleno = {10},
numpages = {27},
keywords = {information seeker satisfaction, Community question answering}
}

@article{10.1145/3337799,
author = {Thukral, Deepak and Pandey, Adesh and Gupta, Rishabh and Goyal, Vikram and Chakraborty, Tanmoy},
title = {DiffQue: Estimating Relative Difficulty of Questions in Community Question Answering Services},
year = {2019},
issue_date = {July 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {10},
number = {4},
issn = {2157-6904},
url = {https://doi.org/10.1145/3337799},
doi = {10.1145/3337799},
abstract = {Automatic estimation of relative difficulty of a pair of questions is an important and challenging problem in community question answering (CQA) services. There are limited studies that addressed this problem. Past studies mostly leveraged expertise of users answering the questions and barely considered other properties of CQA services such as metadata of users and posts, temporal information, and textual content. In this article, we propose DiffQue, a novel system that maps this problem to a network-aided edge directionality prediction problem. DiffQue&nbsp;starts by constructing a novel network structure that captures different notions of difficulties among a pair of questions. It then measures the relative difficulty of two questions by predicting the direction of a (virtual) edge connecting these two questions in the network. It leverages features extracted from the network structure, metadata of users/posts, and textual description of questions and answers. Experiments on datasets obtained from two CQA sites (further divided into four datasets) with human annotated ground-truth show that DiffQue&nbsp;outperforms four state-of-the-art methods by a significant margin (28.77\% higher F1 score and 28.72\% higher AUC than the best baseline). As opposed to the other baselines, (i) DiffQue&nbsp;appropriately responds to the training noise, (ii) DiffQue&nbsp;is capable of adapting multiple domains (CQA datasets), and (iii) DiffQue&nbsp;can efficiently handle the “cold start” problem that may arise due to the lack of information for newly posted questions or newly arrived users.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = jul,
articleno = {42},
numpages = {27},
keywords = {time-evolving networks, edge directionality prediction, difficulty of questions, Community question answering}
}

@article{10.1145/2180868.2180872,
author = {Pal, Aditya and Harper, F. Maxwell and Konstan, Joseph A.},
title = {Exploring Question Selection Bias to Identify Experts and Potential Experts in Community Question Answering},
year = {2012},
issue_date = {May 2012},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {30},
number = {2},
issn = {1046-8188},
url = {https://doi.org/10.1145/2180868.2180872},
doi = {10.1145/2180868.2180872},
abstract = {Community Question Answering (CQA) services enable their users to exchange knowledge in the form of questions and answers. These communities thrive as a result of a small number of highly active users, typically called experts, who provide a large number of high-quality useful answers. Expert identification techniques enable community managers to take measures to retain the experts in the community. There is further value in identifying the experts during the first few weeks of their participation as it would allow measures to nurture and retain them. In this article we address two problems: (a) How to identify current experts in CQA? and (b) How to identify users who have potential of becoming experts in future (potential experts)? In particular, we propose a probabilistic model that captures the selection preferences of users based on the questions they choose for answering. The probabilistic model allows us to run machine learning methods for identifying experts and potential experts. Our results over several popular CQA datasets indicate that experts differ considerably from ordinary users in their selection preferences; enabling us to predict experts with higher accuracy over several baseline models. We show that selection preferences can be combined with baseline measures to improve the predictive performance even further.},
journal = {ACM Trans. Inf. Syst.},
month = may,
articleno = {10},
numpages = {28},
keywords = {question selection process, community question answering, Expert identification}
}

@article{10.1145/3371388,
author = {Li, Hongfei and Shankar, Ramesh and Stallaert, Jan},
title = {Invested or Indebted: Ex-ante and Ex-post Reciprocity in Online Knowledge Sharing Communities},
year = {2020},
issue_date = {March 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {11},
number = {1},
issn = {2158-656X},
url = {https://doi.org/10.1145/3371388},
doi = {10.1145/3371388},
abstract = {Online communities that curate knowledge critically depend on high-quality contributions from anonymous expert users. Understanding users’ motivation to contribute knowledge helps practitioners design such websites for optimal user contribution and user benefits. Researchers have studied reciprocity as a motivation for users to share knowledge online. In this study, we focus on two different types of reciprocity as drivers of online contribution: ex-post and ex-ante reciprocity. Ex-post reciprocity refers to users who received help from others in the past and pay back by helping others at present. Using a quasi-experiment performed via the instrumental variable method as the identification strategy, we test whether users who received more answers last week answer more questions in the current week on StackOverflow.com. We find a significant positive relationship between ex-post reciprocity and knowledge contribution, and such a reciprocal motivation diminishes with time. Ex-ante reciprocity refers to people helping others in expectation of future help from others. Using data from StackOverflow.com, we take advantage of a natural experiment with a difference-in-differences analysis and find evidence supporting the existence of ex-ante reciprocity. This study offers a new taxonomy for reciprocity and new insights on how reciprocity drives online knowledge sharing.},
journal = {ACM Trans. Manage. Inf. Syst.},
month = jan,
articleno = {1},
numpages = {26},
keywords = {reciprocity, knowledge sharing, Q&amp;A website, ex-ante, Ex-post}
}

@article{10.1145/3492855,
author = {Kou, Ziyi and Shang, Lanyu and Zhang, Yang and Wang, Dong},
title = {HC-COVID: A Hierarchical Crowdsource Knowledge Graph Approach to Explainable COVID-19 Misinformation Detection},
year = {2022},
issue_date = {January 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {6},
number = {GROUP},
url = {https://doi.org/10.1145/3492855},
doi = {10.1145/3492855},
abstract = {The proliferation of social media has promoted the spread of misinformation that raises many concerns in our society. This paper focuses on a critical problem of explainable COVID-19 misinformation detection that aims to accurately identify and explain misleading COVID-19 claims on social media. Motivated by the lack of COVID-19 relevant knowledge in existing solutions, we construct a novel crowdsource knowledge graph based approach to incorporate the COVID-19 knowledge facts by leveraging the collaborative efforts of expert and non-expert crowd workers. Two important challenges exist in developing our solution: i) how to effectively coordinate the crowd efforts from both expert and non-expert workers to generate the relevant knowledge facts for detecting COVID-19 misinformation; ii) How to leverage the knowledge facts from the constructed knowledge graph to accurately explain the detected COVID-19 misinformation. To address the above challenges, we develop HC-COVID, a hierarchical crowdsource knowledge graph based framework that explicitly models the COVID-19 knowledge facts contributed by crowd workers with different levels of expertise and accurately identifies the related knowledge facts to explain the detection results. We evaluate HC-COVID using two public real-world datasets on social media. Evaluation results demonstrate that HC-COVID significantly outperforms state-of-the-art baselines in terms of the detection accuracy of misleading COVID-19 claims and the quality of the explanations.},
journal = {Proc. ACM Hum.-Comput. Interact.},
month = jan,
articleno = {36},
numpages = {25},
keywords = {human-ai collaboration, explainable misinformation detection, covid19}
}

@article{10.1145/1113830.1113833,
author = {Regehr, John and Reid, Alastair and Webb, Kirk},
title = {Eliminating stack overflow by abstract interpretation},
year = {2005},
issue_date = {November 2005},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {4},
number = {4},
issn = {1539-9087},
url = {https://doi.org/10.1145/1113830.1113833},
doi = {10.1145/1113830.1113833},
abstract = {An important correctness criterion for software running on embedded microcontrollers is stack safety: a guarantee that the call stack does not overflow. Our first contribution is a method for statically guaranteeing stack safety of interrupt-driven embedded software using an approach based on context-sensitive dataflow analysis of object code. We have implemented a prototype stack analysis tool that targets software for Atmel AVR microcontrollers and tested it on embedded applications compiled from up to 30,000 lines of C. We experimentally validate the accuracy of the tool, which runs in under 10 sec on the largest programs that we tested. The second contribution of this paper is the development of two novel ways to reduce stack memory requirements of embedded software.},
journal = {ACM Trans. Embed. Comput. Syst.},
month = nov,
pages = {751–778},
numpages = {28},
keywords = {sensor network, interrupt-driven, dataflow analysis, context sensitive, call stack, abstract interpretation, Microcontroller}
}

@article{10.1145/1132462.1132467,
author = {Choi, Yoonseo and Han, Hwansoo},
title = {Optimal register reassignment for register stack overflow minimization},
year = {2006},
issue_date = {March 2006},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {3},
number = {1},
issn = {1544-3566},
url = {https://doi.org/10.1145/1132462.1132467},
doi = {10.1145/1132462.1132467},
abstract = {Architectures with a register stack can implement efficient calling conventions. Using the overlapping of callers' and callees' registers, callers are able to pass parameters to callees without a memory stack. The most recent instance of a register stack can be found in the Intel Itanium architecture. A hardware component called the register stack engine (RSE) provides an illusion of an infinite-length register stack using a memory-backed process to handle overflow and underflow for a physically limited number of registers. Despite such hardware support, some applications suffer from the overhead required to handle register stack overflow and underflow. The memory latency associated with the overflow and underflow of a register stack can be reduced by generating multiple register allocation instructions within a procedure [Settle et al. 2003]. Live analysis is utilized to find a set of registers that are not required to keep their values across procedure boundaries. However, among those dead registers, only the registers that are consecutively located in a certain part of the register stack frame can be removed. We propose a compiler-supported register reassignment technique that reduces RSE overflow/underflow further. By reassigning registers based on live analysis, our technique forces as many dead registers to be removed as possible. We define the problem of optimal register reassignment, which minimizes interprocedural register stack heights considering multiple call sites within a procedure. We present how this problem is related to a path-finding problem in a graph called a sequence graph. We also propose an efficient heuristic algorithm for the problem. Finally, we present the measurement of effects of the proposed techniques on SPEC CINT2000 benchmark suite and the analysis of the results. The result shows that our approach reduces the RSE cycles by 6.4\% and total cpu cycles by 1.7\% on average.},
journal = {ACM Trans. Archit. Code Optim.},
month = mar,
pages = {90–114},
numpages = {25},
keywords = {sequence graph, register stack, register allocation, Register assignment}
}

@article{10.1145/3274399,
author = {Oliveira, Nigini and Muller, Michael and Andrade, Nazareno and Reinecke, Katharina},
title = {The Exchange in StackExchange: Divergences between Stack Overflow and its Culturally Diverse Participants},
year = {2018},
issue_date = {November 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {2},
number = {CSCW},
url = {https://doi.org/10.1145/3274399},
doi = {10.1145/3274399},
abstract = {StackExchange is a network of Question \&amp; Answer (Q&amp;A) sites that support collaborative knowledge exchange on a variety of topics. Prior research found a significant imbalance between those who contribute content to Q&amp;A sites (predominantly people from Western countries) and those who passively use the site (the so-called "lurkers"). One possible explanation for such participation differences between countries could be a mismatch between culturally related preferences of some users and the values ingrained in the design of the site. To examine this hypothesis, we conducted a value-sensitive analysis of the design of the StackExchange site Stack Overflow and contrasted our findings with those of participants from societies with varying cultural backgrounds using a series of focus groups and interviews. Our results reveal tensions between collectivist values, such as the openness for social interactions, and the performance-oriented, individualist values embedded in Stack Overflow's design and community guidelines. This finding confirms that socio-technical sites like Stack Overflow reflect the inherent values of their designers, knowledge that can be leveraged to foster participation equity.},
journal = {Proc. ACM Hum.-Comput. Interact.},
month = nov,
articleno = {130},
numpages = {22},
keywords = {value-sensitive design, stack overflow, question \&amp; answer sites, online collaboration, cross-cultural studies}
}

@article{10.1145/3274381,
author = {Lu, Zhicong and Heo, Seongkook and Wigdor, Daniel J.},
title = {StreamWiki: Enabling Viewers of Knowledge Sharing Live Streams to Collaboratively Generate Archival Documentation for Effective In-Stream and Post Hoc Learning},
year = {2018},
issue_date = {November 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {2},
number = {CSCW},
url = {https://doi.org/10.1145/3274381},
doi = {10.1145/3274381},
abstract = {Knowledge-sharing live streams are distinct from traditional educational videos, at least because of the large concurrently-viewing audience and the real-time discussions between viewers and the streamer. Though this creates unique opportunities for interactive learning, it also brings a challenge for creating a useful archive for post hoc learning. This paper presents the results of interviews with knowledge sharing streamers, their moderators, and viewers to understand current experiences and needs for sharing and learning knowledge through live streaming. Based on those findings, we built StreamWiki, a tool which leverages the viewers during live streams to produce useful archives of the interactive learning experience. On StreamWiki, moderators initiate tasks that viewers complete by conducting microtasks, such as writing a summary, commenting, and voting for informative comments. As a result, a summary document is built in real time. Through the tests of our prototype with streamers and viewers, we found that StreamWiki could help understanding the content and the context of the stream, during the stream and for post hoc learning.},
journal = {Proc. ACM Hum.-Comput. Interact.},
month = nov,
articleno = {112},
numpages = {26},
keywords = {live streaming, learning, knowledge sharing, knowledge building, collaborative documentation}
}

@article{10.1145/3134667,
author = {Chen, Chunyang and Xing, Zhenchang and Liu, Yang},
title = {By the Community \&amp; For the Community: A Deep Learning Approach to Assist Collaborative Editing in Q&amp;A Sites},
year = {2017},
issue_date = {November 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {1},
number = {CSCW},
url = {https://doi.org/10.1145/3134667},
doi = {10.1145/3134667},
abstract = {Community edits to questions and answers (called post edits) plays an important role in improving content quality in Stack Overflow. Our study of post edits in Stack Overflow shows that a large number of edits are about formatting, grammar and spelling. These post edits usually involve small-scale sentence edits and our survey of trusted contributors suggests that most of them care much or very much about such small sentence edits. To assist users in making small sentence edits, we develop an edit-assistance tool for identifying minor textual issues in posts and recommending sentence edits for correction. We formulate the sentence editing task as a machine translation problem, in which an original sentence is "translated" into an edited sentence. Our tool implements a character-level Recurrent Neural Network (RNN) encoder-decoder model, trained with about 6.8 millions original-edited sentence pairs from Stack Overflow post edits. We evaluate our edit assistance tool using a large-scale archival post edits, a field study of assisting a novice post editor, and a survey of trusted contributors. Our evaluation demonstrates the feasibility of training a deep learning model with post edits by the community and then using the trained model to assist post editing for the community.},
journal = {Proc. ACM Hum.-Comput. Interact.},
month = dec,
articleno = {32},
numpages = {21},
keywords = {deep learning, collaborative editing, Q&amp;A sites}
}

@article{10.1613/jair.1.13304,
author = {Liu, Chong and Wang, Yu-Xiang},
title = {Doubly Robust Crowdsourcing},
year = {2022},
issue_date = {May 2022},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {73},
issn = {1076-9757},
url = {https://doi.org/10.1613/jair.1.13304},
doi = {10.1613/jair.1.13304},
abstract = {Large-scale labeled dataset is the indispensable fuel that ignites the AI revolution as we see today. Most such datasets are constructed using crowdsourcing services such as Amazon Mechanical Turk which provides noisy labels from non-experts at a fair price. The sheer size of such datasets mandates that it is only feasible to collect a few labels per data point. We formulate the problem of test-time label aggregation as a statistical estimation problem of inferring the expected voting score. By imitating workers with supervised learners and using them in a doubly robust estimation framework, we prove that the variance of estimation can be substantially reduced, even if the learner is a poor approximation. Synthetic and real-world experiments show that by combining the doubly robust approach with adaptive worker/item selection rules, we often need much lower label cost to achieve nearly the same accuracy as in the ideal world where all workers label all data points.},
journal = {J. Artif. Int. Res.},
month = may,
numpages = {21}
}

@article{10.1145/3544018,
author = {Miao, Xiaoye and Peng, Huanhuan and Gao, Yunjun and Zhang, Zongfu and Yin, Jianwei},
title = {On Dynamically Pricing Crowdsourcing Tasks},
year = {2023},
issue_date = {February 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {17},
number = {2},
issn = {1556-4681},
url = {https://doi.org/10.1145/3544018},
doi = {10.1145/3544018},
abstract = {Crowdsourcing techniques have been extensively explored in the past decade, including task allocation, quality assessment, and so on. Most of professional crowdsourcing platforms adopt the fixed pricing scheme to offer a fixed price for crowd tasks. It is neither incentive for crowd workers to produce good performance, nor profitable for the requester to gain high utility with low budget. In this article, we study the problem of pricing crowdsourcing tasks with optional bonuses. We propose a dynamic pricing mechanism, named CrowdPricer for incentively delivering bonuses to the crowd workers of completing tasks, in addition to offering a base payment for completing a task. We leverage a deep time sequence model to learn the effect of bonuses on workers’ quality for crowd tasks. CrowdPricer makes decisions on whether to provide bonuses on workers, so as to maximize the requester’s utility in expectation. We present an efficient bonus delivery algorithm under the help of beam search technique, in order to efficiently solve the decision making problem. Extensive experiments using both a real crowdsourcing platform and simulations demonstrate that CrowdPricer yields the higher utility for the requester. It also obtains more correct crowd answers than the state-of-the-art pricing methods.},
journal = {ACM Trans. Knowl. Discov. Data},
month = feb,
articleno = {30},
numpages = {27},
keywords = {optimization, deep learning model, pricing mechanism, Crowdsourcing}
}

@article{10.1145/3589346,
author = {Aguirre, Carlos and Cao, Shiye and Mahmood, Amama and Huang, Chien-Ming},
title = {Crowdsourcing Thumbnail Captions: Data Collection and Validation},
year = {2023},
issue_date = {September 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {13},
number = {3},
issn = {2160-6455},
url = {https://doi.org/10.1145/3589346},
doi = {10.1145/3589346},
abstract = {Speech interfaces, such as personal assistants and screen readers, read image captions to users. Typically, however, only one caption is available per image, which may not be adequate for all situations (e.g., browsing large quantities of images). Long captions provide a deeper understanding of an image but require more time to listen to, whereas shorter captions may not allow for such thorough comprehension yet have the advantage of being faster to consume. We explore how to effectively collect both thumbnail captions—succinct image descriptions meant to be consumed quickly—and comprehensive captions—which allow individuals to understand visual content in greater detail. We consider text-based instructions and time-constrained methods to collect descriptions at these two levels of detail and find that a time-constrained method is the most effective for collecting thumbnail captions while preserving caption accuracy. Additionally, we verify that caption authors using this time-constrained method are still able to focus on the most important regions of an image by tracking their eye gaze. We evaluate our collected captions along human-rated axes—correctness, fluency, amount of detail, and mentions of important concepts—and discuss the potential for model-based metrics to perform large-scale automatic evaluations in the future.},
journal = {ACM Trans. Interact. Intell. Syst.},
month = sep,
articleno = {14},
numpages = {28},
keywords = {gaze tracking, annotation interfaces, accessibility, crowdsourcing, Image captions}
}

@article{10.1145/3603256,
author = {Fornaroli, Alessandro and Gatica-Perez, Daniel},
title = {Urban Crowdsourcing Platforms across the World: A Systematic Review},
year = {2023},
issue_date = {September 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {4},
number = {3},
url = {https://doi.org/10.1145/3603256},
doi = {10.1145/3603256},
abstract = {Urban crowdsourcing platforms are becoming increasingly important, especially considering the relevance of citizen-centricity in smart cities. This systematic review aims at analyzing existing academic literature on urban crowdsourcing platforms to gather citizen-generated data and shed light on the state of research and development of these tools. Studies describing data-gathering urban crowdsourcing platforms were selected following the PRISMA protocol, for a total of 30 studies, corresponding to 32 platforms. After analyzing the studies at large, this review then proceeds to examine and catalogue the platforms, focusing on their location, purpose, and public data availability. While providing valuable information on existing platforms, the catalogue is subject to different types of bias, including a geographical one, which derive primarily from the chosen methodology to identify platforms worldwide. The article also discusses the implications of such choices.},
journal = {Digit. Gov.: Res. Pract.},
month = sep,
articleno = {15},
numpages = {19},
keywords = {citizen sourcing, Urban crowdsourcing}
}

@article{10.1145/3532670,
author = {Mart\'{\i}, Jos\'{e}},
title = {Crowdsourcing Crisis Management and Democratic Legitimacy},
year = {2022},
issue_date = {April 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {3},
number = {2},
url = {https://doi.org/10.1145/3532670},
doi = {10.1145/3532670},
abstract = {In situations of crisis, governments must make decisions under a great uncertainty, complexity, urgency, high social pressure, and great scarcity of resources. We need to maximize the quality of decisions and ensure that we find practical and effective solutions for our problems. The stakes are simply too high. The risk, however, is to give up democratic legitimacy for, in the best case, some form of output legitimacy or technocracy. On the other hand, decisions made in times of emergency are tremendously consequential for our citizens, affecting their fundamental rights and welfare, and it is for decisions like this that democratic legitimacy seems to be crucial. We may be in the apparent dilemma of having to choose between the quality of public decision-making and its democratic legitimacy. This article claims that this is a false dilemma, at least if we take it as an either/or choice. Rather than choosing one value or the other, what we need is to find a proper balance between them on a case-by-case basis. And the article argues that one very promising way to do it is by applying the ideas of crowdlaw and crowdsourcing crisis management based on the potential of collective intelligence, which in turn grounds an ideal of participatory, deliberative, and collaborative democracy.},
journal = {Digit. Gov.: Res. Pract.},
month = nov,
articleno = {15},
numpages = {15},
keywords = {quality of decision-making, citizen participation, citizen engagement, collective intelligence, democratic legitimacy, democracy, collaborative governance, crisis management, crowdsourcing, CrowdLaw}
}

@article{10.1145/3586998,
author = {Li, Huiru and Jiang, Liangxiao and Xue, Siqing},
title = {Neighborhood Weighted Voting-Based Noise Correction for Crowdsourcing},
year = {2023},
issue_date = {August 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {17},
number = {7},
issn = {1556-4681},
url = {https://doi.org/10.1145/3586998},
doi = {10.1145/3586998},
abstract = {In crowdsourcing scenarios, we can obtain each instance’s multiple noisy labels set from different crowd workers and then use a ground truth inference algorithm to infer its integrated label. Despite the effectiveness of ground truth inference algorithms, a certain level of noise still remains in the integrated labels. To reduce the impact of noise, many noise correction algorithms have been proposed in recent years. To the best of our knowledge, however, nearly all existing noise correction algorithms only exploit each instance’s own multiple noisy label sets but ignore the multiple noisy label sets of its neighbors. Here neighbors refer to the nearest instances found in the feature space based on the distance metric learning. In this article, we propose neighborhood weighted voting-based noise correction (NWVNC). In NWVNC, we at first take advantage of the multiple noisy label sets of each instance’s neighbors (including itself) to estimate the probability that it belongs to its integrated label. Then, we use the estimated probability to identify and filter noise instances and thus obtain a clean set and a noise set. Finally, we train three heterogeneous classifiers on the clean set and correct the noise instances by the consensus voting of three trained classifiers. The experimental results on 34 simulated and two real-world crowdsourced datasets show that NWVNC significantly outperforms all the other state-of-the-art noise correction algorithms used for comparison.},
journal = {ACM Trans. Knowl. Discov. Data},
month = apr,
articleno = {96},
numpages = {18},
keywords = {consensus voting, neighborhood weighted voting, noise correction, Crowdsourcing learning}
}

@article{10.1145/3594721,
author = {Daquino, Marilena and Wigham, Mari and Daga, Enrico and Giagnolini, Lucia and Tomasi, Francesca},
title = {CLEF. A Linked Open Data Native System for Crowdsourcing},
year = {2023},
issue_date = {September 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {16},
number = {3},
issn = {1556-4673},
url = {https://doi.org/10.1145/3594721},
doi = {10.1145/3594721},
abstract = {Collaborative data collection initiatives are increasingly becoming pivotal to cultural institutions and scholars, to boost the population of born-digital archives. For over a decade, organisations have been leveraging Semantic Web technologies to design their workflows, ensure data quality, and a means for sharing and reusing (Linked Data). Crucially, scholarly projects that leverage cultural heritage data to collaboratively develop new resources would benefit from agile solutions to simplify the Linked Data production workflow via user-friendly interfaces. To date, only a few pioneers have abandoned legacy cataloguing and archiving systems to fully embrace the Linked Open Data (LOD) paradigm and manage their catalogues or research products through LOD-native management systems. In this article we present Crowdsourcing Linked Entities via web Form (CLEF), an agile LOD-native platform for collaborative data collection, peer-review, and publication. We detail design choices as motivated by two case studies, from the Cultural Heritage and scholarly domains respectively, and we discuss benefits of our solution in the light of prior works. In particular, the strong focus on user-friendly interfaces for producing FAIR data, the provenance-aware editorial process, and the integration with consolidated data management workflows, distinguish CLEF as a novel attempt to develop Linked Data platforms for cultural heritage.},
journal = {J. Comput. Cult. Herit.},
month = aug,
articleno = {41},
numpages = {17},
keywords = {Wikidata, Linked Open Data, provenance, Crowdsourcing}
}

@article{10.1145/3593582,
author = {Zhao, Yan and Deng, Liwei and Zheng, Kai},
title = {AdaTaskRec: An Adaptive Task Recommendation Framework in Spatial Crowdsourcing},
year = {2023},
issue_date = {October 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {41},
number = {4},
issn = {1046-8188},
url = {https://doi.org/10.1145/3593582},
doi = {10.1145/3593582},
abstract = {Spatial crowdsourcing is one of the prime movers for the orchestration of location-based tasks, and task recommendation is a crucial means to help workers discover attractive tasks. While a number of existing studies have focused on modeling workers’ geographical preferences in task recommendation, they ignore the phenomenon of workers’ travel intention drifts across geographical areas, i.e., workers tend to have different intentions when they travel in different areas, which discounts the task recommendation quality of existing methods especially for workers that travel in unfamiliar out-of-town areas. To address this problem, we propose an Adaptive Task Recommendation (AdaTaskRec) framework. Specifically, we first give a novel two-module worker preference learning architecture that can calculate workers’ preferences for POIs (that tasks are associated with) in different areas adaptively based on workers’ current locations. If we detect that a worker is in the hometown area, then we apply the hometown preference learning module, which hybrids different strategies to aggregate workers’ travel intentions into their preferences while considering the transition and the sequence patterns among locations. Otherwise, we invoke the out-of-town preference learning module, which is to capture workers’ preferences by learning their travel intentions and transferring their hometown preferences into their out-of-town ones. Additionally, to improve task recommendation effectiveness, we propose a dynamic top-k recommendation method that sets different k values dynamically according to the numbers of neighboring workers and tasks. We also give an extra-reward-based and a fair top-k recommendation method, which introduce the extra rewards for tasks based on their recommendation rounds and consider exposure-based fairness of tasks, respectively. Extensive experiments offer insight into the effectiveness of the proposed framework.},
journal = {ACM Trans. Inf. Syst.},
month = jul,
articleno = {95},
numpages = {32},
keywords = {spatial crowdsourcing, travel intention, Task recommendation}
}

@article{10.1145/3604940,
author = {Maddalena, Eddy and Ib\'{a}\~{n}ez, Luis-Daniel and Reeves, Neal and Simperl, Elena},
title = {Qrowdsmith: Enhancing Paid Microtask Crowdsourcing with Gamification and Furtherance Incentives},
year = {2023},
issue_date = {October 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {14},
number = {5},
issn = {2157-6904},
url = {https://doi.org/10.1145/3604940},
doi = {10.1145/3604940},
abstract = {Microtask crowdsourcing platforms are social intelligence systems in which volunteers, called crowdworkers, complete small, repetitive tasks in return for a small fee. Beyond payments, task requesters are considering non-monetary incentives such as points, badges, and other gamified elements to increase performance and improve crowdworker experience. In this article, we present Qrowdsmith, a platform for gamifying microtask crowdsourcing. To design the system, we explore empirically a range of gamified and financial incentives and analyse their impact on how efficient, effective, and reliable the results are. To maintain participation over time and save costs, we propose furtherance incentives, which are offered to crowdworkers to encourage additional contributions in addition to the fee agreed upfront. In a series of controlled experiments, we find that while gamification can work as furtherance incentives, it impacts negatively on crowdworkers’ performance, both in terms of the quantity and quality of work, as compared to a baseline where they can continue to contribute voluntarily. Gamified incentives are also less effective than paid bonus equivalents. Our results contribute to the understanding of how best to encourage engagement in microtask crowdsourcing activities and design better crowd intelligence systems.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = sep,
articleno = {86},
numpages = {26},
keywords = {Qrowdsmith: Enhancing paid microtask crowdsourcing with gamification and furtherance incentives}
}

@article{10.1145/3556545,
author = {Wu, Gongqing and Zhou, Liangzhu and Xia, Jiazhu and Li, Lei and Bao, Xianyu and Wu, Xindong},
title = {Crowdsourcing Truth Inference Based on Label Confidence Clustering},
year = {2023},
issue_date = {May 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {17},
number = {4},
issn = {1556-4681},
url = {https://doi.org/10.1145/3556545},
doi = {10.1145/3556545},
abstract = {Truth inference can help solve some difficult problems of data integration in crowdsourcing. Crowdsourced workers are not experts and their labeling ability varies greatly; therefore, in practical applications, it is difficult to determine whether the labels collected from a crowdsourcing platform are correct. This article proposes a novel algorithm called truth inference based on label confidence clustering (TILCC) to improve the quality of integrated labels for the single-choice classification problem in crowdsourcing labeling tasks. We obtain the label confidence via worker reliability, which is calculated from multiple noise labels using a truth discovery method, and then we generate the clustering features and use the K-means algorithm to cluster all the tasks into K different clusters. Each cluster corresponds to a specific class, and the tasks in the cluster are assigned a label. Compared with the performances of six state-of-the-art methods, MV, ZenCrowd, PM, CATD, GLAD, and GTIC, on 12 randomly selected real-world datasets, the performance of our algorithm showed many advantages: no need to set complex parameters, faster running speed, and significantly higher accuracy.},
journal = {ACM Trans. Knowl. Discov. Data},
month = feb,
articleno = {46},
numpages = {20},
keywords = {single-choice classification, label confidence, clustering, crowdsourcing truth inference, Truth discovery}
}

@article{10.1145/3555627,
author = {Bragg, Danielle and Glasser, Abraham and Minakov, Fyodor and Caselli, Naomi and Thies, William},
title = {Exploring Collection of Sign Language Videos through Crowdsourcing},
year = {2022},
issue_date = {November 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {6},
number = {CSCW2},
url = {https://doi.org/10.1145/3555627},
doi = {10.1145/3555627},
abstract = {Inadequate sign language data currently impedes advancement of sign language ML and AI. Training on existing datasets results in limited models due to small size, and lack of diverse signers in real-world settings. Complex labeling problems in particular often limit scale. In this work, we explore the potential for crowdsourcing to help overcome these barriers. To do this, we ran a user study with exploratory crowdsourcing tasks designed to support scalability: 1) to record videos of specific content -- thereby enabling automatic, scalable labeling -- and 2) to perform quality control checks for execution consistency -- further reducing post-processing requirements. We also provided workers with a searchable view of the crowdsourced dataset, to boost engagement and transparency and align with Deaf community values. Our user study included 29 participants using our exploratory tasks to record 1906 videos and perform 2331 quality control checks. Our results suggest that a crowd of signers may be able to generate high-quality recordings and perform reliable quality control, and that the signing community values visibility into the resulting dataset.},
journal = {Proc. ACM Hum.-Comput. Interact.},
month = nov,
articleno = {514},
numpages = {24},
keywords = {sign language, machine learning, education, dataset, data, crowdsourcing, corpus, citizen science}
}

@article{10.1007/s00778-021-00713-1,
author = {Zheng, Libin and Chen, Lei and Cheng, Peng},
title = {Privacy-preserving worker allocation in crowdsourcing},
year = {2022},
issue_date = {Jul 2022},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {31},
number = {4},
issn = {1066-8888},
url = {https://doi.org/10.1007/s00778-021-00713-1},
doi = {10.1007/s00778-021-00713-1},
abstract = {Crowdsourcing has been a prevalent way to obtain answers for tasks that need human intelligence. In general, a crowdsourcing platform is responsible for allocating workers to each received task, with high-quality workers in priority. However, the allocation results can in turn yield knowledge about workers’ quality. For example, those unallocated workers are supposed to be less-qualified. They can be upset if such information is known by the public, which is an invasion of their privacy. To alleviate such concerns, we study the privacy-preserving worker allocation problem in this paper, aiming to properly allocate the workers while protecting their privacy. We propose worker allocation methods with the property of differential privacy, which proceed by first computing weights for each potential allocation and then sampling according to the weights. The Markov Chain Monte Carlo-based method is shown in our experiments to improve over the trivial random allocation method by 18.9\% in terms of worker quality on synthetic data. On the real data, it realizes differential privacy with less than 20\% loss on quality even when ϵ=13.},
journal = {The VLDB Journal},
month = jan,
pages = {733–751},
numpages = {19},
keywords = {Worker privacy, Worker allocation, Crowdsourcing, Differential privacy}
}

@article{10.1145/3494522,
author = {Hettiachchi, Danula and Kostakos, Vassilis and Goncalves, Jorge},
title = {A Survey on Task Assignment in Crowdsourcing},
year = {2022},
issue_date = {March 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {55},
number = {3},
issn = {0360-0300},
url = {https://doi.org/10.1145/3494522},
doi = {10.1145/3494522},
abstract = {Quality improvement methods are essential to gathering high-quality crowdsourced data, both for research and industry applications. A popular and broadly applicable method is task assignment that dynamically adjusts crowd workflow parameters. In this survey, we review task assignment methods that address: heterogeneous task assignment, question assignment, and plurality problems in crowdsourcing. We discuss and contrast how these methods estimate worker performance, and highlight potential challenges in their implementation. Finally, we discuss future research directions for task assignment methods, and how crowdsourcing platforms and other stakeholders can benefit from them.},
journal = {ACM Comput. Surv.},
month = feb,
articleno = {49},
numpages = {35},
keywords = {worker attributes, question assignment, plurality problem, heterogeneous task assignment, data quality, Crowdsourcing}
}

@article{10.1145/3503156,
author = {Manerkar, Sanjana and Asnani, Kavita and Khorjuvenkar, Preeti Ravindranath and Desai, Shilpa and Pawar, Jyoti D.},
title = {Konkani WordNet: Corpus-Based Enhancement using Crowdsourcing},
year = {2022},
issue_date = {July 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {21},
number = {4},
issn = {2375-4699},
url = {https://doi.org/10.1145/3503156},
doi = {10.1145/3503156},
abstract = {Konkani is one of the languages included in the eighth schedule of the Indian constitution. It is the official language of Goa and is spoken mainly in Goa and some places in Karnataka and Kerala. Konkani WordNet or Konkani Shabdamalem (k\={o}undefinedkanundefined \'{s}abdamundefinedlundefinedundefined) as it has been referred to, was developed under the Indradhanush WordNet Project Consortium during the period from August 2010 to October 2013. This project was funded by Technology Development for Indian Languages (TDIL), Department of Electronics \&amp; Information Technology (Deity), and Ministry of Communication and Information Technology (MCIT). The work on Konkani WordNet has halted since the end of the project. Currently, the Konkani WordNet contains around 32,370 synsets. However, to make it a powerful resource for NLP applications in the Konkani language, a need is felt for research work toward enhancement of the Konkani WordNet via community involvement. Crowdsourcing is a technique in which the knowledge of the crowd is utilized to accomplish a particular task.In this article, we have presented the details of the crowdsourcing platform named “Konkani Shabdarth” (k\={o}undefinedkanundefined \'{s}abdundefinedrth). Konkani Shabdarth attempts to use the knowledge of Konkani speaking people for creating new synsets and perform the quantitative enhancement of the wordnet. It also intends to work toward enhancing the overall quality of the Konkani WordNet by validating the existing synsets, and adding the missing words to the existing synsets. A text corpus named “Konkani Shabdarth Corpus”, has been created from the Konkani literature while implementing the Konkani Shabdarth tool. Using this corpus, 572 root words that are missing from the Konkani WordNet have been identified which are given as input to Konkani Shabdarth. As of now, total 94 users have registered on the platform, out of which 25 users have actually played the game. Currently, 71 new synsets have been obtained for 21 words. For some of the words, multiple entries for the concept definition have been received. This overlap is essential for automating the process of validating the synsets. Due to the pandemic period, it has been difficult to train and get players to actually play the game and contribute. We studied the impact of adding missing words from other existing Konkani text corpus on the coverage of Konkani WordNet. The expected increase in the percentage coverage of Konkani WordNet has been found to be in the range 20–27 after adding the missing words from the Konkani Shabdarth corpus in comparison to the other corpora for which the increase is in the range 1–10.},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = mar,
articleno = {80},
numpages = {18},
keywords = {crowdsourcing, Konkani Shabdarth, Konkani Wordnet, WordNet}
}

@article{10.1007/s00778-023-00802-3,
author = {Zhao, Yan and Zheng, Kai and Wang, Ziwei and Deng, Liwei and Yang, Bin and Pedersen, Torben Bach and Jensen, Christian S. and Zhou, Xiaofang},
title = {Coalition-based task assignment with priority-aware fairness in spatial crowdsourcing},
year = {2023},
issue_date = {Jan 2024},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {33},
number = {1},
issn = {1066-8888},
url = {https://doi.org/10.1007/s00778-023-00802-3},
doi = {10.1007/s00778-023-00802-3},
abstract = {With the widespread use of networked and geo-positioned mobile devices, e.g., smartphones, Spatial Crowdsourcing (SC), which refers to the assignment of location-based tasks to moving workers, is drawing increasing attention. One of the critical issues in SC is task assignment that allocates tasks to appropriate workers. We propose and study a novel SC problem, namely Coalition-based Task Assignment (CTA), where the spatial tasks (e.g., home improvement and furniture installation) may require more than one worker (forming a coalition) to cooperate to maximize the overall rewards of workers. We design a greedy and an equilibrium-based CTA approach. The greedy approach forms a set of worker coalitions greedily for performing tasks and uses an acceptance probability to identify high-value task assignments. In the equilibrium-based approach, workers form coalitions in sequence and update their strategies (i.e., selecting a best-response task), to maximize their own utility (i.e., the reward of the coalition they belong to) until a Nash equilibrium is reached. Since the equilibrium obtained is not unique and optimal in terms of total rewards, we further propose a simulated annealing scheme to find a better Nash equilibrium. To achieve fair task assignments, we optimize the framework to distribute rewards fairly among workers in a coalition based on their marginal contributions and give workers who arrive first at the SC platform highest priority. Extensive experiments demonstrate the efficiency and effectiveness of the proposed methods on real and synthetic data.},
journal = {The VLDB Journal},
month = jul,
pages = {163–184},
numpages = {22},
keywords = {Coalition, Task assignment, Spatial crowdsourcing, Priority-aware fairness}
}

@article{10.1145/3491048,
author = {Shin, Suho and Choi, Hoyong and Yi, Yung and Ok, Jungseul},
title = {Power of Bonus in Pricing for Crowdsourcing},
year = {2021},
issue_date = {December 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {5},
number = {3},
url = {https://doi.org/10.1145/3491048},
doi = {10.1145/3491048},
abstract = {We consider a simple form of pricing for a crowdsourcing system, where pricing policy is published a priori, and workers then decide their task acceptance. Such a pricing form is widely adopted in practice for its simplicity, e.g., Amazon Mechanical Turk, although additional sophistication to pricing rule can enhance budget efficiency. With the goal of designing efficient and simple pricing rules, we study the impact of the following two design features in pricing policies: (i) personalization tailoring policy worker-by-worker and (ii) bonus payment to qualified task completion. In the Bayesian setting, where the only prior distribution of workers' profiles is available, we first study the Price of Agnosticism (PoA) that quantifies the utility gap between personalized and common pricing policies. We show that PoA is bounded within a constant factor under some mild conditions, and the impact of bonus is essential in common pricing. These analytic results imply that complex personalized pricing can be replaced by simple common pricing once it is equipped with a proper bonus payment. To provide insights on efficient common pricing, we then study the efficient mechanisms of bonus payment for several profile distribution regimes which may exist in practice. We provide primitive experiments on Amazon Mechanical Turk, which support our analytical findings.},
journal = {Proc. ACM Meas. Anal. Comput. Syst.},
month = dec,
articleno = {36},
numpages = {25},
keywords = {crowdsourcing, posted price mechanism, price discrimination, quality-based pricing}
}

@article{10.1109/TNET.2022.3223367,
author = {Shi, Qi and Hao, Dong},
title = {Social Sourcing: Incorporating Social Networks Into Crowdsourcing Contest Design},
year = {2022},
issue_date = {Aug. 2023},
publisher = {IEEE Press},
volume = {31},
number = {4},
issn = {1063-6692},
url = {https://doi.org/10.1109/TNET.2022.3223367},
doi = {10.1109/TNET.2022.3223367},
abstract = {In a crowdsourcing contest, a principal holding a task posts it to a crowd. People in the crowd then compete with each other to win the rewards. Although in real life, a crowd is usually networked and people influence each other via social ties, existing crowdsourcing contest theories do not aim to answer how interpersonal relationships influence people’s incentives and behaviors and thereby affect the crowdsourcing performance. In this work, we novelly take people’s social ties as a key factor in the modeling and designing of agents’ incentives in crowdsourcing contests. We establish two contest mechanisms by which the principal can impel the agents to invite their neighbors to contribute to the task. The first mechanism has a symmetric Bayesian Nash equilibrium, and it is very simple for agents to play and easy for the principal to predict the contest performance. The second mechanism has an asymmetric Bayesian Nash equilibrium, and agents’ behaviors in equilibrium show a vast diversity which is strongly related to their social relations. The Bayesian Nash equilibrium analysis of these new mechanisms reveals that, besides agents’ intrinsic abilities, the social relations among them also play a central role in decision-making. Moreover, we design an effective algorithm to automatically compute the Bayesian Nash equilibrium of the invitation crowdsourcing contest and further adapt it to a large graph dataset. Both theoretical and empirical results show that the new invitation crowdsourcing contests can substantially enlarge the number of participants, whereby the principal can obtain significantly better solutions without a large advertisement expenditure.},
journal = {IEEE/ACM Trans. Netw.},
month = nov,
pages = {1535–1549},
numpages = {15}
}

@article{10.14778/3579075.3579082,
author = {Li, Boyang and Cheng, Yurong and Yuan, Ye and Yang, Yi and Jin, QianQian and Wang, Guoren},
title = {ACTA: Autonomy and Coordination Task Assignment in Spatial Crowdsourcing Platforms},
year = {2023},
issue_date = {January 2023},
publisher = {VLDB Endowment},
volume = {16},
number = {5},
issn = {2150-8097},
url = {https://doi.org/10.14778/3579075.3579082},
doi = {10.14778/3579075.3579082},
abstract = {Spatial platforms have become increasingly important in people's daily lives. Task assignment is a critical problem in these platforms that matches real-time orders to suitable workers. Most studies only focus on independent platforms that are in a competitive relationship. Recently, an emerging service model was proposed, where orders are shared with multiple similar platforms. It aims to solve the imbalance between supply and demand through cooperation. However, it faces the following main challenges: 1) Coordinating independent platforms fairly based on the limited information; 2) Building a task assignment process with personalized algorithms. In this paper, we study real applications and define the Autonomy and Coordination Task Assignment problem (ACTA) to maximize the global revenue and fairness. We propose a framework to solve ACTA that consists of public order sending, local matching, global conflict adjustment and results notification. The framework uses mid-products and public data to train a revenue estimation model to coordinate participants. We further propose dynamic weight task assignment algorithms to guarantee fairness. Through the experiments, we prove that the platforms can obtain higher revenue, which shows the effectiveness and efficiency of our work.},
journal = {Proc. VLDB Endow.},
month = jan,
pages = {1073–1085},
numpages = {13}
}

@article{10.14778/3561261.3561266,
author = {Yang, Yi and Cheng, Yurong and Yuan, Ye and Wang, Guoren and Chen, Lei and Sun, Yongjiao},
title = {Privacy-preserving cooperative online matching over spatial crowdsourcing platforms},
year = {2022},
issue_date = {September 2022},
publisher = {VLDB Endowment},
volume = {16},
number = {1},
issn = {2150-8097},
url = {https://doi.org/10.14778/3561261.3561266},
doi = {10.14778/3561261.3561266},
abstract = {With the continuous development of spatial crowdsourcing platform, online task assignment problem has been widely studied as a typical problem in spatial crowdsourcing. Most of the existing studies are based on a single-platform task assignment to maximize the platform's revenue. Recently, cross online task assignment has been proposed, aiming at increasing the mutual benefit through cooperations. However, existing methods fail to consider the data privacy protection in the process of cooperation and cause the leakage of sensitive data such as the location of a request and the historical data of cooperative platforms. In this paper, we propose Privacy-preserving Cooperative Online Matching (PCOM), which protects the privacy of the users and workers on their respective platforms. We design a PCOM framework and provide theoretical proof that the framework satisfies the differential privacy property. We then propose two PCOM algorithms based on two different privacy-preserving strategies. Extensive experiments on real and synthetic datasets confirm the effectiveness and efficiency of our algorithms.},
journal = {Proc. VLDB Endow.},
month = sep,
pages = {51–63},
numpages = {13}
}

@article{10.1145/3565576,
author = {Wu, Gongqing and Zhuo, Xingrui and Bao, Xianyu and Hu, Xuegang and Hong, Richang and Wu, Xindong},
title = {Crowdsourcing Truth Inference via Reliability-Driven Multi-View Graph Embedding},
year = {2023},
issue_date = {June 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {17},
number = {5},
issn = {1556-4681},
url = {https://doi.org/10.1145/3565576},
doi = {10.1145/3565576},
abstract = {Crowdsourcing truth inference aims to assign a correct answer to each task from candidate answers that are provided by crowdsourced workers. A common approach is to generate workers’ reliabilities to represent the quality of answers. Although crowdsourced triples can be converted into various crowdsourced relationships, the available related methods are not effective in capturing these relationships to alleviate the harm to inference that is caused by conflicting answers. In this research, we propose a Reliability-driven Multi-view Graph Embedding framework for Truth inference (TiReMGE), which explores multiple crowdsourced relationships by organically integrating worker reliabilities into a graph space that is constructed from crowdsourced triples. Specifically, to create an interactive environment, we propose a reliability-driven initialization criterion for initializing vectors of tasks and workers as interactive carriers of reliabilities. From the perspective of multiple crowdsourced relationships, a multi-view graph embedding framework is proposed for reliability information interaction on a task-worker graph, which encodes latent crowdsourced relationships into vectors of workers and tasks for reliability update and truth inference. A heritable reliability updating method based on the Lagrange multiplier method is proposed to obtain reliabilities that match the quality of workers for interaction by a novel constraint law. Our ultimate goal is to minimize the Euclidean distance between the encoded task vector and the answer that is provided by a worker with high reliability. Extensive experimental results on nine real-world datasets demonstrate that TiReMGE significantly outperforms the nine state-of-the-art baselines.},
journal = {ACM Trans. Knowl. Discov. Data},
month = feb,
articleno = {65},
numpages = {26},
keywords = {worker quality match, reliability interaction, graph embedding, Crowdsourcing truth inference}
}

@article{10.1145/3476063,
author = {Qiu, Sihang and Bozzon, Alessandro and Birk, Max V. and Gadiraju, Ujwal},
title = {Using Worker Avatars to Improve Microtask Crowdsourcing},
year = {2021},
issue_date = {October 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {5},
number = {CSCW2},
url = {https://doi.org/10.1145/3476063},
doi = {10.1145/3476063},
abstract = {The future of crowd work has been identified to depend on worker satisfaction, but we lack a thorough understanding of how worker satisfaction can be increased in microtask crowdsourcing. Prior work has shown that one solution is to build tasks that are engaging. To facilitate engagement, two methods that have received attention in recent HCI literature are the use of video games and conversational interfaces. While these are largely different techniques, they aim for the same goal of reducing worker burden and increasing engagement in a task. On one hand, video games have huge motivation potential and translating game design elements for motivational purposes has shown positive effects. Recent work in games research has shown that the use of player avatars is effective in fostering interest, enjoyment, and other aspects pertaining to intrinsic motivation. On the other hand, conversational interfaces have been argued to have advantages over traditional GUIs due to facilitating a more human-like interaction. Conversational microtasking has recently been proposed to improve worker engagement in microtask marketplaces. The contexts of games and crowd work are underlined by the need to motivate and engage participants, yet the potential of using worker avatars to promote self-identification and improve worker satisfaction in microtask crowdsourcing has remained unexplored. Addressing this knowledge gap, we carried out a between-subject study involving 360 crowd workers. We investigated how worker avatars influence quality related outcomes of workers and their perceived experience, in conventional web and novel conversational interfaces. We equipped workers with the functionality of customizing their avatars, and selecting characterizations for their avatars, to understand whether identifying with an avatar can increase the motivation of workers. We found that using worker avatars with conversational interfaces can effectively reduce cognitive workload and increase worker retention. Our results indicate the occurrence of similarity and wishful avatar identification in crowdsourcing. Our findings have important implications in alleviating workers' perceived workload and on the design of crowdsourcing microtasks.},
journal = {Proc. ACM Hum.-Comput. Interact.},
month = oct,
articleno = {322},
numpages = {28},
keywords = {avatar, chatbot, crowdsourcing, motivation, perceived workload}
}

@article{10.1007/s00778-019-00568-7,
author = {Tong, Yongxin and Zhou, Zimu and Zeng, Yuxiang and Chen, Lei and Shahabi, Cyrus},
title = {Spatial crowdsourcing: a survey},
year = {2019},
issue_date = {Jan 2020},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {29},
number = {1},
issn = {1066-8888},
url = {https://doi.org/10.1007/s00778-019-00568-7},
doi = {10.1007/s00778-019-00568-7},
abstract = {Crowdsourcing is a computing paradigm where humans are actively involved in a computing task, especially for tasks that are intrinsically easier for humans than for computers. Spatial crowdsourcing is an increasing popular category of crowdsourcing in the era of mobile Internet and sharing economy, where tasks are spatiotemporal and must be completed at a specific location and time. In fact, spatial crowdsourcing has stimulated a series of recent industrial successes including sharing economy for urban services (Uber and Gigwalk) and spatiotemporal data collection (OpenStreetMap and Waze). This survey dives deep into the challenges and techniques brought by the unique characteristics of spatial crowdsourcing. Particularly, we identify four core algorithmic issues in spatial crowdsourcing: (1) task assignment, (2) quality control, (3) incentive mechanism design, and (4) privacy protection. We conduct a comprehensive and systematic review of existing research on the aforementioned four issues. We also analyze representative spatial crowdsourcing applications and explain how they are enabled by these four technical issues. Finally, we discuss open questions that need to be addressed for future spatial crowdsourcing research and applications.},
journal = {The VLDB Journal},
month = aug,
pages = {217–250},
numpages = {34},
keywords = {Privacy protection, Incentive mechanism, Quality control, Task assignment, Spatial crowdsourcing}
}

@article{10.1145/3597201,
author = {Roitero, Kevin and Barbera, David La and Soprano, Michael and Demartini, Gianluca and Mizzaro, Stefano and Sakai, Tetsuya},
title = {How Many Crowd Workers Do I Need? On Statistical Power when Crowdsourcing Relevance Judgments},
year = {2023},
issue_date = {January 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {42},
number = {1},
issn = {1046-8188},
url = {https://doi.org/10.1145/3597201},
doi = {10.1145/3597201},
abstract = {To scale the size of Information Retrieval collections, crowdsourcing has become a common way to collect relevance judgments at scale. Crowdsourcing experiments usually employ 100–10,000 workers, but such a number is often decided in a heuristic way. The downside is that the resulting dataset does not have any guarantee of meeting predefined statistical requirements as, for example, have enough statistical power to be able to distinguish in a statistically significant way between the relevance of two documents.We propose a methodology adapted from literature on sound topic set size design, based on t-test and ANOVA, which aims at guaranteeing the resulting dataset to meet a predefined set of statistical requirements. We validate our approach on several public datasets.Our results show that we can reliably estimate the recommended number of workers needed to achieve statistical power, and that such estimation is dependent on the topic, while the effect of the relevance scale is limited. Furthermore, we found that such estimation is dependent on worker features such as agreement. Finally, we describe a set of practical estimation strategies that can be used to estimate the worker set size, and we also provide results on the estimation of document set sizes.},
journal = {ACM Trans. Inf. Syst.},
month = aug,
articleno = {21},
numpages = {26},
keywords = {relevance judgments, statistical analysis, Crowdsourcing}
}

@article{10.1145/3569092,
author = {Azizifard, Narges and Gelauff, Lodewijk and Gransard-Desmond, Jean-Olivier and Redi, Miriam and Schifanella, Rossano},
title = {Wiki Loves Monuments: Crowdsourcing the Collective Image of the Worldwide Built Heritage},
year = {2022},
issue_date = {March 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {16},
number = {1},
issn = {1556-4673},
url = {https://doi.org/10.1145/3569092},
doi = {10.1145/3569092},
abstract = {The wide adoption of digital technologies in the cultural heritage sector has promoted the emergence of new, distributed ways of working, communicating, and investigating cultural products and services. In particular, collaborative online platforms and crowdsourcing mechanisms have been widely adopted in the effort to solicit input from the community and promote engagement. In this work, we provide an extensive analysis of the Wiki Loves Monuments initiative, an annual, international photography contest in which volunteers are invited to take pictures of the built cultural heritage and upload them to Wikimedia Commons. We explore the geographical, temporal, and topical dimensions across the 2010–2021 editions. We first adopt a set of CNN-based artificial systems that allow the learning of deep scene features for various scene recognition tasks, exploring cross-country (dis)similarities. To overcome the rigidity of the framework based on scene descriptors, we train a deep convolutional neural network model to label a photo with its country of origin. The resulting model captures the best representation of a heritage site uploaded in a country, and it allows the domain experts to explore the complexity of cross-national architectural styles. Finally, as a validation step, we explore the link between architectural heritage and intangible cultural values, operationalized using the framework developed within the World Value Survey research program. We observe that cross-country cultural similarities match to a fair extent the interrelations emerging in the architectural domain. We think this study contributes to highlighting the richness and the potential of the Wikimedia data and tools ecosystem to act as a scientific object for art historians, iconologists, and archaeologists.},
journal = {J. Comput. Cult. Herit.},
month = dec,
articleno = {20},
numpages = {27},
keywords = {Wiki Loves Monuments, cross-cultural study, Cultural heritage}
}

@article{10.1145/3396863,
author = {Shah, Nihar B. and Zhou, Dengyong},
title = {Approval Voting and Incentives in Crowdsourcing},
year = {2020},
issue_date = {August 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {8},
number = {3},
issn = {2167-8375},
url = {https://doi.org/10.1145/3396863},
doi = {10.1145/3396863},
abstract = {The growing need for labeled training data has made crowdsourcing a vital tool for developing machine learning applications. Here, workers on a crowdsourcing platform are typically shown a list of unlabeled items, and for each of these items, are asked to choose a label from one of the provided options. The workers in crowdsourcing platforms are not experts, thereby making it essential to judiciously elicit the information known to the workers. With respect to this goal, there are two key shortcomings of current systems: (i) the incentives of the workers are not aligned with those of the requesters; and (ii) the interface does not allow workers to convey their knowledge accurately by forcing them to make a single choice among a set of options. In this article, we address these issues by introducing approval voting to utilize the expertise of workers who have partial knowledge of the true answer and coupling it with two strictly proper scoring rules. We additionally establish attractive properties of optimality and uniqueness of our scoring rules. We also conduct preliminary empirical studies on Amazon Mechanical Turk, and the results of these experiments validate our approach.},
journal = {ACM Trans. Econ. Comput.},
month = jun,
articleno = {13},
numpages = {40},
keywords = {Proper scoring rules, incentives, labeling}
}

@article{10.1145/3555137,
author = {Rechkemmer, Amy and Yin, Ming},
title = {Understanding the Microtask Crowdsourcing Experience for Workers with Disabilities: A Comparative View},
year = {2022},
issue_date = {November 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {6},
number = {CSCW2},
url = {https://doi.org/10.1145/3555137},
doi = {10.1145/3555137},
abstract = {Microtask crowdsourcing holds great potential as an employment opportunity with the flexibility and anonymity that individuals with disability may require. Though prior research has explored the accessibility of crowd work, the lived crowd work experiences of the broader community of workers with disability are still largely under-explored, especially when it comes to how their experiences are similar to or different from the experiences of workers without disability. In this work, we aim to obtain a deeper understanding of the microtask crowdsourcing experience for people with disabilities, especially regarding their financial and social experiences of participating in crowd work, along with the benefits and challenges that they encounter through this work. Specifically, we first surveyed 1,200 crowd workers both with and without disability about their experiences using the Amazon Mechanical Turk platform, and the differences we found inspired the design of a follow-up survey to gain greater understanding of the crowd work experience for workers with disability. Our findings reveal that workers with disability receive unique benefits from performing crowd work, such as a greater sense of purpose, but also encounter many challenges, such as completing tasks on time and earning a livable wage, causing them to turn to online communities for assistance. Although many of the challenges they face are not unique to crowd workers with disability, workers with disability may be disproportionately impacted by these challenges. From our findings, we provide implications for crowd platforms, as well as the gig economy as a whole, that seek to promote greater consideration of workers with a diverse range of conditions to create a more valuable work experience for them.},
journal = {Proc. ACM Hum.-Comput. Interact.},
month = nov,
articleno = {412},
numpages = {30},
keywords = {survey, mechanical turk, crowdsourcing, accessibility}
}

@article{10.1145/3555178,
author = {Kou, Ziyi and Zhang, Yang and Zhang, Daniel and Wang, Dong},
title = {CrowdGraph: A Crowdsourcing Multi-modal Knowledge Graph Approach to Explainable Fauxtography Detection},
year = {2022},
issue_date = {November 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {6},
number = {CSCW2},
url = {https://doi.org/10.1145/3555178},
doi = {10.1145/3555178},
abstract = {Human-centric fauxtography is a category of multi-modal posts that spread misleading information on online information distribution and sharing platforms such as online social media. The reason of a human-centric post being fauxtography is closely related to its multi-modal content that consists of diversified human and non-human subjects with complex and implicit relationships. In this paper, we focus on an explainable fauxtography detection problem where the goal is to accurately identify and explain why a human-centric social media post is fauxtography (or not). Our problem is motivated by the limitations of current fauxtography detection solutions that focus primarily on the detection task but ignore the important aspect of explaining their results (e.g., why a certain component of the post delivers the misinformation). Two important challenges exist in solving our problem: 1) it is difficult to capture the implicit relations and attributions of different subjects in a fauxtography post given the fact that many of such knowledge is shared between different crowd workers; 2) it is not a trivial task to create a multi-modal knowledge graph from crowd workers to identify and explain human-centric fauxtography posts with multi-modal contents. To address the above challenges, we develop CrowdGraph, a crowdsourcing based multi-modal knowledge graph approach to address the explainable fauxtography detection problem. We evaluate the performance of CrowdGraph by creating a real-world dataset that consists of human-centric fauxtography posts from Twitter and Reddit. The results show that CrowdGraph not only detects the fauxtography posts more accurately than the state-of-the-arts but also provides well-justified explanations to the detection results with convincing evidence.},
journal = {Proc. ACM Hum.-Comput. Interact.},
month = nov,
articleno = {287},
numpages = {28},
keywords = {social media, multi modal information, crowdsourcing}
}

@article{10.1145/3375194,
author = {de Souza, Cleidson R. B. and Machado, Leticia S. and Melo, Ricardo Rodrigo M.},
title = {On Moderating Software Crowdsourcing Challenges},
year = {2020},
issue_date = {January 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {4},
number = {GROUP},
url = {https://doi.org/10.1145/3375194},
doi = {10.1145/3375194},
abstract = {Crowdsourcing divides a task into small pieces that are carried out by the crowd. In Software Engineering, crowdsourcing divides the software development tasks of to be carried out online by the crow and is simply called Software Crowdsourcing (SW CS). Most SW CS platforms support this emerging software development strategy and operate within a framework of competition among the crowd. Competitive SW CS platforms intentionally minimize communication and collaboration among the parties involved (customer, platform, and crowd) while they compete in the software development tasks. The goal of this paper is to investigate platform moderators in SW CS challenges. Platform moderators are individuals who work for the SW CS platforms to mediate customer and crowd. A qualitative analysis of the content of the communication forums hosted on the TopCoder platform was performed to analyze the messages exchanged by the platform moderators and the crowd. Our empirical results indicate that co-pilots enforce and, at the same time, extend the limitations of the documentation associated with the tasks to support crowd members, provide technical help to crowd members during the competitions, and engage the crowd in the challenges. Co-pilots are organized, work diligently, worrying about being fair, and, at the same time, seeking to find a balance between autonomy and dependency on the customer. We conclude by providing insights to improve the design of software crowdsourcing platforms.},
journal = {Proc. ACM Hum.-Comput. Interact.},
month = jan,
articleno = {14},
numpages = {22},
keywords = {topcoder, software crowdsourcing, platform moderators, crowd, communication, collaboration, challenge}
}

@article{10.1145/3478117,
author = {Ding, Yi and Guo, Baoshen and Zheng, Lin and Lu, Mingming and Zhang, Desheng and Wang, Shuai and Son, Sang Hyuk and He, Tian},
title = {A City-Wide Crowdsourcing Delivery System with Reinforcement Learning},
year = {2021},
issue_date = {Sept 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {5},
number = {3},
url = {https://doi.org/10.1145/3478117},
doi = {10.1145/3478117},
abstract = {The revolution of online shopping in recent years demands corresponding evolution in delivery services in urban areas. To cater to this trend, delivery by the crowd has become an alternative to the traditional delivery services thanks to the advances in ubiquitous computing. Notably, some studies use public transportation for crowdsourcing delivery, given its low-cost delivery network with millions of passengers as potential couriers. However, multiple practical impact factors are not considered in existing public-transport-based crowdsourcing delivery studies due to a lack of data and limited ubiquitous computing infrastructures in the past. In this work, we design a crowdsourcing delivery system based on public transport, considering the practical factors of time constraints, multi-hop delivery, and profits. To incorporate the impact factors, we build a reinforcement learning model to learn the optimal order dispatching strategies from massive passenger data and package data. The order dispatching problem is formulated as a sequential decision making problem for the packages routing, i.e., select the next station for the package. A delivery time estimation module is designed to accelerate the training process and provide statistical delivery time guarantee. Three months of real-world public transportation data and one month of package delivery data from an on-demand delivery platform in Shenzhen are used in the evaluation. Compared with existing crowdsourcing delivery algorithms and widely used baselines, we achieve a 40\% increase in profit rates and a 29\% increase in delivery rates. Comparison with other reinforcement learning algorithms shows that we can improve the profit rate and the delivery rate by 9\% and 8\% by using time estimation in action filtering. We share the data used in the project to the community for other researchers to validate our results and conduct further research.1 [1].},
journal = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
month = sep,
articleno = {97},
numpages = {22},
keywords = {Sharing Economy, Reinforcement Learning, Crowdsourcing, Crowdsourced Labor}
}

@article{10.1145/3287047,
author = {Iwamoto, Eiichi and Matsubara, Masaki and Ota, Chihiro and Nakamura, Satoshi and Terada, Tsutomu and Kitagawa, Hiroyuki and Morishima, Atsuyuki},
title = {Passerby Crowdsourcing: Workers' Behavior and Data Quality Management},
year = {2018},
issue_date = {December 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {2},
number = {4},
url = {https://doi.org/10.1145/3287047},
doi = {10.1145/3287047},
abstract = {Worker recruitment is one of the important problems in crowdsourcing, and many proposals have been presented for placing equipment in physical spaces for recruiting workers. One of the essential challenges of the approach is how to keep people attracted because those who perform tasks at first gradually lose interest and do not access the equipment. This study uses a different approach to the worker recruitment problem. In our approach, we dive into people's personal spaces by projecting task images on the floor, thereby allowing the passersby to effortlessly access tasks while walking. The problem then changes from how to keep people engaged to how to manage data quality because many passersby unconsciously or intentionally walk through the task screen on the floor without doing the task, which produces unintended results. We explore a machine-learning approach to select only the intended results and manage the data quality. The system assesses the workers' intention from their behavior. We identify the features for classifiers based on our observations of the passersby. We then conduct extensive evaluations with real data. The results show that the features are effective in practice, and the classifiers improve the data quality.},
journal = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
month = dec,
articleno = {169},
numpages = {20},
keywords = {Worker recruitment, Long-term practical use, Crowdsourcing}
}

@article{10.1145/3442698,
author = {Peng, Chaoqun and Zhang, Xinglin and Ou, Zhaojing and Zhang, Junna},
title = {Task Planning Considering Location Familiarity in Spatial Crowdsourcing},
year = {2021},
issue_date = {May 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {17},
number = {2},
issn = {1550-4859},
url = {https://doi.org/10.1145/3442698},
doi = {10.1145/3442698},
abstract = {Spatial crowdsourcing (SC) is a popular distributed problem-solving paradigm that harnesses the power of mobile workers (e.g., smartphone users) to perform location-based tasks (e.g., checking product placement or taking landmark photos). Typically, a worker needs to travel physically to the target location to finish the assigned task. Hence, the worker’s familiarity level on the target location directly influences the completion quality of the task. In addition, from the perspective of the SC server, it is desirable to finish all tasks with a low recruitment cost. Combining these issues, we propose a Bi-Objective Task Planning (BOTP) problem in SC, where the server makes a task assignment and schedule for the workers to jointly optimize the workers’ familiarity levels on the locations of assigned tasks and the total cost of worker recruitment. The BOTP problem is proved to be NP-hard and thus intractable. To solve this challenging problem, we propose two algorithms: a divide-and-conquer algorithm based on the constraint method and a heuristic algorithm based on the multi-objective simulated annealing algorithm. The extensive evaluations on a real-world dataset demonstrate the effectiveness of the proposed algorithms.},
journal = {ACM Trans. Sen. Netw.},
month = mar,
articleno = {16},
numpages = {24},
keywords = {task planning, location-based service, location familiarity, Spatial crowdsourcing}
}

@article{10.1145/3524065,
author = {Fisher, Michael A. and Milliken, Lindsay K.},
title = {Crowdsourcing Science and Technology Expertise to Empower Legislative Branch Oversight and Policymaking},
year = {2022},
issue_date = {April 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {3},
number = {2},
url = {https://doi.org/10.1145/3524065},
doi = {10.1145/3524065},
abstract = {As the U.S. and the world struggle through the COVID-19 pandemic, the need for science and technology (S&amp;T) expertise in governance has become even more stark. But in the U.S., the vast majority of legislators and their staffs is generalist, and they have limited resources for engaging with S&amp;T. This can be a barrier to both legislative branch oversight of S&amp;T-related issues and the development of evidence-based public policies, two functions that are especially critical in times of crisis. The obstacles to addressing these gaps have, however, created opportunities for innovations in how lawmakers connect with S&amp;T resources, resulting in new models for scientist-policymaker engagement. Our program, the Congressional Science Policy Initiative, experiments with models for connecting crowdsourced S&amp;T expertise with policymakers, namely, by enriching key congressional hearings with contributions gathered from the science community, organizing advisory councils of scientists and engineers that brief lawmakers, and crowdsourcing technical assistance from the S&amp;T community for legislative initiatives. These activities, which rely on the collective intelligence of the S&amp;T community and have been readily applied to supporting lawmakers’ decision-making processes during the pandemic, have bolstered legislative branch oversight of the executive branch, fact-finding into corporate practices, and evidence-based policymaking.},
journal = {Digit. Gov.: Res. Pract.},
month = sep,
articleno = {11},
numpages = {8},
keywords = {expertise, legislatures, democratization, Crowdsourcing}
}

@article{10.14778/3407790.3407839,
author = {Chen, Zhao and Cheng, Peng and Chen, Lei and Lin, Xuemin and Shahabi, Cyrus},
title = {Fair task assignment in spatial crowdsourcing},
year = {2020},
issue_date = {August 2020},
publisher = {VLDB Endowment},
volume = {13},
number = {12},
issn = {2150-8097},
url = {https://doi.org/10.14778/3407790.3407839},
doi = {10.14778/3407790.3407839},
abstract = {With the pervasiveness of mobile devices, wireless broadband and sharing economy, spatial crowdsourcing is becoming part of our daily life. Existing studies on spatial crowdsourcing usually focus on enhancing the platform interests and customer experiences. In this work, however, we study the fair assignment of tasks to workers in spatial crowdsourcing. That is, we aim to assign tasks, considered as a resource in short supply, to individual spatial workers in a fair manner. In this paper, we first formally define an online bi-objective matching problem, namely the Fair and Effective Task Assignment (FETA) problem, with its special cases/variants of it to capture most typical spatial crowdsourcing scenarios. We propose corresponding solutions for each variant of FETA. Particularly, we show that the dynamic sequential variant, which is a generalization of an existing fairness scheduling problem, can be solved with an O(n) fairness cost bound (n is the total number of workers), and give an O(n/m) fairness cost bound for the m-sized general batch case (m is the minimum batch size). Finally, we evaluate the effectiveness and efficiency of our algorithm on both synthetic and real data sets.},
journal = {Proc. VLDB Endow.},
month = jul,
pages = {2479–2492},
numpages = {14}
}

@article{10.1145/3492854,
author = {Sun, Yuling and Ma, Xiaojuan and Ye, Kai and He, Liang},
title = {Investigating Crowdworkers' Identify, Perception and Practices in Micro-Task Crowdsourcing},
year = {2022},
issue_date = {January 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {6},
number = {GROUP},
url = {https://doi.org/10.1145/3492854},
doi = {10.1145/3492854},
abstract = {Crowdsourcing is rapidly gaining popularity among academic and business communities. Yet, our understanding of this work way is still in its incipient stage, in particular regarding the increasingly large and diverse crowdworkers. As such, we aim to understand crowdworkers' perception and experience to themselves and their work from their own perspective. We explore this by a mix-methods study of crowdworkers in Ali, one of prominent micro-task crowdsourcing platforms in China. Our findings highlight crowdworker in Ali is not only a coded name, but also an identity with some positive attitudes and beliefs towards work and life. In particular, this identity provides many socio-psychological benefits for crowdworkers, which further contributes to their consistent engagement in Ali and proactive practices to improve crowdworker communities and Ali platform collaboratively. We according suggest that taking crowdworker identity as a lens for crowdsourcing research, and turning attention towards construction and expressions of crowdworkers' identity and values in their own context.},
journal = {Proc. ACM Hum.-Comput. Interact.},
month = jan,
articleno = {35},
numpages = {20},
keywords = {proactive practices, perception, micro-task crowdsourcing, identity, group identity, crowdsourcing in china, ali crowdsourcing}
}

@article{10.1145/3610183,
author = {Narimanzadeh, Hasti and Badie-Modiri, Arash and Smirnova, Iuliia G. and Chen, Ted Hsuan Yun},
title = {Crowdsourcing Subjective Annotations Using Pairwise Comparisons Reduces Bias and Error Compared to the Majority-vote Method},
year = {2023},
issue_date = {October 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {CSCW2},
url = {https://doi.org/10.1145/3610183},
doi = {10.1145/3610183},
abstract = {How to better reduce measurement variability and bias introduced by subjectivity in crowdsourced labelling remains an open question. We introduce a theoretical framework for understanding how random error and measurement bias enter into crowdsourced annotations of subjective constructs. We then propose a pipeline that combines pairwise comparison labelling with Elo scoring, and demonstrate that it outperforms the ubiquitous majority-voting method in reducing both types of measurement error. To assess the performance of the labelling approaches, we constructed an agent-based model of crowdsourced labelling that lets us introduce different types of subjectivity into the tasks. We find that under most conditions with task subjectivity, the comparison approach produced higher f1 scores. Further, the comparison approach is less susceptible to inflating bias, which majority voting tends to do. To facilitate applications, we show with simulated and real-world data that the number of required random comparisons for the same classification accuracy scales log-linearly O(N log N) with the number of labelled items. We also implemented the Elo system as an open-source Python package.},
journal = {Proc. ACM Hum.-Comput. Interact.},
month = oct,
articleno = {334},
numpages = {29},
keywords = {comparison method, crowdsourcing, majority-vote method, subjectivity}
}

@article{10.14778/3538598.3538609,
author = {Liang, Yihuai and Li, Yan and Shin, Byeong-Seok},
title = {Decentralized crowdsourcing for human intelligence tasks with efficient on-chain cost},
year = {2022},
issue_date = {May 2022},
publisher = {VLDB Endowment},
volume = {15},
number = {9},
issn = {2150-8097},
url = {https://doi.org/10.14778/3538598.3538609},
doi = {10.14778/3538598.3538609},
abstract = {Crowdsourcing for Human Intelligence Tasks (HIT) has been widely used to crowdsource human knowledge, such as image annotation for machine learning. We use a public blockchain to play the role of traditional centralized HIT systems, such that the blockchain deals with cryptocurrency payments and acts as a trustworthy judge to resolve disputes between a worker and a requester in a decentralized setting, preventing false-reporting and free-riding. Our approach neither uses expensive cryptographic tools, such as zero-knowledge proofs, nor sends the worker's answers to the blockchain. Compared with prior works, our approach significantly reduces on-chain cost: it only requires O(1) on-chain storage and O(logN) smart contract computation, where N is the question number of a HIT. Additionally, our approach uses known answers or gold standards to determine the worker's answer quality. To motivate the requester to use honest known answers, the requester cannot learn the worker's answers if the answer quality does not meet the requirement. We further provide formal security definitions for our decentralized HIT and prove security of our construction.},
journal = {Proc. VLDB Endow.},
month = may,
pages = {1875–1888},
numpages = {14}
}

@article{10.1145/3555613,
author = {Malkin, Nathan and Wagner, David and Egelman, Serge},
title = {Can Humans Detect Malicious Always-Listening Assistants? A Framework for Crowdsourcing Test Drives},
year = {2022},
issue_date = {November 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {6},
number = {CSCW2},
url = {https://doi.org/10.1145/3555613},
doi = {10.1145/3555613},
abstract = {As intelligent voice assistants become more widespread and the scope of their listening increases, they become attractive targets for attackers. In the future, a malicious actor could train voice assistants to listen to audio outside their purview, creating a threat to users' privacy and security. How can this misbehavior be detected? Due to the ambiguities of natural language, people may need to work in conjunction with algorithms to determine whether a given conversation should be heard. To investigate how accurately humans can perform this task, we developed a framework for people to conduct "Test Drives" of always-listening services: after submitting sample conversations, users receive instant feedback about whether these would have been captured. Leveraging a Wizard of Oz interface, we conducted a study with 200 participants to determine whether they could detect one of four types of attacks on three different services. We studied the behavior of individuals, as well as groups working collaboratively, and investigated the effects of task framing on performance. We found that individuals were able to successfully detect malicious apps at varying rates (7.5\% to 75\%), depending on the type of malicious attack, and that groups were highly successful when considered collectively. Our results suggest that the Test Drive framework can be an effective tool for studying user behaviors and concerns, as well as a potentially welcome addition to voice assistant app stores, where it could decrease privacy concerns surrounding always-listening services.},
journal = {Proc. ACM Hum.-Comput. Interact.},
month = nov,
articleno = {500},
numpages = {28},
keywords = {usable security and privacy, passive listening, intelligent assistants, crowdsourcing}
}

@article{10.1145/3569090,
author = {Cheng, Danzhao and Ch’ng, Eugene},
title = {Harnessing Collective Differences in Crowdsourcing Behaviour for Mass Photogrammetry of 3D Cultural Heritage},
year = {2022},
issue_date = {March 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {16},
number = {1},
issn = {1556-4673},
url = {https://doi.org/10.1145/3569090},
doi = {10.1145/3569090},
abstract = {Disorganised and self-organised crowdsourcing activities that harness collective behaviours to achieve a specific level of performance and task completeness are not well understood. Such phenomena become indistinct when highly varied environments are present, particularly for crowdsourcing photogrammetry-based 3D models. Mass photogrammetry can democratise traditional close-range photogrammetry procedures by outsourcing image acquisition tasks to a crowd of non-experts to capture geographically scattered 3D objects. To improve public engagement, we need to understand how individual behaviour in collective efforts work in traditional disorganised crowdsourcing and how it can be organised for better performance. This research aims to investigate the effectiveness of disorganised and self-organised collaborative crowdsourcing. It examines the collaborative dynamics among participants and the trends we could leverage if team structures were incorporated. Two scenarios were proposed and constructed: asynchronous crowdsourcing, which implicitly aggregates isolated contributions from disorganised individuals; and synchronous collaborative crowdsourcing, which assigns participants into a crowd-based self-organised team. Our experiment demonstrated that a self-organised team working in synchrony can effectively improve crowdsourcing photogrammetric 3D models in terms of model completeness and user experience. Through our study, we demonstrated that this crowdsourcing mechanism can provide a social context where participants can exchange information via implicit communication, and collectively build a shared mental model that pertains to their responsibilities and task goals. It stimulates participants’ prosocial motivation and reinforces their commitment. With more time and effort invested, their positive sense of ownership increases, fostering higher dedication and better contribution. Our findings shed further light on the potentials of adopting team structures to encourage effective collaborations in conventionally individual-based voluntary crowdsourcing settings, especially in the digital heritage domain.},
journal = {J. Comput. Cult. Herit.},
month = dec,
articleno = {19},
numpages = {23},
keywords = {cultural heritage, task allocation, crowd behaviour, collaboration dynamics, team structures, mass photogrammetry, Crowdsourcing}
}

@article{10.1145/3555649,
author = {Tsvetkova, Milena and M\"{u}ller, Sebastian and Vuculescu, Oana and Ham, Haylee and Sergeev, Rinat A.},
title = {Relative Feedback Increases Disparities in Effort and Performance in Crowdsourcing Contests: Evidence from a Quasi-Experiment on Topcoder},
year = {2022},
issue_date = {November 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {6},
number = {CSCW2},
url = {https://doi.org/10.1145/3555649},
doi = {10.1145/3555649},
abstract = {Rankings and leaderboards are often used in crowdsourcing contests and online communities to motivate individual contributions but feedback based on social comparison can also have negative effects. Here, we study the unequal effects of such feedback on individual effort and performance for individuals of different ability. We hypothesize that the effects of social comparison differ for top performers and bottom performers in a way that the inequality between the two increases. We use a quasi-experimental design to test our predictions with data from Topcoder, a large online crowdsourcing platform that publishes computer programming contests. We find that in contests where the submitted code is evaluated against others' submissions, rather than using an absolute scale, top performers increase their effort while bottom performers decrease it. As a result, relative scoring leads to better outcomes for those at the top but lower engagement for bottom performers. Our findings expose an important but overlooked drawback from using gamified competitions, rankings, and relative evaluations, with potential implications for crowdsourcing markets, online learning environments, online communities, and organizations in general.},
journal = {Proc. ACM Hum.-Comput. Interact.},
month = nov,
articleno = {536},
numpages = {27},
keywords = {task performance, task effort, feedback giving, engagement, crowdsourcing contests}
}

@article{10.1145/3463932,
author = {Machado, Leticia S. and Melo, Ricardo Rodrigo M. and de Souza, Cleidson R. B. and Prikladnicki, Rafael},
title = {Collaborative Behavior and Winning Challenges in Competitive Software Crowdsourcing},
year = {2021},
issue_date = {July 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {5},
number = {GROUP},
url = {https://doi.org/10.1145/3463932},
doi = {10.1145/3463932},
abstract = {Software Crowdsourcing (SW CS) allows a requester to increase the speed of its software development efforts by submitting a task to be performed by the crowd. SW CS is usually structured around software platforms, which are used by crowd members to identify a task suited for them, gather information about this task, and finally, submit a solution for it. In competitive software crowdsourcing, members of the crowd independently create solutions while competing against each other by monetary rewards for task completion. While competition usually reduces collaboration, in this paper, we investigated how crowd members create a collaborative behavior during programming challenges using online forums to help each other, share useful information, and discuss important documents and artifacts. We also investigated different collaborative behaviours by crowd members and and how this collaboration is associated with crowd members' improved outcome in the challenges. These results are based on analysis of the online forums from Topcoder, one of the largest competitive SW CS platforms},
journal = {Proc. ACM Hum.-Comput. Interact.},
month = jul,
articleno = {220},
numpages = {25},
keywords = {topcoder, software crowdsourcing, communication, collaboration}
}

@article{10.1109/TNET.2021.3105427,
author = {Shi, Zhiguo and Yang, Guang and Gong, Xiaowen and He, Shibo and Chen, Jiming},
title = {Quality-Aware Incentive Mechanisms Under Social Influences in Data Crowdsourcing},
year = {2021},
issue_date = {Feb. 2022},
publisher = {IEEE Press},
volume = {30},
number = {1},
issn = {1063-6692},
url = {https://doi.org/10.1109/TNET.2021.3105427},
doi = {10.1109/TNET.2021.3105427},
abstract = {Incentive mechanism design and quality control are two key challenges in data crowdsourcing, because of the need for recruitment of crowd users and their limited capabilities. Without considering users’ social influences, existing mechanisms often result in low efficiency in terms of the platform’s cost. In this paper, we exploit social influences among users as incentives to motivate users’ participation, in order to reduce the cost of recruiting users. Based on social influences, we design incentive mechanisms with the goal of achieving high quality of crowdsourced data and low cost of incentivizing users’ participation. Specifically, we consider three scenarios. In the full information scenario, we design task assignment and user recruitment mechanisms to optimize the data quality while reducing the incentive cost. In the partial information scenario, users’ qualities and costs are unknown. We exploit the correlation between tasks to overcome the information asymmetry, for both cases of opportunistic crowdsourcing and participatory crowdsourcing. Further, in the dynamic social influence scenario, we investigate the dynamics of users’ social influences and design extra rewards for users to make full use of the social influence and achieve maximum cost saving. We evaluate the incentive mechanisms using numerical results, which demonstrate their effectiveness.},
journal = {IEEE/ACM Trans. Netw.},
month = aug,
pages = {176–189},
numpages = {14}
}

@article{10.1145/3479586,
author = {Pei, Weiping and Yang, Zhiju and Chen, Monchu and Yue, Chuan},
title = {Quality Control in Crowdsourcing based on Fine-Grained Behavioral Features},
year = {2021},
issue_date = {October 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {5},
number = {CSCW2},
url = {https://doi.org/10.1145/3479586},
doi = {10.1145/3479586},
abstract = {Crowdsourcing is popular for large-scale data collection and labeling, but a major challenge is on detecting low-quality submissions. Recent studies have demonstrated that behavioral features of workers are highly correlated with data quality and can be useful in quality control. However, these studies primarily leveraged coarsely extracted behavioral features, and did not further explore quality control at the fine-grained level, i.e., the annotation unit level. In this paper, we investigate the feasibility and benefits of using fine-grained behavioral features, which are the behavioral features finely extracted from a worker's individual interactions with each single unit in a subtask, for quality control in crowdsourcing. We design and implement a framework named Fine-grained Behavior-based Quality Control (FBQC) that specifically extracts fine-grained behavioral features to provide three quality control mechanisms: (1) quality prediction for objective tasks, (2) suspicious behavior detection for subjective tasks, and (3) unsupervised worker categorization. Using the FBQC framework, we conduct two real-world crowdsourcing experiments and demonstrate that using fine-grained behavioral features is feasible and beneficial in all three quality control mechanisms. Our work provides clues and implications for helping job requesters or crowdsourcing platforms to further achieve better quality control.},
journal = {Proc. ACM Hum.-Comput. Interact.},
month = oct,
articleno = {442},
numpages = {28},
keywords = {behavior analysis, crowdsourcing, quality control}
}

@article{10.1145/3407182,
author = {Mazumdar, Pramit and Patra, Bidyut Kr. and Babu, Korra Sathya},
title = {Cold-start Point-of-interest Recommendation through Crowdsourcing},
year = {2020},
issue_date = {November 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {14},
number = {4},
issn = {1559-1131},
url = {https://doi.org/10.1145/3407182},
doi = {10.1145/3407182},
abstract = {Recommender system is a popular tool that aims to provide personalized suggestions to user about items, products, services, and so on. Recommender system has effectively been used in online social networks, especially the location-based social networks for providing suggestions for interesting places known as POIs (points-of-interest). Popular recommender systems explore historical data to learn users’ preferences and, subsequently, they recommend locations to an active user. This strategy faces a major problem when a new POI or business evolves in a city. New business has no historical user experience data. Thus, a recommender system fails to gather enough knowledge about the new businesses, resulting in ignoring them during recommendations. This scenario is popularly known as a cold-start POI problem. Users never get recommendations of the new businesses in a city even though they can be relevant to a user. Also, from a business owner’s perspective, such a recommendation strategy does not help its reachability among users. Therefore, it is important for a recommender system to remain updated with new businesses in a city and ensure that all relevant POIs are recommended to a user irrespective of their lifetime. A POI recommendation approach is proposed in this work that can effectively handle the new businesses, or the cold-start POI problem, in a city. We crowdsource descriptions of cold-start POIs from various online social networks. The reviews of users are exploited here to learn the inherent features at the existing POIs and the new crowdsourced POIs. Finally, the proposed approach recommends top-K POIs consisting of the existing and new POIs. We perform experiments on the real-world Yelp dataset, which is one of the largest available data resources containing details on a wide range of businesses, users, and reviews. The proposed approach is compared with four existing POI recommendation approaches. The obtained results show that our approach outperforms others in handling cold-start POIs.},
journal = {ACM Trans. Web},
month = aug,
articleno = {19},
numpages = {36},
keywords = {crowdsourcing, clustering, Yelp network, Recommender systems}
}

@article{10.1145/3392837,
author = {Qiu, Sihang and Gadiraju, Ujwal and Bozzon, Alessandro},
title = {Estimating Conversational Styles in Conversational Microtask Crowdsourcing},
year = {2020},
issue_date = {May 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {4},
number = {CSCW1},
url = {https://doi.org/10.1145/3392837},
doi = {10.1145/3392837},
abstract = {Crowdsourcing marketplaces have provided a large number of opportunities for online workers to earn a living. To improve satisfaction and engagement of such workers, who are vital for the sustainability of the marketplaces, recent works have used conversational interfaces to support the execution of a variety of crowdsourcing tasks. The rationale behind using conversational interfaces stems from the potential engagement that conversation can stimulate. Prior works in psychology have also shown that conversational styles can play an important role in communication. There are unexplored opportunities to estimate a worker's conversational style with an end goal of improving worker satisfaction, engagement and quality. Addressing this knowledge gap, we investigate the role of conversational styles in conversational microtask crowdsourcing. To this end, we design a conversational interface which supports task execution, and we propose methods to estimate the conversational style of a worker. Our experimental setup was designed to empirically observe how conversational styles of workers relate with quality-related outcomes. Results show that even a naive supervised classifier can predict the conversation style with high accuracy (80\%), and crowd workers with an Involvement conversational style provided a significantly higher output quality, exhibited a higher user engagement and perceived less cognitive task load in comparison to their counterparts. Our findings have important implications on task design with respect to improving worker performance and their engagement in microtask crowdsourcing.},
journal = {Proc. ACM Hum.-Comput. Interact.},
month = may,
articleno = {32},
numpages = {23},
keywords = {work outcomes, user engagement, microtask crowdsourcing, conversational style, cognitive task load.}
}

@article{10.1145/3291933,
author = {Gummidi, Srinivasa Raghavendra Bhuvan and Xie, Xike and Pedersen, Torben Bach},
title = {A Survey of Spatial Crowdsourcing},
year = {2019},
issue_date = {June 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {44},
number = {2},
issn = {0362-5915},
url = {https://doi.org/10.1145/3291933},
doi = {10.1145/3291933},
abstract = {Widespread use of advanced mobile devices has led to the emergence of a new class of crowdsourcing called spatial crowdsourcing. Spatial crowdsourcing advances the potential of a crowd to perform tasks related to real-world scenarios involving physical locations, which were not feasible with conventional crowdsourcing methods. The main feature of spatial crowdsourcing is the presence of spatial tasks that require workers to be physically present at a particular location for task fulfillment. Research related to this new paradigm has gained momentum in recent years, necessitating a comprehensive survey to offer a bird’s-eye view of the current state of spatial crowdsourcing literature. In this article, we discuss the spatial crowdsourcing infrastructure and identify the fundamental differences between spatial and conventional crowdsourcing. Furthermore, we provide a comprehensive view of the existing literature by introducing a taxonomy, elucidate the issues/challenges faced by different components of spatial crowdsourcing, and suggest potential research directions for the future.},
journal = {ACM Trans. Database Syst.},
month = mar,
articleno = {8},
numpages = {46},
keywords = {task scheduling, task matching, task assignment, spatial databases, spatial crowdsourcing, rewards, quality assurance, location privacy, incentive mechanism, Algorithms}
}

@article{10.1145/3368268,
author = {Costa, Camila F. and Nascimento, Mario A.},
title = {In-Route Task Selection in Spatial Crowdsourcing},
year = {2019},
issue_date = {June 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {6},
number = {2},
issn = {2374-0353},
url = {https://doi.org/10.1145/3368268},
doi = {10.1145/3368268},
abstract = {Consider a city’s road network and a worker who is traveling on a given path from a starting point s to a destination d (e.g., from school or work to home) in said network. Consider further that there is a set of tasks in the network available to be performed, where each such task takes a certain amount of time to be completed and yields a positive reward if completed, and, finally, that the worker is willing to deviate from his/her path as long as the travel time to the selected tasks plus the time taken for completing them does not exceed a given time budget. We call this problem the In-Route Task Selection (IRTS) problem and consider two variants thereof. In the first one, named IRTS-SP, we assume that the worker only specifies s and d and he/she wants to consider alternative paths that deviate (cost-wise) as little as possible from the cost of the shortest path connecting s and d. In the second variant, named IRTS-PP, we assume that the worker has a preferred path from s to d and wants to travel along that one path for as long as possible. The latter is practically relevant in cases where the worker has a path other than the shortest one that is more desirable for non-objective reasons, e.g., availability of public transit, bicycle-friendliness or perceived safety. Common to both variants though, we assume that the worker wants to maximize the rewards collected by completing tasks. Clearly, there are now two conflicting criteria for the worker to contemplate when considering which tasks to perform: minimizing path deviation and maximizing collected reward. In this context, we investigate both IRTS variants using the skyline paradigm in order to obtain the set of non-dominated solutions w.r.t. the tradeoffs between earned rewards and deviation from either the cost of the shortest path, in the case of IRTS-SP, or the actual preferred path, in the case of IRTS-PP. Returning the skyline set of solutions to workers is of practical interest as it empowers them, e.g., it allows them to decide, at query time, which tasks suit them better. We propose exact and heuristic approaches in order to solve both variants of the IRTS problem. Our experiments, using real city-scale datasets, show that while the exact approaches serve as benchmarks, they do not scale due to the NP-hardness of the problems. The overall best heuristic approach, on the other hand, can solve relatively large instances of the IRTS problems within practical query processing time, e.g., at par with less effective greedy heuristics, while still producing very good approximate skyline sets, e.g., often yielding less than 10\% relative error w.r.t. the exact solution.},
journal = {ACM Trans. Spatial Algorithms Syst.},
month = dec,
articleno = {7},
numpages = {45},
keywords = {spatial crowdsourcing, skyline, road networks, In-route queries}
}

@article{10.1145/3449213,
author = {Mishra, Swati and Rzeszotarski, Jeffrey M.},
title = {Crowdsourcing and Evaluating Concept-driven Explanations of Machine Learning Models},
year = {2021},
issue_date = {April 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {5},
number = {CSCW1},
url = {https://doi.org/10.1145/3449213},
doi = {10.1145/3449213},
abstract = {An important challenge in building explainable artificially intelligent (AI) systems is designing interpretable explanations. AI models often use low-level data features which may be hard for humans to interpret. Recent research suggests that situating machine decisions in abstract, human understandable concepts can help. However, it is challenging to determine the right level of conceptual mapping. In this research, we explore granularity (of data features) and context (of data instances) as dimensions underpinning conceptual mappings. Based on these measures, we explore strategies for designing explanations in classification models. We introduce an end-to-end concept elicitation pipeline that supports gathering high-level concepts for a given data set. Through crowd-sourced experiments, we examine how providing conceptual information shapes the effectiveness of explanations, finding that a balance between coarse and fine-grained explanations help users better estimate model predictions. We organize our findings into systematic themes that can inform design considerations for future systems.},
journal = {Proc. ACM Hum.-Comput. Interact.},
month = apr,
articleno = {139},
numpages = {26},
keywords = {classification, concepts, explanations, machine learning}
}

@article{10.1145/3397180,
author = {Miao, Xin and Kang, Yanrong and Ma, Qiang and Liu, Kebin and Chen, Lei},
title = {Quality-aware Online Task Assignment in Mobile Crowdsourcing},
year = {2020},
issue_date = {August 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {16},
number = {3},
issn = {1550-4859},
url = {https://doi.org/10.1145/3397180},
doi = {10.1145/3397180},
abstract = {In recent years, mobile crowdsourcing has emerged as a powerful computation paradigm to harness human power to perform spatial tasks such as collecting real-time traffic information and checking product prices in a specific supermarket. A fundamental problem of mobile crowdsourcing is: When both tasks and crowd workers appear in the platforms dynamically, how to assign an appropriate set of tasks to each worker. Most existing studies focus on efficient assignment algorithms based on bipartite graph matching. However, they overlook an important fact that crowd workers might be unreliable. Thus, their task assignment schemes cannot ensure the overall quality. In this article, we investigate the Quality-aware Online Task Assignment (QAOTA) problem in mobile crowdsourcing. We propose a probabilistic model to measure the quality of tasks and a hitchhiking model to characterize workers’ behavior patterns. We model task assignment as a quality maximization problem and derive a polynomial-time online assignment algorithm. Through rigorous analysis, we prove that the proposed algorithm approximates the offline optimal solution with a competitive ratio of 10/7. Finally, we demonstrate the efficiency and effectiveness of our solution through intensive experiments.},
journal = {ACM Trans. Sen. Netw.},
month = jul,
articleno = {30},
numpages = {21},
keywords = {task assignment, Crowdsourcing}
}

@article{10.1145/3476053,
author = {Wang, Liang and Yu, Zhiwen and Yang, Dingqi and Wang, Tian and Wang, En and Guo, Bin and Zhang, Daqing},
title = {Task Execution Quality Maximization for Mobile Crowdsourcing in Geo-Social Networks},
year = {2021},
issue_date = {October 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {5},
number = {CSCW2},
url = {https://doi.org/10.1145/3476053},
doi = {10.1145/3476053},
abstract = {With the rapid development of smart devices and high-quality wireless technologies, mobile crowdsourcing (MCS) has been drawing increasing attention with its great potential in collaboratively completing complicated tasks on a large scale. A key issue toward successful MCS is participant recruitment, where a MCS platform directly recruits suitable crowd participants to execute outsourced tasks by physically traveling to specified locations. Recently, a novel recruitment strategy, namely Word-of-Mouth(WoM)-based MCS, has emerged to effectively improve recruitment effectiveness, by fully exploring users' mobility traces and social relationships on geo-social networks. Against this background, we study in this paper a novel problem, namely Expected Task Execution Quality Maximization (ETEQM) for MCS in geo-social networks, which strives to search a subset of seed users to maximize the expected task execution quality of all recruited participants, under a given incentive budget. To characterize the MCS task propagation process over geo-social networks, we first adopt a propagation tree structure to model the autonomous recruitment between the referrers and the referrals. Based on the model, we then formalize the task execution quality and devise a novel incentive mechanism by harnessing the business strategy of multi-level marketing. We formulate our ETEQM problem as a combinatorial optimization problem, and analyze its NP hardness and high-dimensional characteristics. Based on a cooperative co-evolution framework, we proposed a divide-and-conquer problem-solving approach named ETEQM-CC. We conduct extensive simulation experiments and a case study, verifying the effectiveness of our proposed approach.},
journal = {Proc. ACM Hum.-Comput. Interact.},
month = oct,
articleno = {312},
numpages = {29},
keywords = {cooperative co-evolution, geo-social networks, mobile crowdsourcing, task propagation model}
}

@article{10.14778/3137765.3137827,
author = {Tong, Yongxin and Chen, Lei and Shahabi, Cyrus},
title = {Spatial crowdsourcing: challenges, techniques, and applications},
year = {2017},
issue_date = {August 2017},
publisher = {VLDB Endowment},
volume = {10},
number = {12},
issn = {2150-8097},
url = {https://doi.org/10.14778/3137765.3137827},
doi = {10.14778/3137765.3137827},
abstract = {Crowdsourcing is a new computing paradigm where humans are actively enrolled to participate in the procedure of computing, especially for tasks that are intrinsically easier for humans than for computers. The popularity of mobile computing and sharing economy has extended conventional web-based crowdsourcing to spatial crowdsourcing (SC), where spatial data such as location, mobility and the associated contextual information, plays a central role. In fact, spatial crowdsourcing has stimulated a series of recent industrial successes including Citizen Sensing (Waze), P2P ride-sharing (Uber) and Real-time Online-To-Offline (O2O) services (Instacart and Postmates).In this tutorial, we review the paradigm shift from web-based crowdsourcing to spatial crowdsourcing. We dive deep into the challenges and techniques brought by the unique spatio-temporal characteristics of spatial crowdsourcing. Particularly, we survey new designs in task assignment, quality control, incentive mechanism design and privacy protection on spatial crowdsourcing platforms, as well as the new trend to incorporate crowdsourcing to enhance existing spatial data processing techniques. We also discuss case studies of representative spatial crowdsourcing systems and raise open questions and current challenges for the audience to easily comprehend the tutorial and to advance this important research area.},
journal = {Proc. VLDB Endow.},
month = aug,
pages = {1988–1991},
numpages = {4}
}

@article{10.1145/3229047,
author = {Neiat, Azadeh Ghari and Bouguettaya, Athman and Mistry, Sajib},
title = {Incentive-Based Crowdsourcing of Hotspot Services},
year = {2019},
issue_date = {February 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {19},
number = {1},
issn = {1533-5399},
url = {https://doi.org/10.1145/3229047},
doi = {10.1145/3229047},
abstract = {We present a new spatio-temporal incentive-based approach to achieve a geographically balanced coverage of crowdsourced services. The proposed approach is based on a new spatio-temporal incentive model that considers multiple parameters including location entropy, time of day, and spatio-temporal density to encourage the participation of crowdsourced service providers. We present a greedy network flow algorithm that offers incentives to redistribute crowdsourced service providers to improve the crowdsourced coverage balance within an area. A novel participation probability model is also introduced to estimate the expected number of crowdsourced service providers’ movement based on spatio-temporal features. Experimental results validate the efficiency and effectiveness of the proposed approach.},
journal = {ACM Trans. Internet Technol.},
month = jan,
articleno = {5},
numpages = {24},
keywords = {task assignment, spatiotemporal data, spatio-temporal incentive model, spatio-temporal crowdsourcing, sensor cloud, network flow, mobile crowdsourcing, crowdsourced service, coverage distribution, WiFi hotspot coverage, IoT services, IoT}
}

@article{10.1145/3610044,
author = {Pei, Weiping and Likhtenshteyn, Yanina and Yue, Chuan},
title = {A Tale of Two Communities: Privacy of Third Party App Users in Crowdsourcing - The Case of Receipt Transcription},
year = {2023},
issue_date = {October 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {CSCW2},
url = {https://doi.org/10.1145/3610044},
doi = {10.1145/3610044},
abstract = {Mobile and web apps are increasingly relying on the data generated or provided by users such as from their uploaded documents and images. Unfortunately, those apps may raise significant user privacy concerns. Specifically, to train or adapt their models for accurately processing huge amounts of data continuously collected from millions of app users, app or service providers have widely adopted the approach of crowdsourcing for recruiting crowd workers to manually annotate or transcribe the sampled ever-changing user data. However, when users' data are uploaded through apps and then become widely accessible to hundreds of thousands of anonymous crowd workers, many human-in-the-loop related privacy questions arise concerning both the app user community and the crowd worker community. In this paper, we propose to investigate the privacy risks brought by this significant trend of large-scale crowd-powered processing of app users' data generated in their daily activities. We consider the representative case of receipt scanning apps that have millions of users, and focus on the corresponding receipt transcription tasks that appear popularly on crowdsourcing platforms. We design and conduct an app user survey study (n=108) to explore how app users perceive privacy in the context of using receipt scanning apps. We also design and conduct a crowd worker survey study (n=102) to explore crowd workers' experiences on receipt and other types of transcription tasks as well as their attitudes towards such tasks. Overall, we found that most app users and crowd workers expressed strong concerns about the potential privacy risks to receipt owners, and they also had a very high level of agreement with the need for protecting receipt owners' privacy. Our work provides insights on app users' potential privacy risks in crowdsourcing, and highlights the need and challenges for protecting third party users' privacy on crowdsourcing platforms. We have responsibly disclosed our findings to the related crowdsourcing platform and app providers.},
journal = {Proc. ACM Hum.-Comput. Interact.},
month = oct,
articleno = {253},
numpages = {43},
keywords = {app user, crowdsourcing, privacy, receipt transcription}
}

@article{10.5555/3128489.3128560,
author = {Nguyen, Quoc Viet and Duong, Chi Thang and Nguyen, Thanh Tam and Weidlich, Matthias and Aberer, Karl and Yin, Hongzhi and Zhou, Xiaofang},
title = {Argument discovery via crowdsourcing},
year = {2017},
issue_date = {August    2017},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {26},
number = {4},
issn = {1066-8888},
abstract = {The amount of controversial issues being discussed on the Web has been growing dramatically. In articles, blogs, and wikis, people express their points of view in the form of arguments, i.e., claims that are supported by evidence. Discovery of arguments has a large potential for informing decision-making. However, argument discovery is hindered by the sheer amount of available Web data and its unstructured, free-text representation. The former calls for automatic text-mining approaches, whereas the latter implies a need for manual processing to extract the structure of arguments. In this paper, we propose a crowdsourcing-based approach to build a corpus of arguments, an argumentation base, thereby mediating the trade-off of automatic text-mining and manual processing in argument discovery. We develop an end-to-end process that minimizes the crowd cost while maximizing the quality of crowd answers by: (1) ranking argumentative texts, (2) pro-actively eliciting user input to extract arguments from these texts, and (3) aggregating heterogeneous crowd answers. Our experiments with real-world datasets highlight that our method discovers virtually all arguments in documents when processing only 25\% of the text with more than 80\% precision, using only 50\% of the budget consumed by a baseline algorithm.},
journal = {The VLDB Journal},
month = aug,
pages = {511–535},
numpages = {25},
keywords = {Web mining, Graphical models, Crowdsourcing}
}

@article{10.1145/3310227,
author = {Zhou, Yao and Ying, Lei and He, Jingrui},
title = {Multi-task Crowdsourcing via an Optimization Framework},
year = {2019},
issue_date = {June 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {13},
number = {3},
issn = {1556-4681},
url = {https://doi.org/10.1145/3310227},
doi = {10.1145/3310227},
abstract = {The unprecedented amounts of data have catalyzed the trend of combining human insights with machine learning techniques, which facilitate the use of crowdsourcing to enlist label information both effectively and efficiently. One crucial challenge in crowdsourcing is the diverse worker quality, which determines the accuracy of the label information provided by such workers. Motivated by the observations that same set of tasks are typically labeled by the same set of workers, we studied their behaviors across multiple related tasks and proposed an optimization framework for learning from task and worker dual heterogeneity. The proposed method uses a weight tensor to represent the workers’ behaviors across multiple tasks, and seeks to find the optimal solution of the tensor by exploiting its structured information. Then, we propose an iterative algorithm to solve the optimization problem and analyze its computational complexity. To infer the true label of an example, we construct a worker ensemble based on the estimated tensor, whose decisions will be weighted using a set of entropy weight. We also prove that the gradient of the most time-consuming updating block is separable with respect to the workers, which leads to a randomized algorithm with faster speed. Moreover, we extend the learning framework to accommodate to the multi-class setting. Finally, we test the performance of our framework on several datasets, and demonstrate its superiority over state-of-the-art techniques.},
journal = {ACM Trans. Knowl. Discov. Data},
month = may,
articleno = {27},
numpages = {26},
keywords = {tensor representation, optimization, entropy ensemble, crowdsourcing, Multi-task learning}
}

@article{10.1145/3487580,
author = {Xu, Jia and Zhou, Yuanhang and Chen, Gongyu and Ding, Yuqing and Yang, Dejun and Liu, Linfeng},
title = {Topic-aware Incentive Mechanism for Task Diffusion in Mobile Crowdsourcing through Social Network},
year = {2021},
issue_date = {February 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {22},
number = {1},
issn = {1533-5399},
url = {https://doi.org/10.1145/3487580},
doi = {10.1145/3487580},
abstract = {Crowdsourcing has become an efficient paradigm to utilize human intelligence to perform tasks that are challenging for machines. Many incentive mechanisms for crowdsourcing systems have been proposed. However, most of existing incentive mechanisms assume that there are sufficient participants to perform crowdsourcing tasks. In large-scale crowdsourcing scenarios, this assumption may be not applicable. To address this issue, we diffuse the crowdsourcing tasks in social network to increase the number of participants. To make the task diffusion more applicable to crowdsourcing system, we enhance the classic Independent Cascade model so the influence is strongly connected with both the types and topics of tasks. Based on the tailored task diffusion model, we formulate the Budget Feasible Task Diffusion (BFTD) problem for maximizing the value function of platform with constrained budget. We design a parameter estimation algorithm based on Expectation Maximization algorithm to estimate the parameters in proposed task diffusion model. Benefitting from the submodular property of the objective function, we apply the budget-feasible incentive mechanism, which satisfies desirable properties of computational efficiency, individual rationality, budget-feasible, truthfulness, and guaranteed approximation, to stimulate the task diffusers. The simulation results based on two real-world datasets show that our incentive mechanism can improve the number of active users and the task completion rate by 9.8\% and 11\%, on average.},
journal = {ACM Trans. Internet Technol.},
month = dec,
articleno = {23},
numpages = {23},
keywords = {EM algorithm, reverse auction, incentive mechanism, social network, Mobile crowdsourcing}
}

@article{10.1145/3476073,
author = {Hettiachchi, Danula and Schaekermann, Mike and McKinney, Tristan J. and Lease, Matthew},
title = {The Challenge of Variable Effort Crowdsourcing and How Visible Gold Can Help},
year = {2021},
issue_date = {October 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {5},
number = {CSCW2},
url = {https://doi.org/10.1145/3476073},
doi = {10.1145/3476073},
abstract = {We consider a class of variable effort human annotation tasks in which the number of labels required per item can greatly vary (e.g., finding all faces in an image, named entities in a text, bird calls in an audio recording, etc.). In such tasks, some items require far more effort than others to annotate. Furthermore, the per-item annotation effort is not known until after each item is annotated since determining the number of labels required is an implicit part of the annotation task itself. On an image bounding-box task with crowdsourced annotators, we show that annotator accuracy and recall consistently drop as effort increases. We hypothesize reasons for this drop and investigate a set of approaches to counteract it. Firstly, we benchmark on this task a set of general best-practice methods for quality crowdsourcing. Notably, only one of these methods actually improves quality: the use of visible gold questions that provide periodic feedback to workers on their accuracy as they work. Given these promising results, we then investigate and evaluate variants of the visible gold approach, yielding further improvement. Final results show a 7\% improvement in bounding-box accuracy over the baseline. We discuss the generality of the visible gold approach and promising directions for future research.},
journal = {Proc. ACM Hum.-Comput. Interact.},
month = oct,
articleno = {332},
numpages = {26},
keywords = {crowdsourcing, data quality, gold standards, object detection, worker training}
}

@article{10.1145/3328906,
author = {Acer, Utku G\"{u}nay and Broeck, Marc van den and Forlivesi, Claudio and Heller, Florian and Kawsar, Fahim},
title = {Scaling Crowdsourcing with Mobile Workforce: A Case Study with Belgian Postal Service},
year = {2019},
issue_date = {June 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {3},
number = {2},
url = {https://doi.org/10.1145/3328906},
doi = {10.1145/3328906},
abstract = {Traditional urban-scale crowdsourcing approaches suffer from three caveats - lack of complete spatiotemporal coverage, lack of accurate information and lack of sustained engagement of crowd workers. In this paper, we argue that these caveats can be addressed by embedding crowdsourcing tasks into the daily routine of mobile workforces that roam around an urban area. As a use case, we take the bpost who deliver the letters and parcels to the citizens across entire Belgium. We present a study that explores the behavioural attributes of these mobile postal workers both quantitatively (6.3K) and qualitatively (6) to assess the opportunity of leveraging them for crowdsourcing tasks. We report their mobility pattern, workflow, and behavioural traits which collectively inform the design of a purpose-built crowdsourcing solution. In particular, our solution operates on two key techniques - route augmentation, and on-wearable interruptibility management. Together, these mechanisms enhance the spatial coverage, response accuracy and increase workers' engagement with crowdsourcing tasks. We describe these principal components in a wearable smartwatch application supported by a data management infrastructure. Finally, we report a first-of-its-kind real-world trial with ten postal workers for two weeks to assess the quality of road signs at the city centre of Antwerp. Our findings suggest that our solution was effective in achieving 89\% spatial coverage and increasing response rate (83.6\%) and accuracy (100\%) of the crowdsourcing tasks. Although limited in scale, these and the rest of our findings highlight the way of building an efficient and purposeful crowdsourcing solution of the future.},
journal = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
month = jun,
articleno = {35},
numpages = {32},
keywords = {wearable computing, mobile crowdsourcing, interruptibility, behaviour modelling}
}

@article{10.1145/3274428,
author = {Skorupska, Kinga and Nunez, Manuel and Kopec, Wieslaw and Nielek, Radoslaw},
title = {Older Adults and Crowdsourcing: Android TV App for Evaluating TEDx Subtitle Quality},
year = {2018},
issue_date = {November 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {2},
number = {CSCW},
url = {https://doi.org/10.1145/3274428},
doi = {10.1145/3274428},
abstract = {In this paper we describe the insights from an exploratory qualitative pilot study testing the feasibility of a solution that would encourage older adults to participate in online crowdsourcing tasks in a non-computer scenario. Therefore, we developed an Android TV application using Amara API to retrieve subtitles for TEDx talks which allows the participants to detect and categorize errors to support the quality of the translation and transcription processes. It relies on the older adults' innate skills as long-time native language users and the motivating factors of this socially and personally beneficial task. The study allowed us to verify the underlying concept of using Smart TVs as interfaces for crowdsourcing, as well as possible barriers, including the interface, configuration issues, topics and the process itself. We have also assessed the older adults' interaction and engagement with this TV-enabled online crowdsourcing task and we are convinced that the design of our setup addresses some key barriers to crowdsourcing by older adults. It also validates avenues for further research in this area focused on such considerations as autonomy and freedom of choice, familiarity, physical and cognitive comfort as well as building confidence and the edutainment value.},
journal = {Proc. ACM Hum.-Comput. Interact.},
month = nov,
articleno = {159},
numpages = {23},
keywords = {volunteering, subtitling, software engineering, social inclusion, smart tv, older adults, edutainment, crowdsourcing, application development, android tv}
}

@article{10.1145/3487607,
author = {Reynante, Brandon and Dow, Steven P. and Mahyar, Narges},
title = {A Framework for Open Civic Design: Integrating Public Participation, Crowdsourcing, and Design Thinking},
year = {2021},
issue_date = {October 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {2},
number = {4},
url = {https://doi.org/10.1145/3487607},
doi = {10.1145/3487607},
abstract = {Civic problems are often too complex to solve through traditional top-down strategies. Various governments and civic initiatives have explored more community-driven strategies where citizens get involved with defining problems and innovating solutions. While certain people may feel more empowered, the public at large often does not have accessible, flexible, and meaningful ways to engage. Prior theoretical frameworks for public participation typically offer a one-size-fits-all model based on face-to-face engagement and fail to recognize the barriers faced by even the most engaged citizens. In this article, we explore a vision for open civic design where we integrate theoretical frameworks from public engagement, crowdsourcing, and design thinking to consider the role technology can play in lowering barriers to large-scale participation, scaffolding problem-solving activities, and providing flexible options that cater to individuals’ skills, availability, and interests. We describe our novel theoretical framework and analyze the key goals associated with this vision: (1) to promote inclusive and sustained participation in civics; (2) to facilitate effective management of large-scale participation; and (3) to provide a structured process for achieving effective solutions. We present case studies of existing civic design initiatives and discuss challenges, limitations, and future work related to operationalizing, implementing, and testing this framework.},
journal = {Digit. Gov.: Res. Pract.},
month = dec,
articleno = {31},
numpages = {22},
keywords = {conceptual framework, design thinking, crowdsourcing, public participation, digital civics, Civic design}
}

@article{10.1109/TNET.2018.2812785,
author = {Ma, Qian and Gao, Lin and Liu, Ya-Feng and Huang, Jianwei},
title = {Incentivizing Wi-Fi Network Crowdsourcing: A Contract Theoretic Approach},
year = {2018},
issue_date = {June 2018},
publisher = {IEEE Press},
volume = {26},
number = {3},
issn = {1063-6692},
url = {https://doi.org/10.1109/TNET.2018.2812785},
doi = {10.1109/TNET.2018.2812785},
abstract = {Crowdsourced wireless community network enables the individual users to share their private Wi-Fi access points APs with each other, hence it can achieve a large Wi-Fi coverage with a small deployment cost via crowdsourcing. This paper presents a novel contract-based incentive framework to incentivize such a Wi-Fi network crowdsourcing under incomplete information where each user has certain private information such as mobility pattern and Wi-Fi access quality. In the proposed framework, the network operator designs and offers a set of contract items to users, each consisting of a Wi-Fi access price that a user can charge others for accessing his AP and a subscription fee that a user needs to pay the operator for joining the community. Different from the existing contracts in the literature, in our contract model, each user’s best choice depends not only on his private information but also on other user’s choices. This greatly complicates the contract design, as the operator needs to analyze the equilibrium choices of all users, rather than the best choice of each single user. We first derive the feasible contract that guarantees the user’s truthful information disclosure based on the equilibrium analysis of the user choice, and then derive the optimal and feasible contract that yields a maximal profit for the operator. Our analysis shows that a user who provides a higher Wi-Fi access quality is more likely to choose a higher Wi-Fi access price and subscription fee, regardless of the user mobility pattern. Simulation results further show that when increasing the average Wi-Fi access quality of users, the operator can gain more profit, but counter-intuitively offer lower Wi-Fi access prices and subscription fees for users.},
journal = {IEEE/ACM Trans. Netw.},
month = jun,
pages = {1035–1048},
numpages = {14}
}

@article{10.5555/3546258.3546448,
author = {Song, Changyue and Liu, Kaibo and Zhang, Xi},
title = {Collusion detection and ground truth inference in crowdsourcing for labeling tasks},
year = {2021},
issue_date = {January 2021},
publisher = {JMLR.org},
volume = {22},
number = {1},
issn = {1532-4435},
abstract = {Crowdsourcing has been a prompt and cost-effective way of obtaining labels in many machine learning applications. In the literature, a number of algorithms have been developed to infer the ground truth based on the collected labels. However, most existing studies assume workers to be independent and are vulnerable to worker collusion. This paper aims at detecting the collusive behaviors of workers in labeling tasks. Specifically, we consider collusion in a pairwise manner and propose a penalized pairwise profile likelihood method based on the adaptive LASSO penalty for collusion detection. Many models that describe the behavior of independent workers can be incorporated into our proposed framework as the baseline model. We further investigate the theoretical properties of the proposed method that guarantee the asymptotic performance. An algorithm based on expectation-maximization algorithm and coordinate descent is proposed to numerically maximize the penalized pairwise profile likelihood function for parameter estimation. To the best of our knowledge, this is the first statistical model that simultaneously detects collusion, learns workers' capabilities, and infers the ground true labels. Numerical studies using synthetic and real data sets are also conducted to verify the performance of the method.},
journal = {J. Mach. Learn. Res.},
month = jan,
articleno = {190},
numpages = {45},
keywords = {pairwise profile likelihood, collusion, crowdsourcing, adaptive LASSO}
}

@article{10.14778/3007263.3007323,
author = {Amer-Yahia, Sihem and Roy, Senjuti Basu},
title = {Human factors in crowdsourcing},
year = {2016},
issue_date = {September 2016},
publisher = {VLDB Endowment},
volume = {9},
number = {13},
issn = {2150-8097},
url = {https://doi.org/10.14778/3007263.3007323},
doi = {10.14778/3007263.3007323},
abstract = {Today, crowdsourcing is used to "taskify" any job ranging from simple receipt transcription to collaborative editing, fan-subbing, citizen science, and citizen journalism. The crowd is typically volatile, its arrival and departure asynchronous, and its levels of attention and accuracy diverse. Tasks vary in complexity and may necessitate the participation of workers with varying degrees of expertise. Sometimes, workers need to collaborate explicitly and build on each other's contributions to complete a single task. For example, in disaster reporting, CrowdMap allows geographically closed people with diverse and complementary skills, to work together to report details about the course of a typhoon or the aftermath of an earthquake.This uber-ization of human labor requires the understanding of workers motivation in completing a task, their ability to work together in collaborative tasks, as well as, helping workers find relevant tasks. For over 40 years, organization studies have thoroughly examined human factors that affect workers in physical workplaces. More recently, computer scientists have developed algorithms that verify and leverage those findings in a virtual marketplace, in this case, a crowdsourcing platform.The goal of this tutorial is to review those two areas and discuss how their combination may improve workers' experience, task throughput and outcome quality for both micro-tasks and collaborative tasks. We will start with a coverage of motivation theory, team formation, and learning worker profiles. We will then address open research questions that result from this review.},
journal = {Proc. VLDB Endow.},
month = sep,
pages = {1615–1618},
numpages = {4}
}

@article{10.14778/3007263.3007293,
author = {Ikeda, Kosetsu and Morishima, Atsuyuki and Rahman, Habibur and Roy, Senjuti Basu and Thirumuruganathan, Saravanan and Amer-Yahia, Sihem and Das, Gautam},
title = {Collaborative crowdsourcing with crowd4U},
year = {2016},
issue_date = {September 2016},
publisher = {VLDB Endowment},
volume = {9},
number = {13},
issn = {2150-8097},
url = {https://doi.org/10.14778/3007263.3007293},
doi = {10.14778/3007263.3007293},
abstract = {Collaborative crowdsourcing is an emerging paradigm where a set of workers, often with diverse and complementary skills, form groups and work together to complete complex tasks. While crowdsourcing has been used successfully in many applications, collaboration is essential for achieving a high quality outcome for a number of emerging applications such as text translation, citizen journalism and surveillance tasks. However, no crowdsourcing platform today enables the end-to-end deployment of collaborative tasks. We demonstrate Crowd4U, a volunteer-based system that enables the deployment of diverse crowdsourcing tasks with complex data-flows, in a declarative manner. In addition to treating workers and tasks as rich entities, Crowd4U also provides an easy-to-use form-based task UI. Crowd4U implements worker-to-task assignment algorithms that are appropriate for each kind of task. Once workers are assigned to tasks, appropriate worker collaboration schemes are enforced in order to enable effective result coordination.},
journal = {Proc. VLDB Endow.},
month = sep,
pages = {1497–1500},
numpages = {4}
}

@article{10.1145/3078842,
author = {Cheng, Shih-Fen and Chen, Cen and Kandappu, Thivya and Lau, Hoong Chuin and Misra, Archan and Jaiman, Nikita and Tandriansyah, Randy and Koh, Desmond},
title = {Scalable Urban Mobile Crowdsourcing: Handling Uncertainty in Worker Movement},
year = {2017},
issue_date = {May 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {9},
number = {3},
issn = {2157-6904},
url = {https://doi.org/10.1145/3078842},
doi = {10.1145/3078842},
abstract = {In this article, we investigate effective ways of utilizing crowdworkers in providing various urban services. The task recommendation platform that we design can match tasks to crowdworkers based on workers’ historical trajectories and time budget limits, thus making recommendations personal and efficient. One major challenge we manage to address is the handling of crowdworker’s trajectory uncertainties. In this article, we explicitly allow multiple routine routes to be probabilistically associated with each worker. We formulate this problem as an integer linear program whose goal is to maximize the expected total utility achieved by all workers. We further exploit the separable structures of the formulation and apply the Lagrangian relaxation technique to scale up computation. Numerical experiments have been performed over the instances generated using the realistic public transit dataset in Singapore. The results show that we can find significantly better solutions than the deterministic formulation, and in most cases we can find solutions that are very close to the theoretical performance limit. To demonstrate the practicality of our approach, we deployed our recommendation engine to a campus-scale field trial, and we demonstrate that workers receiving our recommendations incur fewer detours and complete more tasks, and are more efficient against workers relying on their own planning (25\% more for top workers who receive recommendations). This is achieved despite having highly uncertain worker trajectories. We also demonstrate how to further improve the robustness of the system by using a simple multi-coverage mechanism.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = dec,
articleno = {26},
numpages = {24},
keywords = {user behavior, uncertainty modeling, spatial crowdsourcing, participatory sensing, empirical study, context-aware, Mobile crowdsourcing}
}

@article{10.1145/3403931,
author = {Maddalena, Eddy and Ib\'{a}\~{n}ez, Luis-Daniel and Simperl, Elena},
title = {Mapping Points of Interest Through Street View Imagery and Paid Crowdsourcing},
year = {2020},
issue_date = {October 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {11},
number = {5},
issn = {2157-6904},
url = {https://doi.org/10.1145/3403931},
doi = {10.1145/3403931},
abstract = {We present the Virtual City Explorer (VCE), an online crowdsourcing platform for the collection of rich geotagged information in urban environments. Compared to other volunteered geographic information approaches, which are constrained by the number and availability of mapping enthusiasts on the ground, the VCE uses digital street imagery to allow people to virtually explore a city from anywhere in the world, using a browser or a mobile phone. In addition, contributions in VCE are designed as paid microtasks—small jobs that can be carried out without any specific knowledge of the local area or previous mapping expertise in exchange for a fee. We tested the VCE in two cities to map points of interest (PoIs) in transport and mobility, using FigureEight to recruit participants. We were able to show that our platform enables crowdworkers to submit PoI location seamlessly, cover almost all of the tested areas, and discover several PoIs not reported by other approaches. This allows the VCE to complement existing approaches that leverage experts or grassroot communities.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = aug,
articleno = {63},
numpages = {28},
keywords = {urban auditing, microtasks, mapping, geospatial information, geographic information, Crowdsourcing}
}

@article{10.1109/TNET.2020.3018448,
author = {Xia, Hui and Zhang, Rui and Cheng, Xiangguo and Qiu, Tie and Wu, Dapeng Oliver},
title = {Two-Stage Game Design of Payoff Decision-Making Scheme for Crowdsourcing Dilemmas},
year = {2020},
issue_date = {Dec. 2020},
publisher = {IEEE Press},
volume = {28},
number = {6},
issn = {1063-6692},
url = {https://doi.org/10.1109/TNET.2020.3018448},
doi = {10.1109/TNET.2020.3018448},
abstract = {Crowdsourcing uses collective intelligence to finish complicated tasks and is widely applied in many fields. However, the crowdsourcing dilemmas between the task requester and the task completer restrict the efficiency of system severely, e.g., the cooperation dilemma leads to the failure in the interactions and the quality of service dilemma results in the inability of task completer to provide high-quality service. Current research usually focuses on solving only one aforementioned dilemma and fails to integrate perfectly with the service architectural pattern of crowdsourcing systems. In this article, combined with the crowdsourcing interaction phase, we limit the objects that cause dilemma and propose a &lt;inline-formula&gt; &lt;tex-math notation="LaTeX"&gt;$boldsymbol {t}$ &lt;/tex-math&gt;&lt;/inline-formula&gt;wo-stage &lt;inline-formula&gt; &lt;tex-math notation="LaTeX"&gt;$boldsymbol {g}$ &lt;/tex-math&gt;&lt;/inline-formula&gt;ame &lt;inline-formula&gt; &lt;tex-math notation="LaTeX"&gt;$boldsymbol {p}$ &lt;/tex-math&gt;&lt;/inline-formula&gt;ayoff &lt;inline-formula&gt; &lt;tex-math notation="LaTeX"&gt;$boldsymbol {d}$ &lt;/tex-math&gt;&lt;/inline-formula&gt;ecision-making scheme (&lt;italic&gt;TGPD&lt;/italic&gt;) to overcome these shortcomings. To solve the cooperation dilemma between the requester and the crowdsourcing platform, we first propose a dynamic payment method based on the reputation-quality rules for the task requester, and then develop a cos-evaluation algorithm to estimate platform’s cost, last design a co-determine algorithm to determine whether the platform adopts a cooperative strategy. To address the quality of service dilemma between the crowdsourcing platform and the workers, we first present an auction-screening method to estimate the reasonable recruitment range of workers which can be optimized by the result of cos-evaluation algorithm, and then use a reward distribution method to motivate workers to complete tasks with high quality and on time. The experimental results indicate that our new scheme successfully increases the worker’s and platforms’ payoffs at the same time, improves the accuracy of screening workers, enhances the worker’s quality of service, and decreases the platform’s cost.},
journal = {IEEE/ACM Trans. Netw.},
month = dec,
pages = {2741–2754},
numpages = {14}
}

@article{10.1145/3391707,
author = {Huang, Yapei and Tian, Yun and Liu, Zhijie and Jin, Xiaowei and Liu, Yanan and Zhao, Shifeng and Tian, Daxin},
title = {A Traffic Density Estimation Model Based on Crowdsourcing Privacy Protection},
year = {2020},
issue_date = {August 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {11},
number = {4},
issn = {2157-6904},
url = {https://doi.org/10.1145/3391707},
doi = {10.1145/3391707},
abstract = {Acquiring traffic condition information is of great significance in transportation guidance, urban planning, and route recommendation. To date, traffic density data are generally acquired by road sound analysis, video data analysis, or in-vehicle network communication, which are usually financially or temporally expensive. Another way to get traffic conditions is to collect track data by crowdsourcing. However, this way lead to a greater risk of leaking users’ privacy. To avoid the risk, this article proposes a traffic density estimation model based on crowdsourcing privacy protection. First, in the acquisition process of the track data by crowdsourcing, dual servers are employed for transmission, and homomorphic encryption is carried out to encrypt the data to protect the data from being leaked during transmission. Second, sampling is implemented for randomization and anonymization to reduce the spatial continuity and temporal continuity of position data. In this way, the intermediate server cannot acquire users’ original data, and the main server cannot obtain users’ personal information. Finally, before data transmission, Laplace noising is performed on the users’ local position data to further protect the original location information. The proposed algorithm in this study realizes that only users have their original track data, and the servers involved in the work cannot infer the original track data, which ensures the real security of user privacy. The proposed algorithm was verified with the track data from the Didi Gaia Data Opening Plan. The experimental results showed that the proposed algorithm could still maintain the validity of data analysis results and the security of user data privacy after homomorphic encryption, noise addition, and sample collection, and displayed good robustness and scalability.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = may,
articleno = {46},
numpages = {18},
keywords = {traffic flow, sample, encryption, crowdsourcing, Differential privacy}
}

@article{10.1145/3347514,
author = {Chen, Yanjiao and Wang, Xu and Li, Baochun and Zhang, Qian},
title = {An Incentive Mechanism for Crowdsourcing Systems with Network Effects},
year = {2019},
issue_date = {November 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {19},
number = {4},
issn = {1533-5399},
url = {https://doi.org/10.1145/3347514},
doi = {10.1145/3347514},
abstract = {In a crowdsourcing system, it is important for the crowdsourcer to engineer extrinsic rewards to incentivize the participants. With mobile social networking, a user enjoys an intrinsic benefit when she aligns her behavior with the behavior of others. Referred to as network effects, such an intrinsic benefit becomes more significant as more users join and contribute to the crowdsourcing system. But should a crowdsourcer design her extrinsic rewards differently when such network effects are taken into consideration? In this article, we incorporate network effects as a contributing factor to intrinsic rewards, and study its influence on the design of extrinsic rewards. We show that the number of participating users and their contributions to the crowdsourcing system evolve to a steady equilibrium, thanks to subtle interactions between intrinsic rewards due to network effects and extrinsic rewards offered by the crowdsourcer. Taken network effects into consideration, we design progressively more sophisticated extrinsic reward mechanisms, and propose new and optimal strategies for a crowdsourcer to obtain a higher utility. Through simulations and examples, we demonstrate that with our new strategies, a crowdsourcer is able to attract more participants with higher contributed efforts; and the participants gain higher utilities from both intrinsic and extrinsic rewards.},
journal = {ACM Trans. Internet Technol.},
month = sep,
articleno = {49},
numpages = {21},
keywords = {network effects, intrinsic rewards, incentive mechanism, Crowdsourcing}
}

@article{10.1145/3359209,
author = {Venkatagiri, Sukrit and Thebault-Spieker, Jacob and Kohler, Rachel and Purviance, John and Mansur, Rifat Sabbir and Luther, Kurt},
title = {GroundTruth: Augmenting Expert Image Geolocation with Crowdsourcing and Shared Representations},
year = {2019},
issue_date = {November 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {3},
number = {CSCW},
url = {https://doi.org/10.1145/3359209},
doi = {10.1145/3359209},
abstract = {Expert investigators bring advanced skills and deep experience to analyze visual evidence, but they face limits on their time and attention. In contrast, crowds of novices can be highly scalable and parallelizable, but lack expertise. In this paper, we introduce the concept of shared representations for crowd--augmented expert work, focusing on the complex sensemaking task of image geolocation performed by professional journalists and human rights investigators. We built GroundTruth, an online system that uses three shared representations-a diagram, grid, and heatmap-to allow experts to work with crowds in real time to geolocate images. Our mixed-methods evaluation with 11 experts and 567 crowd workers found that GroundTruth helped experts geolocate images, and revealed challenges and success strategies for expert-crowd interaction. We also discuss designing shared representations for visual search, sensemaking, and beyond.},
journal = {Proc. ACM Hum.-Comput. Interact.},
month = nov,
articleno = {107},
numpages = {30},
keywords = {visual search, verification, shared representations, sensemaking, real-time crowdsourcing, misinformation, journalism, investigation, geolocation, expert, crowdsourcing, crowd-augmented expert work, crowd}
}

@article{10.1145/3479531,
author = {Ram\'{\i}rez, Jorge and Sayin, Burcu and Baez, Marcos and Casati, Fabio and Cernuzzi, Luca and Benatallah, Boualem and Demartini, Gianluca},
title = {On the State of Reporting in Crowdsourcing Experiments and a Checklist to Aid Current Practices},
year = {2021},
issue_date = {October 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {5},
number = {CSCW2},
url = {https://doi.org/10.1145/3479531},
doi = {10.1145/3479531},
abstract = {Crowdsourcing is being increasingly adopted as a platform to run studies with human subjects. Running a crowdsourcing experiment involves several choices and strategies to successfully port an experimental design into an otherwise uncontrolled research environment, e.g., sampling crowd workers, mapping experimental conditions to micro-tasks, or ensure quality contributions. While several guidelines inform researchers in these choices, guidance of how and what to report from crowdsourcing experiments has been largely overlooked. If under-reported, implementation choices constitute variability sources that can affect the experiment's reproducibility and prevent a fair assessment of research outcomes. In this paper, we examine the current state of reporting of crowdsourcing experiments and offer guidance to address associated reporting issues. We start by identifying sensible implementation choices, relying on existing literature and interviews with experts, to then extensively analyze the reporting of 171 crowdsourcing experiments. Informed by this process, we propose a checklist for reporting crowdsourcing experiments.},
journal = {Proc. ACM Hum.-Comput. Interact.},
month = oct,
articleno = {387},
numpages = {34},
keywords = {crowdsourcing, crowdsourcing experiments, reporting, reproducibility}
}

@article{10.1145/3078852,
author = {Feyisetan, Oluwaseyi and Simperl, Elena},
title = {Social Incentives in Paid Collaborative Crowdsourcing},
year = {2017},
issue_date = {November 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {8},
number = {6},
issn = {2157-6904},
url = {https://doi.org/10.1145/3078852},
doi = {10.1145/3078852},
abstract = {Paid microtask crowdsourcing has traditionally been approached as an individual activity, with units of work created and completed independently by the members of the crowd. Other forms of crowdsourcing have, however, embraced more varied models, which allow for a greater level of participant interaction and collaboration. This article studies the feasibility and uptake of such an approach in the context of paid microtasks. Specifically, we compare engagement, task output, and task accuracy in a paired-worker model with the traditional, single-worker version. Our experiments indicate that collaboration leads to better accuracy and more output, which, in turn, translates into lower costs. We then explore the role of the social flow and social pressure generated by collaborating partners as sources of incentives for improved performance. We utilise a Bayesian method in conjunction with interface interaction behaviours to detect when one of the workers in a pair tries to exit the task. Upon this realisation, the other worker is presented with the opportunity to contact the exiting partner to stay: either for personal financial reasons (i.e., they have not completed enough tasks to qualify for a payment) or for fun (i.e., they are enjoying the task). The findings reveal that: (1) these socially motivated incentives can act as furtherance mechanisms to help workers attain and exceed their task requirements and produce better results than baseline collaborations; (2) microtask crowd workers are empathic (as opposed to selfish) agents, willing to go the extra mile to help their partners get paid; and, (3) social furtherance incentives create a win-win scenario for the requester and for the workers by helping more workers get paid by re-engaging them before they drop out.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = jul,
articleno = {73},
numpages = {31},
keywords = {social pressure, social flow, incentives, Crowdsourcing}
}

@article{10.1145/3152889,
author = {Dumitrache, Anca and Aroyo, Lora and Welty, Chris},
title = {Crowdsourcing Ground Truth for Medical Relation Extraction},
year = {2018},
issue_date = {June 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {8},
number = {2},
issn = {2160-6455},
url = {https://doi.org/10.1145/3152889},
doi = {10.1145/3152889},
abstract = {Cognitive computing systems require human labeled data for evaluation and often for training. The standard practice used in gathering this data minimizes disagreement between annotators, and we have found this results in data that fails to account for the ambiguity inherent in language. We have proposed the CrowdTruth method for collecting ground truth through crowdsourcing, which reconsiders the role of people in machine learning based on the observation that disagreement between annotators provides a useful signal for phenomena such as ambiguity in the text. We report on using this method to build an annotated data set for medical relation extraction for the cause and treat relations, and how this data performed in a supervised training experiment. We demonstrate that by modeling ambiguity, labeled data gathered from crowd workers can (1) reach the level of quality of domain experts for this task while reducing the cost, and (2) provide better training data at scale than distant supervision. We further propose and validate new weighted measures for precision, recall, and F-measure, which account for ambiguity in both human and machine performance on this task.},
journal = {ACM Trans. Interact. Intell. Syst.},
month = jul,
articleno = {11},
numpages = {20},
keywords = {relation extraction, natural language ambiguity, inter-annotator disagreement, crowdtruth, crowd truth, clinical natural language processing, Ground truth}
}

@article{10.1145/3359130,
author = {van Berkel, Niels and Goncalves, Jorge and Hettiachchi, Danula and Wijenayake, Senuri and Kelly, Ryan M. and Kostakos, Vassilis},
title = {Crowdsourcing Perceptions of Fair Predictors for Machine Learning: A Recidivism Case Study},
year = {2019},
issue_date = {November 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {3},
number = {CSCW},
url = {https://doi.org/10.1145/3359130},
doi = {10.1145/3359130},
abstract = {The increased reliance on algorithmic decision-making in socially impactful processes has intensified the calls for algorithms that are unbiased and procedurally fair. Identifying fair predictors is an essential step in the construction of equitable algorithms, but the lack of ground-truth in fair predictor selection makes this a challenging task. In our study, we recruit 90 crowdworkers to judge the inclusion of various predictors for recidivism. We divide participants across three conditions with varying group composition. Our results show that participants were able to make informed decisions on predictor selection. We find that agreement with the majority vote is higher when participants are part of a more diverse group. The presented workflow, which provides a scalable and practical approach to reach a diverse audience, allows researchers to capture participants' perceptions of fairness in private while simultaneously allowing for structured participant discussion.},
journal = {Proc. ACM Hum.-Comput. Interact.},
month = nov,
articleno = {28},
numpages = {21},
keywords = {perceived fairness, modelling bias, intelligible models, fairness, crowdsourcing, crime, chatbots, bias, artificial intelligence, algorithmic decision making}
}

@article{10.1049/iet-com.2018.5356,
author = {Hou, Jian and Luo, Shuyun and Xu, Weiqiang and Wang, Lili},
title = {Fairness‐based multi‐task reward allocation in mobile crowdsourcing system},
year = {2019},
issue_date = {October 2019},
publisher = {John Wiley \&amp; Sons, Inc.},
address = {USA},
volume = {13},
number = {16},
url = {https://doi.org/10.1049/iet-com.2018.5356},
doi = {10.1049/iet-com.2018.5356},
abstract = {Mobile crowdsourcing‐based applications, widely popular, exploit the sensing data crowdsourced from smartphone users without putting any burden on the extra cost of data sensing and collection. However, user participation in crowdsourcing incurs resource cost, such as battery, bandwidth, thus it is critical to design incentive mechanisms for propelling user's participation. Previous diverse incentive mechanisms designed for crowdsourcing applications only focus on users' contribution for reward allocation, while ignore another important property, i.e. fairness, users' reward should be corresponding with their cost. In this study, the authors first introduce a new concept called rate of return (RoR), defined as the ratio of received reward and incurred cost for each user, to demonstrate the property of fairness. With the goal of guarantee, the fairness of reward allocation for each user in a multiple‐task system, three algorithms, consensus‐based reward allocation, consensus‐based balanced topology reward allocation and Gossip‐based reward allocation are proposed for the demands of various scenarios, in which the RoR values are synchronised by optimising the fairness function in either centralised or decentralised manner. Through rigorous theoretical analysis and extensive simulations, it is finally demonstrated that the proposed reward allocation algorithms have the good property of fairness with quick convergence.},
journal = {IET Communications},
month = oct,
pages = {2506–2511},
numpages = {6},
keywords = {diverse incentive mechanisms, resource cost, user participation, data sensing, smartphone users, sensing data, mobile crowdsourcing‐based applications, mobile crowdsourcing system, fairness‐based multitask reward allocation, reward allocation algorithms, fairness function, Gossip‐based reward allocation, consensus‐based balanced topology reward allocation, consensus‐based reward allocation, multiple‐task system, received reward, optimisation, resource allocation, incentive schemes, mobile computing, smart phones}
}

@article{10.1145/3479572,
author = {Fogliato, Riccardo and Chouldechova, Alexandra and Lipton, Zachary},
title = {The Impact of Algorithmic Risk Assessments on Human Predictions and its Analysis via Crowdsourcing Studies},
year = {2021},
issue_date = {October 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {5},
number = {CSCW2},
url = {https://doi.org/10.1145/3479572},
doi = {10.1145/3479572},
abstract = {As algorithmic risk assessment instruments (RAIs) are increasingly adopted to assist decision makers, their predictive performance and potential to promote inequity have come under scrutiny. However, while most studies examine these tools in isolation, researchers have come to recognize that assessing their impact requires understanding the behavior of their human interactants. In this paper, building off of several recent crowdsourcing works focused on criminal justice, we conduct a vignette study in which laypersons are tasked with predicting future re-arrests. Our key findings are as follows: (1) Participants often predict that an offender will be rearrested even when they deem the likelihood of re-arrest to be well below 50\%; (2) Participants do not anchor on the RAI's predictions; (3) The time spent on the survey varies widely across participants and most cases are assessed in less than 10 seconds; (4) Judicial decisions, unlike participants' predictions, depend in part on factors that are orthogonal to the likelihood of re-arrest. These results highlight the influence of several crucial but often overlooked design decisions and concerns around generalizability when constructing crowdsourcing studies to analyze the impacts of RAI},
journal = {Proc. ACM Hum.-Comput. Interact.},
month = oct,
articleno = {428},
numpages = {24},
keywords = {algorithm-assisted decision-making, algorithmic risk assessment instruments, human in-the-loop, user study}
}

@article{10.14778/3055540.3055547,
author = {Zheng, Yudian and Li, Guoliang and Li, Yuanbing and Shan, Caihua and Cheng, Reynold},
title = {Truth inference in crowdsourcing: is the problem solved?},
year = {2017},
issue_date = {January 2017},
publisher = {VLDB Endowment},
volume = {10},
number = {5},
issn = {2150-8097},
url = {https://doi.org/10.14778/3055540.3055547},
doi = {10.14778/3055540.3055547},
abstract = {Crowdsourcing has emerged as a novel problem-solving paradigm, which facilitates addressing problems that are hard for computers, e.g., entity resolution and sentiment analysis. However, due to the openness of crowdsourcing, workers may yield low-quality answers, and a redundancy-based method is widely employed, which first assigns each task to multiple workers and then infers the correct answer (called truth) for the task based on the answers of the assigned workers. A fundamental problem in this method is Truth Inference, which decides how to effectively infer the truth. Recently, the database community and data mining community independently study this problem and propose various algorithms. However, these algorithms are not compared extensively under the same framework and it is hard for practitioners to select appropriate algorithms. To alleviate this problem, we provide a detailed survey on 17 existing algorithms and perform a comprehensive evaluation using 5 real datasets. We make all codes and datasets public for future research. Through experiments we find that existing algorithms are not stable across different datasets and there is no algorithm that outperforms others consistently. We believe that the truth inference problem is not fully solved, and identify the limitations of existing algorithms and point out promising research directions.},
journal = {Proc. VLDB Endow.},
month = jan,
pages = {541–552},
numpages = {12}
}

@article{10.14778/3275536.3275541,
author = {Yang, Jingru and Fan, Ju and Wei, Zhewei and Li, Guoliang and Liu, Tongyu and Du, Xiaoyong},
title = {Cost-effective data annotation using game-based crowdsourcing},
year = {2018},
issue_date = {September 2018},
publisher = {VLDB Endowment},
volume = {12},
number = {1},
issn = {2150-8097},
url = {https://doi.org/10.14778/3275536.3275541},
doi = {10.14778/3275536.3275541},
abstract = {Large-scale data annotation is indispensable for many applications, such as machine learning and data integration. However, existing annotation solutions either incur expensive cost for large datasets or produce noisy results. This paper introduces a cost-effective annotation approach, and focuses on the labeling rule generation problem that aims to generate high-quality rules to largely reduce the labeling cost while preserving quality. To address the problem, we first generate candidate rules, and then devise a game-based crowdsourcing approach CROWDGAME to select high-quality rules by considering coverage and precision. CROWDGAME employs two groups of crowd workers: one group answers rule validation tasks (whether a rule is valid) to play a role of rule generator, while the other group answers tuple checking tasks (whether the annotated label of a data tuple is correct) to play a role of rule refuter. We let the two groups play a two-player game: rule generator identifies high-quality rules with large coverage and precision, while rule refuter tries to refute its opponent rule generator by checking some tuples that provide enough evidence to reject rules covering the tuples. This paper studies the challenges in CROWDGAME. The first is to balance the trade-off between coverage and precision. We define the loss of a rule by considering the two factors. The second is rule precision estimation. We utilize Bayesian estimation to combine both rule validation and tuple checking tasks. The third is to select crowdsourcing tasks to fulfill the game-based framework for minimizing the loss. We introduce a minimax strategy and develop efficient task selection algorithms. We conduct experiments on entity matching and relation extraction, and the results show that our method outperforms state-of-the-art solutions.},
journal = {Proc. VLDB Endow.},
month = sep,
pages = {57–70},
numpages = {14}
}

@article{10.1145/3148148,
author = {Daniel, Florian and Kucherbaev, Pavel and Cappiello, Cinzia and Benatallah, Boualem and Allahbakhsh, Mohammad},
title = {Quality Control in Crowdsourcing: A Survey of Quality Attributes, Assessment Techniques, and Assurance Actions},
year = {2018},
issue_date = {January 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {51},
number = {1},
issn = {0360-0300},
url = {https://doi.org/10.1145/3148148},
doi = {10.1145/3148148},
abstract = {Crowdsourcing enables one to leverage on the intelligence and wisdom of potentially large groups of individuals toward solving problems. Common problems approached with crowdsourcing are labeling images, translating or transcribing text, providing opinions or ideas, and similar—all tasks that computers are not good at or where they may even fail altogether. The introduction of humans into computations and/or everyday work, however, also poses critical, novel challenges in terms of quality control, as the crowd is typically composed of people with unknown and very diverse abilities, skills, interests, personal objectives, and technological resources. This survey studies quality in the context of crowdsourcing along several dimensions, so as to define and characterize it and to understand the current state of the art. Specifically, this survey derives a quality model for crowdsourcing tasks, identifies the methods and techniques that can be used to assess the attributes of the model, and the actions and strategies that help prevent and mitigate quality problems. An analysis of how these features are supported by the state of the art further identifies open issues and informs an outlook on hot future research directions.},
journal = {ACM Comput. Surv.},
month = jan,
articleno = {7},
numpages = {40},
keywords = {quality model, attributes, assurance, assessment, Crowdsourcing}
}

@article{10.5555/2789272.2886801,
author = {Moreno, Pablo G. and Art\'{e}s-Rodr\'{\i}guez, Antonio and Teh, Yee Whye and Perez-Cruz, Fernando},
title = {Bayesian nonparametric crowdsourcing},
year = {2015},
issue_date = {January 2015},
publisher = {JMLR.org},
volume = {16},
number = {1},
issn = {1532-4435},
abstract = {Crowdsourcing has been proven to be an effective and efficient tool to annotate large data-sets. User annotations are often noisy, so methods to combine the annotations to produce reliable estimates of the ground truth are necessary. We claim that considering the existence of clusters of users in this combination step can improve the performance. This is especially important in early stages of crowdsourcing implementations, where the number of annotations is low. At this stage there is not enough information to accurately estimate the bias introduced by each annotator separately, so we have to resort to models that consider the statistical links among them. In addition, finding these clusters is interesting in itself as knowing the behavior of the pool of annotators allows implementing efficient active learning strategies. Based on this, we propose in this paper two new fully unsupervised models based on a Chinese restaurant process (CRP) prior and a hierarchical structure that allows inferring these groups jointly with the ground truth and the properties of the users. Efficient inference algorithms based on Gibbs sampling with auxiliary variables are proposed. Finally, we perform experiments, both on synthetic and real databases, to show the advantages of our models over state-of-the-art algorithms.},
journal = {J. Mach. Learn. Res.},
month = jan,
pages = {1607–1627},
numpages = {21},
keywords = {multiple annotators, hierarchical clustering, Gibbs sampling, Dirichlet process, Bayesian nonparametrics}
}

@article{10.1109/TASLP.2016.2634123,
author = {Granell, Emilio and Martinez-Hinarejos, Carlos-D. and Granell, Emilio and Martinez-Hinarejos, Carlos-D},
title = {Multimodal Crowdsourcing for Transcribing Handwritten Documents},
year = {2017},
issue_date = {February 2017},
publisher = {IEEE Press},
volume = {25},
number = {2},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2016.2634123},
doi = {10.1109/TASLP.2016.2634123},
abstract = {Transcription of handwritten documents is an important research topic for multiple applications, such as document classification or information extraction. In the case of historical documents, their transcription allows to preserve cultural heritage because of the amount of historical data contained in those documents. The transcription process can employ state-of-the-art handwritten text recognition systems in order to obtain an initial transcription. This transcription is usually not good enough for the quality standards, but that may speed up the final transcription of the expert. In this framework, the use of collaborative transcription applications crowdsourcing has risen in the recent years, but these platforms are mainly limited by the use of non-mobile devices. Thus, the recruiting initiatives get reduced to a smaller set of potential volunteers. In this paper, an alternative that allows the use of mobile devices is presented. The proposal consists of using speech dictation of handwritten text lines. Then, by using multimodal combination of speech and handwritten text images, a draft transcription can be obtained, presenting more quality than that obtained by only using handwritten text recognition. The speech dictation platform is implemented as a mobile device application, which allows for a wider range of population for recruiting volunteers. A real acquisition on the contents of a Spanish historical handwritten book was obtained with the platform. This data was used to perform experiments on the behaviour of the proposed framework. Some experiments were performed to study how to optimise the collaborators effort in terms of number of collaborations, including how many lines and which lines should be selected for the speech dictation.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = feb,
pages = {409–419},
numpages = {11}
}

@article{10.1145/3555595,
author = {Huang, Keman and Zhou, Jilei and Chen, Shao},
title = {Being a Solo Endeavor or Team Worker in Crowdsourcing Contests? It is a Long-term Decision You Need to Make},
year = {2022},
issue_date = {November 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {6},
number = {CSCW2},
url = {https://doi.org/10.1145/3555595},
doi = {10.1145/3555595},
abstract = {Workers in crowdsourcing are evolving from one-off, independent micro-workers to on-demand collaborators with a long-term orientation. They were expected to collaborate as transient teams to solve more complex, non-trivial tasks. However, collaboration as a team may not be as prevalent as possible, given the lack of support for synchronous collaboration and the "competition, collaboration but transient" nature of crowdsourcing. Aiming at unfolding how individuals collaborate as a transient team and how such teamwork can affect an individual's long-term success, this study investigates the individuals' collaborations on Kaggle, a crowdsourcing contest platform for data analysis. The analysis reveals a growing trend of collaborating as a transient team, which is influenced by contest designs like complexity and reward. However, compared with working independently, the surplus of teamwork in a contest varies over time. Furthermore, the teamwork experience is beneficial for individuals in the short term and long term. Our study distinguishes the team-related intellectual capital and solo-related intellectual capital, and finds a path dependency effect for the individual to work solely or collectively. These findings allow us to contribute insights into the collaborative strategies for crowd workers, contest designers, and platform operators like Kaggle.},
journal = {Proc. ACM Hum.-Comput. Interact.},
month = nov,
articleno = {494},
numpages = {32},
keywords = {transient team, team worker, solo endeavor, long-term aspect, crowdsourcing contests, crowdsourcing, Kaggle}
}

@article{10.1145/3460865,
author = {Wu, Hanlu and Ma, Tengfei and Wu, Lingfei and Xu, Fangli and Ji, Shouling},
title = {Exploiting Heterogeneous Graph Neural Networks with Latent Worker/Task Correlation Information for Label Aggregation in Crowdsourcing},
year = {2021},
issue_date = {April 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {16},
number = {2},
issn = {1556-4681},
url = {https://doi.org/10.1145/3460865},
doi = {10.1145/3460865},
abstract = {Crowdsourcing has attracted much attention for its convenience to collect labels from non-expert workers instead of experts. However, due to the high level of noise from the non-experts, a label aggregation model that infers the true label from noisy crowdsourced labels is required. In this article, we propose a novel framework based on graph neural networks for aggregating crowd labels. We construct a heterogeneous graph between workers and tasks and derive a new graph neural network to learn the representations of nodes and the true labels. Besides, we exploit the unknown latent interaction between the same type of nodes (workers or tasks) by adding a homogeneous attention layer in the graph neural networks. Experimental results on 13 real-world datasets show superior performance over state-of-the-art models.},
journal = {ACM Trans. Knowl. Discov. Data},
month = sep,
articleno = {27},
numpages = {18},
keywords = {label aggregation, graph neural network, Crowdsourcing}
}

@article{10.14778/3236187.3236196,
author = {Cheng, Peng and Jian, Xun and Chen, Lei},
title = {An experimental evaluation of task assignment in spatial crowdsourcing},
year = {2018},
issue_date = {July 2018},
publisher = {VLDB Endowment},
volume = {11},
number = {11},
issn = {2150-8097},
url = {https://doi.org/10.14778/3236187.3236196},
doi = {10.14778/3236187.3236196},
abstract = {Recently, with the rapid development of mobile devices and the crowdsourcing platforms, the spatial crowdsourcing has attracted much attention from the database community. Specifically, spatial crowdsourcing refers to sending a location-based request to workers according to their positions, and workers need to physically move to specified locations to conduct tasks. Many works have studied task assignment problems in spatial crowdsourcing, however, their problem settings are different from each other. Thus, it is hard to compare the performances of existing algorithms on task assignment in spatial crowdsourcing. In this paper, we present a comprehensive experimental comparison of most existing algorithms on task assignment in spatial crowdsourcing. Specifically, we first give general definitions about spatial workers and spatial tasks based on definitions in the existing works such that the existing algorithms can be applied on the same synthetic and real data sets. Then, we provide a uniform implementation for all the tested algorithms of task assignment problems in spatial crowdsourcing (open sourced). Finally, based on the results on both synthetic and real data sets, we discuss the strengths and weaknesses of tested algorithms, which can guide future research on the same area and practical implementations of spatial crowdsourcing systems.},
journal = {Proc. VLDB Endow.},
month = jul,
pages = {1428–1440},
numpages = {13}
}

@article{10.1145/3226028,
author = {Jiang, Jiuchuan and An, Bo and Jiang, Yichuan and Lin, Donghui and Bu, Zhan and Cao, Jie and Hao, Zhifeng},
title = {Understanding Crowdsourcing Systems from a Multiagent Perspective and Approach},
year = {2018},
issue_date = {June 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {13},
number = {2},
issn = {1556-4665},
url = {https://doi.org/10.1145/3226028},
doi = {10.1145/3226028},
abstract = {Crowdsourcing has recently been significantly explored. Although related surveys have been conducted regarding this subject, each has mainly consisted of a review of a single aspect of crowdsourcing systems or on the application of crowdsourcing in a specific application domain. A crowdsourcing system is a comprehensive set of multiple entities, including various elements and processes. Multiagent computing has already been widely envisioned as a powerful paradigm for modeling autonomous multi-entity systems with adaptation to dynamic environments. Therefore, this article presents a novel multiagent perspective and approach to understanding crowdsourcing systems, which can be used to correlate the research on crowdsourcing and multiagent systems and inspire possible interdisciplinary research between the two areas. This article mainly discusses the following two aspects: (1) The multiagent perspective can be used for conducting a comprehensive survey on the state of the art of crowdsourcing, and (2) the multiagent approach can bring about concrete enhancements for crowdsourcing technology and inspire future research directions that enable crowdsourcing research to overcome the typical challenges in crowdsourcing technology. Finally, this article discusses the advantages and disadvantages of the multiagent perspective by comparing it with two other popular perspectives on crowdsourcing: the business perspective and the technical perspective.},
journal = {ACM Trans. Auton. Adapt. Syst.},
month = jul,
articleno = {8},
numpages = {32},
keywords = {multiagent systems, crowdsourcing processes, crowdsourcing elements, Crowdsourcing systems}
}

@article{10.1145/2928269,
author = {Gould, Sandy J. J. and Cox, Anna L. and Brumby, Duncan P.},
title = {Diminished Control in Crowdsourcing: An Investigation of Crowdworker Multitasking Behavior},
year = {2016},
issue_date = {July 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {23},
number = {3},
issn = {1073-0516},
url = {https://doi.org/10.1145/2928269},
doi = {10.1145/2928269},
abstract = {Obtaining high-quality data from crowds can be difficult if contributors do not give tasks sufficient attention. Attention checks are often used to mitigate this problem, but, because the roots of inattention are poorly understood, checks often compel attentive contributors to complete unnecessary work. We investigated a potential source of inattentiveness during crowdwork: multitasking. We found that workers switched to other tasks every 5 minutes, on average. There were indications that increasing switch frequency negatively affected performance. To address this, we tested an intervention that encouraged workers to stay focused on our task after multitasking was detected. We found that our intervention reduced the frequency of task switching. It also improves on existing attention checks because it does not place additional demands on workers who are already focused. Our approach shows that crowds can help to overcome some of the limitations of laboratory studies by affording access to naturalistic multitasking behavior.},
journal = {ACM Trans. Comput.-Hum. Interact.},
month = jun,
articleno = {19},
numpages = {29},
keywords = {transcription, online experimentation, multitasking, methodology, human performance, data entry, cuing, crowdsourcing, Interruptions}
}

@article{10.1145/3274432,
author = {Song, Shuyi and Bu, Jiajun and Artmeier, Andreas and Shi, Keyue and Wang, Ye and Yu, Zhi and Wang, Can},
title = {Crowdsourcing-Based Web Accessibility Evaluation with Golden Maximum Likelihood Inference},
year = {2018},
issue_date = {November 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {2},
number = {CSCW},
url = {https://doi.org/10.1145/3274432},
doi = {10.1145/3274432},
abstract = {Web accessibility evaluation examines how well websites comply with accessibility guidelines which help people with disabilities to perceive, navigate and contribute to the Web. This demanding task usually requires manual assessment by experts with many years of training and experience. However, not enough experts are available to carry out the increasing number of evaluation projects while non-experts often have different opinions about the presence of accessibility barriers. Addressing these issues, we introduce a crowdsourcing system with a novel truth inference algorithm to derive reliable and accurate assessments from conflicting opinions of evaluators. Extensive evaluation on 23,901 complex tasks assessed by 50 people with and without disabilities shows that our approach outperforms state of the art approaches. In addition, we conducted surveys to identify frequent barriers that people with disabilities are facing in their daily lives and the difficulty to access Web pages when they encounter these barriers. The frequencies and severities of barriers correlate with their derived importance in our evaluation project.},
journal = {Proc. ACM Hum.-Comput. Interact.},
month = nov,
articleno = {163},
numpages = {21},
keywords = {web accessibility, user experience, evaluation system, disability, crowdsourcing, collaborative work}
}

@article{10.1145/2897369,
author = {Katsimerou, Christina and Albeda, Joris and Huldtgren, Alina and Heynderickx, Ingrid and Redi, Judith A.},
title = {Crowdsourcing Empathetic Intelligence: The Case of the Annotation of EMMA Database for Emotion and Mood Recognition},
year = {2016},
issue_date = {July 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {4},
issn = {2157-6904},
url = {https://doi.org/10.1145/2897369},
doi = {10.1145/2897369},
abstract = {Unobtrusive recognition of the user's mood is an essential capability for affect-adaptive systems. Mood is a subtle, long-term affective state, often misrecognized even by humans. The challenge to train a machine to recognize it from, for example, a video of the user, is significant, and already begins with the lack of ground truth for supervised learning. Existing affective databases consist mainly of short videos, annotated in terms of expressed emotions rather than mood. In very few cases, we encounter perceived mood annotations, of questionable reliability, however, due to the subjectivity of mood estimation and the small number of coders involved. In this work, we introduce a new database for mood recognition from video. Our database contains 180 long, acted videos, depicting typical daily scenarios, and subtle facial and bodily expressions. The videos cover three visual modalities (face, body, Kinect data), and are annotated in terms of emotions (via G-trace) and mood (via the Self-Assessment Manikin and the AffectButton). To annotate the database exhaustively, we exploit crowdsourcing to reach out to an extensive number of nonexpert coders. We validate the reliability of our crowdsourced annotations by (1) adopting a number of criteria to filter out unreliable coders, and (2) comparing the annotations of a subset of our videos with those collected in a controlled lab setting.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = may,
articleno = {51},
numpages = {27},
keywords = {mood recognition, emotion recognition, crowdsourcing, affective annotation, Multimodal database}
}

@article{10.1145/3140459,
author = {Schmitz, Heinz and Lykourentzou, Ioanna},
title = {Online Sequencing of Non-Decomposable Macrotasks in Expert Crowdsourcing},
year = {2018},
issue_date = {March 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {1},
number = {1},
url = {https://doi.org/10.1145/3140459},
doi = {10.1145/3140459},
abstract = {We introduce the problem of Task Assignment and Sequencing, which models online optimization in expert crowdsourcing settings that involve non-decomposable macrotasks. Non-decomposition is a property of certain types of complex problems, like the formulation of an R&amp;D approach or the definition of a research methodology, which cannot be handled through the "divide-and-conquer" approach typically used in microtask crowdsourcing. In contrast to splitting the macrotask to multiple microtasks and allocating them to several workers in parallel, our model supports the sequential improvement of the macrotask one worker at a time, across distinct time slots of a given timeline, until a sufficient quality level is achieved. Our model assumes an online environment where expert workers are available only at specific time slots and worker/task arrivals are not known a priori. With respect to this setting, we propose TAS-ONLINE, an online algorithm that aims to complete as many tasks as possible within budget, required quality, and a given timeline, without any future input information regarding job release dates or worker availabilities. Experimental results comparing TAS-ONLINE to five benchmarks show that it achieves more completed jobs, lower flow times, and higher job quality. This work bears practical implications for providing performance and quality guarantees to expert crowdsourcing platforms that wish to integrate non-decomposable macrotasks into their offered services.},
journal = {Trans. Soc. Comput.},
month = jan,
articleno = {1},
numpages = {33},
keywords = {online scheduling decisions, macrotask scheduling, cooperative social computing, Crowdsourcing optimization}
}

@article{10.1007/s00778-017-0484-3,
author = {Hung, Nguyen Quoc and Thang, Duong Chi and Tam, Nguyen Thanh and Weidlich, Matthias and Aberer, Karl and Yin, Hongzhi and Zhou, Xiaofang},
title = {Answer validation for generic crowdsourcing tasks with minimal efforts},
year = {2017},
issue_date = {December  2017},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {26},
number = {6},
issn = {1066-8888},
url = {https://doi.org/10.1007/s00778-017-0484-3},
doi = {10.1007/s00778-017-0484-3},
abstract = {Crowdsourcing has been established as an essential means to scale human computation in diverse Web applications, reaching from data integration to information retrieval. Yet, crowd workers have wide-ranging levels of expertise. Large worker populations are heterogeneous and comprise a significant amount of faulty workers. As a consequence, quality insurance for crowd answers is commonly seen as the Achilles heel of crowdsourcing. Although various techniques for quality control have been proposed in recent years, a post-processing phase in which crowd answers are validated is still required. Such validation, however, is typically conducted by experts, whose availability is limited and whose work incurs comparatively high costs. This work aims at guiding an expert in the validation of crowd answers. We present a probabilistic model that helps to identify the most beneficial validation questions in terms of both improvement in result correctness and detection of faulty workers. By seeking expert feedback on the most problematic cases, we are able to obtain a set of high-quality answers, even if the expert does not validate the complete answer set. Our approach is applicable for a broad range of crowdsourcing tasks, including classification and counting. Our comprehensive evaluation using both real-world and synthetic datasets demonstrates that our techniques save up to 60\% of expert efforts compared to baseline methods when striving for perfect result correctness. In absolute terms, for most cases, we achieve close to perfect correctness after expert input has been sought for only 15\% of the crowdsourcing tasks.},
journal = {The VLDB Journal},
month = dec,
pages = {855–880},
numpages = {26},
keywords = {Validation, Probabilistic model, Guiding user feedback, Generic tasks, Crowdsourcing}
}

@article{10.1145/2733379,
author = {Mordacchini, Matteo and Passarella, Andrea and Conti, Marco and Allen, Stuart M. and Chorley, Martin J. and Colombo, Gualtiero B. and Tanasescu, Vlad and Whitaker, Roger M.},
title = {Crowdsourcing through Cognitive Opportunistic Networks},
year = {2015},
issue_date = {June 2015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {10},
number = {2},
issn = {1556-4665},
url = {https://doi.org/10.1145/2733379},
doi = {10.1145/2733379},
abstract = {Until recently crowdsourcing has been primarily conceived as an online activity to harness resources for problem solving. However, the emergence of Opportunistic Networking (ON) has opened up crowdsourcing to the spatial domain. In this article, we bring the ON model for potential crowdsourcing in the smart city environment. We introduce cognitive features of the ON that allow users’ mobile devices to become aware of the surrounding physical environment. Specifically, we exploit cognitive psychology studies on dynamic memory structures and cognitive heuristics—mental models that describe how the human brain handles decision making among complex and real-time stimuli. Combined with ON, these cognitive features allow devices to act as proxies in their users’ cyberworlds and exchange knowledge to deliver awareness of places in an urban environment. This is done through tags associated with locations. They represent features that are perceived by humans about a place. We consider the extent to which this knowledge becomes available to participants using interactions with locations and other nodes. This is assessed taking into account a wide range of cognitive parameters. Outcomes are important because this functionality could support a new type of recommendation system that is independent of the traditional forms of networking.},
journal = {ACM Trans. Auton. Adapt. Syst.},
month = jun,
articleno = {13},
numpages = {29},
keywords = {smart cities, crowdsourcing, cognitive heuristic, Opportunistic networks}
}

@article{10.1145/3324926,
author = {Wang, Tian and Luo, Hao and Zheng, Xi and Xie, Mande},
title = {Crowdsourcing Mechanism for Trust Evaluation in CPCS Based on Intelligent Mobile Edge Computing},
year = {2019},
issue_date = {November 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {10},
number = {6},
issn = {2157-6904},
url = {https://doi.org/10.1145/3324926},
doi = {10.1145/3324926},
abstract = {Both academia and industry have directed tremendous interest toward the combination of Cyber Physical Systems and Cloud Computing, which enables a new breed of applications and services. However, due to the relative long distance between remote cloud and end nodes, Cloud Computing cannot provide effective and direct management for end nodes, which leads to security vulnerabilities. In this article, we first propose a novel trust evaluation mechanism using crowdsourcing and Intelligent Mobile Edge Computing. The mobile edge users with relatively strong computation and storage ability are exploited to provide direct management for end nodes. Through close access to end nodes, mobile edge users can obtain various information of the end nodes and determine whether the node is trustworthy. Then, two incentive mechanisms, i.e., Trustworthy Incentive and Quality-Aware Trustworthy Incentive Mechanisms, are proposed for motivating mobile edge users to conduct trust evaluation. The first one aims to motivate edge users to upload their real information about their capability and costs. The purpose of the second one is to motivate edge users to make trustworthy effort to conduct tasks and report results. Detailed theoretical analysis demonstrates the validity of Quality-Aware Trustworthy Incentive Mechanism from data trustfulness, effort trustfulness, and quality trustfulness, respectively. Extensive experiments are carried out to validate the proposed trust evaluation and incentive mechanisms. The results corroborate that the proposed mechanisms can efficiently stimulate mobile edge users to perform evaluation task and improve the accuracy of trust evaluation.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = oct,
articleno = {62},
numpages = {19},
keywords = {trust evaluation, mobile edge computing, artificial intelligence, Crowdsourcing}
}

@article{10.1109/TNET.2018.2811736,
author = {Chatterjee, Avhishek and Borokhovich, Michael and Varshney, Lav R. and Vishwanath, Sriram and Varshney, Lav R. and Chatterjee, Avhishek and Vishwanath, Sriram and Borokhovich, Michael},
title = {Efficient and Flexible Crowdsourcing of Specialized Tasks With Precedence Constraints},
year = {2018},
issue_date = {April 2018},
publisher = {IEEE Press},
volume = {26},
number = {2},
issn = {1063-6692},
url = {https://doi.org/10.1109/TNET.2018.2811736},
doi = {10.1109/TNET.2018.2811736},
abstract = {Many companies now use crowdsourcing to leverage external as well as internal crowds to perform specialized work, and so methods of improving efficiency are critical. Tasks in crowdsourcing systems with specialized work have multiple steps and each step requires multiple skills. Steps may have different flexibilities in terms of obtaining service from one or multiple agents due to varying levels of dependency among parts of steps. Steps of a task may have precedence constraints among them. Moreover, there are variations in loads of different types of tasks requiring different skill sets and availabilities of agents with different skill sets. Considering these constraints together necessitate the design of novel schemes to allocate steps to agents. In addition, large crowdsourcing systems require allocation schemes that are simple, fast, decentralized, and offer customers task requesters the freedom to choose agents. In this paper, we study the performance limits of such crowdsourcing systems and propose efficient allocation schemes that provably meet the performance limits under these additional requirements. We demonstrate our algorithms on data from a crowdsourcing platform run by a nonprofit company and show significant improvements over current practice.},
journal = {IEEE/ACM Trans. Netw.},
month = apr,
pages = {879–892},
numpages = {14}
}

@article{10.1145/2776896,
author = {Siddharthan, Advaith and Lambin, Christopher and Robinson, Anne-Marie and Sharma, Nirwan and Comont, Richard and O'mahony, Elaine and Mellish, Chris and Wal, Ren\'{e} Van Der},
title = {Crowdsourcing Without a Crowd: Reliable Online Species Identification Using Bayesian Models to Minimize Crowd Size},
year = {2016},
issue_date = {July 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {4},
issn = {2157-6904},
url = {https://doi.org/10.1145/2776896},
doi = {10.1145/2776896},
abstract = {We present an incremental Bayesian model that resolves key issues of crowd size and data quality for consensus labeling. We evaluate our method using data collected from a real-world citizen science program, BeeWatch, which invites members of the public in the United Kingdom to classify (label) photographs of bumblebees as one of 22 possible species. The biological recording domain poses two key and hitherto unaddressed challenges for consensus models of crowdsourcing: (1) the large number of potential species makes classification difficult, and (2) this is compounded by limited crowd availability, stemming from both the inherent difficulty of the task and the lack of relevant skills among the general public. We demonstrate that consensus labels can be reliably found in such circumstances with very small crowd sizes of around three to five users (i.e., through group sourcing). Our incremental Bayesian model, which minimizes crowd size by re-evaluating the quality of the consensus label following each species identification solicited from the crowd, is competitive with a Bayesian approach that uses a larger but fixed crowd size and outperforms majority voting. These results have important ecological applicability: biological recording programs such as BeeWatch can sustain themselves when resources such as taxonomic experts to confirm identifications by photo submitters are scarce (as is typically the case), and feedback can be provided to submitters in a timely fashion. More generally, our model provides benefits to any crowdsourced consensus labeling task where there is a cost (financial or otherwise) associated with soliciting a label.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = may,
articleno = {45},
numpages = {20},
keywords = {consensus model, citizen science, bumblebee identification, biological recording, Crowdsourcing, Bayesian reasoning}
}

@article{10.14778/2732977.2732982,
author = {Vesdapunt, Norases and Bellare, Kedar and Dalvi, Nilesh},
title = {Crowdsourcing algorithms for entity resolution},
year = {2014},
issue_date = {August 2014},
publisher = {VLDB Endowment},
volume = {7},
number = {12},
issn = {2150-8097},
url = {https://doi.org/10.14778/2732977.2732982},
doi = {10.14778/2732977.2732982},
abstract = {In this paper, we study a hybrid human-machine approach for solving the problem of Entity Resolution (ER). The goal of ER is to identify all records in a database that refer to the same underlying entity, and are therefore duplicates of each other. Our input is a graph over all the records in a database, where each edge has a probability denoting our prior belief (based on Machine Learning models) that the pair of records represented by the given edge are duplicates. Our objective is to resolve all the duplicates by asking humans to verify the equality of a subset of edges, leveraging the transitivity of the equality relation to infer the remaining edges (e.g. a = c can be inferred given a = b and b = c). We consider the problem of designing optimal strategies for asking questions to humans that minimize the expected number of questions asked. Using our theoretical framework, we analyze several strategies, and show that a strategy, claimed as "optimal" for this problem in a recent work, can perform arbitrarily bad in theory. We propose alternate strategies with theoretical guarantees. Using both public datasets as well as the production system at Facebook, we show that our techniques are effective in practice.},
journal = {Proc. VLDB Endow.},
month = aug,
pages = {1071–1082},
numpages = {12}
}

@article{10.1145/2729713,
author = {To, Hien and Shahabi, Cyrus and Kazemi, Leyla},
title = {A Server-Assigned Spatial Crowdsourcing Framework},
year = {2015},
issue_date = {August 2015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {1},
number = {1},
issn = {2374-0353},
url = {https://doi.org/10.1145/2729713},
doi = {10.1145/2729713},
abstract = {With the popularity of mobile devices, spatial crowdsourcing is rising as a new framework that enables human workers to solve tasks in the physical world. With spatial crowdsourcing, the goal is to crowdsource a set of spatiotemporal tasks (i.e., tasks related to time and location) to a set of workers, which requires the workers to physically travel to those locations in order to perform the tasks. In this article, we focus on one class of spatial crowdsourcing, in which the workers send their locations to the server and thereafter the server assigns to every worker tasks in proximity to the worker’s location with the aim of maximizing the overall number of assigned tasks. We formally define this maximum task assignment (MTA) problem in spatial crowdsourcing, and identify its challenges. We propose alternative solutions to address these challenges by exploiting the spatial properties of the problem space, including the spatial distribution and the travel cost of the workers. MTA is based on the assumptions that all tasks are of the same type and all workers are equally qualified in performing the tasks. Meanwhile, different types of tasks may require workers with various skill sets or expertise. Subsequently, we extend MTA by taking the expertise of the workers into consideration. We refer to this problem as the maximum score assignment (MSA) problem and show its practicality and generality. Extensive experiments with various synthetic and two real-world datasets show the applicability of our proposed framework.},
journal = {ACM Trans. Spatial Algorithms Syst.},
month = jul,
articleno = {2},
numpages = {28},
keywords = {spatial task assignment, spatial crowdsourcing, participatory sensing, mobile crowdsourcing, Crowdsourcing}
}

@article{10.14778/2824032.2824136,
author = {Gao, Jing and Li, Qi and Zhao, Bo and Fan, Wei and Han, Jiawei},
title = {Truth discovery and crowdsourcing aggregation: a unified perspective},
year = {2015},
issue_date = {August 2015},
publisher = {VLDB Endowment},
volume = {8},
number = {12},
issn = {2150-8097},
url = {https://doi.org/10.14778/2824032.2824136},
doi = {10.14778/2824032.2824136},
abstract = {In the era of Big Data, data entries, even describing the same objects or events, can come from a variety of sources, where a data source can be a web page, a database or a person. Consequently, conflicts among sources become inevitable. To resolve the conflicts and achieve high quality data, truth discovery and crowdsourcing aggregation have been studied intensively. However, although these two topics have a lot in common, they are studied separately and are applied to different domains. To answer the need of a systematic introduction and comparison of the two topics, we present an organized picture on truth discovery and crowdsourcing aggregation in this tutorial. They are compared on both theory and application levels, and their related areas as well as open questions are discussed.},
journal = {Proc. VLDB Endow.},
month = aug,
pages = {2048–2049},
numpages = {2}
}

@article{10.1145/2837029,
author = {Luo, Tie and Das, Sajal K. and Tan, Hwee Pink and Xia, Lirong},
title = {Incentive Mechanism Design for Crowdsourcing: An All-Pay Auction Approach},
year = {2016},
issue_date = {April 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {3},
issn = {2157-6904},
url = {https://doi.org/10.1145/2837029},
doi = {10.1145/2837029},
abstract = {Crowdsourcing can be modeled as a principal-agent problem in which the principal (crowdsourcer) desires to solicit a maximal contribution from a group of agents (participants) while agents are only motivated to act according to their own respective advantages. To reconcile this tension, we propose an all-pay auction approach to incentivize agents to act in the principal’s interest, i.e., maximizing profit, while allowing agents to reap strictly positive utility. Our rationale for advocating all-pay auctions is based on two merits that we identify, namely all-pay auctions (i) compress the common, two-stage “bid-contribute” crowdsourcing process into a single “bid-cum-contribute” stage, and (ii) eliminate the risk of task nonfulfillment. In our proposed approach, we enhance all-pay auctions with two additional features: an adaptive prize and a general crowdsourcing environment. The prize or reward adapts itself as per a function of the unknown winning agent’s contribution, and the environment or setting generally accommodates incomplete and asymmetric information, risk-averse (and risk-neutral) agents, and a stochastic (and deterministic) population. We analytically derive this all-pay auction-based mechanism and extensively evaluate it in comparison to classic and optimized mechanisms. The results demonstrate that our proposed approach remarkably outperforms its counterparts in terms of the principal’s profit, agent’s utility, and social welfare.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = feb,
articleno = {35},
numpages = {26},
keywords = {shading effect, risk aversion, participatory sensing, incomplete information, Mobile crowd sensing, Bayesian Nash equilibrium}
}

@article{10.1145/3086686,
author = {Brade\v{s}ko, Luka and Witbrock, Michael and Starc, Janez and Herga, Zala and Grobelnik, Marko and Mladeni\'{c}, Dunja},
title = {Curious Cat--Mobile, Context-Aware Conversational Crowdsourcing Knowledge Acquisition},
year = {2017},
issue_date = {October 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {35},
number = {4},
issn = {1046-8188},
url = {https://doi.org/10.1145/3086686},
doi = {10.1145/3086686},
abstract = {Scaled acquisition of high-quality structured knowledge has been a longstanding goal of Artificial Intelligence research. Recent advances in crowdsourcing, the sheer number of Internet and mobile users, and the commercial availability of supporting platforms offer new tools for knowledge acquisition. This article applies context-aware knowledge acquisition that simultaneously satisfies users’ immediate information needs while extending its own knowledge using crowdsourcing. The focus is on knowledge acquisition on a mobile device, which makes the approach practical and scalable; in this context, we propose and implement a new KA approach that exploits an existing knowledge base to drive the KA process, communicate with the right people, and check for consistency of the user-provided answers. We tested the viability of the approach in experiments using our platform with real users around the world, and an existing large source of common-sense background knowledge. These experiments show that the approach is promising: the knowledge is estimated to be true and useful for users 95\% of the time. Using context to proactively drive knowledge acquisition increased engagement and effectiveness (the number of new assertions/day/user increased for 175\%). Using pre-existing and newly acquired knowledge also proved beneficial.},
journal = {ACM Trans. Inf. Syst.},
month = aug,
articleno = {33},
numpages = {46},
keywords = {reasoning, location and context based knowledge acquisition, knowledge systems, dialogue systems, crowdsourcing, chatbots, Sensor and location mining}
}

@article{10.1145/3002172,
author = {Maddalena, Eddy and Mizzaro, Stefano and Scholer, Falk and Turpin, Andrew},
title = {On Crowdsourcing Relevance Magnitudes for Information Retrieval Evaluation},
year = {2017},
issue_date = {July 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {35},
number = {3},
issn = {1046-8188},
url = {https://doi.org/10.1145/3002172},
doi = {10.1145/3002172},
abstract = {Magnitude estimation is a psychophysical scaling technique for the measurement of sensation, where observers assign numbers to stimuli in response to their perceived intensity. We investigate the use of magnitude estimation for judging the relevance of documents for information retrieval evaluation, carrying out a large-scale user study across 18 TREC topics and collecting over 50,000 magnitude estimation judgments using crowdsourcing. Our analysis shows that magnitude estimation judgments can be reliably collected using crowdsourcing, are competitive in terms of assessor cost, and are, on average, rank-aligned with ordinal judgments made by expert relevance assessors.We explore the application of magnitude estimation for IR evaluation, calibrating two gain-based effectiveness metrics, nDCG and ERR, directly from user-reported perceptions of relevance. A comparison of TREC system effectiveness rankings based on binary, ordinal, and magnitude estimation relevance shows substantial variation; in particular, the top systems ranked using magnitude estimation and ordinal judgments differ substantially. Analysis of the magnitude estimation scores shows that this effect is due in part to varying perceptions of relevance: different users have different perceptions of the impact of relative differences in document relevance. These results have direct implications for IR evaluation, suggesting that current assumptions about a single view of relevance being sufficient to represent a population of users are unlikely to hold.},
journal = {ACM Trans. Inf. Syst.},
month = jan,
articleno = {19},
numpages = {32},
keywords = {relevance assessments, relevance, evaluation, Magnitude estimation}
}

@article{10.1145/3512946,
author = {Kornfield, Rachel and Mohr, David C. and Ranney, Rachel and Lattie, Emily G. and Meyerhoff, Jonah and Williams, Joseph J. and Reddy, Madhu},
title = {Involving Crowdworkers with Lived Experience in Content-Development for Push-Based Digital Mental Health Tools: Lessons Learned from Crowdsourcing Mental Health Messages},
year = {2022},
issue_date = {April 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {6},
number = {CSCW1},
url = {https://doi.org/10.1145/3512946},
doi = {10.1145/3512946},
abstract = {Digital tools can support individuals managing mental health concerns, but delivering sufficiently engaging content is challenging. This paper seeks to clarify how individuals with mental health concerns can contribute content to improve push-based mental health messaging tools. We recruited crowdworkers with mental health symptoms to evaluate and revise expert-composed content for an automated messaging tool, and to generate new topics and messages. A second wave of crowdworkers evaluated expert and crowdsourced content. Crowdworkers generated topics for messages that had not been prioritized by experts, including self-care, positive thinking, inspiration, relaxation, and reassurance. Peer evaluators rated messages written by experts and peers similarly. Our findings also suggest the importance of personalization, particularly when content adaptation occurs over time as users interact with example messages. These findings demonstrate the potential of crowdsourcing for generating diverse and engaging content for push-based tools, and suggest the need to support users in meaningful content customization.},
journal = {Proc. ACM Hum.-Comput. Interact.},
month = apr,
articleno = {99},
numpages = {30},
keywords = {personalization, peer-to-peer support, mixed-methods research, mental health, digital health interventions, crowdsourcing}
}

@article{10.1109/TNET.2018.2828415,
author = {Klos nee Muller, Sabrina and Tekin, Cem and van der Schaar, Mihaela and Klein, Anja},
title = {Context-Aware Hierarchical Online Learning for Performance Maximization in Mobile Crowdsourcing},
year = {2018},
issue_date = {June 2018},
publisher = {IEEE Press},
volume = {26},
number = {3},
issn = {1063-6692},
url = {https://doi.org/10.1109/TNET.2018.2828415},
doi = {10.1109/TNET.2018.2828415},
abstract = {In mobile crowdsourcing MCS, mobile users accomplish outsourced human intelligence tasks. MCS requires an appropriate task assignment strategy, since different workers may have different performance in terms of acceptance rate and quality. Task assignment is challenging, since a worker’s performance 1 may fluctuate, depending on both the worker’s current personal context and the task context and 2 is not known a priori, but has to be learned over time. Moreover, learning context-specific worker performance requires access to context information, which may not be available at a central entity due to communication overhead or privacy concerns. In addition, evaluating worker performance might require costly quality assessments. In this paper, we propose a context-aware hierarchical online learning algorithm addressing the problem of performance maximization in MCS. In our algorithm, a local controller LC in the mobile device of a worker regularly observes the worker’s context, her/his decisions to accept or decline tasks and the quality in completing tasks. Based on these observations, the LC regularly estimates the worker’s context-specific performance. The mobile crowdsourcing platform MCSP then selects workers based on performance estimates received from the LCs. This hierarchical approach enables the LCs to learn context-specific worker performance and it enables the MCSP to select suitable workers. In addition, our algorithm preserves worker context locally, and it keeps the number of required quality assessments low. We prove that our algorithm converges to the optimal task assignment strategy. Moreover, the algorithm outperforms simpler task assignment strategies in experiments based on synthetic and real data.},
journal = {IEEE/ACM Trans. Netw.},
month = jun,
pages = {1334–1347},
numpages = {14}
}

@article{10.1007/s00778-015-0385-2,
author = {Basu Roy, Senjuti and Lykourentzou, Ioanna and Thirumuruganathan, Saravanan and Amer-Yahia, Sihem and Das, Gautam},
title = {Task assignment optimization in knowledge-intensive crowdsourcing},
year = {2015},
issue_date = {August    2015},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {24},
number = {4},
issn = {1066-8888},
url = {https://doi.org/10.1007/s00778-015-0385-2},
doi = {10.1007/s00778-015-0385-2},
abstract = {We present SmartCrowd, a framework for optimizing task assignment in knowledge-intensive crowdsourcing (KI-C). SmartCrowd distinguishes itself by formulating, for the first time, the problem of worker-to-task assignment in KI-C as an optimization problem, by proposing efficient adaptive algorithms to solve it and by accounting for human factors, such as worker expertise, wage requirements, and availability inside the optimization process. We present rigorous theoretical analyses of the task assignment optimization problem and propose optimal and approximation algorithms with guarantees, which rely on index pre-computation and adaptive maintenance. We perform extensive performance and quality experiments using real and synthetic data to demonstrate that the SmartCrowd approach is necessary to achieve efficient task assignments of high-quality under guaranteed cost budget.},
journal = {The VLDB Journal},
month = aug,
pages = {467–491},
numpages = {25},
keywords = {Optimization, Knowledge-intensive crowdsourcing, Human factors, Collaborative crowdsourcing}
}

@article{10.1145/3569470,
author = {Xie, Zhiqing and Luo, Haiyong and Zhang, Xiaotian and Xiong, Hao and Zhao, Fang and Li, Zhaohui and Ye, Qi and Rong, Bojie and Gao, Jiuchong},
title = {TransFloor: Transparent Floor Localization for Crowdsourcing Instant Delivery},
year = {2023},
issue_date = {December 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {6},
number = {4},
url = {https://doi.org/10.1145/3569470},
doi = {10.1145/3569470},
abstract = {Smart on-demand delivery services require accurate indoor localization to enhance the system-human synergy experience of couriers in complex multi-story malls and platform construction. Floor localization is an essential part of indoor positioning, which can provide floor/altitude data support for upper-level 3D indoor navigation services (e.g., delivery route planning) to improve delivery efficiency and optimize order dispatching strategies. We argue that due to label dependence and device dependence, the existing floor localization methods cannot be flexibly deployed on a large scale in numerous multi-story malls across the country, nor can they apply to all couriers/users on the platform. This paper proposes a novel self-evolving and user-transparent floor localization system named TransFloor, based on crowdsourcing delivery data (e.g., order status and sensors data) without additional label investment and specialized equipment constraints. TransFloor consists of an unsupervised barometer-based module--IOD-TKPD and an NLP-inspired Wi-Fi-based module--Wifi2Vec, and Self-Labeling is a perfect bridge between both to completely achieve label-free and device-independent floor positioning. In addition, TransFloor is designed as a lightweight plugin embedded into the platform without refactoring the existing architecture, and it has been deployed nationwide to adaptively launch real-time accurate 3D/floor positioning services for numerous crowdsourcing couriers. We evaluate TransFloor on real-world records from an instant delivery platform (involving 672,282 orders, 7,390 couriers, and 6,206 merchants in 388 malls during two months). It can achieve an average accuracy of 94.61\% and demonstrate good robustness to device heterogeneity and adaptive durability, outperforming existing state-of-the-art methods. Crucially, it can effectively improve user satisfaction and reduce overdue delivery by providing accurate floor navigation information in complex multi-story malls. As a case study, the platform reduces erroneous order scheduling by 60\% and overdue delivery by 2.7\%, and increases delivery efficiency by reducing courier arrival time by 12.27 seconds accounting for 7.29\%. We believe that the key ideas of TransFloor can be extended to other crowdsourcing scenarios for the public further.},
journal = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
month = jan,
articleno = {189},
numpages = {30},
keywords = {Instant delivery, Floor localization, Crowdsourcing, 3D indoor localization}
}

@article{10.14778/3137765.3137806,
author = {Li, Yan and Kou, Ngai Meng and Wang, Hao and U, Leong Hou and Gong, Zhiguo},
title = {A confidence-aware top-k query processing toolkit on crowdsourcing},
year = {2017},
issue_date = {August 2017},
publisher = {VLDB Endowment},
volume = {10},
number = {12},
issn = {2150-8097},
url = {https://doi.org/10.14778/3137765.3137806},
doi = {10.14778/3137765.3137806},
abstract = {Ranking techniques have been widely used in ubiquitous applications like recommendation, information retrieval, etc. For ranking computation hostile but human friendly items, crowdsourcing is considered as an emerging technique to process the ranking by human power. However, there is a lack of an easy-to-use toolkit for answering crowdsourced top-k query with minimal effort.In this work, we demonstrate an interactive programming toolkit that is a unified solution for answering the crowd-sourced top-k queries. The toolkit employs a new confidence-aware crowdsourced top-k algorithm, SPR. The whole progress of the algorithm is monitored and visualized to end users in a timely manner. Besides the visualized result and the statistics, the system also reports the estimation of the monetary cost and the breakdown of each phase. Based on the estimation, end users can strike a balance between the budget and the quality through the interface of this toolkit.},
journal = {Proc. VLDB Endow.},
month = aug,
pages = {1909–1912},
numpages = {4}
}

@article{10.5555/3013589.3013603,
author = {Venanzi, Matteo and Guiver, John and Kohli, Pushmeet and Jennings, Nicholas R.},
title = {Time-sensitive Bayesian information aggregation for crowdsourcing systems},
year = {2016},
issue_date = {May 2016},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {56},
number = {1},
issn = {1076-9757},
abstract = {Many aspects of the design of efficient crowdsourcing processes, such as defining worker's bonuses, fair prices and time limits of the tasks, involve knowledge of the likely duration of the task at hand. In this work we introduce a new time-sensitive Bayesian aggregation method that simultaneously estimates a task's duration and obtains reliable aggregations of crowdsourced judgments. Our method, called BCCTime, uses latent variables to represent the uncertainty about the workers' completion time, the tasks' duration and the workers' accuracy. To relate the quality of a judgment to the time a worker spends on a task, our model assumes that each task is completed within a latent time window within which all workers with a propensity to genuinely attempt the labelling task (i.e., no spammers) are expected to submit their judgments. In contrast, workers with a lower propensity to valid labelling, such as spammers, bots or lazy labellers, are assumed to perform tasks considerably faster or slower than the time required by normal workers. Specifically, we use efficient message-passing Bayesian inference to learn approximate posterior probabilities of (i) the confusion matrix of each worker, (ii) the propensity to valid labelling of each worker, (iii) the unbiased duration of each task and (iv) the true label of each task. Using two real-world public datasets for entity linking tasks, we show that BCCTime produces up to 11\% more accurate classifications and up to 100\% more informative estimates of a task's duration compared to state-of-the-art methods.},
journal = {J. Artif. Int. Res.},
month = may,
pages = {517–545},
numpages = {29}
}

@article{10.1145/3078853,
author = {Tran, Luan and To, Hien and Fan, Liyue and Shahabi, Cyrus},
title = {A Real-Time Framework for Task Assignment in Hyperlocal Spatial Crowdsourcing},
year = {2018},
issue_date = {May 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {9},
number = {3},
issn = {2157-6904},
url = {https://doi.org/10.1145/3078853},
doi = {10.1145/3078853},
abstract = {Spatial Crowdsourcing (SC) is a novel platform that engages individuals in the act of collecting various types of spatial data. This method of data collection can significantly reduce cost and turnover time and is particularly useful in urban environmental sensing, where traditional means fail to provide fine-grained field data. In this study, we introduce hyperlocal spatial crowdsourcing, where all workers who are located within the spatiotemporal vicinity of a task are eligible to perform the task (e.g., reporting the precipitation level at their area and time). In this setting, there is often a budget constraint, either for every time period or for the entire campaign, on the number of workers to activate to perform tasks. The challenge is thus to maximize the number of assigned tasks under the budget constraint despite the dynamic arrivals of workers and tasks. We introduce a taxonomy of several problem variants, such as budget-per-time-period vs. budget-per-campaign and binary-utility vs. distance-based-utility. We study the hardness of the task assignment problem in the offline setting and propose online heuristics which exploit the spatial and temporal knowledge acquired over time. Our experiments are conducted with spatial crowdsourcing workloads generated by the SCAWG tool, and extensive results show the effectiveness and efficiency of our proposed solutions.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = jan,
articleno = {37},
numpages = {26},
keywords = {participatory sensing, online task assignment, crowdsensing, budget constraints, Spatial crowdsourcing, GIS}
}

@article{10.1145/2746353,
author = {Tranquillini, Stefano and Daniel, Florian and Kucherbaev, Pavel and Casati, Fabio},
title = {Modeling, Enacting, and Integrating Custom Crowdsourcing Processes},
year = {2015},
issue_date = {May 2015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {9},
number = {2},
issn = {1559-1131},
url = {https://doi.org/10.1145/2746353},
doi = {10.1145/2746353},
abstract = {Crowdsourcing (CS) is the outsourcing of a unit of work to a crowd of people via an open call for contributions. Thanks to the availability of online CS platforms, such as Amazon Mechanical Turk or CrowdFlower, the practice has experienced a tremendous growth over the past few years and demonstrated its viability in a variety of fields, such as data collection and analysis or human computation. Yet it is also increasingly struggling with the inherent limitations of these platforms: each platform has its own logic of how to crowdsource work (e.g., marketplace or contest), there is only very little support for structured work (work that requires the coordination of multiple tasks), and it is hard to integrate crowdsourced tasks into state-of-the-art business process management (BPM) or information systems.We attack these three shortcomings by (1) developing a flexible CS platform (we call it Crowd Computer, or CC) that allows one to program custom CS logics for individual and structured tasks, (2) devising a BPMN--based modeling language that allows one to program CC intuitively, (3) equipping the language with a dedicated visual editor, and (4) implementing CC on top of standard BPM technology that can easily be integrated into existing software and processes. We demonstrate the effectiveness of the approach with a case study on the crowd-based mining of mashup model patterns.},
journal = {ACM Trans. Web},
month = may,
articleno = {7},
numpages = {43},
keywords = {tactics, processes, Crowdsourcing, Crowd Computer, BPMN4Crowd}
}

@article{10.1145/2897370,
author = {Chen, Chen and Wo\'{z}niak, Pawe\l{} W. and Romanowski, Andrzej and Obaid, Mohammad and Jaworski, Tomasz and Kucharski, Jacek and Grudzie\'{n}, Krzysztof and Zhao, Shengdong and Fjeld, Morten},
title = {Using Crowdsourcing for Scientific Analysis of Industrial Tomographic Images},
year = {2016},
issue_date = {July 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {4},
issn = {2157-6904},
url = {https://doi.org/10.1145/2897370},
doi = {10.1145/2897370},
abstract = {In this article, we present a novel application domain for human computation, specifically for crowdsourcing, which can help in understanding particle-tracking problems. Through an interdisciplinary inquiry, we built a crowdsourcing system designed to detect tracer particles in industrial tomographic images, and applied it to the problem of bulk solid flow in silos. As images from silo-sensing systems cannot be adequately analyzed using the currently available computational methods, human intelligence is required. However, limited availability of experts, as well as their high cost, motivates employing additional nonexperts. We report on the results of a study that assesses the task completion time and accuracy of employing nonexpert workers to process large datasets of images in order to generate data for bulk flow research. We prove the feasibility of this approach by comparing results from a user study with data generated from a computational algorithm. The study shows that the crowd is more scalable and more economical than an automatic solution. The system can help analyze and understand the physics of flow phenomena to better inform the future design of silos, and is generalized enough to be applicable to other domains.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = jul,
articleno = {52},
numpages = {25},
keywords = {tomography, particle tracking, crowdsourcing, Silo}
}

@article{10.5555/3013558.3013568,
author = {Ho, Chien-Ju and Slivkins, Aleksandrs and Vaughan, Jennifer Wortman},
title = {Adaptive contract design for crowdsourcing markets: bandit algorithms for repeated principal-agent problems},
year = {2016},
issue_date = {January 2016},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {55},
number = {1},
issn = {1076-9757},
abstract = {Crowdsourcing markets have emerged as a popular platform for matching available workers with tasks to complete. The payment for a particular task is typically set by the task's requester, and may be adjusted based on the quality of the completed work, for example, through the use of "bonus" payments. In this paper, we study the requester's problem of dynamically adjusting quality-contingent payments for tasks. We consider a multi-round version of the well-known principal-agent model, whereby in each round a worker makes a strategic choice of the effort level which is not directly observable by the requester. In particular, our formulation significantly generalizes the budget-free online task pricing problems studied in prior work. We treat this problem as a multi-armed bandit problem, with each "arm" representing a potential contract. To cope with the large (and in fact, infinite) number of arms, we propose a new algorithm, AgnosticZooming, which discretizes the contract space into a finite number of regions, effectively treating each region as a single arm. This discretization is adaptively refined, so that more promising regions of the contract space are eventually discretized more finely. We analyze this algorithm, showing that it achieves regret sublinear in the time horizon and substantially improves over non-adaptive discretization (which is the only competing approach in the literature). Our results advance the state of art on several different topics: the theory of crowdsourcing markets, principal-agent problems, multi-armed bandits, and dynamic pricing.},
journal = {J. Artif. Int. Res.},
month = jan,
pages = {317–359},
numpages = {43}
}

@article{10.1109/TNET.2014.2379281,
author = {Zhao, Dong and Li, Xiang-Yang and Ma, Huadong},
title = {Budget-feasible online incentive mechanisms for crowdsourcing tasks truthfully},
year = {2016},
issue_date = {April 2016},
publisher = {IEEE Press},
volume = {24},
number = {2},
issn = {1063-6692},
url = {https://doi.org/10.1109/TNET.2014.2379281},
doi = {10.1109/TNET.2014.2379281},
abstract = {Mobile crowd sensing (MCS) is a new paradigm that takes advantage of pervasive mobile devices to efficiently collect data, enabling numerous novel applications. To achieve good service quality for an MCS application, incentive mechanisms are necessary to attract more user participation. Most existing mechanisms apply only for the offline scenario where all users report their strategic types in advance. On the contrary, we focus on a more realistic scenario where users arrive one by one online in a random order. Based on the online auction model, we investigate the problem that users submit their private types to the crowdsourcer when arriving, and the crowdsourcer aims at selecting a subset of users before a specified deadline for maximizing the value of services (assumed to be a nonnegative monotone submodular function) provided by selected users under a budget constraint. We design two online mechanisms, OMZ and OMG, satisfying the computational efficiency, individual rationality, budget feasibility, truthfulness, consumer sovereignty, and constant competitiveness under the zero arrival-departure interval case and a more general case, respectively. Through extensive simulations, we evaluate the performance and validate the theoretical properties of our online mechanisms.},
journal = {IEEE/ACM Trans. Netw.},
month = apr,
pages = {647–661},
numpages = {15},
keywords = {online auction, mobile crowd sensing, incentive mechanism design, crowdsourcing}
}

@article{10.14778/3554821.3554876,
author = {Wu, Qingshun and Li, Yafei and Li, Huiling and Zhang, Di and Zhu, Guanglei},
title = {AMRAS: a visual analysis system for spatial crowdsourcing},
year = {2022},
issue_date = {August 2022},
publisher = {VLDB Endowment},
volume = {15},
number = {12},
issn = {2150-8097},
url = {https://doi.org/10.14778/3554821.3554876},
doi = {10.14778/3554821.3554876},
abstract = {The wide adoption of GPS-enabled smart devices has greatly promoted spatial crowdsourcing, where the core issue is how to assign tasks to workers efficiently and with high quality. In this paper, we build a novel visual analysis system for spatial crowdsourcing, namely AMRAS, which can not only intuitively present the task allocation for workers under different time window scales to users (e.g., data analysts and managers) in real-time, but also help users analyze task assignment decision model and its learning process. AMRAS has the following novel features. First, AMRAS provides two user-friendly interfaces that allow users to employ simple and easy-to-use console to perform statistical analysis. Secondly, AMRAS provides three powerful visualization tools, such as the visualization of assignment results, assignment process, and assignment decision model, which not only allow users to intuitively analyze the whole process of task assignment, but also help users discover the computational bottleneck of their task assignment solution. Finally, AMRAS enables online access to real-time data, providing users with instant assignment and instant analysis. We have implemented and deployed AMRAS on Alibaba Cloud and demonstrated its usability and efficiency in real-world datasets. The demonstration video of AMRAS has been uploaded to Google Drive.},
journal = {Proc. VLDB Endow.},
month = aug,
pages = {3690–3693},
numpages = {4}
}

@article{10.14778/2794367.2794372,
author = {Cheng, Peng and Lian, Xiang and Chen, Zhao and Fu, Rui and Chen, Lei and Han, Jinsong and Zhao, Jizhong},
title = {Reliable diversity-based spatial crowdsourcing by moving workers},
year = {2015},
issue_date = {June 2015},
publisher = {VLDB Endowment},
volume = {8},
number = {10},
issn = {2150-8097},
url = {https://doi.org/10.14778/2794367.2794372},
doi = {10.14778/2794367.2794372},
abstract = {With the rapid development of mobile devices and the crowdsourcing platforms, the spatial crowdsourcing has attracted much attention from the database community, specifically, spatial crowdsourcing refers to sending a location-based request to workers according to their positions. In this paper, we consider an important spatial crowdsourcing problem, namely reliable diversity-based spatial crowdsourcing (RDB-SC), in which spatial tasks (such as taking videos/photos of a landmark or firework shows, and checking whether or not parking spaces are available) are time-constrained, and workers are moving towards some directions. Our RDB-SC problem is to assign workers to spatial tasks such that the completion reliability and the spatial/temporal diversities of spatial tasks are maximized. We prove that the RDB-SC problem is NP-hard and intractable. Thus, we propose three effective approximation approaches, including greedy, sampling, and divide-and-conquer algorithms. In order to improve the efficiency, we also design an effective cost-model-based index, which can dynamically maintain moving workers and spatial tasks with low cost, and efficiently facilitate the retrieval of RDB-SC answers. Through extensive experiments, we demonstrate the efficiency and effectiveness of our proposed approaches over both real and synthetic datasets.},
journal = {Proc. VLDB Endow.},
month = jun,
pages = {1022–1033},
numpages = {12}
}

@article{10.1145/2870649,
author = {Han, Shuguang and Dai, Peng and Paritosh, Praveen and Huynh, David},
title = {Crowdsourcing Human Annotation on Web Page Structure: Infrastructure Design and Behavior-Based Quality Control},
year = {2016},
issue_date = {July 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {4},
issn = {2157-6904},
url = {https://doi.org/10.1145/2870649},
doi = {10.1145/2870649},
abstract = {Parsing the semantic structure of a web page is a key component of web information extraction. Successful extraction algorithms usually require large-scale training and evaluation datasets, which are difficult to acquire. Recently, crowdsourcing has proven to be an effective method of collecting large-scale training data in domains that do not require much domain knowledge. For more complex domains, researchers have proposed sophisticated quality control mechanisms to replicate tasks in parallel or sequential ways and then aggregate responses from multiple workers. Conventional annotation integration methods often put more trust in the workers with high historical performance; thus, they are called performance-based methods. Recently, Rzeszotarski and Kittur have demonstrated that behavioral features are also highly correlated with annotation quality in several crowdsourcing applications. In this article, we present a new crowdsourcing system, called Wernicke, to provide annotations for web information extraction. Wernicke collects a wide set of behavioral features and, based on these features, predicts annotation quality for a challenging task domain: annotating web page structure. We evaluate the effectiveness of quality control using behavioral features through a case study where 32 workers annotate 200 Q&amp;A web pages from five popular websites. In doing so, we discover several things: (1) Many behavioral features are significant predictors for crowdsourcing quality. (2) The behavioral-feature-based method outperforms performance-based methods in recall prediction, while performing equally with precision prediction. In addition, using behavioral features is less vulnerable to the cold-start problem, and the corresponding prediction model is more generalizable for predicting recall than precision for cross-website quality analysis. (3) One can effectively combine workers’ behavioral information and historical performance information to further reduce prediction errors.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = apr,
articleno = {56},
numpages = {25},
keywords = {worker performance, quality control, behavioral features, Crowdsourcing}
}

@article{10.1145/3191741,
author = {Gleason, Cole and Ahmetovic, Dragan and Savage, Saiph and Toxtli, Carlos and Posthuma, Carl and Asakawa, Chieko and Kitani, Kris M. and Bigham, Jeffrey P.},
title = {Crowdsourcing the Installation and Maintenance of Indoor Localization Infrastructure to Support Blind Navigation},
year = {2018},
issue_date = {March 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {2},
number = {1},
url = {https://doi.org/10.1145/3191741},
doi = {10.1145/3191741},
abstract = {Indoor navigation systems can make unfamiliar buildings more accessible for people with vision impairments, but their adoption is hampered by the effort of installing infrastructure and maintaining it over time. Most solutions in this space require augmenting the environment with add-ons, such as Bluetooth beacons. Installing and calibrating such infrastructure requires time and expertise. Once installed, localization accuracy often degrades over time as batteries die, beacons go missing, or otherwise stop working. Even localization systems installed by experts can become unreliable weeks, months, or years after the installation. To address this problem, we created LuzDeploy: a physical crowdsourcing system that organizes non-experts for the installation and long-term maintenance of a Bluetooth-based navigation system. LuzDeploy simplifies the tasks required to install and maintain the localization infrastructure, thus making a crowdsourcing approach feasible for non-experts. We report on a field deployment where 127 participants installed and maintained a blind navigation system over several months in a 7-story building, completing 455 tasks in total. We compare the accuracy of the system installed by participants to an installation completed by experts with specialized equipment. LuzDeploy aims to improve the sustainability of indoor navigation systems to encourage widespread adoption outside of research settings.},
journal = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
month = mar,
articleno = {9},
numpages = {25},
keywords = {Individuals with visual impairments, Indoor navigation assistance, Physical crowdsourcing, Real-world accessibility, Volunteer crowd work}
}

@article{10.1145/2461912.2462016,
author = {Limpaecher, Alex and Feltman, Nicolas and Treuille, Adrien and Cohen, Michael},
title = {Real-time drawing assistance through crowdsourcing},
year = {2013},
issue_date = {July 2013},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {32},
number = {4},
issn = {0730-0301},
url = {https://doi.org/10.1145/2461912.2462016},
doi = {10.1145/2461912.2462016},
abstract = {We propose a new method for the large-scale collection and analysis of drawings by using a mobile game specifically designed to collect such data. Analyzing this crowdsourced drawing database, we build a spatially varying model of artistic consensus at the stroke level. We then present a surprisingly simple stroke-correction method which uses our artistic consensus model to improve strokes in real-time. Importantly, our auto-corrections run interactively and appear nearly invisible to the user while seamlessly preserving artistic intent. Closing the loop, the game itself serves as a platform for large-scale evaluation of the effectiveness of our stroke correction algorithm.},
journal = {ACM Trans. Graph.},
month = jul,
articleno = {54},
numpages = {8},
keywords = {interactive drawings, crowdsourcing}
}

@article{10.1145/3119930,
author = {Gadiraju, Ujwal and Fetahu, Besnik and Kawase, Ricardo and Siehndel, Patrick and Dietze, Stefan},
title = {Using Worker Self-Assessments for Competence-Based Pre-Selection in Crowdsourcing Microtasks},
year = {2017},
issue_date = {August 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {24},
number = {4},
issn = {1073-0516},
url = {https://doi.org/10.1145/3119930},
doi = {10.1145/3119930},
abstract = {Paid crowdsourcing platforms have evolved into remarkable marketplaces where requesters can tap into human intelligence to serve a multitude of purposes, and the workforce can benefit through monetary returns for investing their efforts. In this work, we focus on individual crowd worker competencies. By drawing from self-assessment theories in psychology, we show that crowd workers often lack awareness about their true level of competence. Due to this, although workers intend to maintain a high reputation, they tend to participate in tasks that are beyond their competence. We reveal the diversity of individual worker competencies, and make a case for competence-based pre-selection in crowdsourcing marketplaces. We show the implications of flawed self-assessments on real-world microtasks, and propose a novel worker pre-selection method that considers accuracy of worker self-assessments. We evaluated our method in a sentiment analysis task and observed an improvement in the accuracy by over 15\%, when compared to traditional performance-based worker pre-selection. Similarly, our proposed method resulted in an improvement in accuracy of nearly 6\% in an image validation task. Our results show that requesters in crowdsourcing platforms can benefit by considering worker self-assessments in addition to their performance for pre-selection.},
journal = {ACM Trans. Comput.-Hum. Interact.},
month = aug,
articleno = {30},
numpages = {26},
keywords = {worker behaviour, self-assessment, pre-selection, pre-screening, performance, microtasks, Crowdsourcing}
}

@article{10.5555/2600623.2600646,
author = {Riley, Derek and Nellen, Gwen and Barrera, Raul and Quevedo, Jesus},
title = {Crowdsourcing traffic simulation to improve signal timing},
year = {2014},
issue_date = {May 2014},
publisher = {Consortium for Computing Sciences in Colleges},
address = {Evansville, IN, USA},
volume = {29},
number = {5},
issn = {1937-4771},
abstract = {Inefficiency in automobile traffic light timing contributes to wasted time of travelers and wasted energy, but improving the timing of traffic lights is a difficult problem due to the inherent complexity of the problem. In this work, we present our crowdsourcing tool that allows Android mobile phone users to play a game challenging them to find optimal light timing for a simulated traffic intersection. To succeed at this game, players must configure the traffic light signals to optimize the throughput of vehicles. Scores and configurations of players are stored in a remote database, which is used to populate a high score list. The game uses crowdsourcing by allowing players to load configurations from the high score list to try to optimize them. We also present a case study of volunteers who tested our prototype.},
journal = {J. Comput. Sci. Coll.},
month = may,
pages = {112–118},
numpages = {7}
}

@article{10.1145/2978655,
author = {Zhou, Yipeng and Chen, Liang and Jing, Mi and Zou, Shenglong and Ma, Richard Tianbai},
title = {Design, Implementation, and Measurement of a Crowdsourcing-Based Content Distribution Platform},
year = {2016},
issue_date = {December 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {12},
number = {5s},
issn = {1551-6857},
url = {https://doi.org/10.1145/2978655},
doi = {10.1145/2978655},
abstract = {Content distribution, especially the distribution of video content, unavoidably consumes bandwidth resources heavily. Internet content providers invest heavily in purchasing content distribution network (CDN) services. By deploying tens of thousands of edge servers close to end users, CDN companies are able to distribute content efficiently and effectively, but at considerable cost. Thus, it is of great importance to develop a new system that distributes content at a lower cost but comparable service quality. In lieu of expensive CDN systems, we implement a crowdsourcing-based content distribution system, Thunder Crystal, by renting bandwidth for content upload/download and storage for content cache from agents. This is a large-scale system with tens of thousands of agents, whose resources significantly amplify Thunder Crystal’s content distribution capacity. The involved agents are either from ordinary Internet users or enterprises. Monetary rewards are paid to agents based on their upload traffic so as to motivate them to keep contributing resources. As far as we know, this is a novel system that has not been studied or implemented before. This article introduces the design principles and implementation details before presenting the measurement study. In summary, with the help of agent devices, Thunder Crystal is able to reduce the content distribution cost by one half and amplify the content distribution capacity by 11 to 15 times.},
journal = {ACM Trans. Multimedia Comput. Commun. Appl.},
month = nov,
articleno = {80},
numpages = {23},
keywords = {video distribution, agent, Crowdsourcing, CDN}
}

@article{10.1145/2856102,
author = {Radanovic, Goran and Faltings, Boi and Jurca, Radu},
title = {Incentives for Effort in Crowdsourcing Using the Peer Truth Serum},
year = {2016},
issue_date = {July 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {4},
issn = {2157-6904},
url = {https://doi.org/10.1145/2856102},
doi = {10.1145/2856102},
abstract = {Crowdsourcing is widely proposed as a method to solve a large variety of judgment tasks, such as classifying website content, peer grading in online courses, or collecting real-world data. As the data reported by workers cannot be verified, there is a tendency to report random data without actually solving the task. This can be countered by making the reward for an answer depend on its consistency with answers given by other workers, an approach called peer consistency. However, it is obvious that the best strategy in such schemes is for all workers to report the same answer without solving the task.Dasgupta and Ghosh [2013] show that, in some cases, exerting high effort can be encouraged in the highest-paying equilibrium. In this article, we present a general mechanism that implements this idea and is applicable to most crowdsourcing settings. Furthermore, we experimentally test the novel mechanism, and validate its theoretical properties.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = mar,
articleno = {48},
numpages = {28},
keywords = {peer prediction, mechanism design, Crowdsourcing}
}

@article{10.5555/2946645.3053498,
author = {Chen, Xi and Jiao, Kevin and Lin, Qihang},
title = {Bayesian decision process for cost-efficient dynamic ranking via crowdsourcing},
year = {2016},
issue_date = {January 2016},
publisher = {JMLR.org},
volume = {17},
number = {1},
issn = {1532-4435},
abstract = {Rank aggregation based on pairwise comparisons over a set of items has a wide range of applications. Although considerable research has been devoted to the development of rank aggregation algorithms, one basic question is how to efficiently collect a large amount of high-quality pairwise comparisons for the ranking purpose. Because of the advent of many crowdsourcing services, a crowd of workers are often hired to conduct pairwise comparisons with a small monetary reward for each pair they compare. Since different workers have different levels of reliability and different pairs have different levels of ambiguity, it is desirable to wisely allocate the limited budget for comparisons among the pairs of items and workers so that the global ranking can be accurately inferred from the comparison results. To this end, we model the active sampling problem in crowdsourced ranking as a Bayesian Markov decision process, which dynamically selects item pairs and workers to improve the ranking accuracy under a budget constraint. We further develop a computationally efficient sampling policy based on knowledge gradient as well as a moment matching technique for posterior approximation. Experimental evaluations on both synthetic and real data show that the proposed policy achieves high ranking accuracy with a lower labeling cost.},
journal = {J. Mach. Learn. Res.},
month = jan,
pages = {7617–7656},
numpages = {40},
keywords = {moment matching, knowledge gradient, dynamic programming, crowdsourced ranking, Markov decision process, Bayesian}
}

@article{10.14778/2735479.2735495,
author = {Vesdapunt, Norases and Bellare, Kedar and Dalvi, Nilesh},
title = {Errata for "Crowdsourcing algorithms for entity resolution": (PVLDB 7(12): 1071-1082)},
year = {2015},
issue_date = {January 2015},
publisher = {VLDB Endowment},
volume = {8},
number = {5},
issn = {2150-8097},
url = {https://doi.org/10.14778/2735479.2735495},
doi = {10.14778/2735479.2735495},
abstract = {We discovered that there was a duplicate figure in our paper. We accidentally put Figure 13(b) for Figure 12(b). We have provided the correct Figure 12(b) above (See Figure 1). Figure 1 plots the recall of various strategies as a function of the number of questions asked for Places dataset. There was no error in the discussion in our paper (See Section 6.2.1 in our paper for more details).},
journal = {Proc. VLDB Endow.},
month = jan,
pages = {641},
numpages = {1}
}

@article{10.1145/3133327,
author = {Prandi, Catia and Mirri, Silvia and Ferretti, Stefano and Salomoni, Paola},
title = {On the Need of Trustworthy Sensing and Crowdsourcing for Urban Accessibility in Smart City},
year = {2017},
issue_date = {February 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {18},
number = {1},
issn = {1533-5399},
url = {https://doi.org/10.1145/3133327},
doi = {10.1145/3133327},
abstract = {Mobility in urban environments is an undisputed key factor that can affect citizens’ well-being and quality of life. This is particularly relevant for those people with disabilities or with reduced mobility who have to face the presence of barriers in urban areas. In this scenario, the availability of information about such architectural elements (together with facilities) can greatly support citizens’ mobility by enhancing their independence and their abilities in conducting daily outdoor activities. With this in mind, we have designed and developed mobile Pervasive Accessibility Social Sensing (mPASS), a system that provides users with personalized paths, computed on the basis of their own preferences and needs, with a customizable and accessible interface. The system collects data from crowdsourcing and crowdsensing to map urban and architectural accessibility by providing reliable information coming from different data sources with different levels of trustworthiness. In this context, reliability can be ensured by properly managing crowdsourced and crowdsensed data, combined when possible with authoritative datasets, provided by disability rights organizations and local authorities. To demonstrate this claim, in this article we present our trustworthiness model and discuss results we have obtained by simulations.},
journal = {ACM Trans. Internet Technol.},
month = oct,
articleno = {4},
numpages = {21},
keywords = {Urban Accessibility, Trustworthiness, Crowdsourcing, Crowdsensing, Credibility}
}

@article{10.14778/2536360.2536374,
author = {Zhang, Chen Jason and Chen, Lei and Jagadish, H. V. and Cao, Chen Caleb},
title = {Reducing uncertainty of schema matching via crowdsourcing},
year = {2013},
issue_date = {July 2013},
publisher = {VLDB Endowment},
volume = {6},
number = {9},
issn = {2150-8097},
url = {https://doi.org/10.14778/2536360.2536374},
doi = {10.14778/2536360.2536374},
abstract = {Schema matching is a central challenge for data integration systems. Automated tools are often uncertain about schema matchings they suggest, and this uncertainty is inherent since it arises from the inability of the schema to fully capture the semantics of the represented data. Human common sense can often help. Inspired by the popularity and the success of easily accessible crowdsourcing platforms, we explore the use of crowdsourcing to reduce the uncertainty of schema matching.Since it is typical to ask simple questions on crowdsourcing platforms, we assume that each question, namely Correspondence Correctness Question (CCQ), is to ask the crowd to decide whether a given correspondence should exist in the correct matching. We propose frameworks and efficient algorithms to dynamically manage the CCQs, in order to maximize the uncertainty reduction within a limited budget of questions. We develop two novel approaches, namely "Single CCQ" and "Multiple CCQ", which adaptively select, publish and manage the questions. We verified the value of our solutions with simulation and real implementation.},
journal = {Proc. VLDB Endow.},
month = jul,
pages = {757–768},
numpages = {12}
}

@article{10.1145/2483669.2483676,
author = {Burrows, Steven and Potthast, Martin and Stein, Benno},
title = {Paraphrase acquisition via crowdsourcing and machine learning},
year = {2013},
issue_date = {June 2013},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {4},
number = {3},
issn = {2157-6904},
url = {https://doi.org/10.1145/2483669.2483676},
doi = {10.1145/2483669.2483676},
abstract = {To paraphrase means to rewrite content while preserving the original meaning. Paraphrasing is important in fields such as text reuse in journalism, anonymizing work, and improving the quality of customer-written reviews. This article contributes to paraphrase acquisition and focuses on two aspects that are not addressed by current research: (1) acquisition via crowdsourcing, and (2) acquisition of passage-level samples. The challenge of the first aspect is automatic quality assurance; without such a means the crowdsourcing paradigm is not effective, and without crowdsourcing the creation of test corpora is unacceptably expensive for realistic order of magnitudes. The second aspect addresses the deficit that most of the previous work in generating and evaluating paraphrases has been conducted using sentence-level paraphrases or shorter; these short-sample analyses are limited in terms of application to plagiarism detection, for example. We present the Webis Crowd Paraphrase Corpus 2011 (Webis-CPC-11), which recently formed part of the PAN 2010 international plagiarism detection competition. This corpus comprises passage-level paraphrases with 4067 positive samples and 3792 negative samples that failed our criteria, using Amazon's Mechanical Turk for crowdsourcing. In this article, we review the lessons learned at PAN 2010, and explain in detail the method used to construct the corpus. The empirical contributions include machine learning experiments to explore if passage-level paraphrases can be identified in a two-class classification problem using paraphrase similarity features, and we find that a k-nearest-neighbor classifier can correctly distinguish between paraphrased and nonparaphrased samples with 0.980 precision at 0.523 recall. This result implies that just under half of our samples must be discarded (remaining 0.477 fraction), but our cost analysis shows that the automation we introduce results in a 18\% financial saving and over 100 hours of time returned to the researchers when repeating a similar corpus design. On the other hand, when building an unrelated corpus requiring, say, 25\% training data for the automated component, we show that the financial outcome is cost neutral, while still returning over 70 hours of time to the researchers. The work presented here is the first to join the paraphrasing and plagiarism communities.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = jul,
articleno = {43},
numpages = {21},
keywords = {plagiarism, cost analysis, corpus, Paraphrase generation, Mechanical Turk}
}

@article{10.14778/3402755.3402809,
author = {Doan, AnHai and Franklin, Michael J. and Kossmann, Donald and Kraska, Tim},
title = {Crowdsourcing applications and platforms: a data management perspective},
year = {2011},
issue_date = {August 2011},
publisher = {VLDB Endowment},
volume = {4},
number = {12},
issn = {2150-8097},
url = {https://doi.org/10.14778/3402755.3402809},
doi = {10.14778/3402755.3402809},
abstract = {Over the past decade, crowdsourcing has emerged as a major problem-solving and data-gathering paradigm on the World-Wide Web. Well-known examples of crowdsourcing include Wikipedia, Linux, Yahoo! Answers, YouTube, Mechanical Turk-based applications, and much effort is being directed toward developing many more.},
journal = {Proc. VLDB Endow.},
month = aug,
pages = {1508–1509},
numpages = {2}
}

@article{10.1145/2814573,
author = {Amor, Iheb Ben and Benbernou, Salima and Ouziri, Mourad and Malik, Zaki and Medjahed, Brahim},
title = {Discovering Best Teams for Data Leak-Aware Crowdsourcing in Social Networks},
year = {2016},
issue_date = {February 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {10},
number = {1},
issn = {1559-1131},
url = {https://doi.org/10.1145/2814573},
doi = {10.1145/2814573},
abstract = {Crowdsourcing is emerging as a powerful paradigm to help perform a wide range of tedious tasks in various enterprise applications. As such applications become more complex, crowdsourcing systems often require the collaboration of several experts connected through professional/social networks and organized in various teams. For instance, a well-known car manufacturer asked fans to contribute ideas for the kinds of technologies that should be incorporated into one of its cars. For that purpose, fans needed to collaborate and form teams competing with each others to come up with the best ideas. However, once teams are formed, each one would like to provide the best solution and treat that solution as a “trade secret,” hence preventing any data leak to its competitors (i.e., the other teams). In this article, we propose a data leak--aware crowdsourcing system called SocialCrowd. We introduce a clustering algorithm that uses social relationships between crowd workers to discover all possible teams while avoiding interteam data leakage. We also define a ranking mechanism to select the “best” team configurations. Our mechanism is based on the semiring approach defined in the area of soft constraints programming. Finally, we present experiments to assess the efficiency of the proposed approach.},
journal = {ACM Trans. Web},
month = feb,
articleno = {2},
numpages = {27},
keywords = {social networks, data leakage, clustering, Crowdsourcing}
}

@article{10.1145/3231934,
author = {Safran, Mejdl and Che, Dunren},
title = {Efficient Learning-Based Recommendation Algorithms for Top-N Tasks and Top-N Workers in Large-Scale Crowdsourcing Systems},
year = {2018},
issue_date = {January 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {37},
number = {1},
issn = {1046-8188},
url = {https://doi.org/10.1145/3231934},
doi = {10.1145/3231934},
abstract = {The task and worker recommendation problems in crowdsourcing systems have brought up unique characteristics that are not present in traditional recommendation scenarios, i.e., the huge flow of tasks with short lifespans, the importance of workers’ capabilities, and the quality of the completed tasks. These unique features make traditional recommendation approaches no longer satisfactory for task and worker recommendation in crowdsourcing systems. In this article, we propose a two-tier data representation scheme (defining a worker--category suitability score and a worker--task attractiveness score) to support personalized task and worker recommendations. We also extend two optimization methods, namely least mean square error and Bayesian personalized rank, to better fit the characteristics of task/worker recommendation in crowdsourcing systems. We then integrate the proposed representation scheme and the extended optimization methods along with the two adapted popular learning models, i.e., matrix factorization and kNN, and result in two lines of top-N recommendation algorithms for crowdsourcing systems: (1) Top-N-Tasks recommendation algorithms for discovering the top-N most suitable tasks for a given worker and (2) Top-N-Workers recommendation algorithms for identifying the top-N best workers for a task requester. An extensive experimental study is conducted that validates the effectiveness and efficiency of a broad spectrum of algorithms, accompanied by our analysis and the insights gained.},
journal = {ACM Trans. Inf. Syst.},
month = oct,
articleno = {2},
numpages = {46},
keywords = {task recommendation, ranking algorithms, machine learning, crowdsourcing, Crowd computing}
}

@article{10.1145/2897510,
author = {Xie, Hong and Lui, John C. S. and Towsley, Don},
title = {Design and Analysis of Incentive and Reputation Mechanisms for Online Crowdsourcing Systems},
year = {2016},
issue_date = {May 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {1},
number = {3},
issn = {2376-3639},
url = {https://doi.org/10.1145/2897510},
doi = {10.1145/2897510},
abstract = {Today, online crowdsourcing services like Amazon Mechanical Turk, UpWork, and Yahoo! Answers are gaining in popularity. For such online services, it is important to attract “workers” to provide high-quality solutions to the “tasks” outsourced by “requesters.” The challenge is that workers have different skill sets and can provide different amounts of effort. In this article, we design a class of incentive and reputation mechanisms to solicit high-quality solutions from workers. Our incentive mechanism allows multiple workers to solve a task, splits the reward among workers based on requester evaluations of the solution quality, and guarantees that high-skilled workers provide high-quality solutions. However, our incentive mechanism suffers the potential risk that a requester will eventually collects low-quality solutions due to fundamental limitations in task assigning accuracy. Our reputation mechanism ensures that low-skilled workers do not provide low-quality solutions by tracking workers’ historical contributions and penalizing those workers having poor reputations. We show that by coupling our reputation mechanism with our incentive mechanism, a requester can collect at least one high-quality solution. We present an optimization framework to select parameters for our reputation mechanism. We show that there is a trade-off between system efficiency (i.e., the number of tasks that can be solved for a given reward) and revenue (i.e., the amount of transaction fees), and we present the optimal trade-off curve between system efficiency and revenue. We demonstrate the applicability and effectiveness of our mechanisms through experiments using a real-world dataset from UpWork. We infer model parameters from this data, use them to determine proper rewards, and select the parameters of our incentive and reputation mechanisms for UpWork. Experimental results show that our incentive and reputation mechanisms achieve 98.82\% of the maximum system efficiency while only sacrificing 4\% of revenue.},
journal = {ACM Trans. Model. Perform. Eval. Comput. Syst.},
month = may,
articleno = {13},
numpages = {27},
keywords = {repeated game, equilibrium, Bayesian game}
}

@article{10.1145/3415203,
author = {Fan, Shaoyang and Gadiraju, Ujwal and Checco, Alessandro and Demartini, Gianluca},
title = {CrowdCO-OP: Sharing Risks and Rewards in Crowdsourcing},
year = {2020},
issue_date = {October 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {4},
number = {CSCW2},
url = {https://doi.org/10.1145/3415203},
doi = {10.1145/3415203},
abstract = {Paid micro-task crowdsourcing has gained in popularity partly due to the increasing need for large-scale manually labelled datasets which are often used to train and evaluate Artificial Intelligence systems. Modern paid crowdsourcing platforms use a piecework approach to rewards, meaning that workers are paid for each task they complete, given that their work quality is considered sufficient by the requester or the platform. Such an approach creates risks for workers; their work may be rejected without being rewarded, and they may be working on poorly rewarded tasks, in light of the disproportionate time required to complete them. As a result, recent research has shown that crowd workers may tend to choose specific, simple, and familiar tasks and avoid new requesters to manage these risks. In this paper, we propose a novel crowdsourcing reward mechanism that allows workers to share these risks and achieve a standardized hourly wage equal for all participating workers. Reward-focused workers can thereby take up challenging and complex HITs without bearing the financial risk of not being rewarded for completed work. We experimentally compare different crowd reward schemes and observe their impact on worker performance and satisfaction. Our results show that 1) workers clearly perceive the benefits of the proposed reward scheme, 2) work effectiveness and efficiency are not impacted as compared to those of the piecework scheme, and 3) the presence of slow workers is limited and does not disrupt the proposed cooperation-based approaches.},
journal = {Proc. ACM Hum.-Comput. Interact.},
month = oct,
articleno = {132},
numpages = {24},
keywords = {worker behavior, reward sharing, human computation, fairness, crowdsourcing}
}

@article{10.14778/2732951.2732966,
author = {To, Hien and Ghinita, Gabriel and Shahabi, Cyrus},
title = {A framework for protecting worker location privacy in spatial crowdsourcing},
year = {2014},
issue_date = {June 2014},
publisher = {VLDB Endowment},
volume = {7},
number = {10},
issn = {2150-8097},
url = {https://doi.org/10.14778/2732951.2732966},
doi = {10.14778/2732951.2732966},
abstract = {Spatial Crowdsourcing (SC) is a transformative platform that engages individuals, groups and communities in the act of collecting, analyzing, and disseminating environmental, social and other spatio-temporal information. The objective of SC is to outsource a set of spatio-temporal tasks to a set of workers, i.e., individuals with mobile devices that perform the tasks by physically traveling to specified locations of interest. However, current solutions require the workers, who in many cases are simply volunteering for a cause, to disclose their locations to untrustworthy entities. In this paper, we introduce a framework for protecting location privacy of workers participating in SC tasks. We argue that existing location privacy techniques are not sufficient for SC, and we propose a mechanism based on differential privacy and geocasting that achieves effective SC services while offering privacy guarantees to workers. We investigate analytical models and task assignment strategies that balance multiple crucial aspects of SC functionality, such as task completion rate, worker travel distance and system overhead. Extensive experimental results on real-world datasets show that the proposed technique protects workers' location privacy without incurring significant performance metrics penalties.},
journal = {Proc. VLDB Endow.},
month = jun,
pages = {919–930},
numpages = {12}
}

@article{10.1145/2483669.2483671,
author = {Resnik, Philip and Buzek, Olivia and Kronrod, Yakov and Hu, Chang and Quinn, Alexander J. and Bederson, Benjamin B.},
title = {Using targeted paraphrasing and monolingual crowdsourcing to improve translation},
year = {2013},
issue_date = {June 2013},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {4},
number = {3},
issn = {2157-6904},
url = {https://doi.org/10.1145/2483669.2483671},
doi = {10.1145/2483669.2483671},
abstract = {Targeted paraphrasing is a new approach to the problem of obtaining cost-effective, reasonable quality translation, which makes use of simple and inexpensive human computations by monolingual speakers in combination with machine translation. The key insight behind the process is that it is possible to spot likely translation errors with only monolingual knowledge of the target language, and it is possible to generate alternative ways to say the same thing (i.e., paraphrases) with only monolingual knowledge of the source language. Formal evaluation demonstrates that this approach can yield substantial improvements in translation quality, and the idea has been integrated into a broader framework for monolingual collaborative translation that produces fully accurate, fully fluent translations for a majority of sentences in a real-world translation task, with no involvement of human bilingual speakers.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = jul,
articleno = {38},
numpages = {21},
keywords = {wisdom of crowds, translation interface, translation, paraphrase, machine translation, human computation, crowdsourcing, Monolingual}
}

@article{10.1145/3434175,
author = {Anjum, Samreen and Lin, Chi and Gurari, Danna},
title = {CrowdMOT: Crowdsourcing Strategies for Tracking Multiple Objects in Videos},
year = {2021},
issue_date = {December 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {4},
number = {CSCW3},
url = {https://doi.org/10.1145/3434175},
doi = {10.1145/3434175},
abstract = {Crowdsourcing is a valuable approach for tracking objects in videos in a more scalable manner than possible with domain experts. However, existing frameworks do not produce high quality results with non-expert crowdworkers, especially for scenarios where objects split. To address this shortcoming, we introduce a crowdsourcing platform called CrowdMOT, and investigate two micro-task design decisions: (1) whether to decompose the task so that each worker is in charge of annotating all objects in a sub-segment of the video versus annotating a single object across the entire video, and (2) whether to show annotations from previous workers to the next individuals working on the task. We conduct experiments on a diversity of videos which show both familiar objects (aka - people) and unfamiliar objects (aka - cells). Our results highlight strategies for efficiently collecting higher quality annotations than observed when using strategies employed by today's state-of-art crowdsourcing system.},
journal = {Proc. ACM Hum.-Comput. Interact.},
month = jan,
articleno = {266},
numpages = {25},
keywords = {video annotation, crowdsourcing, computer vision}
}

@article{10.1007/s00778-013-0324-z,
author = {Demartini, Gianluca and Difallah, Djellel Eddine and Cudr\'{e}-Mauroux, Philippe},
title = {Large-scale linked data integration using probabilistic reasoning and crowdsourcing},
year = {2013},
issue_date = {October   2013},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {22},
number = {5},
issn = {1066-8888},
url = {https://doi.org/10.1007/s00778-013-0324-z},
doi = {10.1007/s00778-013-0324-z},
abstract = {We tackle the problems of semiautomatically matching linked data sets and of linking large collections of Web pages to linked data. Our system, ZenCrowd, (1) uses a three-stage blocking technique in order to obtain the best possible instance matches while minimizing both computational complexity and latency, and (2) identifies entities from natural language text using state-of-the-art techniques and automatically connects them to the linked open data cloud. First, we use structured inverted indices to quickly find potential candidate results from entities that have been indexed in our system. Our system then analyzes the candidate matches and refines them whenever deemed necessary using computationally more expensive queries on a graph database. Finally, we resort to human computation by dynamically generating crowdsourcing tasks in case the algorithmic components fail to come up with convincing results. We integrate all results from the inverted indices, from the graph database and from the crowd using a probabilistic framework in order to make sensible decisions about candidate matches and to identify unreliable human workers. In the following, we give an overview of the architecture of our system and describe in detail our novel three-stage blocking technique and our probabilistic decision framework. We also report on a series of experimental results on a standard data set, showing that our system can achieve a 95 \% average accuracy on instance matching (as compared to the initial 88 \% average accuracy of the purely automatic baseline) while drastically limiting the amount of work performed by the crowd. The experimental evaluation of our system on the entity linking task shows an average relative improvement of 14 \% over our best automatic approach.},
journal = {The VLDB Journal},
month = oct,
pages = {665–687},
numpages = {23},
keywords = {Probabilistic reasoning, Instance matching, Entity linking, Data integration, Crowdsourcing}
}

@article{10.1145/3421712,
author = {Tu, Jinzheng and Yu, Guoxian and Wang, Jun and Domeniconi, Carlotta and Guo, Maozu and Zhang, Xiangliang},
title = {CrowdWT: Crowdsourcing via Joint Modeling of Workers and Tasks},
year = {2020},
issue_date = {February 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {15},
number = {1},
issn = {1556-4681},
url = {https://doi.org/10.1145/3421712},
doi = {10.1145/3421712},
abstract = {Crowdsourcing is a relatively inexpensive and efficient mechanism to collect annotations of data from the open Internet. Crowdsourcing workers are paid for the provided annotations, but the task requester usually has a limited budget. It is desirable to wisely assign the appropriate task to the right workers, so the overall annotation quality is maximized while the cost is reduced. In this article, we propose a novel task assignment strategy (CrowdWT) to capture the complex interactions between tasks and workers, and properly assign tasks to workers. CrowdWT first develops a Worker Bias Model (WBM) to jointly model the worker’s bias, the ground truths of tasks, and the task features. WBM constructs a mapping between task features and worker annotations to dynamically assign the task to a group of workers, who are more likely to give correct annotations for the task. CrowdWT further introduces a Task Difficulty Model (TDM), which builds a Kernel ridge regressor based on task features to quantify the intrinsic difficulty of tasks and thus to assign the difficult tasks to more reliable workers. Finally, CrowdWT combines WBM and TDM into a unified model to dynamically assign tasks to a group of workers and recall more reliable and even expert workers to annotate the difficult tasks. Our experimental results on two real-world datasets and two semi-synthetic datasets show that CrowdWT achieves high-quality answers within a limited budget, and has the best performance against competitive methods.&lt;?vsp -1.5pt?&gt;},
journal = {ACM Trans. Knowl. Discov. Data},
month = dec,
articleno = {12},
numpages = {24},
keywords = {worker bias model, task difficulty model, task assignment, annotation quality, Crowdsourcing}
}

@article{10.1145/3108935,
author = {Peng, Xin and Gu, Jingxiao and Tan, Tian Huat and Sun, Jun and Yu, Yijun and Nuseibeh, Bashar and Zhao, Wenyun},
title = {CrowdService: Optimizing Mobile Crowdsourcing and Service Composition},
year = {2018},
issue_date = {May 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {18},
number = {2},
issn = {1533-5399},
url = {https://doi.org/10.1145/3108935},
doi = {10.1145/3108935},
abstract = {Some user needs can only be met by leveraging the capabilities of others to undertake particular tasks that require intelligence and labor. Crowdsourcing such capabilities is one way to achieve this. But providing a service that leverages crowd intelligence and labor is a challenge, since various factors need to be considered to enable reliable service provisioning. For example, the selection of an optimal set of workers from those who bid to perform a task needs to be made based on their reliability, expected reward, and distance to the target locations. Moreover, for an application involving multiple services, the overall cost and time constraints must be optimally allocated to each involved service. In this article, we develop a framework, named CrowdService, that supplies crowd intelligence and labor as publicly accessible crowd services via mobile crowdsourcing. The article extends our earlier work by providing an approach for constraints synthesis and worker selection. It employs a genetic algorithm to dynamically synthesize and update near-optimal cost and time constraints for each crowd service involved in a composite service and selects a near-optimal set of workers for each crowd service to be executed. We implement the proposed framework on Android platforms and evaluate its effectiveness, scalability, and usability in both experimental and user studies.},
journal = {ACM Trans. Internet Technol.},
month = jan,
articleno = {19},
numpages = {25},
keywords = {service composition, reliability, collaboration, Mobile crowdsourcing}
}

@article{10.1145/3230665,
author = {Wilson, Shomir and Schaub, Florian and Liu, Frederick and Sathyendra, Kanthashree Mysore and Smullen, Daniel and Zimmeck, Sebastian and Ramanath, Rohan and Story, Peter and Liu, Fei and Sadeh, Norman and Smith, Noah A.},
title = {Analyzing Privacy Policies at Scale: From Crowdsourcing to Automated Annotations},
year = {2018},
issue_date = {February 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {13},
number = {1},
issn = {1559-1131},
url = {https://doi.org/10.1145/3230665},
doi = {10.1145/3230665},
abstract = {Website privacy policies are often long and difficult to understand. While research shows that Internet users care about their privacy, they do not have the time to understand the policies of every website they visit, and most users hardly ever read privacy policies. Some recent efforts have aimed to use a combination of crowdsourcing, machine learning, and natural language processing to interpret privacy policies at scale, thus producing annotations for use in interfaces that inform Internet users of salient policy details. However, little attention has been devoted to studying the accuracy of crowdsourced privacy policy annotations, how crowdworker productivity can be enhanced for such a task, and the levels of granularity that are feasible for automatic analysis of privacy policies. In this article, we present a trajectory of work addressing each of these topics. We include analyses of crowdworker performance, evaluation of a method to make a privacy-policy oriented task easier for crowdworkers, a coarse-grained approach to labeling segments of policy text with descriptive themes, and a fine-grained approach to identifying user choices described in policy text. Together, the results from these efforts show the effectiveness of using automated and semi-automated methods for extracting from privacy policies the data practice details that are salient to Internet users’ interests.},
journal = {ACM Trans. Web},
month = dec,
articleno = {1},
numpages = {29},
keywords = {privacy policies, natural language processing, machine learning, human computer interaction (HCI), crowdsourcing, Privacy}
}

@article{10.1145/3397333,
author = {Huang, Danny Yuxing and Apthorpe, Noah and Li, Frank and Acar, Gunes and Feamster, Nick},
title = {IoT Inspector: Crowdsourcing Labeled Network Traffic from Smart Home Devices at Scale},
year = {2020},
issue_date = {June 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {4},
number = {2},
url = {https://doi.org/10.1145/3397333},
doi = {10.1145/3397333},
abstract = {The proliferation of smart home devices has created new opportunities for empirical research in ubiquitous computing, ranging from security and privacy to personal health. Yet, data from smart home deployments are hard to come by, and existing empirical studies of smart home devices typically involve only a small number of devices in lab settings. To contribute to data-driven smart home research, we crowdsource the largest known dataset of labeled network traffic from smart home devices from within real-world home networks. To do so, we developed and released IoT Inspector, an open-source tool that allows users to observe the traffic from smart home devices on their own home networks. Between April 10, 2019 and January 21, 2020, 5,404 users have installed IoT Inspector, allowing us to collect labeled network traffic from 54,094 smart home devices. At the time of publication, IoT Inspector is still gaining users and collecting data from more devices. We demonstrate how this data enables new research into smart homes through two case studies focused on security and privacy. First, we find that many device vendors, including Amazon and Google, use outdated TLS versions and send unencrypted traffic, sometimes to advertising and tracking services. Second, we discover that smart TVs from at least 10 vendors communicated with advertising and tracking services. Finally, we find widespread cross-border communications, sometimes unencrypted, between devices and Internet services that are located in countries with potentially poor privacy practices. To facilitate future reproducible research in smart homes, we will release the IoT Inspector data to the public.},
journal = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
month = jun,
articleno = {46},
numpages = {21},
keywords = {smart home, security, privacy, network measurement, Internet-of-Things}
}

@article{10.1145/3130916,
author = {Goncalves, Jorge and Hosio, Simo and van Berkel, Niels and Ahmed, Furqan and Kostakos, Vassilis},
title = {CrowdPickUp: Crowdsourcing Task Pickup in the Wild},
year = {2017},
issue_date = {September 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {1},
number = {3},
url = {https://doi.org/10.1145/3130916},
doi = {10.1145/3130916},
abstract = {We develop and evaluate a new ubiquitous crowdsourcing platform called CrowdPickUp, that combines the advantages of mobile and situated crowdsourcing to overcome their respective limitations. In a 19-day long field study with 70 participants, we evaluate the quality of work that CrowdPickUp produces. In particular, we measure quality in terms of worker performance in a variety of tasks (requiring local knowledge, location-based, general) while using a number of different quality control mechanisms, and also capture workers’ perceptions of the platform. Our findings show that workers of CrowdPickUp contributed data of comparable quality to previously presented crowdsourcing deployments while at the same time allowing for a wide breadth of tasks to be deployed. Finally, we offer insights towards the continued exploration of this research agenda.},
journal = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
month = sep,
articleno = {51},
numpages = {22},
keywords = {worker performance, ubiquitous crowdsourcing, tasks, situated crowdsourcing, mobile crowdsourcing, location-based, local knowledge, crowdsourcing}
}

@article{10.1145/3415181,
author = {Hettiachchi, Danula and van Berkel, Niels and Kostakos, Vassilis and Goncalves, Jorge},
title = {CrowdCog: A Cognitive Skill based System for Heterogeneous Task Assignment and Recommendation in Crowdsourcing},
year = {2020},
issue_date = {October 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {4},
number = {CSCW2},
url = {https://doi.org/10.1145/3415181},
doi = {10.1145/3415181},
abstract = {While crowd workers typically complete a variety of tasks in crowdsourcing platforms, there is no widely accepted method to successfully match workers to different types of tasks. Researchers have considered using worker demographics, behavioural traces, and prior task completion records to optimise task assignment. However, optimum task assignment remains a challenging research problem due to limitations of proposed approaches, which in turn can have a significant impact on the future of crowdsourcing. We present 'CrowdCog', an online dynamic system that performs both task assignment and task recommendations, by relying on fast-paced online cognitive tests to estimate worker performance across a variety of tasks. Our work extends prior work that highlights the effect of workers' cognitive ability on crowdsourcing task performance. Our study, deployed on Amazon Mechanical Turk, involved 574 workers and 983 HITs that span across four typical crowd tasks (Classification, Counting, Transcription, and Sentiment Analysis). Our results show that both our assignment method and recommendation method result in a significant performance increase (5\% to 20\%) as compared to a generic or random task assignment. Our findings pave the way for the use of quick cognitive tests to provide robust recommendations and assignments to crowd workers.},
journal = {Proc. ACM Hum.-Comput. Interact.},
month = oct,
articleno = {110},
numpages = {22},
keywords = {dynamic task assignment, crowdsourcing, cognitive abilities}
}

@article{10.1109/TNET.2015.2421897,
author = {Yang, Dejun and Xue, Guoliang and Fang, Xi and Tang, Jian},
title = {Incentive mechanisms for crowdsensing: crowdsourcing with smartphones},
year = {2016},
issue_date = {June 2016},
publisher = {IEEE Press},
volume = {24},
number = {3},
issn = {1063-6692},
url = {https://doi.org/10.1109/TNET.2015.2421897},
doi = {10.1109/TNET.2015.2421897},
abstract = {Smartphones are programmable and equipped with a set of cheap but powerful embedded sensors, such as accelerometer, digital compass, gyroscope, GPS, microphone, and camera. These sensors can collectively monitor a diverse range of human activities and the surrounding environment. Crowdsensing is a new paradigm which takes advantage of the pervasive smartphones to sense, collect, and analyze data beyond the scale of what was previously possible. With the crowdsensing system, a crowdsourcer can recruit smartphone users to provide sensing service. Existing crowdsensing applications and systems lack good incentive mechanisms that can attract more user participation. To address this issue, we design incentive mechanisms for crowdsensing. We consider two system models: the crowdsourcer-centric model where the crowdsourcer provides a reward shared by participating users, and the user-centric model where users have more control over the payment they will receive. For the crowdsourcer-centric model, we design an incentive mechanism using a Stackelberg game, where the crowdsourcer is the leader while the users are the followers. We show how to compute the unique Stackelberg Equilibrium, at which the utility of the crowdsourcer is maximized, and none of the users can improve its utility by unilaterally deviating from its current strategy. For the user-centric model, we design an auction-based incentive mechanism, which is computationally efficient, individually rational, profitable, and truthful. Through extensive simulations, we evaluate the performance and validate the theoretical properties of our incentive mechanisms.},
journal = {IEEE/ACM Trans. Netw.},
month = jun,
pages = {1732–1744},
numpages = {13},
keywords = {incentive mechanism, crowdsourcing, crowdsensing, Stackelberg game}
}

@article{10.14778/3137765.3137821,
author = {Jonathan, Christopher and Mokbel, Mohamed F.},
title = {A demonstration of stella: a crowdsourcing-based geotagging framework},
year = {2017},
issue_date = {August 2017},
publisher = {VLDB Endowment},
volume = {10},
number = {12},
issn = {2150-8097},
url = {https://doi.org/10.14778/3137765.3137821},
doi = {10.14778/3137765.3137821},
abstract = {This paper demonstrates Stella; an efficient crowdsourcing-based geotagging framework for any types of objects. In this demonstration, we showcase the effectiveness of Stella in geotagging images via two different scenarios: (1) we provide a graphical interface to show the process of a geotagging process that have been done by using Amazon Mechanical Turk, (2) we seek help from the conference attendees to propose an image to be geotagged or to help us geotag an image by using our application during the demonstration period. At the end of the demonstration period, we will show the geotagging result.},
journal = {Proc. VLDB Endow.},
month = aug,
pages = {1969–1972},
numpages = {4}
}

@article{10.1145/2661229.2661287,
author = {Zhu, Jun-Yan and Agarwala, Aseem and Efros, Alexei A. and Shechtman, Eli and Wang, Jue},
title = {Mirror mirror: crowdsourcing better portraits},
year = {2014},
issue_date = {November 2014},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {33},
number = {6},
issn = {0730-0301},
url = {https://doi.org/10.1145/2661229.2661287},
doi = {10.1145/2661229.2661287},
abstract = {We describe a method for providing feedback on portrait expressions, and for selecting the most attractive expressions from large video/photo collections. We capture a video of a subject's face while they are engaged in a task designed to elicit a range of positive emotions. We then use crowdsourcing to score the captured expressions for their attractiveness. We use these scores to train a model that can automatically predict attractiveness of different expressions of a given person. We also train a cross-subject model that evaluates portrait attractiveness of novel subjects and show how it can be used to automatically mine attractive photos from personal photo collections. Furthermore, we show how, with a little bit ($5-worth) of extra crowdsourcing, we can substantially improve the cross-subject model by "fine-tuning" it to a new individual using active learning. Finally, we demonstrate a training app that helps people learn how to mimic their best expressions.},
journal = {ACM Trans. Graph.},
month = nov,
articleno = {234},
numpages = {12},
keywords = {portraits, crowdsourcing, aesthetic visual quality assessment}
}

@article{10.1145/2717513,
author = {Hara, Kotaro and Azenkot, Shiri and Campbell, Megan and Bennett, Cynthia L. and Le, Vicki and Pannella, Sean and Moore, Robert and Minckler, Kelly and Ng, Rochelle H. and Froehlich, Jon E.},
title = {Improving Public Transit Accessibility for Blind Riders by Crowdsourcing Bus Stop Landmark Locations with Google Street View: An Extended Analysis},
year = {2015},
issue_date = {March 2015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {6},
number = {2},
issn = {1936-7228},
url = {https://doi.org/10.1145/2717513},
doi = {10.1145/2717513},
abstract = {Low-vision and blind bus riders often rely on known physical landmarks to help locate and verify bus stop locations (e.g., by searching for an expected shelter, bench, or newspaper bin). However, there are currently few, if any, methods to determine this information a priori via computational tools or services. In this article, we introduce and evaluate a new scalable method for collecting bus stop location and landmark descriptions by combining online crowdsourcing and Google Street View (GSV). We conduct and report on three studies: (i) a formative interview study of 18 people with visual impairments to inform the design of our crowdsourcing tool, (ii) a comparative study examining differences between physical bus stop audit data and audits conducted virtually with GSV, and (iii) an online study of 153 crowd workers on Amazon Mechanical Turk to examine the feasibility of crowdsourcing bus stop audits using our custom tool with GSV. Our findings reemphasize the importance of landmarks in nonvisual navigation, demonstrate that GSV is a viable bus stop audit dataset, and show that minimally trained crowd workers can find and identify bus stop landmarks with 82.5\% accuracy across 150 bus stop locations (87.3\% with simple quality control).},
journal = {ACM Trans. Access. Comput.},
month = mar,
articleno = {5},
numpages = {23},
keywords = {remote data collection, low-vision and blind users, bus stop auditing, accessible bus stops, Mechanical Turk, Google Street View, Crowdsourcing accessibility}
}

@article{10.1145/3364697,
author = {Dimri, Anuj and Singh, Harsimran and Aggarwal, Naveen and Raman, Bhaskaran and Ramakrishnan, K. K. and Bansal, Divya},
title = {BaroSense: Using Barometer for Road Traffic Congestion Detection and Path Estimation with Crowdsourcing},
year = {2019},
issue_date = {February 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {16},
number = {1},
issn = {1550-4859},
url = {https://doi.org/10.1145/3364697},
doi = {10.1145/3364697},
abstract = {Traffic congestion on urban roadways is a serious problem requiring novel ways to detect and mitigate it. Determining the routes that lead to the traffic congestion segment is also vital in devising mitigation strategies. Further, crowdsourcing this information allows for use of these strategies quickly and in places where infrastructure is not available. In this work, we present an unconventional method, using the barometer sensor of mobile phones to (a) detect road traffic congestion and (b) estimate the paths that lead to the congested road segment. We make the observation that roads are not completely flat and very often, altitude varies along the road. The barometer sensor chips are sensitive enough to measure these variations and consume very little energy of the mobile phone, compared to other sensors such as the GPS or accelerometer. We devise a feature set to map the rate of change of this altitude as the user moves into activities characterized as “still” and “motion,” which are further used by the traffic congestion detection algorithm (RoadSphygmo) to classify the group of users as being in “moving,” “congestion,” &nbsp;or “stuck” states. To estimate the paths that lead to the congested road segment, we compare the user’s barometer sensor readings with a pre-stored road signature of barometer values using Dynamic Time Warping (DTW). We show that by using correlation of barometer sensor values, we can determine if users are in the same vehicle. We crowdsource this information from multiple mobile phones and use majority voting technique to improve the accuracy of traffic congestion detection and path estimation. We find a significant increase in the accuracies using crowdsourced information as compared to individual mobile phones. Further, we show that we can use barometer sensor for other applications such as bus occupancy, boarding/deboarding of a vehicle, and so on. The validation of the state determined by RoadSphygmo is done by comparing it with average GPS speed calculated during the same time period. The path estimation is validated over different intersections and considering various cases of commuter travel. The results obtained are promising and show that the traffic state determination and the estimation of the path taken by the commuter can achieve high accuracy.},
journal = {ACM Trans. Sen. Netw.},
month = nov,
articleno = {4},
numpages = {24},
keywords = {traffic congestion detection, smartphones, path estimation, crowdsourcing, barometer sensor, Activity recognition}
}

@article{10.1145/3363450,
author = {Li, Wei and Chen, Haiquan and Ku, Wei-Shinn and Qin, Xiao},
title = {Turbo-GTS: A Fast Framework of Optimizing Task Throughput for Large-Scale Mobile Crowdsourcing},
year = {2020},
issue_date = {March 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {6},
number = {1},
issn = {2374-0353},
url = {https://doi.org/10.1145/3363450},
doi = {10.1145/3363450},
abstract = {In mobile crowdsourcing, workers are financially motivated to perform as many self-selected tasks as possible to maximize their revenue. Unfortunately, the existing task scheduling approaches in mobile crowdsourcing fail to consider task execution duration and do not scale for massive tasks and large geographic areas. In this article, we propose a novel framework, Turbo-GTS, in support of large-scale geo-task scheduling, with the objective of identifying an optimal task assignment for each worker to maximize the total number of tasks that can be completed for an entire worker group, given the geographic locations of each task and each worker. Since the exact solution to the geo-task scheduling problem is computationally intractable, we first propose two sub-optimal approaches (least cost neighbor with particle filtering and non-urgency degree particle filtering with iterative clustering) based on particle filtering and DBSCAN for the single-worker geo-task scheduling problem. We then extend our work to solve the multi-worker geo-task scheduling problem by proposing two space partitioning-based methods (QT-NNH and QT-NUD), which leverage point-region quadtree to ensure workload balancing. The effectiveness and efficiency of the four proposed approximate solutions are verified by our extensive experiments using both real and synthetic data. Compared to state-of-the-art approaches, our proposed solutions are able to return a higher number of completed tasks for the worker group while reducing the computation cost by up to three orders of magnitude when coping with massive tasks distributed in large geographic areas.},
journal = {ACM Trans. Spatial Algorithms Syst.},
month = jan,
articleno = {1},
numpages = {29},
keywords = {quadtree, particle filtering, Mobile crowdsourcing}
}

@article{10.1145/3237188,
author = {Song, Jean Y. and Fok, Raymond and Kim, Juho and Lasecki, Walter S.},
title = {FourEyes: Leveraging Tool Diversity as a Means to Improve Aggregate Accuracy in Crowdsourcing},
year = {2019},
issue_date = {March 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {10},
number = {1},
issn = {2160-6455},
url = {https://doi.org/10.1145/3237188},
doi = {10.1145/3237188},
abstract = {Crowdsourcing is a common means of collecting image segmentation training data for use in a variety of computer vision applications. However, designing accurate crowd-powered image segmentation systems is challenging, because defining object boundaries in an image requires significant fine motor skills and hand-eye coordination, which makes these tasks error-prone. Typically, special segmentation tools are created and then answers from multiple workers are aggregated to generate more accurate results. However, individual tool designs can bias how and where people make mistakes, resulting in shared errors that remain even after aggregation. In this article, we introduce a novel crowdsourcing approach that leverages tool diversity as a means of improving aggregate crowd performance. Our idea is that given a diverse set of tools, answer aggregation done across tools can help improve the collective performance by offsetting systematic biases induced by the individual tools themselves. To demonstrate the effectiveness of the proposed approach, we design four different tools and present FourEyes, a crowd-powered image segmentation system that uses aggregation across different tools. We then conduct a series of studies that evaluate different aggregation conditions and show that using multiple tools can significantly improve aggregate accuracy. Furthermore, we investigate the idea of applying post-processing for multi-tool aggregation in terms of correction mechanism. We introduce a novel region-based method for synthesizing more accurate bounds for image segmentation tasks through averaging surrounding annotations. In addition, we explore the effect of adjusting the threshold parameter of an EM-based aggregation method. Our results suggest that not only the individual tool’s design, but also the correction mechanism, can affect the performance of multi-tool aggregation. This article extends a work presented at ACM IUI 2018&nbsp;[46] by providing a novel region-based error-correction method and additional in-depth evaluation of the proposed approach.},
journal = {ACM Trans. Interact. Intell. Syst.},
month = aug,
articleno = {3},
numpages = {30},
keywords = {tool diversity, semantic image segmentation, multi-tool aggregation, human computation, computer vision, Crowdsourcing}
}

@article{10.1145/3449193,
author = {Yang, Kexin Bella and Nagashima, Tomohiro and Yao, Junhui and Williams, Joseph Jay and Holstein, Kenneth and Aleven, Vincent},
title = {Can Crowds Customize Instructional Materials with Minimal Expert Guidance? Exploring Teacher-guided Crowdsourcing for Improving Hints in an AI-based Tutor},
year = {2021},
issue_date = {April 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {5},
number = {CSCW1},
url = {https://doi.org/10.1145/3449193},
doi = {10.1145/3449193},
abstract = {AI-based educational technologies may be most welcome in classrooms when they align with teachers' goals, preferences, and instructional practices. Teachers, however, have scarce time to make such customizations themselves. How might the crowd be leveraged to help time-strapped teachers? Crowdsourcing pipelines have traditionally focused on content generation. It is an open question how a pipeline might be designed so the crowd can succeed in a revision/customization task. In this paper, we explore an initial version of a teacher-guided crowdsourcing pipeline designed to improve the adaptive math hints of an AI-based tutoring system so they fit teachers' preferences, while requiring minimal expert guidance. In two experiments involving 144 math teachers and 481 crowdworkers, we found that such an expert-guided revision pipeline could save experts' time and produce better crowd-revised hints (in terms of teacher satisfaction) than two comparison conditions. The revised hints however, did not improve on the existing hints in the AI tutor, which were carefully-written but still have room for improvement and customization. Further analysis revealed that the main challenge for crowdworkers may lie in understanding teachers' brief written comments and implementing them in the form of effective edits, without introducing new problems. We also found that teachers preferred their own revisions over other sources of hints, and exhibited varying preferences for hints. Overall, the results confirm that there is a clear need for customizing hints to individual teachers' preferences. They also highlight the need for more elaborate scaffolds so the crowd can have specific knowledge of the requirements that teachers have for hints. The study represents a first exploration in the literature of how to support crowds with minimal expert guidance in revising and customizing instructional materials.},
journal = {Proc. ACM Hum.-Comput. Interact.},
month = apr,
articleno = {119},
numpages = {24},
keywords = {ai in education, expert-facilitated crowdsourcing, human computation, learning at scale, teachersourcing}
}

@article{10.1145/3134724,
author = {Retelny, Daniela and Bernstein, Michael S. and Valentine, Melissa A.},
title = {No Workflow Can Ever Be Enough: How Crowdsourcing Workflows Constrain Complex Work},
year = {2017},
issue_date = {November 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {1},
number = {CSCW},
url = {https://doi.org/10.1145/3134724},
doi = {10.1145/3134724},
abstract = {The dominant crowdsourcing infrastructure today is the workflow, which decomposes goals into small independent tasks. However, complex goals such as design and engineering have remained stubbornly difficult to achieve with crowdsourcing workflows. Is this due to a lack of imagination, or a more fundamental limit? This paper explores this question through in-depth case studies of 22 workers across six workflow-based crowd teams, each pursuing a complex and interdependent web development goal. We used an inductive mixed method approach to analyze behavior trace data, chat logs, survey responses and work artifacts to understand how workers enacted and adapted the crowdsourcing workflows. Our results indicate that workflows served as useful coordination artifacts, but in many cases critically inhibited crowd workers from pursuing real-time adaptations to their work plans. However, the CSCW and organizational behavior literature argues that all sufficiently complex goals require open-ended adaptation. If complex work requires adaptation but traditional static crowdsourcing workflows can't support it, our results suggest that complex work may remain a fundamental limitation of workflow-based crowdsourcing infrastructures.},
journal = {Proc. ACM Hum.-Comput. Interact.},
month = dec,
articleno = {89},
numpages = {23},
keywords = {workflows, human computation, crowdsourcing}
}

@article{10.5555/2946645.3053447,
author = {Shah, Nihar B. and Zhou, Dengyong},
title = {Double or nothing: multiplicative incentive mechanisms for crowdsourcing},
year = {2016},
issue_date = {January 2016},
publisher = {JMLR.org},
volume = {17},
number = {1},
issn = {1532-4435},
abstract = {Crowdsourcing has gained immense popularity in machine learning applications for obtaining large amounts of labeled data. Crowdsourcing is cheap and fast, but suffers from the problem of low-quality data. To address this fundamental challenge in crowdsourcing, we propose a simple payment mechanism to incentivize workers to answer only the questions that they are sure of and skip the rest. We show that surprisingly, under a mild and natural "no-free-lunch" requirement, this mechanism is the one and only incentive-compatible payment mechanism possible. We also show that among all possible incentive-compatible mechanisms (that may or may not satisfy no-free-lunch), our mechanism makes the smallest possible payment to spammers. We further extend our results to a more general setting in which workers are required to provide a quantized confidence for each question. Interestingly, this unique mechanism takes a "multiplicative" form. The simplicity of the mechanism is an added benefit. In preliminary experiments involving over 900 worker-task pairs, we observe a significant drop in the error rates under this unique mechanism for the same or lower monetary expenditure.},
journal = {J. Mach. Learn. Res.},
month = jan,
pages = {5725–5776},
numpages = {52},
keywords = {supervised learning, proper scoring rules, mechanism design, high-quality labels, crowdsourcing}
}

@article{10.14778/2824032.2824062,
author = {Haas, Daniel and Ansel, Jason and Gu, Lydia and Marcus, Adam},
title = {Argonaut: macrotask crowdsourcing for complex data processing},
year = {2015},
issue_date = {August 2015},
publisher = {VLDB Endowment},
volume = {8},
number = {12},
issn = {2150-8097},
url = {https://doi.org/10.14778/2824032.2824062},
doi = {10.14778/2824032.2824062},
abstract = {Crowdsourced workflows are used in research and industry to solve a variety of tasks. The databases community has used crowd workers in query operators/optimization and for tasks such as entity resolution. Such research utilizes microtasks where crowd workers are asked to answer simple yes/no or multiple choice questions with little training. Typically, microtasks are used with voting algorithms to combine redundant responses from multiple crowd workers to achieve result quality. Microtasks are powerful, but fail in cases where larger context (e.g., domain knowledge) or significant time investment is needed to solve a problem, for example in large-document structured data extraction.In this paper, we consider context-heavy data processing tasks that may require many hours of work, and refer to such tasks as macrotasks. Leveraging the infrastructure and worker pools of existing crowdsourcing platforms, we automate macrotask scheduling, evaluation, and pay scales. A key challenge in macrotask-powered work, however, is evaluating the quality of a worker's output, since ground truth is seldom available and redundancy-based quality control schemes are impractical. We present Argonaut, a framework that improves macrotask powered work quality using a hierarchical review. Argonaut uses a predictive model of worker quality to select trusted workers to perform review, and a separate predictive model of task quality to decide which tasks to review. Finally, Argonaut can identify the ideal trade-off between a single phase of review and multiple phases of review given a constrained review budget in order to maximize overall output quality. We evaluate an industrial use of Argonaut to power a structured data extraction pipeline that has utilized over half a million hours of crowd worker input to complete millions of macrotasks. We show that Argonaut can capture up to 118\% more errors than random spot-check reviews in review budget-constrained environments with up to two review layers.},
journal = {Proc. VLDB Endow.},
month = aug,
pages = {1642–1653},
numpages = {12}
}

@article{10.14778/2733004.2733047,
author = {Chen, Zhao and Fu, Rui and Zhao, Ziyuan and Liu, Zheng and Xia, Leihao and Chen, Lei and Cheng, Peng and Cao, Caleb Chen and Tong, Yongxin and Zhang, Chen Jason},
title = {gMission: a general spatial crowdsourcing platform},
year = {2014},
issue_date = {August 2014},
publisher = {VLDB Endowment},
volume = {7},
number = {13},
issn = {2150-8097},
url = {https://doi.org/10.14778/2733004.2733047},
doi = {10.14778/2733004.2733047},
abstract = {As one of the successful forms of using Wisdom of Crowd, crowdsourcing, has been widely used for many human intrinsic tasks, such as image labeling, natural language understanding, market predication and opinion mining. Meanwhile, with advances in pervasive technology, mobile devices, such as mobile phones and tablets, have become extremely popular. These mobile devices can work as sensors to collect multimedia data(audios, images and videos) and location information. This power makes it possible to implement the new crowdsourcing mode: spatial crowdsourcing. In spatial crowdsourcing, a requester can ask for resources related a specific location, the mobile users who would like to take the task will travel to that place and get the data. Due to the rapid growth of mobile device uses, spatial crowdsourcing is likely to become more popular than general crowdsourcing, such as Amazon Turk and Crowdflower. However, to implement such a platform, effective and efficient solutions for worker incentives, task assignment, result aggregation and data quality control must be developed.In this demo, we will introduce gMission, a general spatial crowdsourcing platform, which features with a collection of novel techniques, including geographic sensing, worker detection, and task recommendation. We introduce the sketch of system architecture and illustrate scenarios via several case analysis.},
journal = {Proc. VLDB Endow.},
month = aug,
pages = {1629–1632},
numpages = {4}
}

@article{10.14778/3025111.3025118,
author = {Zheng, Yudian and Li, Guoliang and Cheng, Reynold},
title = {DOCS: a domain-aware crowdsourcing system using knowledge bases},
year = {2016},
issue_date = {November 2016},
publisher = {VLDB Endowment},
volume = {10},
number = {4},
issn = {2150-8097},
url = {https://doi.org/10.14778/3025111.3025118},
doi = {10.14778/3025111.3025118},
abstract = {Crowdsourcing is a new computing paradigm that harnesses human effort to solve computer-hard problems, such as entity resolution and photo tagging. The crowd (or workers) have diverse qualities and it is important to effectively model a worker's quality. Most of existing worker models assume that workers have the same quality on different tasks. In practice, however, tasks belong to a variety of diverse domains, and workers have different qualities on different domains. For example, a worker who is a basketball fan should have better quality for the task of labeling a photo related to 'Stephen Curry' than the one related to 'Leonardo DiCaprio'. In this paper, we study how to leverage domain knowledge to accurately model a worker's quality. We examine using knowledge base (KB), e.g., Wikipedia and Freebase, to detect the domains of tasks and workers. We develop Domain Vector Estimation, which analyzes the domains of a task with respect to the KB. We also study Truth Inference, which utilizes the domain-sensitive worker model to accurately infer the true answer of a task. We design an Online Task Assignment algorithm, which judiciously and efficiently assigns tasks to appropriate workers. To implement these solutions, we have built DOCS, a system deployed on the Amazon Mechanical Turk. Experiments show that DOCS performs much better than the state-of-the-art approaches.},
journal = {Proc. VLDB Endow.},
month = nov,
pages = {361–372},
numpages = {12}
}

@article{10.14778/2350229.2350263,
author = {Wang, Jiannan and Kraska, Tim and Franklin, Michael J. and Feng, Jianhua},
title = {CrowdER: crowdsourcing entity resolution},
year = {2012},
issue_date = {July 2012},
publisher = {VLDB Endowment},
volume = {5},
number = {11},
issn = {2150-8097},
url = {https://doi.org/10.14778/2350229.2350263},
doi = {10.14778/2350229.2350263},
abstract = {Entity resolution is central to data integration and data cleaning. Algorithmic approaches have been improving in quality, but remain far from perfect. Crowdsourcing platforms offer a more accurate but expensive (and slow) way to bring human insight into the process. Previous work has proposed batching verification tasks for presentation to human workers but even with batching, a human-only approach is infeasible for data sets of even moderate size, due to the large numbers of matches to be tested. Instead, we propose a hybrid human-machine approach in which machines are used to do an initial, coarse pass over all the data, and people are used to verify only the most likely matching pairs. We show that for such a hybrid system, generating the minimum number of verification tasks of a given size is NP-Hard, but we develop a novel two-tiered heuristic approach for creating batched tasks. We describe this method, and present the results of extensive experiments on real data sets using a popular crowdsourcing platform. The experiments show that our hybrid approach achieves both good efficiency and high accuracy compared to machine-only or human-only alternatives.},
journal = {Proc. VLDB Endow.},
month = jul,
pages = {1483–1494},
numpages = {12}
}

@article{10.1145/3134697,
author = {Kou, Yubo and Gui, Xinning and Zhang, Shaozeng and Nardi, Bonnie},
title = {Managing Disruptive Behavior through Non-Hierarchical Governance: Crowdsourcing in League of Legends and Weibo},
year = {2017},
issue_date = {November 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {1},
number = {CSCW},
url = {https://doi.org/10.1145/3134697},
doi = {10.1145/3134697},
abstract = {Disruptive behaviors such as flaming and vandalism have been part of the Internet since its beginning. Various models of hierarchical governance have been established and managed in different online venues, with both successes and failures. Recently, a new model of non-hierarchical governance has emerged using crowdsourcing technology to allow an online community to manage itself. How do people view and work with non-hierarchical governance? In this paper, we present an interview study with people from two sites: the video game League of Legends and Weibo, a microblogging site in China. We found that people were passionate about participation in crowdsourcing, but at the same time, struggled with the system, and acted beyond their designated role within the system. We derive implications for designing online non-hierarchical governance from our research.},
journal = {Proc. ACM Hum.-Comput. Interact.},
month = dec,
articleno = {62},
numpages = {17},
keywords = {weibo, troll, social media, online game, online community, non-hierarchical governance, league of legends, harassment, governance, deviant behavior, crowdsourced governance, big data, anti-social behavior}
}

@article{10.1145/3131275,
author = {Kim, Nam Wook and Bylinskii, Zoya and Borkin, Michelle A. and Gajos, Krzysztof Z. and Oliva, Aude and Durand, Fredo and Pfister, Hanspeter},
title = {BubbleView: An Interface for Crowdsourcing Image Importance Maps and Tracking Visual Attention},
year = {2017},
issue_date = {October 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {24},
number = {5},
issn = {1073-0516},
url = {https://doi.org/10.1145/3131275},
doi = {10.1145/3131275},
abstract = {In this article, we present BubbleView, an alternative methodology for eye tracking using discrete mouse clicks to measure which information people consciously choose to examine. BubbleView is a mouse-contingent, moving-window interface in which participants are presented with a series of blurred images and click to reveal “bubbles” -- small, circular areas of the image at original resolution, similar to having a confined area of focus like the eye fovea. Across 10 experiments with 28 different parameter combinations, we evaluated BubbleView on a variety of image types: information visualizations, natural images, static webpages, and graphic designs, and compared the clicks to eye fixations collected with eye-trackers in controlled lab settings. We found that BubbleView clicks can both (i) successfully approximate eye fixations on different images, and (ii) be used to rank image and design elements by importance. BubbleView is designed to collect clicks on static images, and works best for defined tasks such as describing the content of an information visualization or measuring image importance. BubbleView data is cleaner and more consistent than related methodologies that use continuous mouse movements. Our analyses validate the use of mouse-contingent, moving-window methodologies as approximating eye fixations for different image and task types.},
journal = {ACM Trans. Comput.-Hum. Interact.},
month = nov,
articleno = {36},
numpages = {40},
keywords = {websites, visual attention, saliency, natural scenes, mouse-contingent interface, information visualizations, image importance, graphic designs, eye tracking, crowdsourcing, Human vision}
}

@article{10.5555/2946645.3007055,
author = {Zhang, Yuchen and Chen, Xi and Zhou, Dengyong and Jordan, Michael I.},
title = {Spectral methods meet EM: a provably optimal algorithm for crowdsourcing},
year = {2016},
issue_date = {January 2016},
publisher = {JMLR.org},
volume = {17},
number = {1},
issn = {1532-4435},
abstract = {Crowdsourcing is a popular paradigm for effectively collecting labels at low cost. The Dawid-Skene estimator has been widely used for inferring the true labels from the noisy labels provided by non-expert crowdsourcing workers. However, since the estimator maximizes a non-convex log-likelihood function, it is hard to theoretically justify its performance. In this paper, we propose a two-stage efficient algorithm for multi-class crowd labeling problems. The first stage uses the spectral method to obtain an initial estimate of parameters. Then the second stage refines the estimation by optimizing the objective function of the Dawid-Skene estimator via the EM algorithm. We show that our algorithm achieves the optimal convergence rate up to a logarithmic factor. We conduct extensive experiments on synthetic and real datasets. Experimental results demonstrate that the proposed algorithm is comparable to the most accurate empirical approach, while outperforming several other recently proposed methods.},
journal = {J. Mach. Learn. Res.},
month = jan,
pages = {3537–3580},
numpages = {44},
keywords = {spectral methods, non-convex optimization, minimax rate, crowdsourcing, EM, Dawid-Skene model}
}

@article{10.14778/2824032.2824109,
author = {Chu, Xu and Morcos, John and Ilyas, Ihab F. and Ouzzani, Mourad and Papotti, Paolo and Tang, Nan and Ye, Yin},
title = {KATARA: reliable data cleaning with knowledge bases and crowdsourcing},
year = {2015},
issue_date = {August 2015},
publisher = {VLDB Endowment},
volume = {8},
number = {12},
issn = {2150-8097},
url = {https://doi.org/10.14778/2824032.2824109},
doi = {10.14778/2824032.2824109},
abstract = {Data cleaning with guaranteed reliability is hard to achieve without accessing external sources, since the truth is not necessarily discoverable from the data at hand. Furthermore, even in the presence of external sources, mainly knowledge bases and humans, effectively leveraging them still faces many challenges, such as aligning heterogeneous data sources and decomposing a complex task into simpler units that can be consumed by humans. We present Katara, a novel end-to-end data cleaning system powered by knowledge bases and crowdsourcing. Given a table, a kb, and a crowd, Katara (i) interprets the table semantics w.r.t. the given kb; (ii) identifies correct and wrong data; and (iii) generates top-k possible repairs for the wrong data. Users will have the opportunity to experience the following features of Katara: (1) Easy specification: Users can define a Katara job with a browser-based specification; (2) Pattern validation: Users can help the system to resolve the ambiguity of different table patterns (i.e., table semantics) discovered by Katara; (3) Data annotation: Users can play the role of internal crowd workers, helping Katara annotate data. Moreover, Katara will visualize the annotated data as correct data validated by the kb, correct data jointly validated by the kb and the crowd, or erroneous tuples along with their possible repairs.},
journal = {Proc. VLDB Endow.},
month = aug,
pages = {1952–1955},
numpages = {4}
}

@article{10.5555/3122009.3242050,
author = {Vaughan, Jennifer Wortman},
title = {Making better use of the crowd: how crowdsourcing can advance machine learning research},
year = {2017},
issue_date = {January 2017},
publisher = {JMLR.org},
volume = {18},
number = {1},
issn = {1532-4435},
abstract = {This survey provides a comprehensive overview of the landscape of crowdsourcing research, targeted at the machine learning community. We begin with an overview of the ways in which crowdsourcing can be used to advance machine learning research, focusing on four application areas: 1) data generation, 2) evaluation and debugging of models, 3) hybrid intelligence systems that leverage the complementary strengths of humans and machines to expand the capabilities of AI, and 4) crowdsourced behavioral experiments that improve our understanding of how humans interact with machine learning systems and technology more broadly. We next review the extensive literature on the behavior of crowdworkers themselves. This research, which explores the prevalence of dishonesty among crowdworkers, how workers respond to both monetary incentives and intrinsic forms of motivation, and how crowdworkers interact with each other, has immediate implications that we distill into best practices that researchers should follow when using crowdsourcing in their own research. We conclude with a discussion of additional tips and best practices that are crucial to the success of any project that uses crowdsourcing, but rarely mentioned in the literature.},
journal = {J. Mach. Learn. Res.},
month = jan,
pages = {7026–7071},
numpages = {46},
keywords = {model evaluation, mechanical turk, incentives, hybrid intelligence, data generation, crowdsourcing, behavioral experiments}
}

@article{10.14778/2336664.2336676,
author = {Liu, Xuan and Lu, Meiyu and Ooi, Beng Chin and Shen, Yanyan and Wu, Sai and Zhang, Meihui},
title = {CDAS: a crowdsourcing data analytics system},
year = {2012},
issue_date = {June 2012},
publisher = {VLDB Endowment},
volume = {5},
number = {10},
issn = {2150-8097},
url = {https://doi.org/10.14778/2336664.2336676},
doi = {10.14778/2336664.2336676},
abstract = {Some complex problems, such as image tagging and natural language processing, are very challenging for computers, where even state-of-the-art technology is yet able to provide satisfactory accuracy. Therefore, rather than relying solely on developing new and better algorithms to handle such tasks, we look to the crowdsourcing solution -- employing human participation -- to make good the shortfall in current technology. Crowdsourcing is a good supplement to many computer tasks. A complex job may be divided into computer-oriented tasks and human-oriented tasks, which are then assigned to machines and humans respectively.To leverage the power of crowdsourcing, we design and implement a Crowdsourcing Data Analytics System, CDAS. CDAS is a framework designed to support the deployment of various crowdsourcing applications. The core part of CDAS is a quality-sensitive answering model, which guides the crowdsourcing engine to process and monitor the human tasks. In this paper, we introduce the principles of our quality-sensitive model. To satisfy user required accuracy, the model guides the crowdsourcing query engine for the design and processing of the corresponding crowdsourcing jobs. It provides an estimated accuracy for each generated result based on the human workers' historical performances. When verifying the quality of the result, the model employs an online strategy to reduce waiting time. To show the effectiveness of the model, we implement and deploy two analytics jobs on CDAS, a twitter sentiment analytics job and an image tagging job. We use real Twitter and Flickr data as our queries respectively. We compare our approaches with state-of-the-art classification and image annotation techniques. The results show that the human-assisted methods can indeed achieve a much higher accuracy. By embedding the quality-sensitive model into crowdsourcing query engine, we effectively reduce the processing cost while maintaining the required query answer quality.},
journal = {Proc. VLDB Endow.},
month = jun,
pages = {1040–1051},
numpages = {12}
}

@article{10.14778/2367502.2367555,
author = {Park, Hyunjung and Garcia-Molina, Hector and Pang, Richard and Polyzotis, Neoklis and Parameswaran, Aditya and Widom, Jennifer},
title = {Deco: a system for declarative crowdsourcing},
year = {2012},
issue_date = {August 2012},
publisher = {VLDB Endowment},
volume = {5},
number = {12},
issn = {2150-8097},
url = {https://doi.org/10.14778/2367502.2367555},
doi = {10.14778/2367502.2367555},
abstract = {Deco is a system that enables declarative crowdsourcing: answering SQL queries posed over data gathered from the crowd as well as existing relational data. Deco implements a novel push-pull hybrid execution model in order to support a flexible data model and a precise query semantics, while coping with the combination of latency, monetary cost, and uncertainty of crowdsourcing. We demonstrate Deco using two crowdsourcing platforms: Amazon Mechanical Turk and an in-house platform, to show how Deco provides a convenient means of collecting and querying crowdsourced data.},
journal = {Proc. VLDB Endow.},
month = aug,
pages = {1990–1993},
numpages = {4}
}

@article{10.5555/1747137.1747149,
author = {Gibson, Neal and Talburt, John},
title = {Hive: crowdsourcing education data},
year = {2010},
issue_date = {May 2010},
publisher = {Consortium for Computing Sciences in Colleges},
address = {Evansville, IN, USA},
volume = {25},
number = {5},
issn = {1937-4771},
abstract = {Data Driven Decision Making is a well-worn mantra in education, best personified by the enormous amount of data created by the yearly student assessments of literacy, math, and science mandated by No Child Left Behind. The U.S. Department of Education has recently begun giving states grants to create longitudinal data systems. The purpose of these grants is to help states organize and present these data in meaningful ways so that they can be used to improve student achievement. However, little research has been done on best practices on how educators should use these data. The Arkansas Department of Education initiated a year-long study on data use by educators and, based on this research, has developed an interactive data visualization application that incorporates social networking tools which allow educators to collaborate on data analysis.},
journal = {J. Comput. Sci. Coll.},
month = may,
pages = {72–78},
numpages = {7}
}

@article{10.14778/3067421.3067431,
author = {Jain, Ayush and Sarma, Akash Das and Parameswaran, Aditya and Widom, Jennifer},
title = {Understanding workers, developing effective tasks, and enhancing marketplace dynamics: a study of a large crowdsourcing marketplace},
year = {2017},
issue_date = {March 2017},
publisher = {VLDB Endowment},
volume = {10},
number = {7},
issn = {2150-8097},
url = {https://doi.org/10.14778/3067421.3067431},
doi = {10.14778/3067421.3067431},
abstract = {We conduct an experimental analysis of a dataset comprising over 27 million microtasks performed by over 70,000 workers issued to a large crowdsourcing marketplace between 2012--2016. Using this data---never before analyzed in an academic context---we shed light on three crucial aspects of crowdsourcing: (1) Task design---helping requesters understand what constitutes an effective task, and how to go about designing one; (2) Marketplace dynamics --- helping marketplace administrators and designers understand the interaction between tasks and workers, and the corresponding marketplace load; and (3) Worker behavior --- understanding worker attention spans, lifetimes, and general behavior, for the improvement of the crowdsourcing ecosystem as a whole.},
journal = {Proc. VLDB Endow.},
month = mar,
pages = {829–840},
numpages = {12}
}

@article{10.14778/2733004.2733024,
author = {Sun, Chong and Rampalli, Narasimhan and Yang, Frank and Doan, AnHai},
title = {Chimera: large-scale classification using machine learning, rules, and crowdsourcing},
year = {2014},
issue_date = {August 2014},
publisher = {VLDB Endowment},
volume = {7},
number = {13},
issn = {2150-8097},
url = {https://doi.org/10.14778/2733004.2733024},
doi = {10.14778/2733004.2733024},
abstract = {Large-scale classification is an increasingly critical Big Data problem. So far, however, very little has been published on how this is done in practice. In this paper we describe Chimera, our solution to classify tens of millions of products into 5000+ product types at WalmartLabs. We show that at this scale, many conventional assumptions regarding learning and crowdsourcing break down, and that existing solutions cease to work. We describe how Chimera employs a combination of learning, rules (created by in-house analysts), and crowdsourcing to achieve accurate, continuously improving, and cost-effective classification. We discuss a set of lessons learned for other similar Big Data systems. In particular, we argue that at large scales crowdsourcing is critical, but must be used in combination with learning, rules, and in-house analysts. We also argue that using rules (in conjunction with learning) is a must, and that more research attention should be paid to helping analysts create and manage (tens of thousands of) rules more effectively.},
journal = {Proc. VLDB Endow.},
month = aug,
pages = {1529–1540},
numpages = {12}
}

@article{10.1145/3130942,
author = {Liu, Ruilin and Yang, Yu and Kwak, Daehan and Zhang, Desheng and Iftode, Liviu and Nath, Badri},
title = {Your Search Path Tells Others Where to Park: Towards Fine-Grained Parking Availability Crowdsourcing Using Parking Decision Models},
year = {2017},
issue_date = {September 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {1},
number = {3},
url = {https://doi.org/10.1145/3130942},
doi = {10.1145/3130942},
abstract = {A main challenge faced by the state-of-the-art parking sensing systems is to infer the state of the spots not covered by participants’ parking/unparking events (called background availability) when the system penetration rate is limited. In this paper, we tackle this problem by exploring an empirical phenomenon that ignoring a spot along a driver’s parking search trajectory is likely due to the unavailability. However, complications caused by drivers’ preferences, e.g. ignoring the spots too far from the driver’s destination, have to be addressed based on human parking decisions. We build a model based on a dataset of more than 55,000 real parking decisions to predict the probability that a driver would take a spot, assuming the spot is available. Then, we present a crowdsourcing system, called ParkScan, which leverages the learned parking decision model in collaboration with the hidden Markov model to estimate background parking spot availability. We evaluated ParkScan with real-world data from both off-street scenarios (i.e., two public parking lots) and an on-street parking scenario (i.e., 35 urban blocks in Seattle). Both of the experiments showed that with a 5\% penetration rate, ParkScan reduces over 12.9\% of availability estimation errors for all the spots during parking peak hours, compared to the baseline using only the historical data. Also, even with a single participant driver, ParkScan cuts off at least 15\% of the estimation errors for the spots along the driver’s parking search trajectory.},
journal = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
month = sep,
articleno = {78},
numpages = {27},
keywords = {Parking, Mobile Sensing, Human Decision Modeling, Crowdsourcing}
}

@article{10.14778/2735479.2735482,
author = {Wu, Ting and Chen, Lei and Hui, Pan and Zhang, Chen Jason and Li, Weikai},
title = {Hear the whole story: towards the diversity of opinion in crowdsourcing markets},
year = {2015},
issue_date = {January 2015},
publisher = {VLDB Endowment},
volume = {8},
number = {5},
issn = {2150-8097},
url = {https://doi.org/10.14778/2735479.2735482},
doi = {10.14778/2735479.2735482},
abstract = {The recent surge in popularity of crowdsourcing has brought with it a new opportunity for engaging human intelligence in the process of data analysis. Crowdsourcing provides a fundamental mechanism for enabling online workers to participate in tasks that are either too difficult to be solved solely by a computer or too expensive to employ experts to perform. In the field of social science, four elements are required to form a wise crowd - Diversity of Opinion, Independence, Decentralization and Aggregation. However, while the other three elements are already studied and implemented in current crowdsourcing platforms, the 'Diversity of Opinion' has not been functionally enabled. In this paper, we address the algorithmic optimizations towards the diversity of opinion of crowdsourcing marketplaces.From a computational perspective, in order to build a wise crowd, we need to quantitatively modeling the diversity, and take it into consideration for constructing the crowd. In a crowdsourcing marketplace, we usually encounter two basic paradigms for worker selection: building a crowd to wait for tasks to come and selecting workers for a given task. Therefore, we propose our Similarity-driven Model (S-Model) and Task-driven Model (T-Model) for both of the paradigms. Under both of the models, we propose efficient and effective algorithms to enlist a budgeted number of workers, which have the optimal diversity. We have verified our solutions with extensive experiments on both synthetic datasets and real data sets.},
journal = {Proc. VLDB Endow.},
month = jan,
pages = {485–496},
numpages = {12}
}

@article{10.14778/2367502.2367537,
author = {Morishima, Atsuyuki and Shinagawa, Norihide and Mitsuishi, Tomomi and Aoki, Hideto and Fukusumi, Shun},
title = {CyLog/Crowd4U: a declarative platform for complex data-centric crowdsourcing},
year = {2012},
issue_date = {August 2012},
publisher = {VLDB Endowment},
volume = {5},
number = {12},
issn = {2150-8097},
url = {https://doi.org/10.14778/2367502.2367537},
doi = {10.14778/2367502.2367537},
abstract = {This demo presents a principled approach to the problems of data-centric human/machine computations with Crowd4U, a crowdsourcing platform equipped with a suite of tools for rapid development of crowdsourcing applications. Using the demo, we show that declarative database abstraction can be used as a powerful tool to design, implement, and analyze data-centric crowdsourcing applications. The power of Crowd4U comes from CyLog, a database abstraction that handles complex data-centric human/machine computations. CyLog is a Datalog-like language that incorporates a principled feedback system for humans at the language level so that the semantics of the computation not closed in machines can be defined based on the game theory. We believe that the demo clearly shows that database abstraction can be a promising basis for designing complex data-centric applications requiring human/machine computations.},
journal = {Proc. VLDB Endow.},
month = aug,
pages = {1918–1921},
numpages = {4}
}

