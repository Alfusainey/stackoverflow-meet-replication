@inproceedings{10.1145/2678025.2700997,
author = {Weld, Daniel S.},
title = {Intelligent Control of Crowdsourcing},
year = {2015},
isbn = {9781450333061},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2678025.2700997},
doi = {10.1145/2678025.2700997},
abstract = {Crowd-sourcing labor markets (e.g., Amazon Mechanical Turk) are booming, because they enable rapid construction of complex workflows that seamlessly mix human computation with computer automation. Example applications range from photo tagging to audio-visual transcription and interlingual translation. Similarly, workflows on citizen science sites (e.g. GalaxyZoo) have allowed ordinary people to pool their effort and make interesting discoveries. Unfortunately, constructing a good workflow is difficult, be- cause the quality of the work performed by humans is highly variable. Typically, a task designer will experiment with several alternative workflows to accomplish a task, varying the amount of redundant labor, until she devises a control strategy that delivers acceptable performance. Fortunately, this control challenge can often be formulated as an automated planning problem ripe for algorithms from the probabilistic planning and reinforcement learning literature. I describe our recent work on the decision-theoretic control of crowd sourcing and suggest open problems for future research.},
booktitle = {Proceedings of the 20th International Conference on Intelligent User Interfaces},
pages = {1},
numpages = {1},
keywords = {planning, crowdsourcing, adaptive interfaces},
location = {Atlanta, Georgia, USA},
series = {IUI '15}
}

@inproceedings{10.1145/3334480.3383099,
author = {Liang, Qianhui and Wang, Meijie and Nagakura, Takehiko},
title = {Urban Immersion: A Web-based Crowdsourcing Platform for Collecting Urban Space Perception Data},
year = {2020},
isbn = {9781450368193},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3334480.3383099},
doi = {10.1145/3334480.3383099},
abstract = {We introduce Urban Immersion, a web-based platform for collecting crowdsourcing immersive perception data of urban space. Current research based on crowdsourcing data mainly utilize 2D representation tools, and it has limitations in the studies involving spatial features. Thus, in our design and implementation of the platform, which aims to help architects, urban researchers, or people in spatial management to understand their users' preferences, we incorporate webVR and 360-video techniques to display both realistic and abstract representation of the urban environment. We chose the city Shanghai for the first round of the experiment to test our crowdsourcing platform. We first did internal testing for the prototyping of the platform and then published it in social media and invited 771 people to participate in rating their perception preference. We did a rating analysis and visual mapping of the 5735 valid data we collected. We evaluated this round of crowdsourcing data collection on Urban Immersion platform. We checked the effectiveness of applying 3D representation techniques to crowdsourcing platforms and proposed how we could improve the platform in future work.},
booktitle = {Extended Abstracts of the 2020 CHI Conference on Human Factors in Computing Systems},
pages = {1–8},
numpages = {8},
keywords = {crowdsourcing, urban perception, web-based interaction},
location = {Honolulu, HI, USA},
series = {CHI EA '20}
}

@inproceedings{10.1007/978-3-030-23207-8_43,
author = {Reisert, Paul and Vallejo, Gisela and Inoue, Naoya and Gurevych, Iryna and Inui, Kentaro},
title = {An Annotation Protocol for Collecting User-Generated Counter-Arguments Using Crowdsourcing},
year = {2019},
isbn = {978-3-030-23206-1},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-23207-8_43},
doi = {10.1007/978-3-030-23207-8_43},
abstract = {Constructive feedback is important for improving critical thinking skills. However, little work has been done to automatically generate such feedback for an argument. In this work, we experiment with an annotation protocol for collecting user-generated counter-arguments via crowdsourcing. We conduct two parallel crowdsourcing experiments, where workers are instructed to produce (i) a counter-argument, and (ii) a counter-argument after identifying a fallacy. Our analysis indicates that we can collect counter-arguments that are useful as constructive feedback, especially when workers are first asked to identify a fallacy type.},
booktitle = {Artificial Intelligence in Education: 20th International Conference, AIED 2019, Chicago, IL, USA, June 25-29, 2019, Proceedings, Part II},
pages = {232–236},
numpages = {5},
keywords = {Critical thinking, Counter-argument, Fallacy, Crowdsourcing, Annotation study, Constructive feedback},
location = {Chicago, IL, USA}
}

@inproceedings{10.1145/3326285.3329043,
author = {Li, Juan and Wu, Jie and Zhu, Yanmin},
title = {Selecting optimal mobile users for long-term environmental monitoring by crowdsourcing},
year = {2019},
isbn = {9781450367783},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3326285.3329043},
doi = {10.1145/3326285.3329043},
abstract = {Urban environmental monitoring related to such issues as air pollution and noise helps people understand their living environments and promotes urban construction. It is more and more important nowadays. By crowdsourcing, we can get mobile users at a low cost to collect measurement at different locations. This paper studies how to select optimal mobile users to construct an accurate monitoring map under a limited budget. We extend the noise Gaussian Process model to construct the data utility model. Because the monitoring map is updated in each time slot, we try to maximize the time-averaged data utility under the time-averaged budget constraint. This problem is particularly challenging given the unknown future information and the difficulty of solving the one-slot problem: maximizing a non-monotone sub-modular objective under the budget constraint. To address these challenges, we first make use of Lyapunov optimization to decompose the long-term optimization problem into a series of real-time problems which do not require a priori knowledge about the future information. We then propose a time-efficient online algorithm to solve the NP-hard one-slot problem. As long as the algorithm for the one-slot problem has a competitive ratio e, the time-averaged data utility of our online algorithm has a small gap compared with e times the optimal one. Evaluations based on the real air pollution data in Beijing [2] and real human trajectory data [1] show the efficiency of our approach.},
booktitle = {Proceedings of the International Symposium on Quality of Service},
articleno = {8},
numpages = {10},
keywords = {non-monotone submodular function, long-term problem, gaussian process, environmental monitoring, crowdsourcing},
location = {Phoenix, Arizona},
series = {IWQoS '19}
}

@inproceedings{10.1145/3092305.3092309,
author = {Acer, Utku Gunay and van den Broeck, Marc and Godon, Marc and Forlivesi, Claudio and Kawsar, Fahim},
title = {Can Mobile Workforce Revolutionize Country-Scale Crowdsourcing?},
year = {2017},
isbn = {9781450349581},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3092305.3092309},
doi = {10.1145/3092305.3092309},
abstract = {Traditional urban-scale crowdsourcing approaches suffer from three caveats - lack of complete spatiotemporal coverage, lack of accurate information and lack of sustained engagement of crowd workers. We argue that mobile workforces roaming around the city (and the larger country) can overcome all three caveats if their daily activity routines embed crowdsourcing tasks. To this end, in this paper, we report a first-of-its-kind study in which we explore behavioral attributes of mobile postal workers both quantitatively (6.3K) and qualitatively (6) to assess the opportunities of leveraging them for country-scale crowdsourcing tasks. Based on our observations, we develop a crowdsourcing infrastructure with carefully designed data collection strategies, and a corresponding wearable data collection application. We briefly present this solution and discuss its potential in country-scale crowdsourcing applications.},
booktitle = {Proceedings of the 4th International on Workshop on Physical Analytics},
pages = {31–36},
numpages = {6},
keywords = {mobile computing, crowdsourcin},
location = {Niagara Falls, New York, USA},
series = {WPA '17}
}

@inproceedings{10.5555/3060832.3060916,
author = {Wang, Lu and Zhou, Zhi-Hua},
title = {Cost-saving effect of crowdsourcing learning},
year = {2016},
isbn = {9781577357704},
publisher = {AAAI Press},
abstract = {Crowdsourcing is widely adopted in many domains as a popular paradigm to outsource work to individuals. In the machine learning community, crowdsourcing is commonly used as a cost-saving way to collect labels for training data. While a lot of effort has been spent on developing methods for inferring labels from a crowd, few work concentrates on the theoretical foundation of crowdsourcing learning. In this paper, we theoretically study the cost-saving effect of crowdsourcing learning, and present an upper bound for the minimally-sufficient number of crowd labels for effective crowdsourcing learning. Our results provide an understanding about how to allocate crowd labels efficiently, and are verified empirically.},
booktitle = {Proceedings of the Twenty-Fifth International Joint Conference on Artificial Intelligence},
pages = {2111–2117},
numpages = {7},
location = {New York, New York, USA},
series = {IJCAI'16}
}

@inproceedings{10.1145/3313831.3376473,
author = {Wang, Yihong and Papangelis, Konstantinos and Saker, Michael and Lykourentzou, Ioanna and Chamberlain, Alan and Khan, Vassilis-Javed},
title = {Crowdsourcing in China: Exploring the Work Experiences of Solo Crowdworkers and Crowdfarm Workers},
year = {2020},
isbn = {9781450367080},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3313831.3376473},
doi = {10.1145/3313831.3376473},
abstract = {Recent research highlights the potential of crowdsourcing in China. Yet very few studies explore the workplace context and experiences of Chinese crowdworkers. Those that do, focus mainly on the work experiences of solo crowdworkers but do not deal with issues pertaining to the substantial amount of people working in 'crowdfarms'. This article addresses this gap as one of its primary concerns. Drawing on a study that involves 48 participants, our research explores, compares and contrasts the work experiences of solo crowdworkers to those of crowdfarm workers. Our findings illustrate that the work experiences and context of the solo workers and crowdfarm workers are substantially different, with regards to their motivations, the ways they engage with crowdsourcing, the tasks they work on, and the crowdsourcing platforms they utilize. Overall, our study contributes to furthering the understandings on the work experiences of crowdworkers in China.},
booktitle = {Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems},
pages = {1–13},
numpages = {13},
keywords = {crowdfarms, crowdsourcing, crowdworkers, motivations and attitudes, platform satisfaction, reputation management, tasks, work experience, work life balance},
location = {Honolulu, HI, USA},
series = {CHI '20}
}

@inproceedings{10.1007/978-3-030-22649-7_23,
author = {Saremi, Razieh and Yang, Ye and Khanfor, Abdullah},
title = {Ant Colony Optimization to Reduce Schedule Acceleration in Crowdsourcing Software Development},
year = {2019},
isbn = {978-3-030-22648-0},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-22649-7_23},
doi = {10.1007/978-3-030-22649-7_23},
abstract = {The complexity of software tasks and the variety of developer skill sets requires to accomplish the tasks, provides a challenge in the planning process for software project managers. Uncertainty based on crowd workers’ different time zone and first language adds a layer of complexity to the CSD task scheduling. Therefore, accessing a scheduling model which can ease task allocation to improve task success and decrease project duration is essential. Existing models are either focused on the task allocation based on workers quality, or task availability in the crowdsourced platform. To create a flexible and effective model in CSD, we present an Ant Colony Optimization algorithm. The proposed approach shows a plan based on a list of available tasks in the platform and available workers based on their performance and rating metrics. The presented model is composed of four components: task fitness, workers’ attraction, task-worker availability, and task scheduler. Experimental results on 408 projects demonstrate that the proposed method reduced project duration on average 74 days.},
booktitle = {Human Interface and the Management of Information. Information in Intelligent Systems: Thematic Area, HIMI 2019, Held as Part of the 21st HCI International Conference, HCII 2019, Orlando, FL, USA, July 26-31, 2019, Proceedings, Part II},
pages = {286–300},
numpages = {15},
keywords = {Topcoder, Workers’ availability, Task similarity, Task fitness, Ant Colony Optimization, Crowdsourced software development},
location = {Orlando, FL, USA}
}

@inproceedings{10.5555/3306127.3332087,
author = {Yang, Yi and Bai, Quan and Liu, Qing},
title = {Modeling Random Guessing and Task Difficulty for Truth Inference in Crowdsourcing},
year = {2019},
isbn = {9781450363099},
publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
address = {Richland, SC},
abstract = {This paper addresses the challenge of truth inference in crowdsourcing applications. We propose a generative method that jointly models tasks' difficulties, workers' abilities and guessing behavior to estimate the truths of crowdsourced tasks, which leads to a more accurate estimation on the workers' abilities and tasks' truths. Experiments demonstrate that the proposed method is more effective for estimating truths of crowdsourced tasks compared with the state-of-art methods.},
booktitle = {Proceedings of the 18th International Conference on Autonomous Agents and MultiAgent Systems},
pages = {2288–2290},
numpages = {3},
keywords = {crowdsourcing truth inference, crowdsourcing},
location = {Montreal QC, Canada},
series = {AAMAS '19}
}

@inproceedings{10.1109/ICASSP.2017.7952910,
author = {Pag\`{e}s-Zamora, Alba and Giannakis, Georgios B. and L\'{o}pez-Valcarce, Roberto and Gim\'{e}nez-Febrer, Pere},
title = {Robust clustering of data collected via crowdsourcing},
year = {2017},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICASSP.2017.7952910},
doi = {10.1109/ICASSP.2017.7952910},
abstract = {Crowdsourcing approaches rely on the collection of multiple individuals to solve problems that require analysis of large data sets in a timely accurate manner. The inexperience of participants or annotators motivates well robust techniques. Focusing on clustering setups, the data provided by all annotators is suitably modeled here as a mixture of Gaussian components plus a uniformly distributed random variable to capture outliers. The proposed algorithm is based on the expectation-maximization algorithm and allows for soft assignments of data to clusters, to rate annotators according to their performance, and to estimate the number of Gaussian components in the non-Gaussian/Gaussian mixture model, in a jointly manner.},
booktitle = {2017 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
pages = {4014–4018},
numpages = {5},
location = {New Orleans, LA, USA}
}

@inproceedings{10.1145/3297156.3297239,
author = {Gu, Yonggen and Chen, Jiashen and Wu, Xiaohong},
title = {An Implement of Smart Contract Based Decentralized Online Crowdsourcing Mechanism},
year = {2018},
isbn = {9781450366069},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3297156.3297239},
doi = {10.1145/3297156.3297239},
abstract = {With the gradual promotion, crowdsourcing has become an efficient way to solve problems that are very complicated for computers and simple for human crowd intelligence in recent years. Traditional crowdsourcing is based on a central system where requesters post tasks on a crowdsourcing central server or platform, however, this centralized model currently faces various challenges such as prohibitive cost, single point of failure, and vulnerability to malicious attacks. To this end, this paper proposes a smart contract-based decentralized online crowdsourcing mechanism, which includes task assignment rules and reward payment rules, etc. The mechanism has the characteristics like decentralization, unalterable, truthfulness and so on. In addition, the corresponding smart contract is designed, so that the mechanism can really run and process the actual data, and the effectiveness is shown by experiments. In this way, the entire crowdsourcing process no longer requires the participation of trusted third-party agencies, information and privacy security is guaranteed, and the cost is lower.},
booktitle = {Proceedings of the 2018 2nd International Conference on Computer Science and Artificial Intelligence},
pages = {195–199},
numpages = {5},
keywords = {smart contract, decentralization, blockchain, Ethereum, Crowdsourcing},
location = {Shenzhen, China},
series = {CSAI '18}
}

@inproceedings{10.1109/GLOCOM.2018.8647346,
author = {Shu, Jiangang and Liu, Ximeng and Yang, Kan and Zhang, Yinghui and Jia, Xiaohua and Deng, Robert H.},
title = {SybMatch: Sybil Detection for Privacy-Preserving Task Matching in Crowdsourcing},
year = {2018},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/GLOCOM.2018.8647346},
doi = {10.1109/GLOCOM.2018.8647346},
abstract = {The past decade has witnessed the rise of crowdsourcing, and privacy in crowdsourcing has also gained rising concern in the meantime. In this paper, we focus on the privacy leaks and sybil attacks during the task matching, and propose a privacy-preserving task matching scheme, called SybMatch. The SybMatch scheme can simultaneously protect the privacy of publishers and subscribers against semi-honest crowdsourcing service provider, and meanwhile support the sybil detection against greedy subscribers and efficient user revocation. Detailed security analysis and thorough performance evaluation show that the SybMatch scheme is secure and efficient.},
booktitle = {2018 IEEE Global Communications Conference (GLOBECOM)},
pages = {1–6},
numpages = {6},
location = {Abu Dhabi, United Arab Emirates}
}

@inproceedings{10.1109/GLOCOM.2018.8647174,
author = {Li, Weiwei and Zhang, Kuan and Su, Zhou and Lu, Rongxing and Wang, Ying},
title = {Anomalous Path Detection for Spatial Crowdsourcing-Based Indoor Navigation System},
year = {2018},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/GLOCOM.2018.8647174},
doi = {10.1109/GLOCOM.2018.8647174},
abstract = {Indoor navigation system provides customized path planning for requesters who are unfamiliar with the indoor environment, such as shopping mall and airport. Spatial crowd-sourcing technology can be applied to indoor navigation to offer fundamental services related to location. However, spatial crowdsourcing-based indoor navigation is vulnerable to the intrusion of injected anomalous paths from attackers. In this paper, we propose an anomalous path detection (APD) scheme to classify attackers according to their reputation management and abnormal trajectory sequence. Specifically, we first develop a crowdsourcing system to support the indoor location service using the fog as the spatial crowdsourcing server. Then, we identify two levels of attackers, i.e., the malicious responders and the semi-honest responders in the indoor environment according to their attacking purposes. Through the responders' historical records from the fog server, we analyze a series of trajectory sequences consisting of the distance between the current position and the destination to distinguish the semi-honest responders from the normal. In addition, we propose a semi-supervised learning with hidden Markov model (HMM) to detect the semi-honest responders. Finally, the extensive simulations show that the APD scheme can achieve higher accuracy with the acceptable false rate.},
booktitle = {2018 IEEE Global Communications Conference (GLOBECOM)},
pages = {1–7},
numpages = {7},
location = {Abu Dhabi, United Arab Emirates}
}

@inproceedings{10.1145/3394171.3413619,
author = {Li, Jing and Ling, Suiyi and Wang, Junle and Le Callet, Patrick},
title = {A Probabilistic Graphical Model for Analyzing the Subjective Visual Quality Assessment Data from Crowdsourcing},
year = {2020},
isbn = {9781450379885},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3394171.3413619},
doi = {10.1145/3394171.3413619},
abstract = {The swift development of the multimedia technology has raised dramatically the users' expectation on the quality of experience. To obtain the ground-truth perceptual quality for model training, subjective assessment is necessary. Crowdsourcing platform provides us a convenient and feasible way to run large-scale experiments. However, the obtained perceptual quality labels are generally noisy. In this paper, we propose a probabilistic graphical annotation model to infer the underlying ground truth and discovering the annotator's behavior. In the proposed model, the ground truth quality label is considered following a categorical distribution rather than a unique number, i.e., different reliable opinions on the perceptual quality are allowed. In addition, different annotator's behaviors in crowdsourcing are modeled, which allows us to identify the possibility that the annotator makes noisy labels during the test. The proposed model has been tested on both simulated data and real-world data, where it always shows superior performance than the other state-of-the-art models in terms of accuracy and robustness.},
booktitle = {Proceedings of the 28th ACM International Conference on Multimedia},
pages = {3339–3347},
numpages = {9},
keywords = {quality assessment, probabilistic graphic model, ground truth, crowdsourcing, annotator behavior},
location = {Seattle, WA, USA},
series = {MM '20}
}

@inproceedings{10.1109/CDC.2017.8264132,
author = {Liu, Xiangyang and Baras, John S.},
title = {Crowdsourcing with multi-dimensional trust and active learning},
year = {2017},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/CDC.2017.8264132},
doi = {10.1109/CDC.2017.8264132},
abstract = {We consider a typical crowdsourcing task that aggregates input from multiple workers as a problem in information fusion. To cope with the issue of noisy and sometimes malicious input from users, trust is used to model workers expertise. We propose a probabilistic model to jointly infer multi-dimensional trust of workers, multi-domain properties of questions, and true labels of questions. Our model is flexible and extensible to incorporate metadata associated with questions. To show that, we further propose two extended models, one of which handles input tasks with real-valued features and the other handles tasks with text features by incorporating topic models. In order to decrease entropies and reduce error rates more quickly with fewer annotations from workers, we further propose strategies for selecting which questions to ask and which workers to assign the questions to based on multidimension characteristics of questions and workers trust values in those dimensions. We evaluate our models and algorithms on real-world data sets. These results can be applied for fusion of information from multiple data sources like sensors, human input, machine learning results, or a hybrid of them.},
booktitle = {2017 IEEE 56th Annual Conference on Decision and Control (CDC)},
pages = {3224–3231},
numpages = {8},
location = {Melbourne, Australia}
}

@inproceedings{10.1145/2998181.2998311,
author = {Kandappu, Thivya and Misra, Archan and Tandriansyah, Randy},
title = {Collaboration Trumps Homophily in Urban Mobile Crowdsourcing},
year = {2017},
isbn = {9781450343350},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2998181.2998311},
doi = {10.1145/2998181.2998311},
abstract = {This paper establishes the power of dynamic collaborative task completion among workers for urban mobile crowd-sourcing. Collaboration is defined via the notion of peer referrals, whereby a worker who has accepted a location-specific task, but is unlikely to visit that location, offloads the task to a willing friend. Such a collaborative framework might be particularly useful for task bundles, especially for bundles that have higher geographic dispersion. The challenge, however, comes from the high similarity observed in the spatio-temporal pattern of task completion among friends. Using extensive real-world crowd-sourcing studies conducted over 7 weeks and 1000+ workers on a campus-based crowd-sourcing platform, we quantify the effect of such "task completion homophily", and show that incorporating such peer-preferences can improve worker-specific models of task preferences by over 30\%. We then show that such collaborative offloading works in spite of such spatio-temporal similarity, primarily because workers refer tasks to their close friends, who in turn perform such peer-requested tasks (with over 95\% completion rate) even if they experience detours that are significantly larger (often more than twice) than what they normally tolerate for platform-recommended tasks.},
booktitle = {Proceedings of the 2017 ACM Conference on Computer Supported Cooperative Work and Social Computing},
pages = {902–915},
numpages = {14},
keywords = {social-ties, homophily, crowd-sourcing, collaboration},
location = {Portland, Oregon, USA},
series = {CSCW '17}
}

@inproceedings{10.1145/3323503.3360634,
author = {Yagui, Marcela Mayumi Mauricio and Vivacqua, Adriana S.},
title = {A crowdsourcing web system for curating empirical knowledge in linked open data},
year = {2019},
isbn = {9781450367639},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3323503.3360634},
doi = {10.1145/3323503.3360634},
abstract = {Traditional and Empirical knowledge databases are becoming essential for the preservation of a region's culture. Without proper curation this knowledge might be lost over time. The goal of this work is to present a web system to support the co-curation of data derived from empirical knowledge. Our proposal also provides the interconnection between visitors' contributions with data already available in Linked Open Data repositories. The web system was evaluated by specialists from the Rio de Janeiro Botanical Garden, who verified the impact of the system implantation in cultural environments related to medicinal plants.},
booktitle = {Proceedings of the 25th Brazillian Symposium on Multimedia and the Web},
pages = {441–444},
numpages = {4},
keywords = {linked open data, cultural heritage, crowdsourcing, content curation, collaborative systems},
location = {Rio de Janeiro, Brazil},
series = {WebMedia '19}
}

@inproceedings{10.1145/3018661.3018688,
author = {Li, Yaliang and Du, Nan and Liu, Chaochun and Xie, Yusheng and Fan, Wei and Li, Qi and Gao, Jing and Sun, Huan},
title = {Reliable Medical Diagnosis from Crowdsourcing: Discover Trustworthy Answers from Non-Experts},
year = {2017},
isbn = {9781450346757},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3018661.3018688},
doi = {10.1145/3018661.3018688},
abstract = {Nowadays, increasingly more people are receiving medical diagnoses from healthcare-related question answering platforms as people can get diagnoses quickly and conveniently. However, such diagnoses from non-expert crowdsourcing users are noisy or even wrong due to the lack of medical domain knowledge, which can cause serious consequences. To unleash the power of crowdsourcing on healthcare question answering, it is important to identify trustworthy answers and filter out noisy ones from user-generated data. Truth discovery methods estimate user reliability degrees and infer trustworthy information simultaneously, and thus these methods can be adopted to discover trustworthy diagnoses from crowdsourced answers. However, existing truth discovery methods do not take into account the rich semantic meanings of the answers. In the light of this challenge, we propose a method to automatically capture the semantic meanings of answers, where answers are represented as real-valued vectors in the semantic space. To learn such vector representations from noisy user-generated data, we tightly combine the truth discovery and vector learning processes. In this way, the learned vector representations enable truth discovery method to model the semantic relations among answers, and the information trustworthiness inferred by truth discovery can help the procedure of vector representation learning. To demonstrate the effectiveness of the proposed method, we collect a large-scale real-world dataset that involves 219,527 medical diagnosis questions and 23,657 non-expert users. Experimental results show that the proposed method improves the accuracy of identified trustworthy answers due to the successful consideration of answers' semantic meanings. Further, we demonstrate the fast convergence and good scalability of the proposed method, which makes it practical for real-world applications.},
booktitle = {Proceedings of the Tenth ACM International Conference on Web Search and Data Mining},
pages = {253–261},
numpages = {9},
keywords = {truth discovery, semantic meanings, medical question answering},
location = {Cambridge, United Kingdom},
series = {WSDM '17}
}

@inproceedings{10.5555/3192424.3192461,
author = {Choi, Hongkyu and Lee, Kyumin and Webb, Steve},
title = {Detecting malicious campaigns in crowdsourcing platforms},
year = {2016},
isbn = {9781509028467},
publisher = {IEEE Press},
abstract = {Crowdsourcing systems enable new opportunities for requesters with limited funds to accomplish various tasks using human computation. However, the power of human computation is abused by malicious requesters who create malicious campaigns to manipulate information in web systems such as social networking sites, online review sites, and search engines. To mitigate the impact and reach of these malicious campaigns to targeted sites, we propose and evaluate a machine learning based classification approach for detecting malicious campaigns in crowdsourcing platforms as a first line of defense. Specifically, we (i) conduct a comprehensive analysis to understand the characteristics of malicious campaigns and legitimate campaigns in crowdsourcing platforms, (ii) propose various features to distinguish between malicious campaigns and legitimate campaigns, and (iii) evaluate a classification approach against baselines. Our experimental results show that our proposed approaches effectively detect malicious campaigns with low false negative and false positive rates.},
booktitle = {Proceedings of the 2016 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining},
pages = {197–202},
numpages = {6},
location = {Davis, California},
series = {ASONAM '16}
}

@inproceedings{10.1109/WCNC.2018.8377007,
author = {Chen, Xiao},
title = {Task trading for crowdsourcing in opportunistic mobile social networks},
year = {2018},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/WCNC.2018.8377007},
doi = {10.1109/WCNC.2018.8377007},
abstract = {With the explosive proliferation of mobile devices, mobile crowdsourcing has become a new paradigm involving a crowd of mobile users to collectively take large-scale tasks from requesters in mobile social networks (MSNs). In this paper, we study task allocation in crowdsourcing in Opportunistic Mobile Social Networks (OMSNs) which are formed opportunistically when people gather together at social events. Specifically, we aim to minimize the total working hours of the users to finish these tasks. Different from other algorithms, we hope to raise the efficiency of the whole network by task trading inspired by the comparative advantage in macroeconomy. We first prove that our defined problem is NP-hard and then propose a heuristic task trading algorithm TTA by which users can trade when they meet opportunistically. Simulation results comparing our proposed algorithm with the one without considering trading and the brute force algorithm to find the minimum total number of hours show that our proposed algorithm can substantially reduce the total number of hours to finish all the allocated tasks and is very close to the benchmark brute force algorithm.},
booktitle = {2018 IEEE Wireless Communications and Networking Conference (WCNC)},
pages = {1–6},
numpages = {6},
location = {Barcelona, Spain}
}

@inproceedings{10.1109/INCoS.2015.74,
author = {Mladenow, Andreas and Bauer, Christine and Strauss, Christine and Gregus, Michal},
title = {Collaboration and Locality in Crowdsourcing},
year = {2015},
isbn = {9781467376952},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/INCoS.2015.74},
doi = {10.1109/INCoS.2015.74},
abstract = {As novel forms of crowdsourcing emerge on the market, we emphasize that the important aspect of location-dependency is more complex than assumed and, thus, suggest a typology along two dimensions of locality: the first dimension refers to whether or not the crowdsourcees interact while being collocated or dispersed, the second dimension refers to the locality of the crowdsourcees in relation to the crowdsourcer's locality (local vs. remote crowd). The resulting four types of crowdsourcing are underpinned by real-world examples. Potential advantages and challenges of the four types are discussed, particularly with respect to motivation and value. The suggested categorization shall provide the necessary basis for future research, as a systematic approach is essential to enable, yield and foster sustainability in a novel interdisciplinary research field like location-based crowdsourcing.},
booktitle = {Proceedings of the 2015 International Conference on  Intelligent Networking and Collaborative Systems},
pages = {1–6},
numpages = {6},
keywords = {Web-based Communities, Tournament-based Crowdsourcing, Taxonomy, Social Networks, Location-based Crowdsourcing, Crowdsourcing, Collaborative Systems, Collaboration-based Crowdsourcing, Collaboration},
series = {INCOS '15}
}

@inproceedings{10.1007/978-3-030-77750-0_32,
author = {Rapp, Maximilian and Kr\"{o}ger, Niclas and Scheerer, Samira},
title = {Inside-Out: How Internal Social Media Platforms Can Accelerate Innovation and Push External Crowdsourcing Towards New Frontiers},
year = {2021},
isbn = {978-3-030-77749-4},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-77750-0_32},
doi = {10.1007/978-3-030-77750-0_32},
abstract = {Continuous Improvement Processes (CIP) in companies and organizations alike have been part of a widespread metamorphosis to a more strategic internal crowdsourcing process with professional campaigns as well as sophisticated ideation platforms to gather knowledge and experiences from the organizations’ employees and stakeholders. While its counterpart, namely external crowdsourcing with users, customers, or external stakeholders is a matter of myriad research, the use, process, metamorphosis, and environments of internal crowds are lacking a deeper understanding through in-depth analysis. In this paper, we will answer 1) why most organizations use either internal or external crowdsourcing, and 2) what key success factors exist for effective internal campaigns. In order to answer these questions, we accompanied 10 organizations using an active research approach based on a variety of data, including interviews. We sum up by consolidating all findings in managerial implications for practical execution.},
booktitle = {HCI in Business, Government and Organizations: 8th International Conference, HCIBGO 2021, Held as Part of the 23rd HCI International Conference, HCII 2021, Virtual Event, July 24–29, 2021, Proceedings},
pages = {500–514},
numpages = {15},
keywords = {Open innovation, Co-creation, Ideation, Community, Crowdsourcing, Continuous Improvement Process, Employee integration}
}

@inproceedings{10.1145/3379157.3391304,
author = {Shekh.Khalil, Naziha and Dogruer, Ecem and Elosta, Abdulmohimen K. O. and Eraslan, Sukru and Yesilada, Yeliz and Harper, Simon},
title = {EyeCrowdata: Towards a Web-based Crowdsourcing Platform for Web-related Eye-Tracking Data},
year = {2020},
isbn = {9781450371353},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3379157.3391304},
doi = {10.1145/3379157.3391304},
abstract = {Eye-tracking studies are commonly used for identifying the usability problems of Web pages and gaining insights into how the design of Web pages can be improved for better user experience. Similar to other user studies, eye-tracking studies should be carefully designed and conducted by considering ethical issues and confounding factors, and therefore these studies typically require a considerable amount of time. Recruiting a large number of participants is also an important issue as eye-tracking sessions may not be conducted in parallel in case of limited resources such as equipment and researchers. Previous work highlighted the need for a Web-based platform to crowdsource Web-related eye-tracking data and facilitate data sharing, thus allowing the replication of existing analysis. Previous work also presented a preliminary structured literature review on what kinds of metrics are required for such a platform. In this paper, we also focus on Web-related eye-tracking studies, and we present an overview of the extended version of the structured literature review along with a prototype for a Web-based platform for crowdsourcing Web-related eye-tracking data called EyeCrowdata.},
booktitle = {ACM Symposium on Eye Tracking Research and Applications},
articleno = {31},
numpages = {6},
keywords = {replicability, eye tracking, data sharing, crowdsourcing, Web},
location = {Stuttgart, Germany},
series = {ETRA '20 Adjunct}
}

@inproceedings{10.3233/978-1-61499-672-9-1573,
author = {Liu, Siyuan and Fan, Xiuyi and Miao, Chunyan},
title = {Identifying and rewarding subcrowds in crowdsourcing},
year = {2016},
isbn = {9781614996712},
publisher = {IOS Press},
address = {NLD},
url = {https://doi.org/10.3233/978-1-61499-672-9-1573},
doi = {10.3233/978-1-61499-672-9-1573},
abstract = {Identifying and rewarding truthful workers are key to the sustainability of crowdsourcing platforms. In this paper, we present a clustering based rewarding mechanism that rewards workers based on their truthfulness while accommodating the differences in workers' preferences. Experimental results show that the proposed approach can effectively discover subcrowds under various conditions, and truthful workers are better rewarded than less truthful ones.},
booktitle = {Proceedings of the Twenty-Second European Conference on Artificial Intelligence},
pages = {1573–1574},
numpages = {2},
location = {The Hague, The Netherlands},
series = {ECAI'16}
}

@inproceedings{10.1145/3308560.3317081,
author = {K. Chaithanya Manam, V. and Jampani, Dwarakanath and Zaim, Mariam and Wu, Meng-Han and J. Quinn, Alexander},
title = {TaskMate: A Mechanism to Improve the Quality of Instructions in Crowdsourcing},
year = {2019},
isbn = {9781450366755},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3308560.3317081},
doi = {10.1145/3308560.3317081},
abstract = {Developing instructions for microtask crowd workers requires time to ensure consistent interpretations by crowd workers. Even with substantial effort, workers may still misinterpret the instructions due to ambiguous language and structure in the task design. Prior work demonstrated methods for facilitating iterative improvement with help from the requester. However, any participation by the requester reduces the time saved by delegating the work—and hence the utility of using crowdsourcing. We present TaskMate, a system for facilitating worker-led refinement of task instructions with minimal involvement by the requester. Small teams of workers search for ambiguities and vote on the interpretation they believe the requester intended. This paper describes the workflow, our implementation, and our preliminary evaluation.},
booktitle = {Companion Proceedings of The 2019 World Wide Web Conference},
pages = {1121–1130},
numpages = {10},
keywords = {workflow, task instructions, ambiguities, Crowdsourcing},
location = {San Francisco, USA},
series = {WWW '19}
}

@inproceedings{10.5555/3021955.3022041,
author = {Quirino, Wancharle S. and Santos, Celso A.S. and Calles, Juan X.E.A. and F., Fernando Tinelli},
title = {Crowdsourcing strategies for smart cities applications},
year = {2016},
isbn = {9788576693178},
publisher = {Brazilian Computer Society},
address = {Porto Alegre, BRA},
abstract = {Crowdsourcing is a problem-solving model through the contribution of a large number of people and has a low cost among its main advantages. On the other hand, smart cities today comprise a multidisciplinary challenge, where they have the objective of sustainable development and improving the quality of life of its inhabitants. Thus, we see in crowdsourcing a resource capable of contributing to building smart cities. This paper investigates the existing intersection between the fields of smart cities and crowdsourcing and discover the gaps and challenges that characterize this applications context. As a result, we propose some strategies to facilitate the development of crowdsourcing applications. These strategies are then applied to the construction of several applications of this type, two of which are discussed at the end of the article.},
booktitle = {Proceedings of the XII Brazilian Symposium on Information Systems on Brazilian Symposium on Information Systems: Information Systems in the Cloud Computing Era - Volume 1},
pages = {510–517},
numpages = {8},
keywords = {smart cities, platform, applications, Crowdsourcing},
location = {Florianopolis, Santa Catarina, Brazil},
series = {SBSI '16}
}

@inproceedings{10.1145/3127404.3127443,
author = {Sun, Yong and Wang, Jun and Tan, Wenan},
title = {Online Algorithms of Task Allocation in Spatial Crowdsourcing},
year = {2017},
isbn = {9781450353526},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3127404.3127443},
doi = {10.1145/3127404.3127443},
abstract = {Recently, spatial collaborations1 and crowdsourcing has emerged as a novel typical pattern for applying to a range of problems. A key problem of spatial collaboration is to allocate suitable workers to nearby tasks in a real-time online way. Traditional crowdsourcing algorithms always consider the quality of worker with prior knowledge. However, in online crowdsourcing context, the quality of crowd-workers is unknown and uncertain. It is so hard for such task crowdsourcing process in an inherently online and dynamic environment. To solve this spatial crowdsourcing problem, the branch-and-bound R-tree data structure is employed in our algorithms to prune the search tree of the nearby crowd-workers. Furthermore, we introduce a new online algorithm to deal with the uncertain crowdsourcing problems. Theoretical analysis and extensive experiments are conducted for validation purpose; and the experimental results show that our algorithms outperform several existing algorithms in terms of computation time in dealing with the increasing number of crowdsourcing task executing candidates.},
booktitle = {Proceedings of the 12th Chinese Conference on Computer Supported Cooperative Work and Social Computing},
pages = {205–208},
numpages = {4},
keywords = {task allocation, online algorithms, collaborative computing, Spatial crowdsourcing},
location = {Chongqing, China},
series = {ChineseCSCW '17}
}

@inproceedings{10.1109/MDM.2015.55,
author = {Alfarrarjeh, Abdullah and Emrich, Tobias and Shahabi, Cyrus},
title = {Scalable Spatial Crowdsourcing: A Study of Distributed Algorithms},
year = {2015},
isbn = {9781479999729},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/MDM.2015.55},
doi = {10.1109/MDM.2015.55},
abstract = {Recently spatial crowd sourcing was introduced as a natural extension to traditional crowd sourcing allowing for tasks to have a geospatial component, i.e., A task can only be performed if a worker is physically present at the location of the task. The problem of assigning spatial tasks to workers in a spatial crowd sourcing system can be formulated as a weighted bipartite b-matching graph problem that can be solved optimally by existing methods for the minimum cost maximum flow problem. However, these methods are still too complex to run repeatedly for an online system, especially when the number of incoming workers and tasks increases. Hence, we propose a class of approaches that utilizes an online partitioning method to reduce the problem space across a set of cloud servers to construct independent bipartite graphs and solve the assignment problem in parallel. Our approaches solve the spatial task assignment approximately but competitive to the exact solution. We experimentally verify that our approximate approaches outperform the centralized and Map Reduce version of the exact approach with acceptable accuracy and thus suitable for online spatial crowd sourcing at scale.},
booktitle = {Proceedings of the 2015 16th IEEE International Conference on Mobile Data Management - Volume 01},
pages = {134–144},
numpages = {11},
keywords = {spatial task assignment, spatial crowdsouring, online partitioning, distributed spatial task assignment},
series = {MDM '15}
}

@inproceedings{10.1007/978-3-319-68059-0_46,
author = {Riganova, Michaela and Balata, Jan and Mikovec, Zdenek},
title = {Crowdsourcing of Accessibility Attributes on Sidewalk-Based Geodatabase},
year = {2017},
isbn = {9783319680583},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-319-68059-0_46},
doi = {10.1007/978-3-319-68059-0_46},
abstract = {Although the issue of limited mobility affects a large portion of the population, current navigation systems working with roadway-based geodatabases are designed primarily for cars and therefore cannot efficiently help. Usage of the professionally created sidewalk-based geodatabase is a solution. However, the professional geographical "on-site reconnaissance" is labor demanding. In this poster, we report on results of preliminary research focused on a design of the gamified collection of accessibility attributes by non-expert crowd, which will reduce the data collection cost. Preliminary results suggest the feasibility of the approach supported by a proper guidance of non-experts and creativity of achieving precise measurements.},
booktitle = {16th IFIP TC 13 International Conference on Human-Computer Interaction --- INTERACT 2017 - Volume 10516},
pages = {436–440},
numpages = {5}
}

@inproceedings{10.1145/3173574.3173641,
author = {Jang, Esther and Barela, Mary Claire and Johnson, Matt and Martinez, Philip and Festin, Cedric and Lynn, Margaret and Dionisio, Josephine and Heimerl, Kurtis},
title = {Crowdsourcing Rural Network Maintenance and Repair via Network Messaging},
year = {2018},
isbn = {9781450356206},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3173574.3173641},
doi = {10.1145/3173574.3173641},
abstract = {Repair and maintenance requirements limit the successful operation of rural infrastructure. Current best practices are centralized management, which requires travel from urban areas and is prohibitively expensive, or intensively training community members, which limits scaling. We explore an alternative model: crowdsourcing repair from the community. Leveraging a Community Cellular Network in the remote Philippines, we sent SMS to all active network subscribers (n = 63) requesting technical support. From the pool of physical respondents, we explored their ability to repair through mock failures and conducted semi-structured interviews about their experiences with repair. We learned that community members would be eager to practice repair if allowed, would network to recruit more expertise, and seemingly have the collective capacity to resolve some common failures. They are most successful when repairs map directly to their lived experiences. We suggest infrastructure design considerations that could make repairs more tractable and argue for an inclusive approach.},
booktitle = {Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems},
pages = {1–12},
numpages = {12},
keywords = {technology for development, rural development, internet access, ictd, crowdsourcing},
location = {Montreal QC, Canada},
series = {CHI '18}
}

@inproceedings{10.1145/2487575.2487593,
author = {Mo, Kaixiang and Zhong, Erheng and Yang, Qiang},
title = {Cross-task crowdsourcing},
year = {2013},
isbn = {9781450321747},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2487575.2487593},
doi = {10.1145/2487575.2487593},
abstract = {Crowdsourcing is an effective method for collecting labeled data for various data mining tasks. It is critical to ensure the veracity of the produced data because responses collected from different users may be noisy and unreliable. Previous works solve this veracity problem by estimating both the user ability and question difficulty based on the knowledge in each task individually. In this case, each single task needs large amounts of data to provide accurate estimations. However, in practice, budgets provided by customers for a given target task may be limited, and hence each question can be presented to only a few users where each user can answer only a few questions. This data sparsity problem can cause previous approaches to perform poorly due to the overfitting problem on rare data and eventually damage the data veracity. Fortunately, in real-world applications, users can answer questions from multiple historical tasks. For example, one can annotate images as well as label the sentiment of a given title. In this paper, we employ transfer learning, which borrows knowledge from auxiliary historical tasks to improve the data veracity in a given target task. The motivation is that users have stable characteristics across different crowdsourcing tasks and thus data from different tasks can be exploited collectively to estimate users' abilities in the target task. We propose a hierarchical Bayesian model, TLC (Transfer Learning for Crowdsourcing), to implement this idea by considering the overlapping users as a bridge. In addition, to avoid possible negative impact, TLC introduces task-specific factors to model task differences. The experimental results show that TLC significantly improves the accuracy over several state-of-the-art non-transfer-learning approaches under very limited budget in various labeling tasks.},
booktitle = {Proceedings of the 19th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {677–685},
numpages = {9},
keywords = {transfer learning, crowdsourcing},
location = {Chicago, Illinois, USA},
series = {KDD '13}
}

@inproceedings{10.1007/978-3-030-58805-2_22,
author = {Tanaka, Kohei and Wakatsuki, Daisuke and Minagawa, Hiroki},
title = {A Study Examining a Real-Time Sign Language-to-Text Interpretation System Using Crowdsourcing},
year = {2020},
isbn = {978-3-030-58804-5},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-58805-2_22},
doi = {10.1007/978-3-030-58805-2_22},
abstract = {The current study examined how to use crowdsourcing to convert sign language-to-text. Generally in Japan, a sign language interpreter reads and vocalizes the sign language of the speaker, and caption typists generate captions from the vocalization. However, this method doubles labor costs and delays caption provision. Therefore, we developed a system that interprets sign language-to-caption text via crowdsourcing, with non-experts performing interpretations. While many individuals classified as deaf/hard-of-hearing (DHH) who can read sign language are suitable for this task, not all of them possess adequate typing skills. To address this, our system divides live sign language video into shorter segments, distributing them to workers. After the worker interprets and types the segments to text, the system generates captions through integration of these texts. Furthermore, we provide a user interface for playback speed control and one second rewinding in order to improve the ease with which tasks are completed. Our system can establish an environment that not only allows the interpretation of sign language-to-caption text, but also provides an opportunity for DHH individuals to assist those that are unable read sign language. We conducted a test using our prototype system for sign language-to-text interpretation. The mean time it took a worker to finish a task was 26&nbsp;s for a 9&nbsp;s segment. The combined total rate of missing text and collision between segments was 66\%. Analysis of questionnaire responses found that workers assigned fewer tasks considered the tasks more enjoyable.},
booktitle = {Computers Helping People with Special Needs: 17th International Conference, ICCHP 2020, Lecco, Italy, September 9–11, 2020, Proceedings, Part II},
pages = {186–194},
numpages = {9},
keywords = {Real-time sign language-to-text interpretation, Deaf and hard-of-hearing, Crowdsourcing},
location = {Lecco, Italy}
}

@inproceedings{10.1145/3334480.3382999,
author = {Seong, Eunjin and Kim, Seungjun},
title = {Designing a Crowdsourcing System for the Elderly: A Gamified Approach to Speech Collection},
year = {2020},
isbn = {9781450368193},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3334480.3382999},
doi = {10.1145/3334480.3382999},
abstract = {Despite the need for representation of older adults in crowdsourced data, crowd work is generally not designed for older adults and participation by older adults is low. In this paper, we demonstrate a process for designing crowd work for older adults; identifying their needs, designing an approach to foster their participation, and verifying its effectiveness. We found when older people feel connected to others while doing crowd work, they are highly motivated. Furthermore, gamification is an effective tool for fostering their engagement when aligned with their needs and values, as opposed to the needs and values of younger participants. Lastly, we suggest important considerations and opportunities for designing crowd work approaches for senior citizens.},
booktitle = {Extended Abstracts of the 2020 CHI Conference on Human Factors in Computing Systems},
pages = {1–9},
numpages = {9},
keywords = {aging society, crowd work, crowdsourcing, gamification, older adults, speech collection},
location = {Honolulu, HI, USA},
series = {CHI EA '20}
}

@inproceedings{10.1145/3334480.3375228,
author = {Khan, Vassilis-Javed and Papangelis, Konstantinos and Markopoulos, Panos},
title = {Completing a Crowdsourcing Task Instead of an Assignment; What do University Students Think?},
year = {2020},
isbn = {9781450368193},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3334480.3375228},
doi = {10.1145/3334480.3375228},
abstract = {University educators actively seek realistic projects to include in their educational activities. However, finding an actually realistic project is not trivial. The rise of crowdsourcing platforms, in which a variety of tasks are offered in the form of an open call, might be an alternative source to help educators scale up project- based learning. But how do university students feel about executing crowdsourcing tasks instead of their typical assignments? In a study with 24 industrial design students we investigate students' attitudes on introducing crowdsourcing tasks as assignments. Based on our study we offer four suggestions to universities that consider integrating crowdsourcing tasks in their educational activities.},
booktitle = {Extended Abstracts of the 2020 CHI Conference on Human Factors in Computing Systems},
pages = {1–8},
numpages = {8},
keywords = {crowdsourcing, higher education, project based learning, university students' attitude},
location = {Honolulu, HI, USA},
series = {CHI EA '20}
}

@inproceedings{10.1007/978-3-319-26148-5_30,
author = {Slaimi, Fatma and Sellami, Sana and Boucelma, Omar and Hassine, Ahlem Ben},
title = {Crowdsourcing for Web Service Discovery},
year = {2015},
isbn = {9783319261478},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-319-26148-5_30},
doi = {10.1007/978-3-319-26148-5_30},
abstract = {Over last decade, research in Web service discovery has brought a variety of techniques to find out responses for a Web service request. While the accuracy of matchmaking approaches has continuously improved, human contributions remain a key ingredient of the process. In this paper, we propose an approach called Crowd4WS Crowdsourcing for Web service discovery to complement and refine matchmaking approaches by using crowdsourcing techniques. We describe our approach and present the results of experiments on a known collection of RESTful services described with hRESTS.},
booktitle = {Proceedings of the Confederated International Conferences on On the Move to Meaningful Internet Systems: OTM 2015 Conferences - Volume 9415},
pages = {451–464},
numpages = {14},
keywords = {Web services discovery, Matchmaking, Crowdsourcing}
}

@inproceedings{10.1109/WCNC.2018.8377240,
author = {Cui, Jingmei and Sun, Yu-E and Huang, He and Guo, Hansong and Du, Yang and Yang, Wenjian and Li, Meixuan},
title = {TCAM: A truthful combinatorial auction mechanism for crowdsourcing systems},
year = {2018},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/WCNC.2018.8377240},
doi = {10.1109/WCNC.2018.8377240},
abstract = {Crowdsourcing has shown its efficiency in obtaining information by harnessing the intelligence of a large crowd of human workers. It is essential to employ incentive mechanisms, typically auction, to motivate workers and collect sufficient data, since performing crowdsourcing tasks will always consume considerable resources, e.g., CPU or battery resource. To this end, we focus on the problem of heterogeneous task allocation with budget constraint in the crowdsourcing systems and propose a truthful auction mechanism which can maximize the profit of the task requester. In this paper, we first prove the NP-hardness of the studied problem and design a near-optimal task allocation mechanism with partial enumeration which can maximize the profit of the requester. Then, we judiciously design a bid-independent payment calculation mechanism to ensure the truthfulness of the participants. Finally, we prove that the proposed crowdsourcing task auction mechanism can achieve truthfulness and individual rationality. The extensive simulation results also corroborate with our theoretical analysis.},
booktitle = {2018 IEEE Wireless Communications and Networking Conference (WCNC)},
pages = {1–6},
numpages = {6},
location = {Barcelona, Spain}
}

@inproceedings{10.1145/2858036.2858115,
author = {Krishna, Ranjay A. and Hata, Kenji and Chen, Stephanie and Kravitz, Joshua and Shamma, David A. and Fei-Fei, Li and Bernstein, Michael S.},
title = {Embracing Error to Enable Rapid Crowdsourcing},
year = {2016},
isbn = {9781450333627},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2858036.2858115},
doi = {10.1145/2858036.2858115},
abstract = {Microtask crowdsourcing has enabled dataset advances in social science and machine learning, but existing crowdsourcing schemes are too expensive to scale up with the expanding volume of data. To scale and widen the applicability of crowdsourcing, we present a technique that produces extremely rapid judgments for binary and categorical labels. Rather than punishing all errors, which causes workers to proceed slowly and deliberately, our technique speeds up workers' judgments to the point where errors are acceptable and even expected. We demonstrate that it is possible to rectify these errors by randomizing task order and modeling response latency. We evaluate our technique on a breadth of common labeling tasks such as image verification, word similarity, sentiment analysis and topic classification. Where prior work typically achieves a 0.25x to 1x speedup over fixed majority vote, our approach often achieves an order of magnitude (10x) speedup.},
booktitle = {Proceedings of the 2016 CHI Conference on Human Factors in Computing Systems},
pages = {3167–3179},
numpages = {13},
keywords = {human computation, crowdsourcing, RSVP},
location = {San Jose, California, USA},
series = {CHI '16}
}

@inproceedings{10.1145/3357729.3357748,
author = {Lee, Helena H. and Achananuparp, Palakorn and Liu, Yue and Lim, Ee-Peng and Varshney, Lav R.},
title = {Estimating Glycemic Impact of Cooking Recipes via Online Crowdsourcing and Machine Learning},
year = {2019},
isbn = {9781450372084},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3357729.3357748},
doi = {10.1145/3357729.3357748},
abstract = {Consumption of diets with low glycemic impact is highly recommended for diabetics and pre-diabetics as it helps maintain their blood glucose levels. However, laboratory analysis of dietary glycemic potency is time-consuming and expensive. In this paper, we explore a data-driven approach utilizing online crowdsourcing and machine learning to estimate the glycemic impact of cooking recipes. We show that a commonly used healthiness metric may not always be effective in determining recipes suitable for diabetics, thus emphasizing the importance of the glycemic-impact estimation task. Our best classification model, trained on nutritional and crowdsourced data obtained from Amazon Mechanical Turk (AMT), can accurately identify recipes which are unhealthful for diabetics.},
booktitle = {Proceedings of the 9th International Conference on Digital Public Health},
pages = {31–35},
numpages = {5},
keywords = {recipe embeddings, recipe classification, glycemic impact},
location = {Marseille, France},
series = {DPH2019}
}

@inproceedings{10.1007/978-3-030-01391-2_1,
author = {Chai, Yan-sheng and Ma, Huang-lei and Xing, Lin-quan and Wang, Xu and Li, Bo-han},
title = {Implementation of Bus Value-Added Service Platform via Crowdsourcing Incentive},
year = {2018},
isbn = {978-3-030-01390-5},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-01391-2_1},
doi = {10.1007/978-3-030-01391-2_1},
abstract = {Sharing economy is prevailing. The network of cars and shared bicycles is convenient for people to travel. We investigate the issue of value-added service based on crowdsourcing for campus shuttles. We can provide diverse services between users by solving matching problems. The service concludes positioning and location services, requesting designating. The efficient incentive mechanisms make the shuttle bus transportation parcel convenient. We use KNN algorithm to establish KD tree to index different parcels nodes. In our app demo, we show how the application execute and how to improve the user experience who involve the orders.},
booktitle = {Advances in Conceptual Modeling: ER 2018 Workshops Emp-ER, MoBiD, MREBA, QMMQ, SCME, Xi’an, China, October 22-25, 2018, Proceedings},
pages = {3–6},
numpages = {4},
keywords = {Spatio-temporal, KD-tree, Crowdsourcing, Incentive},
location = {Xi'an, China}
}

@inproceedings{10.5555/3178876.3258514,
author = {Demartini, Gianluca and Bozzon, Alessandro},
title = {Session details: Crowdsourcing and Human Computation for the Web},
year = {2018},
isbn = {9781450356398},
publisher = {International World Wide Web Conferences Steering Committee},
address = {Republic and Canton of Geneva, CHE},
booktitle = {Proceedings of the 2018 World Wide Web Conference},
location = {Lyon, France},
series = {WWW '18}
}

@inproceedings{10.1109/GLOCOM.2018.8647916,
author = {Li, Zan and Zhao, Xiaohui and Zhao, Zhongliang and Hu, Fengye and Liang, Hui and Braun, Torsten},
title = {Crowdsensing Indoor Walking Paths with Massive Noisy Crowdsourcing User Traces},
year = {2018},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/GLOCOM.2018.8647916},
doi = {10.1109/GLOCOM.2018.8647916},
abstract = {Crowdsensing indoor walking paths based on crowdsourcing traces collected from normal users has recently become an emerging topic for indoor positioning, which can reduce the labor effort of building radio maps and improve positioning accuracy when a floor plan is unavailable. In this work, we design an indoor walking path crowdsensing system with massive noisy crowdsourcing traces. In this system, we propose a robust iterative trace merging algorithm based on WiFi access points as markers (named 'WiFi-RITA') to merge massive noisy traces. The algorithm formulates the trace merging problem as an optimization problem in which each trace is controlled to translate and rotate to minimize the limitation of distances among traces defined by WiFi access points as markers. WiFi-RITA is robust to the rotation errors and uncertain absolute locations of user traces, and can efficiently work for a large number of user traces. We further adopt a landmark matching algorithm to match the merged traces to the target building and take a 2- dimensional histogram approach to remove outlier traces. With such procedures, we generate walking paths of a large-scale building with a mean accuracy of 2.1m.},
booktitle = {2018 IEEE Global Communications Conference (GLOBECOM)},
pages = {1–6},
numpages = {6},
location = {Abu Dhabi, United Arab Emirates}
}

@inproceedings{10.5555/3327345.3327455,
author = {Hu, Zehong and Liang, Yitao and Zhang, Jie and Li, Zhao and Liu, Yang},
title = {Inference aided reinforcement learning for incentive mechanism design in crowdsourcing},
year = {2018},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Incentive mechanisms for crowdsourcing are designed to incentivize financially self-interested workers to generate and report high-quality labels. Existing mechanisms are often developed as one-shot static solutions, assuming a certain level of knowledge about worker models (expertise levels, costs of exerting efforts, etc.). In this paper, we propose a novel inference aided reinforcement mechanism that learns to incentivize high-quality data sequentially and requires no such prior assumptions. Specifically, we first design a Gibbs sampling augmented Bayesian inference algorithm to estimate workers' labeling strategies from the collected labels at each step. Then we propose a reinforcement incentive learning (RIL) method, building on top of the above estimates, to uncover how workers respond to different payments. RIL dynamically determines the payment without accessing any ground-truth labels. We theoretically prove that RIL is able to incentivize rational workers to provide high-quality labels. Empirical results show that our mechanism performs consistently well under both rational and non-fully rational (adaptive learning) worker models. Besides, the payments offered by RIL are more robust and have lower variances compared to the existing one-shot mechanisms.},
booktitle = {Proceedings of the 32nd International Conference on Neural Information Processing Systems},
pages = {5512–5522},
numpages = {11},
location = {Montr\'{e}al, Canada},
series = {NIPS'18}
}

@inproceedings{10.5555/3017447.3017508,
author = {Goh, Dion Hoe-Lian and Pe-Than, Ei Pa Pa and Lee, Chei Sian},
title = {Crowdsourcing mobile content through games: an analysis of contribution patterns},
year = {2016},
publisher = {American Society for Information Science},
address = {USA},
abstract = {Crowdsourcing of mobile content has become a major way of populating information-rich online environments. One approach to motivate participation is via games. That is, a crowdsourcing game is built upon the desire of individuals to be entertained while generating useful outputs as byproducts of gameplay. A gap in current research is that actual usage patterns of crowdsourcing games have not been investigated adequately. We address this gap by comparing content creation patterns in a game for crowdsourcing mobile content against a non-game version. Our analysis of 3024 contributions in both apps reveal 10 categories, divided into: (1) those that conform more to the notion of mobile content utilized to learn about a specific place or for navigational purposes; and (2) those that were about the content creator himself/herself, or in relation to other users or other non-playing individuals, with the location as a backdrop, similar to status updates in social media platforms like Twitter. We argue that both categories are potentially useful in that they meet different needs, and together could serve to recruit and sustain participation in the longer term. Further, the distribution of categories varied across the apps, indicating that the features afforded by games shape behavior differently from non-game-based approaches to crowdsourcing.},
booktitle = {Proceedings of the 79th ASIS&amp;T Annual Meeting: Creating Knowledge, Enhancing Lives through Information \&amp; Technology},
articleno = {61},
numpages = {10},
keywords = {mobile content, human computation, evaluation, crowdsourcing games, content analysis},
location = {Copenhagen, Denmark},
series = {ASIST '16}
}

@inproceedings{10.1145/3411764.3445477,
author = {Chiang, Chia-En and Chen, Yu-Chun and Lin, Fang-Yu and Feng, Felicia and Wu, Hao-An and Lee, Hao-Ping and Yang, Chang-Hsuan and Chang, Yung-Ju},
title = {“I Got Some Free Time”: Investigating Task-execution and Task-effort Metrics in Mobile Crowdsourcing Tasks},
year = {2021},
isbn = {9781450380966},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3411764.3445477},
doi = {10.1145/3411764.3445477},
abstract = {Using a mixed-methods approach over six weeks, we studied 30 smartphone users’ task choices, task execution and effort devoted to two commercial mobile crowdsourcing platforms in the wild. We focused on the influence of activity contexts, characterized by breakpoint situations and activity attributes. In line with their stated preferences, the participants were more likely to proactively perform mobile crowdsourcing tasks during transitions between activities than during an ongoing activity and during long breaks, respectively. Their task choices were influenced by various activity attributes, and more impacted by their current and preceding activities than their upcoming ones. Two of our three target outcomes, task execution and task choice, were also influenced by individuals’ stress and energy levels. Our qualitative data provide further insights into participants’ decisions about which crowdsourcing tasks to perform and when; and our results’ implications for the design of future mobile crowdsourcing task-prompting mechanisms are also discussed.},
booktitle = {Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems},
articleno = {648},
numpages = {14},
keywords = {qualitative analysis, notification, mixed-effect logistic regression, interruption, Mobile crowdsourcing, ESM},
location = {Yokohama, Japan},
series = {CHI '21}
}

@inproceedings{10.1145/3208903.3208927,
author = {Nasirifard, Pezhman and Rivera, Jose and Zhou, Qunjie and Schreiber, Klaus Bernd and Jacobsen, Hans-Arno},
title = {A Crowdsourcing Approach for the Inference of Distribution Grids},
year = {2018},
isbn = {9781450357678},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3208903.3208927},
doi = {10.1145/3208903.3208927},
abstract = {Maintaining a complete and up-to-date model of the distribution grid is a challenging task, and the scarcity of open models represents a significant bottleneck for researchers in this area. In this work, we address these challenges by introducing a crowdsourcing framework for the collection of open data on distribution grid devices and an algorithm to infer the topological model of the distribution grids. We use the crowd and smartphones to collect an image and the geographical position of power distribution grid devices. Since power distribution lines are usually underground and cannot be mapped, we use spatial data analytics on the collected data in combination with other open data sources to infer the topology of the distribution grid. This paper describes and evaluates our crowdsourcing and inference approach. To evaluate our approach, we organized and conducted a crowdsourcing campaign to map and infer a sizeable district in Munich, Germany. The results are compared with the ground truth of the distribution system operator. Our field experiments show that using the crowd to recognize power distribution elements, a precision of up to 82\% and a recall of up to 65\% can be obtained. The numerical evaluation of our inference algorithm demonstrates that the model we inferred based on the acquired official DSO grid dataset achieves a power length accuracy of 88\% compared to the ground truth. These results confirm our approach as a practical method to infer real power distribution grid models.},
booktitle = {Proceedings of the Ninth International Conference on Future Energy Systems},
pages = {187–199},
numpages = {13},
keywords = {Power grids, Power distribution, Geographic information systems, Distribution grid inference, Crowdsourcing},
location = {Karlsruhe, Germany},
series = {e-Energy '18}
}

@inproceedings{10.1007/978-3-030-62056-1_53,
author = {Bevins, Alisha and McPhaul, Nina and Duncan, Brittany A.},
title = {Content Is King: Impact of Task Design for Eliciting Participant Agreement in&nbsp;Crowdsourcing for HRI},
year = {2020},
isbn = {978-3-030-62055-4},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-62056-1_53},
doi = {10.1007/978-3-030-62056-1_53},
abstract = {This work investigates how the design of crowdsourced tasks can influence responses. As a formative line of inquiry, this study sought to understand how users would respond either through movement, response, or shift of focus to varying flight paths from a drone. When designing an experiment, running several proto-studies can help with generating a dataset that is actionable, but it has been unclear how differences in things such as phrasing or pre- and post-surveys can impact the results. Leveraging methods from psychology, computer-supported cooperative work, and the human-robot interaction communities this work explored the best practices and lessons learned for crowdsourcing to reduce time to actionable data for defining new communication paradigms. The lessons learned in this work will be applicable broadly within the human-robot interaction community, even outside those who are interested in defining flight paths, because they provide a scaffold on which to build future experiments seeking to communicate using non-anthropomorphic robots. Important results and recommendations include: increased negative affect with increased question quantity, completion time being relatively consistent based on total number of responses rather than number of videos, responses being more related to the video than the question, and necessity of varying question lengths to maintain engagement.},
booktitle = {Social Robotics: 12th International Conference, ICSR 2020, Golden, CO, USA, November 14–18, 2020, Proceedings},
pages = {640–651},
numpages = {12},
keywords = {Aerial vehicle, Gesture, Crowdsourced},
location = {Golden, CO, USA}
}

@inproceedings{10.1145/3106426.3106501,
author = {Ashikawa, Masayuki and Kawamura, Takahiro and Ohsuga, Akihiko},
title = {Crowdsourcing worker development based on probabilistic task network},
year = {2017},
isbn = {9781450349512},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3106426.3106501},
doi = {10.1145/3106426.3106501},
abstract = {Crowdsourcing platforms provide an attractive solution for processing numerous tasks at low cost. However, insufficient quality control remains a major concern. In the present study, we propose a grade-based training method for workers. Our training method utilizes probabilistic networks to estimate correlations between tasks based on workers' records for 18.5 million tasks and then allocates pre-learning tasks to the workers to raise the accuracy of target tasks according to the task correlations. In an experiment, the method automatically allocated 31 pre-learning task categories for 9 target task categories, and after the training of the pre-learning tasks, we confirmed that the accuracy of the target tasks was raised by 7.8 points on average. We thus confirmed that the task correlations can be estimated using a large amount of worker records, and that these are useful for the grade-based training of low-quality workers.},
booktitle = {Proceedings of the International Conference on Web Intelligence},
pages = {855–862},
numpages = {8},
keywords = {education, crowdsourcing, bayesian network},
location = {Leipzig, Germany},
series = {WI '17}
}

@inproceedings{10.1145/3106426.3106446,
author = {Kang, Qiyu and Tay, Wee Peng},
title = {Sequential multi-class labeling in crowdsourcing: a ulam-renyi game approach},
year = {2017},
isbn = {9781450349512},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3106426.3106446},
doi = {10.1145/3106426.3106446},
abstract = {We consider a crowdsourcing platform where workers are posed questions by a crowdsourcer, who then uses their responses to determine the hidden state of a multi-class labeling problem. Workers may be unreliable, therefore by designing the questions using error correction coding approaches, the crowdsourcer can achieve a more reliable overall result. We propose to perform sequential questioning in which workers are asked q-ary questions sequentially, and questions are determined based on the workers' previous responses. We propose an optimization framework to determine the best q and questioning strategy to use, subject to a crowdsourcer budget constraint. For a fixed q, this problem is equivalent to finding an optimal questioning strategy to a q-ary Ulam-R\'{e}nyi game, which is in general intractable. We propose a heuristic to find a suboptimal strategy, and demonstrate through simulations that our solution outperforms another error correction coding strategy that does not utilize previous workers' responses. Simulations also suggest that q can in general be chosen to be much smaller than the number of classes in the multi-class labeling problem.},
booktitle = {Proceedings of the International Conference on Web Intelligence},
pages = {245–251},
numpages = {7},
keywords = {ulam-r\'{e}nyi game, question design, multi-class labeling, crowdsourcing, cooperative work},
location = {Leipzig, Germany},
series = {WI '17}
}

@inproceedings{10.1145/3012709.3018007,
author = {Pipelidis, Georgios and Su, Xiang and Prehofer, Christian},
title = {Generation of indoor navigable maps with crowdsourcing},
year = {2016},
isbn = {9781450348607},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3012709.3018007},
doi = {10.1145/3012709.3018007},
abstract = {This paper presents our research in developing a model for the dynamic generation of indoor maps with crowdsourcing. With approximation of the user traces, we generate a point cloud and develop the topology of the space from time based segmentation of the traces. Moreover, we add semantic information for navigation and localization enabled maps. We discuss motivation, research objectives, and detailed research methods in this paper.},
booktitle = {Proceedings of the 15th International Conference on Mobile and Ubiquitous Multimedia},
pages = {385–387},
numpages = {3},
keywords = {semantic annotation, indoor mapping, crowdsourcing},
location = {Rovaniemi, Finland},
series = {MUM '16}
}

@inproceedings{10.1145/2818048.2835202,
author = {Celis, L. Elisa and Reddy, Sai Praneeth and Singh, Ishaan Preet and Vaya, Shailesh},
title = {Assignment Techniques for Crowdsourcing Sensitive Tasks},
year = {2016},
isbn = {9781450335928},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2818048.2835202},
doi = {10.1145/2818048.2835202},
abstract = {Protecting the privacy of crowd workers has been an important topic in crowdsourcing, however, task privacy has largely been ignored despite the fact that many tasks, e.g., form digitization, live audio transcription or image tagging often contain sensitive information. Although assigning an entire job to a worker may leak private information, jobs can often be split into small components that individually do not. We study the problem of distributing such tasks to workers with the goal of maximizing task privacy using such an approach.We introduce information loss functions to formally measure the amount of private information leaked as a function of the task assignment. We then design assignment mechanisms for three different assignment settings: PUSH, PULL and a new setting Tug Of War (TOW), which is an intermediate approach that balances flexibility for both workers and requesters. Our assignment algorithms have zero privacy loss for PUSH, and tight theoretical guarantees for PULL. For TOW, our assignment algorithm provably outperforms PULL; importantly the privacy loss is independent of the number of tasks, even when workers collude. We further analyze the performance and privacy tradeoffs empirically on simulated and real-world collusion networks and find that our algorithms outperform the theoretical guarantees.},
booktitle = {Proceedings of the 19th ACM Conference on Computer-Supported Cooperative Work \&amp; Social Computing},
pages = {836–847},
numpages = {12},
keywords = {Social Networks, Privacy, Microtasks, Crowdsourcing},
location = {San Francisco, California, USA},
series = {CSCW '16}
}

@inproceedings{10.1109/GLOCOM.2018.8647720,
author = {Li, Shu and Zhang, Jie and Xie, Dongqing and Yu, Shui and Dou, Wanchun},
title = {High Quality Participant Recruitment of Mobile Crowdsourcing over Big Data},
year = {2018},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/GLOCOM.2018.8647720},
doi = {10.1109/GLOCOM.2018.8647720},
abstract = {With the rich set of embedded sensors installed in smart-phones, an increasing number of applications have been designed based on these mobile sensors rather than on static sensors in urban areas. In Mobile Crowdsourcing (MCS), participant selection is promoted to save energy and entire incentives. Nevertheless, most of the current researches on this problem assume that the system should get the entire information about the participants. As a result, the suitable tasks are always not allocated to the suitable participants. This practice contributes an inaccurate match between a task and participants, which leads to energy and incentives waste. In view of this challenge, we aim to select participants under a more accurate prediction model, rather than assuming that the information of each participant should be obtained in advance. The prediction model is enabled by the big data of participants' historic evaluation, which are used to predict the user action. Furthermore, a greedy method based on an improved singular value decomposition (SVD), named as SVD_G, is proposed to solve this problem. Finally, the proposed SVD_G method is validated by using the large-scale dataset collected from a real-world project (DaZhongDianPing APP).},
booktitle = {2018 IEEE Global Communications Conference (GLOBECOM)},
pages = {1–6},
numpages = {6},
location = {Abu Dhabi, United Arab Emirates}
}

@inproceedings{10.1145/3357155.3360478,
author = {Amorim, Ana Maria and Vieira, Vaninha},
title = {Exploratory study on the motivation of brazilian elderly people in crowdsourcing systems},
year = {2019},
isbn = {9781450369718},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3357155.3360478},
doi = {10.1145/3357155.3360478},
abstract = {The number of elderly people in the world has been growing every year as well as their interest in using computer and internet. Crowdsourcing systems can benefit from the participation of these elderly people, however, little is known about the use of these systems by this public. The aim of our research is to propose a motivation model to support understanding crowdsourcing usage by elderly people. In this article, we present preliminary results of an exploratory study performed with interviews and observations with 6 old-aged people performing micro-tasks crowdsourcing known as Human Intelligence Task (HIT). The results indicate the potential of crowdsourcing to become a fun activity and pastime for elderly, as well as supporting the increase of self-esteem, and social engagement.},
booktitle = {Proceedings of the 18th Brazilian Symposium on Human Factors in Computing Systems},
articleno = {65},
numpages = {4},
keywords = {senior workforce, motivation, crowdsourcing},
location = {Vit\'{o}ria, Esp\'{\i}rito Santo, Brazil},
series = {IHC '19}
}

@inproceedings{10.1145/3126973.3126988,
author = {Cui, Lizhen and Zhao, Xudong and Liu, Lei and Yu, Han and Miao, Yuan},
title = {Learning Complex Crowdsourcing Task Allocation Strategies from Humans},
year = {2017},
isbn = {9781450353755},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3126973.3126988},
doi = {10.1145/3126973.3126988},
abstract = {Efficient allocation of complex tasks, which typically include heterogeneous attributes such as value, difficulty, skill required, effort required and deadline, is a challenging open problem in crowdsourcing. Existing approaches are mostly designed based on expert knowledge and fail to leverage on user generated data to capture the complex interaction of crowdsourcing participants' behaviours. In this paper, we propose a data-driven learning approach to address this challenge. The proposed approach combines supervised learning and reinforcement learning to enable agents to imitate human task allocation strategies which have shown good performance. The policy network component selects task allocation strategies and the reputation network component calculates the trends of worker reputation fluctuations. The two networks have been trained and evaluated using a large-scale real human task allocation strategy dataset derived from the Agile Manager game. Extensive experiments based on this dataset demonstrate the validity and efficiency of our approach.},
booktitle = {Proceedings of the 2nd International Conference on Crowd Science and Engineering},
pages = {33–37},
numpages = {5},
keywords = {task allocation, reinforcement learning, Crowdsourcing},
location = {Beijing, China},
series = {ICCSE'17}
}

@inproceedings{10.5555/2891460.2891668,
author = {Baba, Yukino and Kashima, Hisashi and Kinoshita, Kei and Yamaguchi, Goushi and Akiyoshi, Yosuke},
title = {Leveraging crowdsourcing to detect improper tasks in crowdsourcing marketplaces},
year = {2013},
publisher = {AAAI Press},
abstract = {Controlling the quality of tasks is a major challenge in crowdsourcing marketplaces. Most of the existing crowdsourcing services prohibit requesters from posting illegal or objectionable tasks. Operators in the marketplaces have to monitor the tasks continuously to find such improper tasks; however, it is too expensive to manually investigate each task. In this paper, we present the reports of our trial study on automatic detection of improper tasks to support the monitoring of activities by marketplace operators. We perform experiments using real task data from a commercial crowdsourcing marketplace and show that the classifier trained by the operator judgments achieves high accuracy in detecting improper tasks. In addition, to reduce the annotation costs of the operator and improve the classification accuracy, we consider the use of crowdsourcing for task annotation. We hire a group of crowdsourcing (non-expert) workers to monitor posted tasks, and incorporate their judgments into the training data of the classifier. By applying quality control techniques to handle the variability in worker reliability, our results show that the use of non-expert judgments by crowdsourcing workers in combination with expert judgments improves the accuracy of detecting improper crowdsourcing tasks.},
booktitle = {Proceedings of the Twenty-Seventh AAAI Conference on Artificial Intelligence},
pages = {1487–1492},
numpages = {6},
location = {Bellevue, Washington},
series = {AAAI'13}
}

@inproceedings{10.1145/3459043.3459059,
author = {Zhu, Mei-Li},
title = {Project-based learning model design based on crowdsourcing— Take“Game Project Development Practice”as an example∗},
year = {2021},
isbn = {9781450389617},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3459043.3459059},
doi = {10.1145/3459043.3459059},
abstract = {In the process of practical teaching, it needs to include the enthusiasm of students, teachers and teaching environment. Based on the concept of crowdsourcing, combined with the characteristics of digital media professional practice education, this paper studies and constructs the project-based learning teaching model based on crowdsourcing. Teachers and students are regarded as crowdsourcing users. By designing appropriate crowdsourcing strategies and assessment and reward mechanisms, teachers' participation and integration in practical courses can be improved, and students' enthusiasm for learning and practice can be stimulated.},
booktitle = {Proceedings of the 2021 2nd International Conference on Education Development and Studies},
pages = {63–66},
numpages = {4},
keywords = {Project based learning, Practice teaching, Crowdsourcing},
location = {Hilo, HI, USA},
series = {ICEDS '21}
}

@inproceedings{10.5555/3016387.3016480,
author = {Yu, Han and Miao, Chunyan and Shen, Zhiqi and Lin, Jun and Leung, Cyril and Yang, Qiang},
title = {Infusing human factors into Algorithmic Crowdsourcing},
year = {2016},
publisher = {AAAI Press},
booktitle = {Proceedings of the Thirtieth AAAI Conference on Artificial Intelligence},
pages = {4062–4063},
numpages = {2},
location = {Phoenix, Arizona},
series = {AAAI'16}
}

@inproceedings{10.1109/ISI.2018.8587316,
author = {da Silva, M\^{o}nica and Viterbo, Jos\'{e} and Bernardini, Flavia and Maciel, Cristiano},
title = {Identifying Privacy Functional Requirements for Crowdsourcing Applications in Smart Cities},
year = {2018},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ISI.2018.8587316},
doi = {10.1109/ISI.2018.8587316},
abstract = {Information and Communication Technologies are indispensable components of smart cities. Its applications are present in several areas, such as urban mobility, environmental issues and medical systems. In this scenario, the use of crowdsourcing technologies comes to help people to contribute to the development and improvement of the urban digital services. However, using crowdsourced data in smart cities solutions can lead to problems with the security and the privacy of user’s data. The setting of comprehensive Functional Requirements (FR) to ensure data privacy is an approach for preventing the occurrence of such issues. In this work, we intend to identify, from a literature review the main privacy requirements that have been observed in the development of applications that make use of crowdsourced data in Smart Cities scenarios.},
booktitle = {2018 IEEE International Conference on Intelligence and Security Informatics (ISI)},
pages = {106–111},
numpages = {6},
location = {Miami, FL, USA}
}

@inproceedings{10.1109/RO-MAN53752.2022.9900685,
author = {Gonzalez, Antonio Galiza Cerdeira and Lo, WingSum and Mizuuchi, Ikuo},
title = {Talk to Kotaro: a web crowdsourcing study on the impact of phone and prosody choice for synthesized speech on human impression},
year = {2022},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/RO-MAN53752.2022.9900685},
doi = {10.1109/RO-MAN53752.2022.9900685},
abstract = {During the COVID-19 pandemic, many research areas that require in person experiments with human volunteers have been impacted due to lockdowns and other activity-restricting policies. The field of robotics is no exception, and specially human-robot interaction research has been severely impacted. In order to circumvent the difficulty of gathering volunteers in person to interact with a robot, we have decided to build a novel crowdsourcing web platform for hosting our "Talk to Kotaro" experiment. The experiment consists of volunteers talking to a robot avatar and reacting to its semantic-free utterances. The developed web platform, which was built using the Python Flask framework, allows for such interactions while recording audio and video and other relevant data, which will be used for studying human impression estimation on gibberish speech. This paper describes not only the experiment and its preliminary results, but the developed platform itself; such tool is essential during pandemics and very useful for regular times, because it enables crowdsourcing data from all over the world.},
booktitle = {2022 31st IEEE International Conference on Robot and Human Interactive Communication (RO-MAN)},
pages = {244–251},
numpages = {8},
location = {Napoli, Italy}
}

@inproceedings{10.1007/978-3-642-41338-4_30,
author = {Mortensen, Jonathan M.},
title = {Crowdsourcing Ontology Verification},
year = {2013},
isbn = {9783642413377},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-642-41338-4_30},
doi = {10.1007/978-3-642-41338-4_30},
abstract = {As the scale and complexity of ontologies increases, so too do errors and engineering challenges. It is frequently unclear, however, to what degree extralogical ontology errors negatively affect the application that the ontology underpins. For example, "Shoe SubClassOf Foot" may be correct logically, but not in a human interpretation. Indeed, such errors, not caught by reasoning, are likely to be domain-specific, and thus identifying salient ontology errors requires consideration of the domain. There are both automated and manual methods that provide ontology quality assurance. Nevertheless, these methods do not readily scale as ontology size increases, and do not necessarily identify the most salient extralogical errors. Recently, crowdsourcing has enabled solutions to complex problems that computers alone cannot solve. For instance, human workers can quickly and more accurately identify objects in images at scale. Crowdsourcing presents an opportunity to develop methods for ontology quality assurance that overcome the current limitations of scalability and applicability. In this work, I aim (1) to determine the effect of extralogical ontology errors in an example domain, (2) to develop a scalable framework for crowdsourcing ontology verification that overcomes current ontology Q/A method limitations, and (3) to apply this framework to ontologies in use. I will then evaluate the method itself and also its effect in the context of a specific domain. As an example domain, I will use biomedicine, which applies many large-scale ontologies. Thus, this work will enable scalable quality assurance for extralogical errors in biomedical ontologies. Terminology},
booktitle = {Proceedings of the 12th International Semantic Web Conference - Part II},
pages = {448–455},
numpages = {8},
series = {ISWC '13}
}

@inproceedings{10.1145/3251806,
author = {Lawson, Shaun},
title = {Session details: Crowdsourcing Fans \&amp; Friends},
year = {2015},
isbn = {9781450331456},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3251806},
doi = {10.1145/3251806},
booktitle = {Proceedings of the 33rd Annual ACM Conference on Human Factors in Computing Systems},
location = {Seoul, Republic of Korea},
series = {CHI '15}
}

@inproceedings{10.1145/2897659.2897664,
author = {Weidema, Edgar R. Q. and L\'{o}pez, Consuelo and Nayebaziz, Sahand and Spanghero, Fernando and van der Hoek, Andr\'{e}},
title = {Toward microtask crowdsourcing software design work},
year = {2016},
isbn = {9781450341585},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2897659.2897664},
doi = {10.1145/2897659.2897664},
abstract = {The use of crowdsourcing as an approach for performing software engineering work is slowly but surely gaining foothold. Different models of crowdsourcing, however, have had varying levels of success to date. This paper contributes to the discussion a preliminary exploration of microtask crowdsourcing and its potential to generate design solutions. We specifically report on an experiment with Amazon Mechanical Turk workers, who each provided one or more solution alternatives to a small, partial user interface design problem. Early analysis of the results indicates that: (1) it is feasible for a crowd to generate a broad range of alternative solutions, (2) quality of those solutions varies considerably, and (3) the task, despite being small, is seen as difficult by many workers.},
booktitle = {Proceedings of the 3rd International Workshop on CrowdSourcing in Software Engineering},
pages = {41–44},
numpages = {4},
keywords = {software design, microtasks, crowdsourcing, alternatives},
location = {Austin, Texas},
series = {CSI-SE '16}
}

@inproceedings{10.1145/2998181.2998197,
author = {Law, Edith and Gajos, Krzysztof Z. and Wiggins, Andrea and Gray, Mary L. and Williams, Alex},
title = {Crowdsourcing as a Tool for Research: Implications of Uncertainty},
year = {2017},
isbn = {9781450343350},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2998181.2998197},
doi = {10.1145/2998181.2998197},
abstract = {Numerous crowdsourcing platforms are now available to support research as well as commercial goals. However, crowdsourcing is not yet widely adopted by researchers for generating, processing or analyzing research data. This study develops a deeper understanding of the circumstances under which crowdsourcing is a useful, feasible or desirable tool for research, as well as the factors that may influence researchers' decisions around adopting crowdsourcing technology. We conducted semi-structured interviews with 18 researchers in diverse disciplines, spanning the humanities and sciences, to illuminate how research norms and practitioners' dispositions were related to uncertainties around research processes, data, knowledge, delegation and quality. The paper concludes with a discussion of the design implications for future crowdsourcing systems to support research.},
booktitle = {Proceedings of the 2017 ACM Conference on Computer Supported Cooperative Work and Social Computing},
pages = {1544–1561},
numpages = {18},
keywords = {interviews, crowdsourcing for research, citizen science},
location = {Portland, Oregon, USA},
series = {CSCW '17}
}

@inproceedings{10.1145/3286606.3286837,
author = {El Khaili, Mohamed and Bakkoury, Jamila and Khiat, Azeddine and Alloubane, Abdelkarim},
title = {Crowdsourcing by IoT using LabVIEW for Measuring the Air Quality},
year = {2018},
isbn = {9781450365628},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3286606.3286837},
doi = {10.1145/3286606.3286837},
abstract = {Our challenge has two dimensions: social and technological. We want to solve a serious problem that is assessing the air quality in cities. To inform and sensitize people to the air pollution problem, our project will bring the locals in participatory situation and actor for the improvement of air quality.We hear more and more talk about the Internet of Things, connected objects, or even connected world, or even intelligent home; new concepts that invade the world and enhance our way of life. Internet of Things called the third industrial revolution will profoundly change the lives of people with home automation, health and recreation, energy, distribution and our environment with intelligent cities or transport connected. The collection of information remains a major challenge without the participation of a large group of people or partners. The Crowdsourcing allows obtaining information due to a large group of people by the internet.},
booktitle = {Proceedings of the 3rd International Conference on Smart City Applications},
articleno = {60},
numpages = {8},
keywords = {Smart city, LabView, Internet of things, Crowdsourcing, Air quality},
location = {Tetouan, Morocco},
series = {SCA '18}
}

@inproceedings{10.5555/3016387.3016616,
author = {Yu, Han and Miao, Chunyan and Liu, Siyuan and Pan, Zhengxiang and Khalid, N. Syahidah B. and Shen, Zhiqi and Leung, Cyril},
title = {Productive aging through intelligent personalized crowdsourcing},
year = {2016},
publisher = {AAAI Press},
abstract = {The current generation of senior citizens are enjoying unparalleled levels of good health than previous generations. The need for personal fulfilment after retirement has driven many of them to participate in productive aging activities such as volunteering. This paper outlines the Silver Productive (SP) mobile app, a system powered by the RTS-P intelligent personalized task sub-delegation approach with dynamic worker effort pricing functions. It provides an algorithmic crowd-sourcing platform to enable seniors to contribute their effort through productive aging activities and help organizations efficiently utilize seniors' collective productivity.},
booktitle = {Proceedings of the Thirtieth AAAI Conference on Artificial Intelligence},
pages = {4405–4406},
numpages = {2},
location = {Phoenix, Arizona},
series = {AAAI'16}
}

@inproceedings{10.1145/2910896.2925455,
author = {Goh, Dion Hoe-Lian and Pe-Than, Ei Pa Pa and Lee, Chei Sian},
title = {Games for Crowdsourcing Mobile Content: An Analysis of Contribution Patterns},
year = {2016},
isbn = {9781450342292},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2910896.2925455},
doi = {10.1145/2910896.2925455},
abstract = {Crowdsourcing of mobile content through games is becoming a major way of populating information-rich online environments. A current research gap is that actual usage patterns of crowdsourcing games has been inadequately investigated. We address this gap by comparing content creation patterns in a game for crowdsourcing mobile content against a non-game version. Results show distinct differences in the types and distribution of content created.},
booktitle = {Proceedings of the 16th ACM/IEEE-CS on Joint Conference on Digital Libraries},
pages = {249–250},
numpages = {2},
keywords = {mobile content, crowdsourcing games, content analysis},
location = {Newark, New Jersey, USA},
series = {JCDL '16}
}

@inproceedings{10.1109/INFOCOM.2016.7524546,
author = {Chen, Yanjiao and Li, Baochun and Zhang, Qian},
title = {Incentivizing crowdsourcing systems with network effects},
year = {2016},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/INFOCOM.2016.7524546},
doi = {10.1109/INFOCOM.2016.7524546},
abstract = {In a crowdsourcing system, it is important for the crowdsourcer to engineer extrinsic rewards to incentivize the participants. With mobile social networking, a user enjoys an intrinsic benefit when she aligns her behavior with the behavior of others. Referred to as network effects, such an intrinsic benefit becomes more significant as the number of users grows in the crowdsourcing system. But should a crowdsourcer design her extrinsic rewards differently when such network effects are taken into account? In this paper, we, for the first time, consider network effects as a contributing factor to intrinsic rewards, and study its influence on the design of extrinsic rewards. Rather than assuming a fixed participant population, we show that the number of participating users evolves to a steady equilibrium, thanks to subtle interactions between intrinsic rewards due to network effects and extrinsic rewards offered by the crowdsourcer. Taken network effects into consideration, we design progressively more sophisticated extrinsic reward mechanisms, and propose new and optimal strategies for a crowdsourcer to obtain a higher utility. Via extensive simulations, we demonstrate that with our new strategies, a crowdsourcer is able to attract more participants with higher contributed efforts; and participants gain higher utilities from both intrinsic and extrinsic rewards.},
booktitle = {IEEE INFOCOM 2016 - The 35th Annual IEEE International Conference on Computer Communications},
pages = {1–9},
numpages = {9},
location = {San Francisco, CA, USA}
}

@inproceedings{10.1145/2968219.2968591,
author = {Hosio, Simo and Goncalves, Jorge and van Berkel, Niels and Klakegg, Simon},
title = {Crowdsourcing situated \&amp; subjective knowledge for decision support},
year = {2016},
isbn = {9781450344623},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2968219.2968591},
doi = {10.1145/2968219.2968591},
abstract = {In this paper we present a study on crowdsourcing subjective knowledge. We introduce a mobile app that was built for this purpose, and compare results from two datasets collected using the app. One dataset was collected during a workshop and the other one during a one-week long field trial. We present interview findings on mobile knowledge collection. Further, we discuss the types of information that should optimally be collected on the go, and show how our data analysis supports the qualitative findings. This work directly continues our earlier efforts on creating a platform that encapsulates wisdom of the crowd for decision support.},
booktitle = {Proceedings of the 2016 ACM International Joint Conference on Pervasive and Ubiquitous Computing: Adjunct},
pages = {1478–1483},
numpages = {6},
keywords = {wisdom of the crowd, smartphones, mobile crowdsourcing, decision support, crowdsourcing, android},
location = {Heidelberg, Germany},
series = {UbiComp '16}
}

@inproceedings{10.1109/HICSS.2016.235,
author = {Straub, Tim and Teubner, Timm and Weinhardt, Christof},
title = {Risk Taking in Online Crowdsourcing Tournaments},
year = {2016},
isbn = {9780769556703},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/HICSS.2016.235},
doi = {10.1109/HICSS.2016.235},
abstract = {Rankings and tournaments are often used to incentivize task completion and participation in online innovation and design contests and prediction markets. One of the main challenges for platform operators is to encourage high quality contributions and effort. In this study we illustrate that in such tournaments, the participants' ranks interfere with risk taking behavior. We present an online experiment accompanying the FIFA World Cup 2014, considering the interplay of different tournament modes (individual and team rankings), the relative rank, tournament progress, and risk taking. We find that subjects take higher risk as the tournament progresses, where this increase is stronger for subjects competing individually, compared to those competing as teams.},
booktitle = {Proceedings of the 2016 49th Hawaii International Conference on System Sciences (HICSS)},
pages = {1851–1860},
numpages = {10},
series = {HICSS '16}
}

@inproceedings{10.1007/978-3-642-30284-8_43,
author = {Karampinas, Dimitris and Triantafillou, Peter},
title = {Crowdsourcing taxonomies},
year = {2012},
isbn = {9783642302831},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-642-30284-8_43},
doi = {10.1007/978-3-642-30284-8_43},
abstract = {Taxonomies are great for organizing and searching web content. As such, many popular classes of web applications, utilize them. However, their manual generation and maintenance by experts is a time-costly procedure, resulting in static taxonomies. On the other hand, mining and statistical approaches may produce low quality taxonomies. We thus propose a drastically new approach, based on the proven, increased human involvement and desire to tag/annotate web content. We define the required input from humans in the form of explicit structural, e.g., supertype-subtype relationships between concepts. Hence we harvest, via common annotation practices, the collective wisdom of users with respect to the (categorization of) web content they share and access. We further define the principles upon which crowdsourced taxonomy construction algorithms should be based. The resulting problem is NP-Hard. We thus provide and analyze heuristic algorithms that aggregate human input and resolve conflicts. We evaluate our approach with synthetic and real-world crowdsourcing experiments and on a real-world taxonomy.},
booktitle = {Proceedings of the 9th International Conference on The Semantic Web: Research and Applications},
pages = {545–559},
numpages = {15},
keywords = {taxonomy, tagging, crowdsourcing, collective intelligence},
location = {Heraklion, Crete, Greece},
series = {ESWC'12}
}

@inproceedings{10.1109/EATIS.2016.7520143,
author = {Barroso, Bruno L. K. and de Oliveira, Rodolfo R. and Macedo, Hendrik T.},
title = {Mobile crowdsourcing app for smart cities},
year = {2016},
isbn = {9781509024360},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/EATIS.2016.7520143},
doi = {10.1109/EATIS.2016.7520143},
abstract = {As current changes in population disposition are increasingly focused on cities and mega-cities, problems associated with rapid urbanization and high population density are becoming very evident and are affecting the lives of millions of people. Thus, the development of solutions that make the organization and use of urban space more efficient is highly necessary. From this need arises the concept of Smart Cities. Upon searching for applications that embodied the Smart Cities concept in the context of reporting issues in public spaces, a new application was developed. GO! Cidade is an innovative application for iOS which uses the power of mobile crowdsourcing to enable collaborative mapping of the problems in public spaces such as parks, streets or roads. To verify its effectiveness, this application was tested for two months by four people in the city of Aracaju-SE, Brazil.},
booktitle = {Proceedings of the 2016 8th Euro American Conference on Telematics and Information Systems (EATIS)},
pages = {49}
}

@inproceedings{10.1145/3340531.3411863,
author = {Qiu, Chenxi and Squicciarini, Anna and Li, Zhuozhao and Pang, Ce and Yan, Li},
title = {Time-Efficient Geo-Obfuscation to Protect Worker Location Privacy over Road Networks in Spatial Crowdsourcing},
year = {2020},
isbn = {9781450368599},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3340531.3411863},
doi = {10.1145/3340531.3411863},
abstract = {To promote cost-effective task assignment in Spatial Crowdsourcing (SC), workers are required to report their location to servers, which raises serious privacy concerns. As a solution, geo-obfuscation has been widely used to protect the location privacy of SC workers, where workers are allowed to report perturbed location instead of the true location. Yet, most existing geo-obfuscation methods consider workers? mobility on a 2 dimensional (2D) plane, wherein workers can move in arbitrary directions. Unfortunately, 2D-based geo-obfuscation is likely to generate high traveling cost for task assignment over roads, as it cannot accurately estimate the traveling costs distortion caused by location obfuscation. In this paper, we tackle the SC worker location privacy problem over road networks. Considering the network-constrained mobility features of workers, we describe workers? mobility by a weighted directed graph, which considers the dynamic traffic condition and road network topology. Based on the graph model, we design a geo-obfuscation (GO) function for workers to maximize the workers? overall location privacy without compromising the task assignment efficiency. We formulate the problem of deriving the optimal GO function as a linear programming (LP) problem. By using the angular block structure of the LP's constraint matrix, we apply Dantzig-Wolfe decomposition to improve the time-efficiency of the GO function generation. Our experimental results in the real-trace driven simulation and the real-world experiment demonstrate the effectiveness of our approach in terms of both privacy and task assignment efficiency.},
booktitle = {Proceedings of the 29th ACM International Conference on Information \&amp; Knowledge Management},
pages = {1275–1284},
numpages = {10},
keywords = {spatial crowdsourcing, location privacy, geo-obfuscation},
location = {Virtual Event, Ireland},
series = {CIKM '20}
}

@inproceedings{10.1145/3077136.3080679,
author = {Zhang, Jing and Sheng, Victor S. and Li, Tao},
title = {Label Aggregation for Crowdsourcing with Bi-Layer Clustering},
year = {2017},
isbn = {9781450350228},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3077136.3080679},
doi = {10.1145/3077136.3080679},
abstract = {This paper proposes a novel general label aggregation method for both binary and multi-class labeling in crowdsourcing, namely Bi-Layer Clustering (BLC), which clusters two layers of features - the conceptual-level and the physical-level features - to infer true labels of instances. BLC first clusters the instances using the conceptual-level features extracted from their multiple noisy labels and then performs clustering again using the physical-level features. It can facilitate tracking the uncertainty changes of the instances, so that the integrated labels that are likely to be falsely inferred on the conceptual layer can be easily corrected using the estimated labels on the physical layer. Experimental results on two real-world crowdsourcing data sets show that BLC outperforms seven state-of-the-art methods.},
booktitle = {Proceedings of the 40th International ACM SIGIR Conference on Research and Development in Information Retrieval},
pages = {921–924},
numpages = {4},
keywords = {label aggregation, inference, crowdsourcing, clustering},
location = {Shinjuku, Tokyo, Japan},
series = {SIGIR '17}
}

@inproceedings{10.1145/2556288.2556996,
author = {Vaish, Rajan and Wyngarden, Keith and Chen, Jingshu and Cheung, Brandon and Bernstein, Michael S.},
title = {Twitch crowdsourcing: crowd contributions in short bursts of time},
year = {2014},
isbn = {9781450324731},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2556288.2556996},
doi = {10.1145/2556288.2556996},
abstract = {To lower the threshold to participation in crowdsourcing, we present twitch crowdsourcing: crowdsourcing via quick contributions that can be completed in one or two seconds. We introduce Twitch, a mobile phone application that asks users to make a micro-contribution each time they unlock their phone. Twitch takes advantage of the common habit of turning to the mobile phone in spare moments. Twitch crowdsourcing activities span goals such as authoring a census of local human activity, rating stock photos, and extracting structured data from Wikipedia pages. We report a field deployment of Twitch where 82 users made 11,240 crowdsourcing contributions as they used their phone in the course of everyday life. The median Twitch activity took just 1.6 seconds, incurring no statistically distinguishable costs to unlock speed or cognitive load compared to a standard slide-to-unlock interface.},
booktitle = {Proceedings of the SIGCHI Conference on Human Factors in Computing Systems},
pages = {3645–3654},
numpages = {10},
keywords = {mobile crowdsourcing, microtasking, crowdsourcing},
location = {Toronto, Ontario, Canada},
series = {CHI '14}
}

@inproceedings{10.1145/3205651.3205796,
author = {Wang, Han and Ren, Zhilei and Li, Xiaochen and Chen, Xin and Jiang, He},
title = {Solving team making problem for crowdsourcing with hybrid metaheuristic algorithm},
year = {2018},
isbn = {9781450357647},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3205651.3205796},
doi = {10.1145/3205651.3205796},
abstract = {For a typical crowdsourcing process, a task publisher first publishes a task with an acceptable budget. Then hundreds of crowdsourced workers apply for the task with their desired bids. To recruit an adequate Crowdsourced Virtual Team (CVT) while balancing the profits of the task publisher and crowdsourced workers, previous studies proposed various algorithms, including Genetic Algorithm (GA), Alternating Variable Method (AVM), etc. However, the performance is still limited. In this study, we propose a novel hybrid metaheuristic algorithm CVTMaker to help publishers identify ideal CVTs. CVTMaker is effective which combines (1+1) Evolutionary Strategy ((1+1)-ES) and AVM to search solutions. Experimental results show that CVTMaker significantly outperforms GA and AVM over 3,117 and 5,642 of the 6,000 instances respectively.},
booktitle = {Proceedings of the Genetic and Evolutionary Computation Conference Companion},
pages = {318–319},
numpages = {2},
keywords = {virtual team making, local search, hybrid meta-heuristic algorithm, evolution strategy, crowdsourcing},
location = {Kyoto, Japan},
series = {GECCO '18}
}

@inproceedings{10.1145/3063955.3063989,
author = {Fu, Ningjia and Zhang, Jianzhong and Yu, Wenping and Wang, Changhai},
title = {Crowdsourcing-based wifi fingerprint update for indoor localization},
year = {2017},
isbn = {9781450348737},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3063955.3063989},
doi = {10.1145/3063955.3063989},
abstract = {Researches on indoor localization become more and more popular because human spend more life time indoors than outdoors. Among all of the present indoor localization technologies, WiFi fingerprint localization is the most widely used. The method of fingerprint is based on matching the current received signal strength with fingerprints stored in the database to get user's position. This method can get high precision with the simple operation, but it's a labor-intensive work to acquire the fingerprint database which costs much time and human resources. In this paper, we proposed a crowdsourcing method to build an auto-update fingerprint database using the data fed back by numerous users. First we detect the user's step sequence using inertial sensors built in smartphones. Then we establish a Hidden Markov Model (HMM) and propose a Ratio-based Map Matching(RMM) algorithm to match the step sequence with the real path in the map. After the successful match, we bind each fingerprint collected during a walk to its corresponding position, so the auto-update fingerprint database is generated. We did some experiments in a teaching building to evaluate our proposed method, and the results show the accuracy achieved by the method is related to the length of the step sequence. If the step sequence is long enough, the database we generated is very close to the manual measuring results.},
booktitle = {Proceedings of the ACM Turing 50th Celebration Conference - China},
articleno = {34},
numpages = {9},
keywords = {wifi fingerprint, step sequence, indoor localization, crowdsourcing, RMM, HMM},
location = {Shanghai, China},
series = {ACM TURC '17}
}

@inproceedings{10.1145/3254659,
author = {Demartini, Gianluca},
title = {Session details: Crowdsourcing 1},
year = {2014},
isbn = {9781450327442},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3254659},
doi = {10.1145/3254659},
booktitle = {Proceedings of the 23rd International Conference on World Wide Web},
location = {Seoul, Korea},
series = {WWW '14}
}

@inproceedings{10.1145/2740908.2741747,
author = {AlShehry, Majid Ali and Ferguson, Bruce Walker},
title = {A Taxonomy of Crowdsourcing Campaigns},
year = {2015},
isbn = {9781450334730},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2740908.2741747},
doi = {10.1145/2740908.2741747},
abstract = {Crowdsourcing serves different needs of different sets of users. Most existing definitions and taxonomies of crowdsourcing address platform purpose while paying little attention to other parameters of this novel social phenomenon. In this paper, we analyze 41 crowdsourcing campaigns on 21 crowdsourcing platforms to derive 9 key parameters of successful crowdsourcing campaigns and introduce a comprehensive taxonomy of crowdsourcing. Using this taxonomy, we identify crowdsourcing trends in two parameters, platform purpose and contributor motivation. The paper highlights important advantages of using this conceptual model in planning crowdsourcing campaigns and concludes with a discussion of emerging challenges to such campaigns.},
booktitle = {Proceedings of the 24th International Conference on World Wide Web},
pages = {475–479},
numpages = {5},
keywords = {crowdsourcing, crowdfunding, collaboration platform},
location = {Florence, Italy},
series = {WWW '15 Companion}
}

@inproceedings{10.1145/3151759.3151844,
author = {Al-Matham, Rawan N. and Al-Khalifa, Hend S.},
title = {A crowdsourcing web-based system for reporting predatory publishers},
year = {2017},
isbn = {9781450352994},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3151759.3151844},
doi = {10.1145/3151759.3151844},
abstract = {With the increasing number of predatory publishers that involves charging publication fees to authors without providing proper peer-review or editorial refereeing to qualify papers for publication; The need for a web-based system that involves the participation of authors and researchers in reporting such publishers is becoming a must.In this paper, we present the design and implementation of a web-based system for reporting predatory publishers. The system utilizes crowdsourcing to populate its database of predatory publishers. Also, the system helps researchers avoid the pitfall of publishing their research work in untrustworthy venues. Finally, the system provides browser add-ons (for Chrome and Firefox) to seamlessly notify researchers while browsing the web of predatory publishers' websites.},
booktitle = {Proceedings of the 19th International Conference on Information Integration and Web-Based Applications \&amp; Services},
pages = {573–576},
numpages = {4},
keywords = {web-based systems, predatory publishers, open access publishers, crowdsourcing, browser add-ons},
location = {Salzburg, Austria},
series = {iiWAS '17}
}

@inproceedings{10.1145/3415088.3415094,
author = {Kahasha, Emmanuella Iranga and Zuva, Tranos},
title = {Mobile crowdsourcing in crop production for farmers in rural areas of the south kivu (DRC)},
year = {2020},
isbn = {9781450375580},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3415088.3415094},
doi = {10.1145/3415088.3415094},
abstract = {A lack of an effective way to collect farm produce data, record farm input expenses, as well as expenditure on farm chemicals and receive information from other stakeholders (e.g. agriculture advisers) are some challenges encountered by farmers on their daily basis [1]. Gaining access to information and communication technologies can alleviate some of their problems and allow them to benefit from earlier unexploited opportunities [2]. In this research, we studied the factors that influenced farmers in the adoption of mobile crowdsourcing portals for agriculture purposes. A model was used to measure the perception of farmers about the technology after having used the technology for one season. The results showed that there is a strong relationship between the multiple independent factors in the model find the dependent variable "intention to use" for mobile crowdsourcing portals for agriculture. We conclude that mobile crowdsourcing application is perceived highly in enhancing the agricultural development in remote areas with regards to data accessibility, agriculture development in crop production and providing support in decision-making processes.},
booktitle = {Proceedings of the 2nd International Conference on Intelligent and Innovative Computing Applications},
articleno = {6},
numpages = {6},
keywords = {mobile crowdsourcing, mobile application, crop production},
location = {Plaine Magnien, Mauritius},
series = {ICONIC '20}
}

@inproceedings{10.1145/3512576.3512617,
author = {Lawas, Leodivino and Dalino Gorro, Ken and Ranolo, Elmo and Ilano, Anthony},
title = {Exploring and Analyzing Facebook as crowdsourcing platform for traffic updates using Selenium Support Vector Machine and Non-parametric LDA},
year = {2022},
isbn = {9781450384971},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3512576.3512617},
doi = {10.1145/3512576.3512617},
abstract = {Traffic is a major problem in the Philippines. Facebook is one of the social media platforms that is commonly used by Filipinos. Machine learning is a field of computer science that allows computers to perform tasks like human beings. In this study, the proponents explored Facebook as a source of traffic updates and as a source of traffic information. In this paper, as a partial result, a machine learning model was created to classify Facebook posts as related to traffic. To gather Facebook posts, a total of 1000 respondents were asked for consent to scrape their public post using the username link and selenium. The Support vector machine model was trained with 3000 Facebook posts. The SVM model was only trained to 3 classes {Road accident, Road activities and Other}. The SVM model was evaluated using 10-cross fold validation. The result shows that the accuracy is 76\% and the recall is 69\%. To analyze the narrative of the corpus, the Hierarchical Dirichlet Process model was created with the log-likelihood of -4.06 with 10 topic models. The following are the narratives of the corpus: {Traffic Management, Immediate Emergency Response, Seeking help, Busses causes majority of accidents.}},
booktitle = {Proceedings of the 2021 9th International Conference on Information Technology: IoT and Smart City},
pages = {226–230},
numpages = {5},
keywords = {Social Media, LDA, HDP, Facebook},
location = {Guangzhou, China},
series = {ICIT '21}
}

@inproceedings{10.5555/2772879.2773301,
author = {Kamar, Ece and Horvitz, Eric},
title = {Planning for Crowdsourcing Hierarchical Tasks},
year = {2015},
isbn = {9781450334136},
publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
address = {Richland, SC},
abstract = {We show how machine vision, learning, and planning can be combined to solve hierarchical consensus tasks. Hierarchical consensus tasks seek correct answers to a hierarchy of subtasks, where branching depends on answers at preceding levels of the hierarchy. We construct a set of hierarchical classification models that aggregate machine and human effort on different subtasks and use these inferences in planning. Optimal solution of hierarchical tasks is intractable due to the branching of task hierarchy and the long horizon of these tasks. We study Monte Carlo planning procedures that can exploit task structure to constrain the policy space for tractability. We evaluate the procedures on data collected from Galaxy Zoo II in allocating human effort and show that significant gains can be achieved.},
booktitle = {Proceedings of the 2015 International Conference on Autonomous Agents and Multiagent Systems},
pages = {1191–1199},
numpages = {9},
keywords = {monte carlo planning, mdps, crowdsourcing, consensus tasks, complementary computing},
location = {Istanbul, Turkey},
series = {AAMAS '15}
}

@inproceedings{10.1145/3491101.3519744,
author = {Lin, Fang-Yu and Lee, Chia-Yi and Ho, Yi-Ting and Chen, Yao-Kuang and Yen, Grace Yu-Chun and Chang, Yung-Ju},
title = {What Kinds of Experiences Do You Desire? A Preliminary Study of the Desired Experiences of Contributors to Location-Based Mobile Crowdsourcing},
year = {2022},
isbn = {9781450391566},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3491101.3519744},
doi = {10.1145/3491101.3519744},
abstract = {Mobile crowdsourcing enables people to learn location-related information from others with diverse experiences and opinions. However, little research has investigated the expected quality of the location-related information users of mobile-crowdsourcing platforms, and the levels and types of relevant experience such users expect crowd members to possess, respectively. To fill this gap, we first conducted an interview study with 22 participants, which yielded five key information properties of the answers to location-based questions: objectivity, relativity, specificity, temporal regularity, and variability. Based on his//her stated perceptions of these properties of the requested information, we deemed each participant to desire at least one, and up to 10 main qualities of the information, and seven main aspects of contributors’ experience. A follow-up survey study was then used to quantify the characteristics of a list of location-related information according to the information properties that the 139 respondents perceived that information to have.},
booktitle = {Extended Abstracts of the 2022 CHI Conference on Human Factors in Computing Systems},
articleno = {220},
numpages = {7},
keywords = {review, mobile crowdsourcing, location-based, information quality},
location = {New Orleans, LA, USA},
series = {CHI EA '22}
}

@inproceedings{10.1145/3210240.3210804,
author = {Walelgne, Ermias A. and Asrese, Alemnew S. and Bajpai, Vaibhav and Ott, J\"{o}rg and Manner, Jukka},
title = {Using Crowdsourcing Data for Adaptive Video Streaming in Cellular Network},
year = {2018},
isbn = {9781450357203},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3210240.3210804},
doi = {10.1145/3210240.3210804},
booktitle = {Proceedings of the 16th Annual International Conference on Mobile Systems, Applications, and Services},
pages = {512},
numpages = {1},
location = {Munich, Germany},
series = {MobiSys '18}
}

@inproceedings{10.1145/3265689.3265717,
author = {Jiang, Yun and Cui, Lizhen and Cao, Yiming and Liu, Lei and He, Wei and Pan, Li and Zheng, Yongqing and Li, Qingzhong},
title = {Spatial Crowdsourcing Task Assignment Based on the Quality of Workers},
year = {2018},
isbn = {9781450365871},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3265689.3265717},
doi = {10.1145/3265689.3265717},
abstract = {With the rapid development of mobile Internet, a variety of spatial crowdsourcing platforms have emerged and been widely applied. Task assignment is the core issue of spatial crowdsourcing. The existing methods of task assignment aim at assigning tasks to workers as much as possible, which lacks the guarantee of the quality of the tasks' answer. In this paper, two kinds of task assignment strategy based on the quality of workers are proposed to ensure the accuracy of the answer submitted by the workers as high as possible. The classical quality control algorithm, Incremental Quality Inference, is used to obtain the quality of workers. Capable worker strategy and maximum worker distance-quality strategy are proposed and compared with nearest work strategy to carry out task assignment based on the quality of workers computed by Incremental Quality Inference. Experimental results with discounted data in the offline shopping mall from crowdsourcing platform demonstrate the effectiveness of our approach.},
booktitle = {Proceedings of the 3rd International Conference on Crowd Science and Engineering},
articleno = {28},
numpages = {6},
keywords = {worker quality, task assignment strategy, spatial crowdsourcing},
location = {Singapore, Singapore},
series = {ICCSE'18}
}

@inproceedings{10.1109/ICDM.2015.119,
author = {Rahman, Habibur and Roy, Senjuti Basu and Thirumuruganathan, Saravanan and Amer-Yahia, Sihem and Das, Gautam},
title = {Task Assignment Optimization in Collaborative Crowdsourcing},
year = {2015},
isbn = {9781467395045},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/ICDM.2015.119},
doi = {10.1109/ICDM.2015.119},
abstract = {A number of emerging applications, such as, collaborative document editing, sentence translation, and citizen journalism require workers with complementary skills and expertise to form groups and collaborate on complex tasks. While existing research has investigated task assignment for knowledge intensive crowdsourcing, they often ignore the aspect of collaboration among workers, that is central to the success of such tasks. Research in behavioral psychology has indicated that large groups hinder successful collaboration. Taking that into consideration, our work is one of the first to investigate and formalize the notion of collaboration among workers and present theoretical analyses to understand the hardness of optimizing task assignment. We propose efficient approximation algorithms with provable theoretical guarantees and demonstrate the superiority of our algorithms through a comprehensive set of experiments using real-world and synthetic datasets. Finally, we conduct a real world collaborative sentence translation application using Amazon Mechanical Turk that we hope provides a template for evaluating collaborative crowdsourcing tasks in micro-task based crowdsourcing platforms.},
booktitle = {Proceedings of the 2015 IEEE International Conference on Data Mining (ICDM)},
pages = {949–954},
numpages = {6},
series = {ICDM '15}
}

@inproceedings{10.1145/3255630,
author = {Karahalios, Karrie},
title = {Session details: Crowdsourcing complexity},
year = {2014},
isbn = {9781450325400},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3255630},
doi = {10.1145/3255630},
booktitle = {Proceedings of the 17th ACM Conference on Computer Supported Cooperative Work \&amp; Social Computing},
location = {Baltimore, Maryland, USA},
series = {CSCW '14}
}

@inproceedings{10.1145/3308560.3317083,
author = {Aroyo, Lora and Dixon, Lucas and Thain, Nithum and Redfield, Olivia and Rosen, Rachel},
title = {Crowdsourcing Subjective Tasks: The Case Study of Understanding Toxicity in Online Discussions},
year = {2019},
isbn = {9781450366755},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3308560.3317083},
doi = {10.1145/3308560.3317083},
abstract = {Discussing things you care about can be difficult, especially via online platforms, where sharing your opinion leaves you open to the real and immediate threats of abuse and harassment. Due to these threats, people stop expressing themselves and give up on seeking different opinions. Recent research efforts focus on examining the strengths and weaknesses (e.g. potential unintended biases) of using machine learning as a support tool to facilitate safe space for online discussions; for example, through detecting various types of negative online behaviors such as hate speech, online harassment, or cyberbullying. Typically, these efforts build upon sentiment analysis or spam detection in text. However, the toxicity of the language could be a strong indicator for the intensity of the negative behavior. In this paper, we study the topic of toxicity in online conversations by addressing the problems of subjectivity, bias, and ambiguity inherent in this task. We start with an analysis of the characteristics of subjective assessment tasks (e.g. relevance judgment, toxicity judgment, sentiment assessment, etc). Whether we perceive something as relevant or as toxic can be influenced by almost infinite amounts of prior or current context, e.g. culture, background, experiences, education, etc. We survey recent work that tries to understand this phenomenon, and we outline a number of open questions and challenges which shape the research perspectives in this multi-disciplinary field.},
booktitle = {Companion Proceedings of The 2019 World Wide Web Conference},
pages = {1100–1105},
numpages = {6},
keywords = {toxicity, subjectivity, crowdsourcing, ACM proceedings},
location = {San Francisco, USA},
series = {WWW '19}
}

@inproceedings{10.1145/3132525.3134827,
author = {Gleason, Cole},
title = {Crowdsourcing the Installation and Maintenance of Indoor Navigation Infrastructure},
year = {2017},
isbn = {9781450349260},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3132525.3134827},
doi = {10.1145/3132525.3134827},
abstract = {Indoor navigation systems, especially for people with visual impairments, often require intensive data collection and instrumentation to install and maintain over time. The most accurate navigation systems require experts to install many Bluetooth beacons or similar technologies throughout the environment. After hardware installation, data samples must be collected to construct a signal model of the environment to be navigated. The demands of this intense upfront workload and ongoing updates pose barriers for widespread adoption of navigation systems. This document describes LuzDeploy, a system to break these installation and maintenance workflows into small, simple tasks that be completed by volunteers with little to no training. A Facebook Messenger bot coordinates the overall deployment by assigning volunteers to beacon placement, data collection, or quality assurance tasks that take only a few minutes to complete. These tasks may be batched to do more or less work, depending on the amount of time the volunteer is able to contribute. LuzDeploy allows building owners and managers to distribute the difficult task of installation and maintenance. By making these workloads easier for non-experts, LuzDeploy aims to increase adoption of indoor navigation systems and make unfamiliar spaces more accessible to people who wish to navigate independently.},
booktitle = {Proceedings of the 19th International ACM SIGACCESS Conference on Computers and Accessibility},
pages = {411–412},
numpages = {2},
keywords = {physical crowdsourcing, indoor navigation assistance, individuals with visual impairments},
location = {Baltimore, Maryland, USA},
series = {ASSETS '17}
}

@inproceedings{10.1109/HICSS.2015.197,
author = {Whitaker, Roger M. and Chorley, Martin and Allen, Stuart M.},
title = {New Frontiers for Crowdsourcing: The Extended Mind},
year = {2015},
isbn = {9781479973675},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/HICSS.2015.197},
doi = {10.1109/HICSS.2015.197},
abstract = {We introduce the concept of extended mind crowd sourcing (EMC) which capitalises on the way in which humans naturally extend their cognition into the environment, using external objects such as smartphones and applications to augment their mental capacity. This phenomenon means that human computation is embedded in data and devices, representing a new way through which human cognition can be accessed for collective discoveries. We relate EMC to existing sociological and psychological concepts and argue that it lies at the intersection of human computation, social computing and crowd sourcing. EMC is a way in which new problems and discoveries can be tackled, for example as necessitated by "wicked" problems, ethnography and culture. We relate EMC to diverse disciplines and point to ways in which the concept may develop in future. We exemplify EMC by presenting a case study where participation in location-based social networks is used to discover the correlation between mobility and human personality traits. This has involved participation from 43 countries and resulted in analysis of over half a million check-ins at street-level locations.},
booktitle = {Proceedings of the 2015 48th Hawaii International Conference on System Sciences},
pages = {1635–1644},
numpages = {10},
keywords = {social computing, personality, participatory computing, networked individualism, location-based social networks, extended mind, distributed cognition, crowdsourcing, Foursquare},
series = {HICSS '15}
}

@inproceedings{10.1109/INFOCOM.2018.8486277,
author = {Zhang, Jianhui and Lu, Pengqian and Li, Zhi and Gan, Jiayu},
title = {Distributed Trip Selection Game for Public Bike System with Crowdsourcing},
year = {2018},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/INFOCOM.2018.8486277},
doi = {10.1109/INFOCOM.2018.8486277},
abstract = {Public Bike Systems (PBSs) offer convenient and green travel service and become popular around the world. In many cities, the local governments build thousands of fixed stations for PBS to alleviate the city traffic jam and solve the last-mile problem. However, the increasing use of PBSs leads to new congestion problems in the form that users have, such as no bike to rent or no dock to return the bike. Further, users wish to receive assistance on deciding how to select bike trips with minimal time cost while taking congestion into account. Meanwhile, crowdsourcing attracted increasing attention in recent years. This paper applies it to help users share information and select bike trips before the bikes or docks are occupied. An interesting and important problem is how to help users select bike trips so that the time consumed on the trips can be minimized. We model the problem as a Bike Trip Selection (BTS) game which is shown to be equivalent to the symmetric network congestion game. This equivalence allows us to design a BTS algorithm by which the users can find at least one Nash Equilibria (NE) distributively. Furthermore, this paper evaluates the algorithm based on real datasets collected from the PBS of Hangzhou City in China. We also design a BTS system including an Android APP and a server to conduct the experiment for the distributed BTS algorithm in practice,},
booktitle = {IEEE INFOCOM 2018 - IEEE Conference on Computer Communications},
pages = {2717–2725},
numpages = {9},
location = {Honolulu, HI, USA}
}

@inproceedings{10.1145/3152494.3152495,
author = {Bhattacharyya, Malay and Mridha, Sankar Kumar},
title = {Studying the influence of requesters in posted-price crowdsourcing},
year = {2018},
isbn = {9781450363419},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3152494.3152495},
doi = {10.1145/3152494.3152495},
abstract = {Crowd-powered systems have recently emerged as useful models for solving complex tasks online by combining machine intelligence with crowd intelligence. These models are mainly of two types - collaborative and competitive. Studying the behavior of the participating crowd workers and requester experiences might yield useful insights about both these models. Analyzing the behaviors of crowd workers has been in major focus for the past several years, whereas requester behaviors have rarely been studied. In this paper, we study the activity of requesters in Flightfox, a competitive crowdsourcing environment, that works on posted-price mechanism to gain new insights about the model. We analyze the task completion data from Flightfox to investigate the patterns of interest and activity of the requesters. It came into view that requesters have an important role in the crowd-powered systems. Again, the behavioral psychology of the requesters appear to inspire the sustainability of the studied model. The global pattern of the requester activities is found to reflect scale-free behavior. Overall, we highlighted some interesting influential characteristics of the requesters in crowdsourcing platforms that were hitherto unknown.},
booktitle = {Proceedings of the ACM India Joint International Conference on Data Science and Management of Data},
pages = {1–10},
numpages = {10},
keywords = {network analysis, competitive crowdsourcing, behavioral analysis, Flightfox},
location = {Goa, India},
series = {CODS-COMAD '18}
}

@inproceedings{10.5220/0007811005780585,
author = {Hussain, Nasir},
title = {Categorical Classification of Factors Effecting Knowledge Management in Software Crowdsourcing: Hypothetical Framework},
year = {2019},
isbn = {9789897583759},
publisher = {SCITEPRESS - Science and Technology Publications, Lda},
address = {Setubal, PRT},
url = {https://doi.org/10.5220/0007811005780585},
doi = {10.5220/0007811005780585},
abstract = {Within Software Crowdsourcing, Knowledge Management has a great significance in both academia and industry as a valuable tool used to manage knowledge from the crowd. The aim of this research is to ascertain which success are and which are failure factors of Knowledge Management in Software Crowdsourcing. Literature review techniques and Quantitative Research techniques were applied in order to establish the success and failure factors. By utilizing the literature review method a total of twelve success factors were established of which seven is supported. Eight failure factures were established out of which six are supported. Subsequent to the analysis, a framework is presented in which the factors are further linked to the implementation of Knowledge Management in Software Crowdsourcing. This research and its suggested framework will also prove useful for academics to further gain a comprehensive view of Knowledge Management factors in Software Crowdsourcing for use in future studies.},
booktitle = {Proceedings of the 14th International Conference on Evaluation of Novel Approaches to Software Engineering},
pages = {578–585},
numpages = {8},
keywords = {Success and Failure Factors, Software Crowdsourcing, Quantitative Research., Knowledge Management},
location = {Heraklion, Crete, Greece},
series = {ENASE 2019}
}

@inproceedings{10.5555/3295222.3295339,
author = {T\'{a}nczos, Ervin and Nowak, Robert and Mankoff, Bob},
title = {A KL-LUCB bandit algorithm for large-scale crowdsourcing},
year = {2017},
isbn = {9781510860964},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {This paper focuses on best-arm identification in multi-armed bandits with bounded rewards. We develop an algorithm that is a fusion of lil-UCB and KL-LUCB, offering the best qualities of the two algorithms in one method. This is achieved by proving a novel anytime confidence bound for the mean of bounded distributions, which is the analogue of the LIL-type bounds recently developed for sub-Gaussian distributions. We corroborate our theoretical results with numerical experiments based on the New Yorker Cartoon Caption Contest.},
booktitle = {Proceedings of the 31st International Conference on Neural Information Processing Systems},
pages = {5896–5905},
numpages = {10},
location = {Long Beach, California, USA},
series = {NIPS'17}
}

@inproceedings{10.1007/978-3-030-19274-7_15,
author = {Chen, Rong and Li, Bo and Xing, Hu and Wang, Yijing},
title = {CrowDIY: How to Design and Adapt Collaborative Crowdsourcing Workflows Under Budget Constraints},
year = {2019},
isbn = {978-3-030-19273-0},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-19274-7_15},
doi = {10.1007/978-3-030-19274-7_15},
abstract = {Workflow quality is a key determinant of crowdsourcing complex work, but finding ways to task design and plan has proved illusive. Instead, we formulate it as an optimization problem with budget constraints and fewer decision variables to set. We propose a two-staged approach CrowDIY that can not only estimate task attributes based on previous tasks but also optimize them with budget constraints in order to publish tasks more wisely in a timely manner. Several experimental studies have been conducted, and the results show compelling evidence that, under different conditions, the proposed approach can effectively reduce the workload of workflow design and plan, while avoiding commonly encountered trial-and-error in crowdsourcing workflows and leading up to successful complex outcomes.},
booktitle = {Web Engineering: 19th International Conference, ICWE 2019, Daejeon, South Korea, June 11–14, 2019, Proceedings},
pages = {203–210},
numpages = {8},
keywords = {Crowdsourcing workflow, Workflow design and plan, Task publishing, Optimization},
location = {Daejeon, Korea (Republic of)}
}

@inproceedings{10.5555/3045390.3045455,
author = {Gao, Chao and Lu, Yu and Zhou, Dengyong},
title = {Exact exponent in optimal rates for crowdsourcing},
year = {2016},
publisher = {JMLR.org},
abstract = {In many machine learning applications, crowd-sourcing has become the primary means for label collection. In this paper, we study the optimal error rate for aggregating labels provided by a set of non-expert workers. Under the classic Dawid-Skene model, we establish matching upper and lower bounds with an exact exponent mI(π) in which m is the number of workers and I(π) the average Chernoff information that characterizes the workers' collective ability. Such an exact characterization of the error exponent allows us to state a precise sample size requirement m &gt; 1/I(π) log 1/ε in order to achieve an ε misclassification error. In addition, our results imply the optimality of various EM algorithms for crowd-sourcing initialized by consistent estimators.},
booktitle = {Proceedings of the 33rd International Conference on International Conference on Machine Learning - Volume 48},
pages = {603–611},
numpages = {9},
location = {New York, NY, USA},
series = {ICML'16}
}

@inproceedings{10.1145/3394486.3403125,
author = {Li, Ang and Duan, Yixiao and Yang, Huanrui and Chen, Yiran and Yang, Jianlei},
title = {TIPRDC: Task-Independent Privacy-Respecting Data Crowdsourcing Framework for Deep Learning with Anonymized Intermediate Representations},
year = {2020},
isbn = {9781450379984},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3394486.3403125},
doi = {10.1145/3394486.3403125},
abstract = {The success of deep learning partially benefits from the availability of various large-scale datasets. These datasets are often crowdsourced from individual users and contain private information like gender, age, etc. The emerging privacy concerns from users on data sharing hinder the generation or use of crowdsourcing datasets and lead to hunger of training data for new deep learning applications. One naive solution is to pre-process the raw data to extract features at the user-side, and then only the extracted features will be sent to the data collector. Unfortunately, attackers can still exploit these extracted features to train an adversary classifier to infer private attributes. Some prior arts leveraged game theory to protect private attributes. However, these defenses are designed for known primary learning tasks, the extracted features work poorly for unknown learning tasks. To tackle the case where the learning task may be unknown or changing, we present TIPRDC, a task-independent privacy-respecting data crowdsourcing framework with anonymized intermediate representation. The goal of this framework is to learn a feature extractor that can hide the privacy information from the intermediate representations; while maximally retaining the original information embedded in the raw data for the data collector to accomplish unknown learning tasks. We design a hybrid training method to learn the anonymized intermediate representation: (1) an adversarial training process for hiding private information from features; (2) maximally retain original information using a neural-network-based mutual information estimator. We extensively evaluate TIPRDC and compare it with existing methods using two image datasets and one text dataset. Our results show that TIPRDC substantially outperforms other existing methods. Our work is the first task-independent privacy-respecting data crowdsourcing framework.},
booktitle = {Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery \&amp; Data Mining},
pages = {824–832},
numpages = {9},
keywords = {privacy-respecting data crowdsourcing, deep learning, anonymized intermediate representations},
location = {Virtual Event, CA, USA},
series = {KDD '20}
}

@inproceedings{10.1109/ICST.2013.44,
author = {Dolstra, Eelco and Vliegendhart, Raynor and Pouwelse, Johan},
title = {Crowdsourcing GUI Tests},
year = {2013},
isbn = {9780769549682},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/ICST.2013.44},
doi = {10.1109/ICST.2013.44},
abstract = {Graphical user interfaces are difficult to test: automated tests are hard to create and maintain, while manual tests are time-consuming, expensive and hard to integrate in a continuous testing process. In this paper, we show that it is possible to crowd source GUI tests, that is, to outsource them to individuals drawn from a large pool of workers on the Internet, by instantiating virtual machines (VMs) running the system under test and letting testers access the VMs through their web browsers. This enables semi-automated continuous testing of GUIs and usability experiments with large numbers of participants at low cost. Several large experiments on the Amazon Mechanical Turk demonstrate that our approach is technically feasible and sufficiently reliable.},
booktitle = {Proceedings of the 2013 IEEE Sixth International Conference on Software Testing, Verification and Validation},
pages = {332–341},
numpages = {10},
keywords = {virtualization, usability studies, crowdsourcing, continuous testing, Mechanical Turk, GUI testing},
series = {ICST '13}
}

@inproceedings{10.5555/3237383.3238050,
author = {Luo, Yuan and Jennings, Nicholas R.},
title = {A Differential Privacy Mechanism with Network Effects for Crowdsourcing Systems},
year = {2018},
publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
address = {Richland, SC},
abstract = {In crowdsourcing systems, it is important for the crowdsource campaign initiator to incentivize users to share their data to produce results of the desired computational accuracy. This problem becomes especially challenging when users are concerned about the privacy of their data. To overcome this challenge, existing work often aims to provide users with differential privacy guarantees to incentivize privacy-sensitive users to share their data. However, this work neglects the network effect that a user enjoys greater privacy protection when he aligns his participation behaviour with that of other users. To explore the network effect and provide a suitable differential privacy guarantee, we design PINE (Privacy Incentivization with Network Effects). PINE is a mechanism that maximizes the initiator's payoff while providing participating users with privacy protections.},
booktitle = {Proceedings of the 17th International Conference on Autonomous Agents and MultiAgent Systems},
pages = {1998–2000},
numpages = {3},
keywords = {network effects, incentive mechanism, differential privacy, crowdsourcing systems},
location = {Stockholm, Sweden},
series = {AAMAS '18}
}

@inproceedings{10.1145/2897659.2897666,
author = {Machado, Leticia and Prikladnicki, Rafael and Meneguzzi, Felipe and de Souza, Cleidson R. B. and Carmel, Erran},
title = {Task allocation for crowdsourcing using AI planning},
year = {2016},
isbn = {9781450341585},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2897659.2897666},
doi = {10.1145/2897659.2897666},
abstract = {Crowdsourcing is a relatively new phenomenon in computer science and software engineering. In crowdsourcing a task is delivered to a crowd of participants who will work on this task. Task allocation is then an important aspect in the context of crowdsourcing. If done properly, it delivers successful results based on the answers provided by the crowd. However, task allocation in crowdsourcing is not a trivial problem. Factors like a task's requirements, the knowledge required for its resolution, and the size and heterogeneity of the participants in the crowd all impact task allocation, and therefore, the expected quality of the task results. In this case, the execution of actions from a plan, which assist the dynamic tasks' allocation in crowdsourcing systems, become relevant as an alternative solution. This paper formalizes task allocation in crowdsourcing scenarios as an artificial intelligence planning problem. Our results suggest that task allocation has several challenges when it is observed in distributed, undefined and dynamic environments, like in crowdsourcing scenarios. Our goal is to evaluate if automated planning is appropriate for providing a plan to match skills of crowd workers for the right tasks in software engineering projects. Preliminary results are presented in this paper.},
booktitle = {Proceedings of the 3rd International Workshop on CrowdSourcing in Software Engineering},
pages = {36–40},
numpages = {5},
keywords = {task allocation, software engineering, crowdsourcing, automated planning},
location = {Austin, Texas},
series = {CSI-SE '16}
}

@inproceedings{10.1145/3243082.3267455,
author = {Silva, Ramon and Fonseca, Augusto and Goldschmidt, Ronaldo and dos Santos, Joel and Bezerra, Eduardo},
title = {A Crowdsourcing Tool for Data Augmentation in Visual Question Answering Tasks},
year = {2018},
isbn = {9781450358675},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3243082.3267455},
doi = {10.1145/3243082.3267455},
abstract = {Visual Question Answering (VQA) is a task that connects the fields of Computer Vision and Natural Language Processing. Taking as input an image I and a natural language question Q about I, a VQA model must be able to produce a coherent answer R (also in natural language) to Q. A particular type of visual question is one in which the question is binary (i.e., a question whose answer belongs to the set {yes, no}). Currently, deep neural networks correspond to the state of the art technique for training of VQA models. Despite its success, the application of neural networks to the VQA task requires a very large amount of data in order to produce models with adequate precision. Datasets currently used for the training of VQA models are the result of laborious manual labeling processes (i.e., made by humans). This context makes relevant the study of approaches to augment these datasets in order to train more accurate prediction models. This paper describes a crowdsourcing tool which can be used in a collaborative manner to augment an existing VQA dataset for binary questions. Our tool actively integrates candidate items from an external data source in order to optimize the selection of queries to be presented to curators.},
booktitle = {Proceedings of the 24th Brazilian Symposium on Multimedia and the Web},
pages = {137–140},
numpages = {4},
keywords = {Image Annotation, Human Computation, Data Augmentation, Crowdsourcing},
location = {Salvador, BA, Brazil},
series = {WebMedia '18}
}

@inproceedings{10.1145/3173574.3173898,
author = {Agapie, Elena and Chinh, Bonnie and Pina, Laura R. and Oviedo, Diana and Welsh, Molly C. and Hsieh, Gary and Munson, Sean},
title = {Crowdsourcing Exercise Plans Aligned with Expert Guidelines and Everyday Constraints},
year = {2018},
isbn = {9781450356206},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3173574.3173898},
doi = {10.1145/3173574.3173898},
abstract = {Exercise plans help people implement behavior change. Crowd workers can help create exercise plans for clients, but their work may result in lower quality plans than produced by experts. We built CrowdFit, a tool that provides feedback about compliance with exercise guidelines and leverages strengths of crowdsourcing to create plans made by non-experts. We evaluated CrowdFit in a comparative study with 46 clients using exercise plans for two weeks. Clients received plans from crowd planners using CrowdFit, crowd planners without CrowdFit, or from expert planners. Compared to crowd planners not using CrowdFit, crowd planners using CrowdFit created plans that are more actionable and more aligned with exercise guidelines. Compared to experts, crowd planners created more actionable plans, and plans that are not significantly different with respect to tailoring, strength and aerobic principles. They struggled, however, to satisfy exercise requirements of amount of exercise. We discuss opportunities for designing technology supporting physical activity planning by non-experts.},
booktitle = {Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems},
pages = {1–13},
numpages = {13},
keywords = {exercise plans, crowdsourcing},
location = {Montreal QC, Canada},
series = {CHI '18}
}

@inproceedings{10.1145/2800835.2800966,
author = {Goncalves, Jorge and Hosio, Simo and Kostakos, Vassilis and Vukovic, Maja and Konomi, Shin'ichi},
title = {Workshop on mobile and situated crowdsourcing},
year = {2015},
isbn = {9781450335751},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2800835.2800966},
doi = {10.1145/2800835.2800966},
abstract = {Crowdsourcing beyond the desktop is increasingly attracting interest due to the rapid proliferation of smart phones and other ubiquitous technologies, such as public displays. This workshop seeks to investigate the current state of the art of mobile and situated crowdsourcing by bringing together researchers of this thriving research agenda.},
booktitle = {Adjunct Proceedings of the 2015 ACM International Joint Conference on Pervasive and Ubiquitous Computing and Proceedings of the 2015 ACM International Symposium on Wearable Computers},
pages = {1339–1342},
numpages = {4},
keywords = {ubiquitous technologies, situated crowdsourcing, mobile crowdsourcing},
location = {Osaka, Japan},
series = {UbiComp/ISWC'15 Adjunct}
}

@inproceedings{10.1145/2851581.2892312,
author = {Manojlovic, Stefan and Gavrilo, Katerina and de Wit, Jan and Khan, Vassilis-Javed and Markopoulos, Panos},
title = {Exploring the Potential of Children in Crowdsourcing},
year = {2016},
isbn = {9781450340823},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2851581.2892312},
doi = {10.1145/2851581.2892312},
abstract = {Recently, companies and academia have turned to crowdsourcing to stimulate creativity and innovation. Although children's creative nature has been well documented in the design process in co-creation for new products and/or services, this has not yet extended to crowdsourcing. With this paper, we investigate -- through crowdsourcing -- the gap between children and crowdsourcing. To gather a diverse sample of participants we used CrowdFlower, a crowdsourcing platform, to generate, evaluate and rank ideas and concepts. Results show that 93\% of parents and 80\% of non-parents would involve children in crowdsourcing. The most valued concept of the crowd was the collaboration between parents and children, who are innovating for companies. This concept involves publishing companies requesting drawings from children for book illustrations.},
booktitle = {Proceedings of the 2016 CHI Conference Extended Abstracts on Human Factors in Computing Systems},
pages = {1250–1256},
numpages = {7},
keywords = {interaction design with children, crowdsourcing},
location = {San Jose, California, USA},
series = {CHI EA '16}
}

@inproceedings{10.1145/3445815.3445818,
author = {Aslan Oguz, Evin and Kosir, Andrej},
title = {An Online Crowdsourcing Experiment to Model the Effects of a Commercial on a User's Consumption Behavior},
year = {2021},
isbn = {9781450388436},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3445815.3445818},
doi = {10.1145/3445815.3445818},
abstract = {Measuring multimedia exposure is challenging, since the impact of multimedia-content on a user is dependent on the user, the multimedia-content, the context, previous exposure, etc. We focus on measuring the impact of a commercial on a user's consumption behavior. We aim to understand the relation between a commercial and its effects on a consumer; whether watching a particular commercial affects an individual's consumption behavior regarding the advertised product. This paper covers a segment of the larger study which is building a model for measuring the multimedia exposure of the users caused by a commercial. The goal of this paper is to test the developed instrument via an online crowdsourcing study, and analyze the participation date and time, and the attentiveness of the users. The aim is to evaluate the applicability of crowdsourcing platforms as an alternative to testing real users in the laboratory. We evaluate the psychometric characteristics (validity and reliability) of the developed instrument. The reliability coefficient α shows good reliability and coefficient ω shows good saturation model.},
booktitle = {Proceedings of the 2020 4th International Conference on Computer Science and Artificial Intelligence},
pages = {15–23},
numpages = {9},
keywords = {online crowdsourcing experiment applicability, multimedia exposure model, instrument reliability},
location = {Zhuhai, China},
series = {CSAI '20}
}

@inproceedings{10.1145/3209582.3209599,
author = {Gong, Xiaowen and Shroff, Ness},
title = {Incentivizing Truthful Data Quality for Quality-Aware Mobile Data Crowdsourcing},
year = {2018},
isbn = {9781450357708},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3209582.3209599},
doi = {10.1145/3209582.3209599},
abstract = {Mobile data crowdsourcing has found a broad range of applications (e.g., spectrum sensing, environmental monitoring) by leveraging the "wisdom" of a potentially large crowd of "workers" (i.e., mobile users). A key metric of crowdsourcing is data accuracy, which relies on the quality of the participating workers' data (e.g., the probability that the data is equal to the ground truth). However, the data quality of a worker can be its own private information (which the worker learns, e.g., based on its location) that it may have incentive to misreport, which can in turn mislead the crowdsourcing requester about the accuracy of the data. This issue is further complicated by the fact that the worker can also manipulate its effort made in the crowdsourcing task and the data reported to the requester, which can also mislead the requester. In this paper, we devise truthful crowdsourcing mechanisms for Quality, Effort, and Data Elicitation (QEDE), which incentivize strategic workers to truthfully report their private worker quality and data to the requester, and make truthful effort as desired by the requester. The truthful design of the QEDE mechanisms overcomes the lack of ground truth and the coupling in the joint elicitation of worker quality, effort, and data. Under the QEDE mechanisms, we characterize the socially optimal and the requester's optimal task assignments, and analyze their performance. We show that the requester's optimal assignment is determined by the largest "virtual valuation" rather than the highest quality among workers, which depends on the worker's quality and the quality's distribution. We evaluate the QEDE mechanisms using simulations which demonstrate the truthfulness of the mechanisms and the performance of the optimal task assignments.},
booktitle = {Proceedings of the Eighteenth ACM International Symposium on Mobile Ad Hoc Networking and Computing},
pages = {161–170},
numpages = {10},
keywords = {incentive mechanism, data quality, Mobile data crowdsourcing},
location = {Los Angeles, CA, USA},
series = {Mobihoc '18}
}

@inproceedings{10.1145/3335550.3335577,
author = {Li, Ruixue and Peng, Can and Sun, Huiliang},
title = {Product Selection Strategy Analysis of Crowdsourcing Platform from the Full Cost Perspective},
year = {2019},
isbn = {9781450362641},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3335550.3335577},
doi = {10.1145/3335550.3335577},
abstract = {From the perspective of full cost, this paper uses Coase's transaction cost theory to analyze the causes of crowdsourcing, and on this basis to analyze the applicability of crowdsourcing platform products. At the same time, based on the crowdsourcing platform--zbj.com, we use the big data technology to grasp and analyze the related data of the crowdsourcing platform's successful cases in the past five months, and use the relevant statistical analysis method to categorize and analyze the industry attributes of the top five orders of the success cases of the zbj.com, in order to verify the theory mentioned in the article.},
booktitle = {Proceedings of the 2019 International Conference on Management Science and Industrial Engineering},
pages = {92–97},
numpages = {6},
keywords = {Selection Strategy Analysis, Full cost, Crowdsourcing platform, Appropriate products},
location = {Phuket, Thailand},
series = {MSIE '19}
}

@inproceedings{10.1145/2660460.2660486,
author = {Almishari, Mishari and Oguz, Ekin and Tsudik, Gene},
title = {Fighting authorship linkability with crowdsourcing},
year = {2014},
isbn = {9781450331982},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2660460.2660486},
doi = {10.1145/2660460.2660486},
abstract = {Massive amounts of contributed content -- including traditional literature, blogs, music, videos, reviews and tweets -- are available on the Internet today, with authors numbering in many millions. Textual information, such as product or service reviews, is an important and increasingly popular type of content that is being used as a foundation of many trendy community-based reviewing sites, such as TripAdvisor and Yelp. Some recent results have shown that, due partly to their specialized/topical nature, sets of reviews authored by the same person are readily linkable based on simple stylometric features. In practice, this means that individuals who author more than a few reviews under different accounts (whether within one site or across multiple sites) can be linked, which represents a significant loss of privacy.In this paper, we start by showing that the problem is actually worse than previously believed. We then explore ways to mitigate authorship linkability in community-based reviewing. We first attempt to harness the global power of crowdsourcing by engaging random strangers into the process of re-writing reviews. As our empirical results (obtained from Amazon Mechanical Turk) clearly demonstrate, crowdsourcing yields impressively sensible reviews that reflect sufficiently different stylometric characteristics such that prior stylometric linkability techniques become largely ineffective. We also consider using machine translation to automatically re-write reviews. Contrary to what was previously believed, our results show that translation decreases authorship linkability as the number of intermediate languages grows. Finally, we explore the combination of crowdsourcing and machine translation and report on results.},
booktitle = {Proceedings of the Second ACM Conference on Online Social Networks},
pages = {69–82},
numpages = {14},
keywords = {stylometry, crowdsourcing, authorship attribution, author linkability, author identification, author anonymization},
location = {Dublin, Ireland},
series = {COSN '14}
}

@inproceedings{10.1109/GLOCOM.2017.8255048,
author = {Yadav, Akash and Sairam, Ashok Singh and Kumar, Anand},
title = {Concurrent Team Formation for Multiple Tasks in Crowdsourcing Platform},
year = {2017},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/GLOCOM.2017.8255048},
doi = {10.1109/GLOCOM.2017.8255048},
abstract = {The tremendous growth of social media technologies has inspired research communities as well as industries to extend the horizon of organizations by recruiting workers available on freelancing sites. Most of the tasks usually require expertise of workers from diverse domains, thus the problem can be reduced to that of team formation. In this work, we address the problem of assigning workers to tasks, where each task requires a set of skills and thus may require more than one worker to successfully complete the task. Given a set of tasks, and a set of workers each with a cost, the objective is to find mutually exclusive set of workers for each task, who can accomplish the task in the most cost-effective manner. The problem being NP-hard, we propose an approximation algorithm that attempts to find the best fit workers based on their collective intelligence for a single task. This approach selects workers in a manner that their expertise complements each other, hence maintaining a balance among the required skills. Such balanced assignment sets require lesser number of workers and reduce the overall cost. The approach is then extended to a set of N tasks. We show that the solution is (2+ α) approximate. The proposed algorithm is evaluated against different assignment schemes. Experimental results using real data show that our approach performs well.},
booktitle = {GLOBECOM 2017 - 2017 IEEE Global Communications Conference},
pages = {1–7},
numpages = {7},
location = {Singapore}
}

@inproceedings{10.1145/3183713.3183732,
author = {Xin, Hao and Meng, Rui and Chen, Lei},
title = {Subjective Knowledge Base Construction Powered By Crowdsourcing and Knowledge Base},
year = {2018},
isbn = {9781450347037},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3183713.3183732},
doi = {10.1145/3183713.3183732},
abstract = {Knowledge base construction (KBC) has become a hot and in-time topic recently with the increasing application need of large-scale knowledge bases (KBs), such as semantic search, QA systems, the Google Knowledge Graph and IBM Watson QA System. Existing KBs mainly focus on encoding the factual facts of the world, e.g., city area and company product, which are regarded as the objective knowledge, whereas the subjective knowledge, which is frequently mentioned in Web queries, has been neglected. The subjective knowledge has no documented ground truth, instead, the truth relies on people's dominant opinion, which can be solicited from online crowd workers. In our work, we propose a KBC framework for subjective knowledge base construction taking advantage of the knowledge from the crowd and existing KBs. We develop a two-staged framework for subjective KB construction which consists of core subjective KB construction and subjective KB enrichment. Firstly, we try to build a core subjective KB mined from existing KBs, where every instance has rich objective properties. Then, we populate the core subjective KB with instances extracted from existing KBs, in which the crowd is leverage to annotate the subjective property of the instances. In order to optimize the crowd annotation process, we formulate the problem of subjective KB enrichment procedure as a cost-aware instance annotation problem and propose two instance annotation algorithms, i.e., adaptive instance annotation and batch-mode instance annotation algorithms. We develop a two-stage system for subjective KB construction which consists of core subjective KB construction and subjective knowledge enrichment. We evaluate our framework on real knowledge bases and a real crowdsourcing platform, the experimental results show that we can derive high quality subjective knowledge facts from existing KBs and crowdsourcing techniques through our proposed framework.},
booktitle = {Proceedings of the 2018 International Conference on Management of Data},
pages = {1349–1361},
numpages = {13},
keywords = {subjective knowledge, knowledge base construction, crowdsourcing},
location = {Houston, TX, USA},
series = {SIGMOD '18}
}

@inproceedings{10.5555/2832249.2832366,
author = {Sun, Yuyin and Singla, Adish and Fox, Dieter and Krause, Andreas},
title = {Building hierarchies of concepts via crowdsourcing},
year = {2015},
isbn = {9781577357384},
publisher = {AAAI Press},
abstract = {Hierarchies of concepts are useful in many applications from navigation to organization of objects. Usually, a hierarchy is created in a centralized manner by employing a group of domain experts, a time-consuming and expensive process. The experts often design one single hierarchy to best explain the semantic relationships among the concepts, and ignore the natural uncertainty that may exist in the process. In this paper, we propose a crowdsourcing system to build a hierarchy and furthermore capture the underlying uncertainty. Our system maintains a distribution over possible hierarchies and actively selects questions to ask using an information gain criterion. We evaluate our methodology on simulated data and on a set of real world application domains. Experimental results show that our system is robust to noise, efficient in picking questions, cost-effective, and builds high quality hierarchies.},
booktitle = {Proceedings of the 24th International Conference on Artificial Intelligence},
pages = {844–851},
numpages = {8},
location = {Buenos Aires, Argentina},
series = {IJCAI'15}
}

@inproceedings{10.1007/978-3-030-29908-8_7,
author = {Truong, Nhat Van-Quoc and Stein, Sebastian and Tran-Thanh, Long and Jennings, Nicholas R.},
title = {What Prize Is Right? How to Learn the Optimal Structure for Crowdsourcing Contests},
year = {2019},
isbn = {978-3-030-29907-1},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-29908-8_7},
doi = {10.1007/978-3-030-29908-8_7},
abstract = {In crowdsourcing, one effective method for encouraging par-ticipants to perform tasks is to run contests where participants compete against each other for rewards. However, there are numerous ways to implement such contests in specific projects. They could vary in their structure (e.g., performance evaluation and the number of prizes) and parameters (e.g., the maximum number of participants and the amount of prize money). Additionally, with a given budget and a time limit, choosing incentives (i.e., contest structures with specific parameter values) that maximise the overall utility is not trivial, as their respective effectiveness in a specific project is usually unknown a priori. Thus, in this paper, we propose a novel algorithm, BOIS (Bayesian-optimisation-based incentive selection), to learn the optimal structure and tune its parameters effectively. In detail, the learning and tuning problems are solved simultaneously by using online learning in combination with Bayesian optimisation. The results of our extensive simulations show that the performance of our algorithm is up&nbsp;to 85\% of the optimal and up&nbsp;to 63\% better than state-of-the-art benchmarks.},
booktitle = {PRICAI 2019: Trends in Artificial Intelligence: 16th Pacific Rim International Conference on Artificial Intelligence, Cuvu, Yanuca Island, Fiji, August 26–30, 2019, Proceedings, Part I},
pages = {85–97},
numpages = {13},
keywords = {Incentive, Crowdsourcing, Bayesian optimisation},
location = {Cuvu, Yanuka Island, Fiji}
}

@inproceedings{10.1145/2647868.2654908,
author = {Seetharaman, Prem and Pardo, Bryan},
title = {Crowdsourcing a Reverberation Descriptor Map},
year = {2014},
isbn = {9781450330633},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2647868.2654908},
doi = {10.1145/2647868.2654908},
abstract = {Audio production is central to every kind of media that involves sound, such as film, television, and music and involves transforming audio into a state ready for consumption by the public. One of the most commonly-used audio production tools is the reverberator. Current interfaces are often complex and hard-to-understand. We seek to simplify these interfaces by letting users communicate their audio production objective with descriptive language (e.g. "Make the drums sound bigger."). To achieve this goal, a system must be able to tell whether the stated goal is appropriate for the selected tool (e.g. making the violin warmer using a panning tool does not make sense). If the goal is appropriate for the tool, it must know what actions lead to the goal. Further, the tool should not impose a vocabulary on users, but rather understand the vocabulary users prefer. In this work, we describe SocialReverb, a project to crowdsource a vocabulary of audio descriptors that can be mapped onto concrete actions using a parametric reverberator. We deployed SocialReverb, on Mechanical Turk, where 513 unique users described 256 instances of reverberation using 2861 unique words. We used this data to build a concept map showing which words are popular descriptors, which ones map consistently to specific reverberation types, and which ones are synonyms. This promises to enable future interfaces that let the user communicate their production needs using natural language.},
booktitle = {Proceedings of the 22nd ACM International Conference on Multimedia},
pages = {587–596},
numpages = {10},
keywords = {interfaces, human computation, audio synonyms, audio engineering, audio descriptors},
location = {Orlando, Florida, USA},
series = {MM '14}
}

@inproceedings{10.1007/978-3-319-91458-9_18,
author = {Tao, Qian and Zeng, Yuxiang and Zhou, Zimu and Tong, Yongxin and Chen, Lei and Xu, Ke},
title = {Multi-Worker-Aware Task Planning in&nbsp;Real-Time Spatial Crowdsourcing},
year = {2018},
isbn = {978-3-319-91457-2},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-319-91458-9_18},
doi = {10.1007/978-3-319-91458-9_18},
abstract = {Spatial crowdsourcing emerges as a new computing paradigm with the development of mobile Internet and the ubiquity of mobile devices. The core of many real-world spatial crowdsourcing applications is to assign suitable tasks to proper workers in real time. Many works only assign a set of tasks to each worker without making the plan how to perform the assigned tasks. Others either make task plans only for a single worker or are unable to operate in real time. In this paper, we propose a new problem called the Multi-Worker-AwareTaskPlanning (MWATP) problem in the online scenario, in which we not only assign tasks to workers but also make plans for them, such that the total utility (revenue) is maximized. We prove that the offline version of MWATP problem is NP-hard, and no online algorithm has a constant competitive ratio on the MWATP problem. Two heuristic algorithms, called Delay-Planning and Fast-Planning, are proposed to solve the problem. Extensive experiments on synthetic and real datasets verify the effectiveness and efficiency of the two proposed algorithms.},
booktitle = {Database Systems for Advanced Applications: 23rd International Conference, DASFAA 2018, Gold Coast, QLD, Australia, May 21-24, 2018, Proceedings, Part II},
pages = {301–317},
numpages = {17},
keywords = {Spatial crowdsourcing, Task assignment, Task planning},
location = {Gold Coast, QLD, Australia}
}

@inproceedings{10.1145/3126673.3126677,
author = {O'Leary, Anthony and O'Raghallaigh, Paidi and Nagle, Tadhg and Sammon, David},
title = {Crowdsourcing from the Community to Resolve Complex Service Requests},
year = {2017},
isbn = {9781450354172},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3126673.3126677},
doi = {10.1145/3126673.3126677},
abstract = {The VMware Community may provide an opportunity to VMware to tap into the collective intelligence of its 2.4 million strong members to generate intelligent responses to complex Service Requests (SRs). Bill Joy, cofounder of Sun Microsystems, put it well when he said: "No matter who you are, most of the smartest people work for someone else..!". The data generated from the vSlua project using an Action Design Research approach shows that the Community resolves lower complexity SR issues efficiently, but begins to struggle as the complexity increases. Almost 50\% of all the SRs were answered and over 50\% of the answered SRs were resolved in under 6 hours.},
booktitle = {Proceedings of the 13th International Symposium on Open Collaboration Companion},
articleno = {5},
numpages = {5},
keywords = {Service Requests, Industry-academia Collaboration, Design Research, Crowdsourcing, Community, Action Research},
location = {Galway, Ireland},
series = {OpenSym '17}
}

@inproceedings{10.1145/1868914.1868921,
author = {Alt, Florian and Shirazi, Alireza Sahami and Schmidt, Albrecht and Kramer, Urs and Nawaz, Zahid},
title = {Location-based crowdsourcing: extending crowdsourcing to the real world},
year = {2010},
isbn = {9781605589343},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1868914.1868921},
doi = {10.1145/1868914.1868921},
abstract = {The WWW and the mobile phone have become an essential means for sharing implicitly and explicitly generated information and a communication platform for many people. With the increasing ubiquity of location sensing included in mobile devices we investigate the arising opportunities for mobile crowdsourcing making use of the real world context. In this paper we assess how the idea of user-generated content, web-based crowdsourcing, and mobile electronic coordination can be combined to extend crowdsourcing beyond the digital domain and link it to tasks in the real world. To explore our concept we implemented a crowd-sourcing platform that integrates location as a parameter for distributing tasks to workers. In the paper we describe the concept and design of the platform and discuss the results of two user studies. Overall the findings show that integrating tasks in the physical world is useful and feasible. We observed that (1) mobile workers prefer to pull tasks rather than getting them pushed, (2) requests for pictures were the most favored tasks, and (3) users tended to solve tasks mainly in close proximity to their homes. Based on this, we discuss issues that should be considered during designing mobile crowdsourcing applications.},
booktitle = {Proceedings of the 6th Nordic Conference on Human-Computer Interaction: Extending Boundaries},
pages = {13–22},
numpages = {10},
keywords = {mobile phone, location, crowdsourcing, context},
location = {Reykjavik, Iceland},
series = {NordiCHI '10}
}

@inproceedings{10.1145/2800835.2801628,
author = {Loke, Seng W.},
title = {On crowdsourcing information maps: cornucopia of the commons for the city},
year = {2015},
isbn = {9781450335751},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2800835.2801628},
doi = {10.1145/2800835.2801628},
abstract = {Information maps about urban phenomena can be constructed via crowdsourcing, creating a potential Cornucopia of the Commons, as information can be reused by many. We present a stylised model of crowdsourcing for building urban information maps, and make observations about the behaviour of contributors.},
booktitle = {Adjunct Proceedings of the 2015 ACM International Joint Conference on Pervasive and Ubiquitous Computing and Proceedings of the 2015 ACM International Symposium on Wearable Computers},
pages = {1527–1533},
numpages = {7},
location = {Osaka, Japan},
series = {UbiComp/ISWC'15 Adjunct}
}

@inproceedings{10.5555/3204094.3204121,
author = {Xue, Qinghan and Chuah, Mooi Choo},
title = {Incentivising high quality crowdsourcing clinical data for disease prediction},
year = {2017},
isbn = {9781509047215},
publisher = {IEEE Press},
abstract = {Predictive modeling is fundamental in transforming large clinical data sets into actionable knowledge which can guide clinical decision making and personalized medicine. Although several studies have merged data mining techniques with statistical analysis to extract hidden patterns from large database, these proposed mechanisms are excessively complex for practical use. Therefore, it is essential that a better tool is developed for disease progression and survival rate predictions. In this paper, we first present how carefully chosen clinical features with our proper data cleaning method improves the accuracy of the the Amyotrophic Lateral Sclerosis (ALS) disease progression and survival rate predictions. In addition, we present an incentive model which provides individual rationality and platform profitability features to encourage hospitals to share high quality data for such predictions. Our evaluation results show promising outcomes for our proposed approaches.},
booktitle = {Proceedings of the Second IEEE/ACM International Conference on Connected Health: Applications, Systems and Engineering Technologies},
pages = {185–194},
numpages = {10},
keywords = {survival rate prediction, platform profitability, individual rationality, incentive, feature selection, ALS},
location = {Philadelphia, Pennsylvania},
series = {CHASE '17}
}

@inproceedings{10.1145/3126673.3126683,
author = {Cullina, Eoin},
title = {A Crowdsourcing Practices Framework for Science Funding Call Processes},
year = {2017},
isbn = {9781450354172},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3126673.3126683},
doi = {10.1145/3126673.3126683},
abstract = {Public scientific research funding agencies (funding agencies) are charged with the task of implementing government science policy and identifying research projects worthy of funding. They play an important role in creating value for society through funding research and informing research policy. However, the work of funding agencies in recent years has been hampered by various challenges in call processes. This research proposes crowdsourcing as a potential solution for funding agencies. Information systems research has engaged with crowdsourcing and the open innovation phenomenon. Crowdsourcing has been utilised by both private organisations and governments in the seeking solutions to similar types of challenges. Despite this fact, no crowdsourcing frameworks have been adapted to address the types of challenges faced by funding agencies in call processes. This research seeks to identify challenges faced by funding agencies for the purposes adapting a crowdsourcing practices framework to address these challenges.},
booktitle = {Proceedings of the 13th International Symposium on Open Collaboration Companion},
articleno = {10},
numpages = {5},
keywords = {scientific research funding agencies, open innovation, Crowdsourcing},
location = {Galway, Ireland},
series = {OpenSym '17}
}

@inproceedings{10.1109/ICSM.2012.6405249,
author = {Sillito, Jonathan and Maurer, Frank and Nasehi, Seyed Mehdi and Burns, Chris},
title = {What makes a good code example? A study of programming Q&amp;A in StackOverflow},
year = {2012},
isbn = {9781467323130},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/ICSM.2012.6405249},
doi = {10.1109/ICSM.2012.6405249},
abstract = {Programmers learning how to use an API or a programming language often rely on code examples to support their learning activities. However, what makes for an effective ode example remains an open question. Finding the haracteristics of the effective examples is essential in improving the appropriateness of these learning aids. To help answer this question we have onducted a qualitative analysis of the questions and answers posted to a programming Q&amp;A web site called StackOverflow. On StackOverflow answers can be voted on, indicating which answers were found helpful by users of the site. By analyzing these well-received answers we identified haracteristics of effective examples. We found that the explanations acompanying examples are as important as the examples themselves. Our findings have implications for the way the API documentation and example set should be developed and evolved as well as the design of the tools assisting the development of these materials.},
booktitle = {Proceedings of the 2012 IEEE International Conference on Software Maintenance (ICSM)},
pages = {25–34},
numpages = {10},
keywords = {social learning, documentation, code example, Web sites, Software maintenance, Programming, Java, Documentation, Conferences, Best practices, API},
series = {ICSM '12}
}

@inproceedings{10.5555/3172077.3172083,
author = {Augustin, Alexandry and Venanzi, Matteo and Rogers, Alex and Jennings, Nicholas R.},
title = {Bayesian aggregation of categorical distributions with applications in crowdsourcing},
year = {2017},
isbn = {9780999241103},
publisher = {AAAI Press},
abstract = {A key problem in crowdsourcing is the aggregation of judgments of proportions. For example, workers might be presented with a news article or an image, and be asked to identify the proportion of each topic, sentiment, object, or colour present in it. These varying judgments then need to be aggregated to form a consensus view of the document's or image's contents. Often, however, these judgments are skewed by workers who provide judgments randomly. Such spammers make the cost of acquiring judgments more expensive and degrade the accuracy of the aggregation. For such cases, we provide a new Bayesian framework for aggregating these responses (expressed in the form of categorical distributions) that for the first time accounts for spammers. We elicit 796 judgments about proportions of objects and colours in images. Experimental results show comparable aggregation accuracy when 60\% of the workers are spammers, as other state of the art approaches do when there are no spammers.},
booktitle = {Proceedings of the 26th International Joint Conference on Artificial Intelligence},
pages = {1411–1417},
numpages = {7},
location = {Melbourne, Australia},
series = {IJCAI'17}
}

@inproceedings{10.1145/3178876.3186033,
author = {Yang, Jie and Drake, Thomas and Damianou, Andreas and Maarek, Yoelle},
title = {Leveraging Crowdsourcing Data for Deep Active Learning An Application: Learning Intents in Alexa},
year = {2018},
isbn = {9781450356398},
publisher = {International World Wide Web Conferences Steering Committee},
address = {Republic and Canton of Geneva, CHE},
url = {https://doi.org/10.1145/3178876.3186033},
doi = {10.1145/3178876.3186033},
abstract = {This paper presents a generic Bayesian framework that enables any deep learning model to actively learn from targeted crowds. Our framework inherits from recent advances in Bayesian deep learning, and extends existing work by considering the targeted crowdsourcing approach, where multiple annotators with unknown expertise contribute an uncontrolled amount (often limited) of annotations. Our framework leverages the low-rank structure in annotations to learn individual annotator expertise, which then helps to infer the true labels from noisy and sparse annotations. It provides a unified Bayesian model to simultaneously infer the true labels and train the deep learning model in order to reach an optimal learning efficacy. Finally, our framework exploits the uncertainty of the deep learning model during prediction as well as the annotators» estimated expertise to minimize the number of required annotations and annotators for optimally training the deep learning model. We evaluate the effectiveness of our framework for intent classification in Alexa (Amazon»s personal assistant), using both synthetic and real-world datasets. Experiments show that our framework can accurately learn annotator expertise, infer true labels, and effectively reduce the amount of annotations in model training as compared to state-of-the-art approaches. We further discuss the potential of our proposed framework in bridging machine learning and crowdsourcing towards improved human-in-the-loop systems.},
booktitle = {Proceedings of the 2018 World Wide Web Conference},
pages = {23–32},
numpages = {10},
keywords = {deep active learning, crowdsourcing, conversational agents},
location = {Lyon, France},
series = {WWW '18}
}

@inproceedings{10.1145/3266237.3266265,
author = {Vaz, Luis and Marczak, Sabrina and Steinmacher, Igor},
title = {An empirical study on task documentation in software crowdsourcing: the case of the topcoder platform},
year = {2018},
isbn = {9781450365031},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3266237.3266265},
doi = {10.1145/3266237.3266265},
abstract = {Software Crowdsourcing, the act of outsourcing software development tasks to a crowd in the form of an open call, happens mediated by a platform and is based on tasks. In the competitive model, the members of the crowd seek for tasks and submit solutions attempting to receive financial rewards In this context, task description plays a relevant role since its understanding supports the choice and development of a task. Little is known about the role of task description as support for these processes. In order to contribute to fill this gap, this paper presents an empirical study exploring the role of documentation when developers select and develop tasks in software crowdsourcing. The TopCoder platform was studied in two stages: a case study with newcomers to crowdsourcing (in the classroom); and a study based on interviews with industry professionals. We identified that the documentation quality influences task selection. Tasks with unclear objective description, without specifying required technologies or environment setup instructions, discourage developers from selecting the task. We also found that poorly specified or incomplete tasks lead developers to look for supplementary material or invest more time and effort than initially estimated. The results provide a better understanding about the importance of task documentation in software crowdsourcing and point out what information is important to the crowd.},
booktitle = {Proceedings of the XXXII Brazilian Symposium on Software Engineering},
pages = {62–71},
numpages = {10},
keywords = {topcoder, software crowdsourcing, documentation},
location = {Sao Carlos, Brazil},
series = {SBES '18}
}

@inproceedings{10.1145/3369740.3372730,
author = {Basak, Jayanta and Bhaumik, Parama and Roy, Siuli and Bandyopadhyay, Somprakash},
title = {A Crowdsourcing based Information System Framework for Coordinated Disaster Management and Building Community Resilience},
year = {2020},
isbn = {9781450377515},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3369740.3372730},
doi = {10.1145/3369740.3372730},
abstract = {Disaster management involves intensive coordination among multiple agencies like police, fire departments, public health, non-govt. agencies, including local volunteers/field workers. Accurate situational information about damage, resource needs, available resources etc., in the affected areas help the disaster management agencies in proper damage and need assessment and prepare suitable resource deployment plan. Crowdsourcing has become a popular approach for information collection where open crowds of people share multimodal situational information (text, images, audio, video etc.) about any event through social media posts. However, the authenticity and reliability of such posts are still debatable. Gathering situational data directly from the affected community (community-sourcing) can supplement social media posts to generate effective insights. In this paper, we attempt to design and develop a multiplatform disaster management information system where both social media-based crowdsourcing and community sourcing techniques are used to accumulate location-specific situational information. Subsequently, a coherent picture of the disaster situation is evolved through the integration of these local snapshots. Here, we explore how community participation, in the context of disaster management, can be enhanced through collaborative knowledge transaction, which eventually will lead towards the development of a resilient community. A field trial of our system is conducted involving a remote village community at Namkhana, West Bengal.},
booktitle = {Proceedings of the 21st International Conference on Distributed Computing and Networking},
articleno = {33},
numpages = {6},
keywords = {Information Crowdsourcing, Disaster Management Information System, Community Resilience},
location = {Kolkata, India},
series = {ICDCN '20}
}

@inproceedings{10.1145/3230467.3230470,
author = {Peng, Fei and Liu, Yu and Lu, Bin},
title = {Research and Application of Block Chain Technology in Crowdsourcing Platform},
year = {2018},
isbn = {9781450364300},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3230467.3230470},
doi = {10.1145/3230467.3230470},
abstract = {Crowdsourcing has become an important mean of value realization in the modern economic environment. Because of the characteristics of decentralization, mutual-trust and non-tampering, block chain technology can establish a distributed trust environment, which is quite suitable for crowdsourcing transactions. Based on the requirements of crowdsourcing transactions and the basic conception of block chain technology, the crowdsourcing trading collaboration mechanism under block chain model is proposed to optimize transaction process and consensus process. Further, the architecture of crowdsourcing platform using the block chain technology is built, providing a framework for the development of the crowdsourcing platform.},
booktitle = {Proceedings of the 2018 International Conference on E-Business and Mobile Commerce},
pages = {1–5},
numpages = {5},
keywords = {Platform architecture, Crowdsourcing, Collaboration mechanism, Block chain technology},
location = {Chengdu, China},
series = {ICEMC '18}
}

@inproceedings{10.1007/978-3-319-68786-5_21,
author = {Sun, Yue and Liu, An and Li, Zhixu and Liu, Guanfeng and Zhao, Lei and Zheng, Kai},
title = {Anonymity-Based Privacy-Preserving Task Assignment in Spatial Crowdsourcing},
year = {2017},
isbn = {978-3-319-68785-8},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-319-68786-5_21},
doi = {10.1007/978-3-319-68786-5_21},
abstract = {The ubiquity of mobile device and wireless networks flourishes the market of Spatial Crowdsourcing (SC), in which location constrained tasks are sent to workers and expected to be performed in some designated locations. To obtain a global optimal task assignment scheme, the SC-server usually needs to collect location information of all workers. During this process, there is a significant security concern, that is, SC-server may not be trustworthy, so it brings about a threat to workers location privacy. In this paper, we focus on the privacy-preserving task assignment in SC. By introducing a semi-honest third party, we present an approach for task assignment in which location privacy of workers can be protected in a k-anonymity manner. We theoretically show that the proposed model is secure against semi-honest adversaries. Experimental results show that our approach is efficient and can scale to real SC applications.},
booktitle = {Web Information Systems Engineering – WISE 2017: 18th International Conference, Puschino, Russia, October 7-11, 2017, Proceedings, Part II},
pages = {263–277},
numpages = {15},
keywords = {Privacy-preserving, Spatial crowdsourcing, Spatial task assignment},
location = {Puschino, Russia}
}

@inproceedings{10.1145/3018896.3018916,
author = {Alabduljabbar, Reham and Al-Dossari, Hmood},
title = {Towards a classification model for tasks in crowdsourcing},
year = {2017},
isbn = {9781450347747},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3018896.3018916},
doi = {10.1145/3018896.3018916},
abstract = {Crowdsourcing is an increasingly popular approach for utilizing the power of the crowd in performing tasks that cannot be solved sufficiently by machines. Text annotation and image labeling are two examples of crowdsourcing tasks that are difficult to automate and human knowledge is often required. However, the quality of the obtained outcome from the crowdsourcing is still problematic. To obtain high-quality results, different quality control mechanisms should be applied to evaluate the different type of tasks. In a previous work, we present a task ontology-based model that can be utilized to identify which quality mechanism is most appropriate based on the task type. In this paper, we complement our previous work by providing a categorization of crowdsourcing tasks. That is, we define the most common task types in the crowdsourcing context. Then, we show how machine learning algorithms can be used to infer automatically the type of the crowdsourced task.},
booktitle = {Proceedings of the Second International Conference on Internet of Things, Data and Cloud Computing},
articleno = {23},
numpages = {7},
keywords = {task, quality control, crowdsourcing, classification, amazon Mturk},
location = {Cambridge, United Kingdom},
series = {ICC '17}
}

@inproceedings{10.1145/3126973.3126994,
author = {Cao, Yiming and Liu, Lei and Cui, Lizhen and Li, Qingzhong},
title = {Empirical Study on Assessment Algorithms with Confidence in Crowdsourcing},
year = {2017},
isbn = {9781450353755},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3126973.3126994},
doi = {10.1145/3126973.3126994},
abstract = {Evaluating the quality of workers is very important in crowdsourcing system and impactful methods are required in order to obtain the most appropriate quality. Previous work have introduced confidence intervals to estimate the quality of workers. However, we have found the size of the confidence interval is wide through analysis of experimental results, which leads to inaccurate worker error rates. In this paper, we propose an optimized algorithm of confidence interval to reduce the size of the confidence interval as narrow as possible and to estimate the quality of workers more precise. We verify our algorithm using the simulated data from our own crowdsourcing platform under realistic settings.},
booktitle = {Proceedings of the 2nd International Conference on Crowd Science and Engineering},
pages = {100–104},
numpages = {5},
keywords = {quality of worker, confidence interval, Crowdsourcing},
location = {Beijing, China},
series = {ICCSE'17}
}

@inproceedings{10.1007/978-3-319-03260-3_35,
author = {Kavaler, David and Posnett, Daryl and Gibler, Clint and Chen, Hao and Devanbu, Premkumar and Filkov, Vladimir},
title = {Using and Asking: APIs Used in the Android Market and Asked about in StackOverflow},
year = {2013},
isbn = {9783319032597},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-319-03260-3_35},
doi = {10.1007/978-3-319-03260-3_35},
abstract = {Programming is knowledge intensive. While it is well understood that programmers spend lots of time looking for information, with few exceptions, there is a significant lack of data on what information they seek, and why. Modern platforms, like Android, comprise complex APIs that often perplex programmers. We ask: which elements are confusing, and why? Increasingly, when programmers need answers, they turn to StackOverflow. This provides a novel opportunity. There are a vast number of applications for Android devices, which can be readily analyzed, and many traces of interactions on StackOverflow. These provide a complementary perspective on using and asking, and allow the two phenomena to be studied together. How does the market demand for the USE of an API drive the market for knowledge about it? Here, we analyze data from Android applications and StackOverflow together, to find out what it is that programmers want to know and why.},
booktitle = {Proceedings of the 5th International Conference on Social Informatics - Volume 8238},
pages = {405–418},
numpages = {14},
location = {Kyoto, Japan},
series = {SocInfo 2013}
}

@inproceedings{10.1145/3411764.3445399,
author = {Rubya, Sabirat and Numainville, Joseph and Yarosh, Svetlana},
title = {Comparing Generic and Community-Situated Crowdsourcing for Data Validation in the Context of Recovery from Substance Use Disorders},
year = {2021},
isbn = {9781450380966},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3411764.3445399},
doi = {10.1145/3411764.3445399},
abstract = {Targeting the right group of workers for crowdsourcing often achieves better quality results. One unique example of targeted crowdsourcing is seeking community-situated workers whose familiarity with the background and the norms of a particular group can help produce better outcome or accuracy. These community-situated crowd workers can be recruited in different ways from generic online crowdsourcing platforms or from online recovery communities. We evaluate three different approaches to recruit generic and community-situated crowd in terms of the time and the cost of recruitment, and the accuracy of task completion. We consider the context of Alcoholics Anonymous (AA), the largest peer support group for recovering alcoholics, and the task of identifying and validating AA meeting information. We discuss the benefits and trade-offs of recruiting paid vs. unpaid community-situated workers and provide implications for future research in the recovery context and relevant domains of HCI, and for the design of crowdsourcing ICT systems.},
booktitle = {Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems},
articleno = {449},
numpages = {17},
keywords = {community-situated crowd, Crowdsourcing, Alcoholics Anonymous},
location = {Yokohama, Japan},
series = {CHI '21}
}

@inproceedings{10.1145/2911451.2914756,
author = {Moshfeghi, Yashar and Huertas-Rosero, Alvaro F. and Jose, Joemon M.},
title = {Identifying Careless Workers in Crowdsourcing Platforms: A Game Theory Approach},
year = {2016},
isbn = {9781450340694},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2911451.2914756},
doi = {10.1145/2911451.2914756},
abstract = {In this paper we introduce a game scenario for crowdsourcing (CS) using incentives as a bait for careless (gambler) workers, who respond to them in a characteristic way. We hypothesise that careless workers are risk-inclined and can be detected in the game scenario by their use of time, and test this hypothesis in two steps: first, we formulate and prove a theorem stating that a risk-inclined worker will react to competition with shorter Task Completion Time (TCT) than a risk-neutral or risk-averse worker. Second, we check if the game scenario introduces a link between TCT and performance, by performing a crowdsourced evaluation using 35 topics from the TREC-8 collection. Experimental evidence confirms our hypothesis, showing that TCT can be used as a powerful discrimination factor to detect careless workers. This is a valuable result in the quest for quality assurance in CS-based micro tasks such as relevance assessment.},
booktitle = {Proceedings of the 39th International ACM SIGIR Conference on Research and Development in Information Retrieval},
pages = {857–860},
numpages = {4},
keywords = {chicken game, crowdsourcing, game theory, relevance assessment},
location = {Pisa, Italy},
series = {SIGIR '16}
}

@inproceedings{10.1145/3242587.3242621,
author = {Ali, Abdullah X. and Morris, Meredith Ringel and Wobbrock, Jacob O.},
title = {Crowdsourcing Similarity Judgments for Agreement Analysis in End-User Elicitation Studies},
year = {2018},
isbn = {9781450359481},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3242587.3242621},
doi = {10.1145/3242587.3242621},
abstract = {End-user elicitation studies are a popular design method, but their data require substantial time and effort to analyze. In this paper, we present Crowdsensus, a crowd-powered tool that enables researchers to efficiently analyze the results of elicitation studies using subjective human judgment and automatic clustering algorithms. In addition to our own analysis, we asked six expert researchers with experience running and analyzing elicitation studies to analyze an end-user elicitation dataset of 10 functions for operating a web-browser, each with 43 voice commands elicited from end-users for a total of 430 voice commands. We used Crowdsensus to gather similarity judgments of these same 430 commands from 410 online crowd workers. The crowd outperformed the experts by arriving at the same results for seven of eight functions and resolving a function where the experts failed to agree. Also, using Crowdsensus was about four times faster than using experts.},
booktitle = {Proceedings of the 31st Annual ACM Symposium on User Interface Software and Technology},
pages = {177–188},
numpages = {12},
keywords = {online crowds, mechanical turk, human computation, end-user elicitation study, crowdsourcing, agreement rate},
location = {Berlin, Germany},
series = {UIST '18}
}

@inproceedings{10.1145/3185089.3185152,
author = {Li, Boshu and Wu, Wenjun and Hu, Zhenhui},
title = {Evaluation of Software Quality for Competition-based Software Crowdsourcing Projects},
year = {2018},
isbn = {9781450354141},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3185089.3185152},
doi = {10.1145/3185089.3185152},
abstract = {Crowdsourcing-based Software Development (CSSD) performs as: many software practitioners use their own experience and technology to participate software development related tasks, through the open platform such as TopCoder. Crowdsourcing software quality issue has caught some researchers' attention, but it is still far from enough, and no work has been done on evaluating crowdsourcing software projects from a macro point of view. In the paper, we apply traditional quality evaluation practice and theory into the evaluation of crowdsourcing-based software quality by proper modification. The main contributions of this paper are: evaluate TopCoder software quality from the perspective of Project Rating and Project Effort respectively, and explore their aggregation strategies. In order to explore the relationship between them, we introduce the definition of quality assurance effort. We believe the final project rating indicator and quality assurance effort can help a project manager to make reasonable decisions on crowdsourcing-based software development tasks.},
booktitle = {Proceedings of the 2018 7th International Conference on Software and Computer Applications},
pages = {102–109},
numpages = {8},
keywords = {TopCoder, Software Quality, Software Competition, Quality Assurance Effort, Project Rating, Project Effort, Crowdsourcing-based Software Development},
location = {Kuantan, Malaysia},
series = {ICSCA '18}
}

@inproceedings{10.1145/2658861.2658948,
author = {Hsu, Jane Yung-jen},
title = {Crowdsourcing agents for smart IoT},
year = {2014},
isbn = {9781450330350},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2658861.2658948},
doi = {10.1145/2658861.2658948},
abstract = {Activity recognition is a key capability for a smart environment to offer timely services and intelligent interactions with people, especially with the growing number of connected devices. While logging data from connected sensors is no longer beyond reach, it is still quite difficult to collect the labels required by machine learning approaches to activity recognition. In this research, crowdsourcing agents are designed to acquire status labels from people situated in the environment. Experiments on crowdsourcing in a typical building on campus have been conducted to improve air conditioning and space utilization. In particular, we will discuss how crowdsourcing agents in the form of simple physical objects can significantly improve user engagement as well as data quality. Collaboration among cyber-physical agents can lead to better user experience and overall performance.},
booktitle = {Proceedings of the Second International Conference on Human-Agent Interaction},
pages = {103},
numpages = {1},
keywords = {internet of things, intelligent agents, cyber-physical systems, crowdsourcing, activity recognition},
location = {Tsukuba, Japan},
series = {HAI '14}
}

@inproceedings{10.1145/3084041.3084058,
author = {Han, Kai and He, Yuntian and Tan, Haisheng and Tang, Shaojie and Huang, He and Luo, Jun},
title = {Online Pricing for Mobile Crowdsourcing with Multi-Minded Users},
year = {2017},
isbn = {9781450349123},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3084041.3084058},
doi = {10.1145/3084041.3084058},
abstract = {Mobile crowdsourcing has been proposed as a promising approach for urban data collection, but it has also brought the critical problem of designing proper mechanisms to incentivize user participation. Most previous work on crowdsourcing incentivization has assumed that each user holds a single private cost for participation or behaves in a "win all or nothing" (a.k.a. "single-minded") manner. However, in some crowdsourcing applications such as Amazon's Mechanical Turk, the users are usually "multi-minded" in the sense that each of them holds heterogeneous private costs for different tasks and only performs a portion of her/his interested tasks according to the announced payments. To address this problem, we propose LIME, an onLine prIcing mechanism to incentivize Multi-minded usErs under the scenario where the users arrive sequentially in an arbitrary order. We show that the design of LIME involves solving a "dummy semi-bandits with multiple knapsacks and random costs" problem, which has not been investigated before, and we also prove that LIME achieves several desirable properties including computational efficiency, budget feasibility, truthfulness, individual rationality and low regret on the utility. Finally, the effectiveness of LIME as well as its superiorities over prior related work are demonstrated through extensive simulations.},
booktitle = {Proceedings of the 18th ACM International Symposium on Mobile Ad Hoc Networking and Computing},
articleno = {18},
numpages = {10},
keywords = {truthful, regret, multi-minded, mobile crowdsourcing, Online pricing},
location = {Chennai, India},
series = {Mobihoc '17}
}

@inproceedings{10.1145/3251626,
author = {Bustamante, Fabi\'{a}n E.},
title = {Session details: On Designing Crowdsourcing Systems},
year = {2015},
isbn = {9781450335393},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3251626},
doi = {10.1145/3251626},
booktitle = {Proceedings of the 2015 ACM SIGCOMM Workshop on Crowdsourcing and Crowdsharing of Big (Internet) Data},
location = {London, United Kingdom},
series = {C2B(1)D '15}
}

@inproceedings{10.1145/2884781.2884865,
author = {Fava, Daniel and Shapiro, Dan and Osborn, Joseph and Sch\"{a}ef, Martin and Whitehead, E. James},
title = {Crowdsourcing program preconditions via a classification game},
year = {2016},
isbn = {9781450339001},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2884781.2884865},
doi = {10.1145/2884781.2884865},
abstract = {Invariant discovery is one of the central problems in software verification. This paper reports on an approach that addresses this problem in a novel way; it crowdsources logical expressions for likely invariants by turning invariant discovery into a computer game. The game, called Binary Fission, employs a classification model. In it, players compose preconditions by separating program states that preserve or violate program assertions. The players have no special expertise in formal methods or programming, and are not specifically aware they are solving verification tasks. We show that Binary Fission players discover concise, general, novel, and human readable program preconditions. Our proof of concept suggests that crowdsourcing offers a feasible and promising path towards the practical application of verification technology.},
booktitle = {Proceedings of the 38th International Conference on Software Engineering},
pages = {1086–1096},
numpages = {11},
location = {Austin, Texas},
series = {ICSE '16}
}

@inproceedings{10.1007/978-3-030-27523-5_11,
author = {Falc\~{a}o, Ana Gabrielle Ramos and Wanderley, Pedro Farias and da Silva Leite, Tiago Henrique and de Souza Baptista, Cl\'{a}udio and de Queiroz, Jos\'{e} Eust\'{a}quio Rangel and de Oliveira, Maxwell Guimar\~{a}es and Rocha, J\'{u}lio Henrique},
title = {Crowdsourcing Urban Issues in Smart Cities: A Usability Assessment of the Crowd4City System},
year = {2019},
isbn = {978-3-030-27522-8},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-27523-5_11},
doi = {10.1007/978-3-030-27523-5_11},
abstract = {Geosocial networks gather large amounts of voluntarily generated information that can be explored in different contexts, including urban areas. In this sense, we developed the Crowd4City system, which puts together city authorities and citizens focusing on the improvement of their urban spaces. In order to ensure the effectiveness of our proposal, we carried out a usability assessment following the ISO 9241, which covers ergonomics of human-computer interaction. For such, this paper describes a case study using Crow4City in a 3-stage evaluation, involving human volunteers, pre-defined tasks, survey analysis and conformity analysis. The statistical indicators show the usability levels which are useful in the analysis of the user’s challenge and motivation on using such a system.},
booktitle = {Electronic Government and the Information Systems Perspective: 8th International Conference, EGOVIS 2019, Linz, Austria, August 26–29, 2019, Proceedings},
pages = {147–159},
numpages = {13},
keywords = {Crowdsourcing, Smart cities, Data reliability, Geosocial networks, Urban issues},
location = {Linz, Austria}
}

@inproceedings{10.1007/978-3-319-96893-3_2,
author = {Li, Jian and Liu, An and Wang, Weiqi and Li, Zhixu and Liu, Guanfeng and Zhao, Lei and Zheng, Kai},
title = {Towards Privacy-Preserving Travel-Time-First Task Assignment in Spatial Crowdsourcing},
year = {2018},
isbn = {978-3-319-96892-6},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-319-96893-3_2},
doi = {10.1007/978-3-319-96893-3_2},
abstract = {With the ubiquity of mobile devices and wireless networks, spatial crowdsourcing (SC) has gained considerable popularity and importance as a new tool of problem-solving. It enables complex tasks at specific locations to be performed by a crowd of nearby workers. In this paper, we study the privacy-preserving travel-time-first task assignment problem where tasks are assigned to workers who can arrive at the required locations first and no private information are revealed to unauthorized parties. Compared with existing work on privacy-preserving task assignment, this problem is novel as tasks are allocated according to travel time rather than travel distance. Moreover, it is challenging as secure computation of travel time requires secure division which is still an open problem nowadays. Observing that current solutions for secure division do not scale well, we propose an efficient algorithm to securely calculate the least common multiple (LCM) of every workers speed, based on which expensive division operation on ciphertexts can be avoided. We formally prove that our protocol is secure against semi-honest adversaries. Through extensive experiments over real datasets, we demonstrate the efficiency and effectiveness of our proposed protocol.},
booktitle = {Web and Big Data: Second International Joint Conference, APWeb-WAIM 2018, Macau, China, July 23-25, 2018, Proceedings, Part II},
pages = {19–34},
numpages = {16},
keywords = {Spatial crowdsourcing, Privacy-preserving, Task assignment},
location = {Macau, China}
}

@inproceedings{10.1145/2882903.2882953,
author = {Das Sarma, Akash and Parameswaran, Aditya and Widom, Jennifer},
title = {Towards Globally Optimal Crowdsourcing Quality Management: The Uniform Worker Setting},
year = {2016},
isbn = {9781450335317},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2882903.2882953},
doi = {10.1145/2882903.2882953},
abstract = {We study crowdsourcing quality management, that is, given worker responses to a set of tasks, our goal is to jointly estimate the true answers for the tasks, as well as the quality of the workers. Prior work on this problem relies primarily on applying Expectation-Maximization (EM) on the underlying maximum likelihood problem to estimate true answers as well as worker quality. Unfortunately, EM only provides a locally optimal solution rather than a globally optimal one. Other solutions to the problem (that do not leverage EM) fail to provide global optimality guarantees as well. In this paper, we focus on filtering, where tasks require the evaluation of a yes/no predicate, and rating, where tasks elicit integer scores from a finite domain. We design algorithms for finding the global optimal estimates of correct task answers and worker quality for the underlying maximum likelihood problem, and characterize the complexity of these algorithms. Our algorithms conceptually consider all mappings from tasks to true answers (typically a very large number), leveraging two key ideas to reduce, by several orders of magnitude, the number of mappings under consideration, while preserving optimality. We also demonstrate that these algorithms often find more accurate estimates than EM-based algorithms. This paper makes an important contribution towards understanding the inherent complexity of globally optimal crowdsourcing quality management.},
booktitle = {Proceedings of the 2016 International Conference on Management of Data},
pages = {47–62},
numpages = {16},
keywords = {rating, quality management, maximum likelihood, human computation, filtering, expectation-maximization, crowdsourcing, EM},
location = {San Francisco, California, USA},
series = {SIGMOD '16}
}

@inproceedings{10.1145/3173574.3173884,
author = {Alallah, Fouad and Neshati, Ali and Sheibani, Nima and Sakamoto, Yumiko and Bunt, Andrea and Irani, Pourang and Hasan, Khalad},
title = {Crowdsourcing vs Laboratory-Style Social Acceptability Studies? Examining the Social Acceptability of Spatial User Interactions for Head-Worn Displays},
year = {2018},
isbn = {9781450356206},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3173574.3173884},
doi = {10.1145/3173574.3173884},
abstract = {The use of crowdsourcing platforms for data collection in HCI research is attractive in their ability to provide rapid access to large and diverse participant samples. As a result, several researchers have conducted studies investigating the similarities and differences between data collected through crowdsourcing and more traditional, laboratory-style data collection. We add to this body of research by examining the feasibility of conducting social acceptability studies via crowdsourcing. Social acceptability can be a key determinant for the early adoption of emerging technologies, and as such, we focus our investigation on social acceptability for Head-Worn Display (HWD) input modalities. Our results indicate that data collected via a crowdsourced experiment and a laboratory-style setting did not differ at a statistically significant level. These results provide initial support for crowdsourcing platforms as viable options for conducting social acceptability research.},
booktitle = {Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems},
pages = {1–7},
numpages = {7},
keywords = {social acceptance, input modalities, head-worn displays, crowdsourcing},
location = {Montreal QC, Canada},
series = {CHI '18}
}

@inproceedings{10.1007/978-3-319-18818-8_43,
author = {Dumitrache, Anca},
title = {Crowdsourcing Disagreement for Collecting Semantic Annotation},
year = {2015},
isbn = {9783319188171},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-319-18818-8_43},
doi = {10.1007/978-3-319-18818-8_43},
abstract = {This paper proposes an approach to gathering semantic annotation, which rejects the notion that human interpretation can have a single ground truth, and is instead based on the observation that disagreement between annotators can signal ambiguity in the input text, as well as how the annotation task has been designed. The purpose of this research is to investigate whether disagreement-aware crowdsourcing is a scalable approach to gather semantic annotation across various tasks and domains. We propose a methodology for answering this question that involves, for each task and domain: defining the crowdsourcing setup, experimental data collection, and evaluating both the setup and the results. We present initial results for the task of medical relation extraction, and propose an evaluation plan for crowdsourcing semantic annotation for several tasks and domains.},
booktitle = {Proceedings of the 12th European Semantic Web Conference on The Semantic Web. Latest Advances and New Domains - Volume 9088},
pages = {701–710},
numpages = {10},
keywords = {Semantic annotation, Semantic ambiguity, Natural language processing, Human computation, Ground truth, Crowdsourcing}
}

@inproceedings{10.5555/3017447.3017460,
author = {Zhitomirsky-Geffet, Maayan and Hajibayova, Lala and Kwa\'{s}nik, Barbara H. and Hamari, Juho and Bullard, Julia and Bowman, Timothy},
title = {Crowdsourcing approaches for knowledge organization systems: crowd collaboration or crowd work?},
year = {2016},
publisher = {American Society for Information Science},
address = {USA},
abstract = {Development of Internet technologies has empowered ordinary users to create, contribute, share and connect with other members of the community. As users learn to exploit the potential of networked communications, they participate in a process, which facilitates a shift from individual to collective contributions and introduces an opportunity for multi-vocal and multi-faceted representation of cultural heritage. Open access to crowdsourced collections requires reconsideration of the traditional authoritative approach of cultural heritage institutions. The arduous nature of the work rendered voluntarily in cultural heritage crowdsourcing initiatives calls for reconsideration of power relationships and giving power to devoted contributors supported by modern "intelligent" technology to regulate the process of representation and organization. Taking into consideration the fact that crowdsourced data are not without flaws, the question is how to better utilize the collective intelligence to create quality information. In this context, various issues such as power, control, trust, inter-contributor consensus, heterogeneity of opinions will be raised and discussed by the panelists. Each of the panelists comes from a different field of expertise (Computer science, Information science, Economics, Communication studies, cultural heritage) and various cultural backgrounds and geographical locations (United States, Europe and Israel). This diversity will be reflected in the presented perspectives on the crowdsourcing topic.},
booktitle = {Proceedings of the 79th ASIS&amp;T Annual Meeting: Creating Knowledge, Enhancing Lives through Information \&amp; Technology},
articleno = {13},
numpages = {6},
keywords = {wisdom of crowds, ontologies, crowdsourcing, crowd work, crowd collaboration, collaborative knowledge organization},
location = {Copenhagen, Denmark},
series = {ASIST '16}
}

@inproceedings{10.1109/MDM.2014.77,
author = {Zadorozhny, Vladimir and Lewis, Michael},
title = {Fusing Information, Crowdsourcing and Mobility},
year = {2014},
isbn = {9781479957057},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/MDM.2014.77},
doi = {10.1109/MDM.2014.77},
abstract = {In this seminar we will consider how concepts of information fusion, crowdsourcing and mobility complement each other and accelerate novel advanced research directions in mobile data management. We will elaborate on each of those concepts and explore their synergy under a prominent scenario of situation assessment in multi-robot search and rescue missions.},
booktitle = {Proceedings of the 2014 IEEE 15th International Conference on Mobile Data Management - Volume 02},
pages = {4–6},
numpages = {3},
series = {MDM '14}
}

@inproceedings{10.1109/MDM.2012.21,
author = {Madria, Sanjay Kumar and Mondal, Anirban},
title = {Crowdsourcing: Dynamic Data Management in Mobile P2P Networks},
year = {2012},
isbn = {9780769547138},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/MDM.2012.21},
doi = {10.1109/MDM.2012.21},
abstract = {Not applicable},
booktitle = {Proceedings of the 2012 IEEE 13th International Conference on Mobile Data Management (Mdm 2012)},
pages = {364–367},
numpages = {4},
keywords = {data replication, crowdsourcing, caching, Mobile P2P},
series = {MDM '12}
}

@inproceedings{10.1145/2502081.2502234,
author = {Soleymani, Mohammad and Larson, Martha},
title = {Crowdsourcing for multimedia research},
year = {2013},
isbn = {9781450324045},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2502081.2502234},
doi = {10.1145/2502081.2502234},
abstract = {Crowdsourcing techniques make use of intelligent contributions of large number of human crowdmembers. This tutorial introduces researchers to the applications of crowdsourcing to multimedia analysis with the aim of allowing them to understand the potentials and limitations of crowdsourcing tools and techniques. We emphasize the fact that crowdsourcing represents a further development along a pre-existing continuum of techniques, and discuss the added advantages that new developments offer. We provide a basic overview of human computation, with an emphasis on example cases in which crowdsourcing has been applied to generate data sets, to improve automatic multimedia content analysis, and to elicit user needs or multimedia system requirements. Different techniques and considerations in using human computation methods to acquire high-quality data and annotations are discussed and demonstrated.},
booktitle = {Proceedings of the 21st ACM International Conference on Multimedia},
pages = {1111–1112},
numpages = {2},
keywords = {user studies, testing, multimedia annotation, crowdsourcing},
location = {Barcelona, Spain},
series = {MM '13}
}

@inproceedings{10.5555/3235838.3235861,
author = {Panjwani, Saurabh and Prakash, Achintya},
title = {Crowdsourcing attacks on biometric systems},
year = {2014},
isbn = {9781931971133},
publisher = {USENIX Association},
address = {USA},
abstract = {We introduce a new approach for attacking and analyzing biometric-based authentication systems, which involves crowdsourcing the search for potential impostors to the system. Our focus is on voice-based authentication, or speaker verification (SV), and we propose a generic method to use crowdsourcing for identifying candidate "mimics" for speakers in a given target population. We then conduct a preliminary analysis of this method with respect to a well-known text-independent SV scheme (the GMM-UBM scheme) using Mechanical Turk as the crowdsourcing platform.Our analysis shows that the new attack method can identify mimics for target speakers with high impersonation success rates: from a pool of 176 candidates, we identified six with an overall false acceptance rate of 44\%, which is higher than what has been reported for professional mimics in prior voice-mimicry experiments. This demonstrates that na\"{\i}ve, untrained users have the potential to carry out impersonation attacks against voice-based systems, although good imitators are rare to find. (We also implement our method with a crowd of amateur mimicry artists and obtain similar results for them.) Match scores for our best mimics were found to be lower than those for automated attacks but, given the relative difficulty of detecting mimicry attacks vis-\'{a}-vis automated ones, our method presents a potent threat to real systems. We discuss implications of our results for the security analysis of SV systems (and of biometric systems, in general) and highlight benefits and challenges associated with the use of crowdsourcing in such analysis.},
booktitle = {Proceedings of the Tenth USENIX Conference on Usable Privacy and Security},
pages = {257–269},
numpages = {13},
location = {Menlo Park, CA},
series = {SOUPS '14}
}

@inproceedings{10.1145/2642918.2647409,
author = {Retelny, Daniela and Robaszkiewicz, S\'{e}bastien and To, Alexandra and Lasecki, Walter S. and Patel, Jay and Rahmati, Negar and Doshi, Tulsee and Valentine, Melissa and Bernstein, Michael S.},
title = {Expert crowdsourcing with flash teams},
year = {2014},
isbn = {9781450330695},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2642918.2647409},
doi = {10.1145/2642918.2647409},
abstract = {We introduce flash teams, a framework for dynamically assembling and managing paid experts from the crowd. Flash teams advance a vision of expert crowd work that accomplishes complex, interdependent goals such as engineering and design. These teams consist of sequences of linked modular tasks and handoffs that can be computationally managed. Interactive systems reason about and manipulate these teams' structures: for example, flash teams can be recombined to form larger organizations and authored automatically in response to a user's request. Flash teams can also hire more people elastically in reaction to task needs, and pipeline intermediate output to accelerate completion times. To enable flash teams, we present Foundry, an end-user authoring platform and runtime manager. Foundry allows users to author modular tasks, then manages teams through handoffs of intermediate work. We demonstrate that Foundry and flash teams enable crowdsourcing of a broad class of goals including design prototyping, course development, and film animation, in half the work time of traditional self-managed teams.},
booktitle = {Proceedings of the 27th Annual ACM Symposium on User Interface Software and Technology},
pages = {75–85},
numpages = {11},
keywords = {flash teams, expert crowd work, crowdsourcing},
location = {Honolulu, Hawaii, USA},
series = {UIST '14}
}

@inproceedings{10.5555/3045118.3045121,
author = {Shah, Nihar B. and Zhou, Dengyong and Peres, Yuval},
title = {Approval voting and incentives in crowdsourcing},
year = {2015},
publisher = {JMLR.org},
abstract = {The growing need for labeled training data has made crowdsourcing an important part of machine learning. The quality of crowdsourced labels is, however, adversely affected by three factors: (1) the workers are not experts; (2) the incentives of the workers are not aligned with those of the requesters; and (3) the interface does not allow workers to convey their knowledge accurately, by forcing them to make a single choice among a set of options. In this paper, we address these issues by introducing approval voting to utilize the expertise of workers who have partial knowledge of the true answer, and coupling it with a ("strictly proper") incentive-compatible compensation mechanism. We show rigorous theoretical guarantees of optimality of our mechanism together with a simple axiomatic characterization. We also conduct preliminary empirical studies on Amazon Mechanical Turk which validate our approach.},
booktitle = {Proceedings of the 32nd International Conference on International Conference on Machine Learning - Volume 37},
pages = {10–19},
numpages = {10},
location = {Lille, France},
series = {ICML'15}
}

@inproceedings{10.1145/3055601.3055607,
author = {Li, Qunwei and Varshney, Pramod K.},
title = {Does Confidence Reporting from the Crowd Benefit Crowdsourcing Performance?},
year = {2017},
isbn = {9781450349772},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3055601.3055607},
doi = {10.1145/3055601.3055607},
abstract = {We explore the design of an effective crowdsourcing system for an M-ary classification task. Crowd workers complete simple binary microtasks whose results are aggregated to give the final classification decision. We consider the scenario where the workers have a reject option so that they are allowed to skip microtasks when they are unable to or choose not to respond to binary microtasks. Additionally, the workers report quantized confidence levels when they are able to submit definitive answers. We present an aggregation approach using a weighted majority voting rule, where each worker's response is assigned an optimized weight to maximize crowd's classification performance. We obtain a couterintuitive result that the classification performance does not benefit from workers reporting quantized confidence. Therefore, the crowdsourcing system designer should employ the reject option without requiring confidence reporting.},
booktitle = {Proceedings of the 2nd International Workshop on Social Sensing},
pages = {49–54},
numpages = {6},
keywords = {reject option, information fusion, distributed inference, crowdsourcing, confidence reporting, Classification},
location = {Pittsburgh, PA, USA},
series = {SocialSens'17}
}

@inproceedings{10.5555/3157382.3157639,
author = {Khetan, Ashish and Oh, Sewoong},
title = {Achieving budget-optimality with adaptive schemes in crowdsourcing},
year = {2016},
isbn = {9781510838819},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Adaptive schemes, where tasks are assigned based on the data collected thus far, are widely used in practical crowdsourcing systems to efficiently allocate the budget. However, existing theoretical analyses of crowdsourcing systems suggest that the gain of adaptive task assignments is minimal. To bridge this gap, we investigate this question under a strictly more general probabilistic model, which has been recently introduced to model practical crowdsourcing datasets. Under this generalized Dawid-Skene model, we characterize the fundamental trade-off between budget and accuracy. We introduce a novel adaptive scheme that matches this fundamental limit. A given budget is allocated over multiple rounds. In each round, a subset of tasks with high enough confidence are classified, and increasing budget is allocated on remaining ones that are potentially more difficult. On each round, decisions are made based on the leading eigenvector of (weighted) non-backtracking operator corresponding to the bipartite assignment graph. We further quantify the gain of adaptivity, by comparing the tradeoff with the one for non-adaptive schemes, and confirm that the gain is significant and can be made arbitrarily large depending on the distribution of the difficulty level of the tasks at hand.},
booktitle = {Proceedings of the 30th International Conference on Neural Information Processing Systems},
pages = {4851–4859},
numpages = {9},
location = {Barcelona, Spain},
series = {NIPS'16}
}

@inproceedings{10.1145/2556288.2556967,
author = {Forlines, Clifton and Miller, Sarah and Guelcher, Leslie and Bruzzi, Robert},
title = {Crowdsourcing the future: predictions made with a social network},
year = {2014},
isbn = {9781450324731},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2556288.2556967},
doi = {10.1145/2556288.2556967},
abstract = {Researchers have long known that aggregate estimations built from the collected opinions of a large group of people often outperform the estimations of individual experts. This phenomenon is generally described as the "Wisdom of Crowds". This approach has shown promise with respect to the task of accurately forecasting future events. Previous research has demonstrated the value of utilizing meta-forecasts (forecasts about what others in the group will predict) when aggregating group predictions. In this paper, we describe an extension to meta-forecasting and demonstrate the value of modeling the familiarity among a population's members (its social network) and applying this model to forecast aggregation. A pair of studies demonstrates the value of taking this model into account, and the described technique produces aggregate forecasts for future events that are significantly better than the standard Wisdom of Crowds approach as well as previous meta-forecasting techniques.},
booktitle = {Proceedings of the SIGCHI Conference on Human Factors in Computing Systems},
pages = {3655–3664},
numpages = {10},
keywords = {social network, meta-forecast, forecasting, crowd-sourcing, bayesian truth serum, aggregation},
location = {Toronto, Ontario, Canada},
series = {CHI '14}
}

@inproceedings{10.1109/GLOCOM.2018.8647268,
author = {Wang, Huiyang and Nguyen, Diep N. and Hoang, Dinh Thai and Dutkiewicz, Eryk and Cheng, Qingqing},
title = {Real-Time Crowdsourcing Incentive for Radio Environment Maps: A Dynamic Pricing Approach},
year = {2018},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/GLOCOM.2018.8647268},
doi = {10.1109/GLOCOM.2018.8647268},
abstract = {To effectively utilize&amp;#x002F;harvest short-lived whitespace that accounts for more than 30\% of the cellular bands, it is critical to build a real-time radio environment map. Note that existing radio spectrum maps&amp;#x002F;databases (e.g., Google Spectrum Database) are updated on a daily or weekly basis. In this paper, we introduce a novel real-time crowdsourcing incentive solution that rewards mobile users who contribute their qualified spectrum sensing data to a radio environment map. First, we develop a feature-based model based on advanced machine learning techniques in order to estimate model parameters of the radio environment map. Based on the prediction model, we then propose a smart dynamic pricing strategy including prepaid and postpaid pricing schemes. The prepaid scheme is to guarantee the minimum payment for participants, and the postpaid scheme is to reward the participants according to their contributions. Importantly, in our model, the postpaid scheme will be adjusted iteratively in a real-time manner based on the contributions of participants to the spectrum map. After that we carry out real experiments through a mobile application and a cloud spectrum database. The experiment results show that our proposed solution can achieve not only better users' utilities, but also a lower overall system cost compared with those of some existing works.},
booktitle = {2018 IEEE Global Communications Conference (GLOBECOM)},
pages = {1–6},
numpages = {6},
location = {Abu Dhabi, United Arab Emirates}
}

@inproceedings{10.1007/978-3-319-04244-2_21,
author = {Challiol, Cecilia and Firmenich, Sergio and Bosetti, Gabriela Alejandra and Gordillo, Silvia E. and Rossi, Gustavo},
title = {Crowdsourcing Mobile Web Applications},
year = {2013},
isbn = {9783319042435},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-319-04244-2_21},
doi = {10.1007/978-3-319-04244-2_21},
abstract = {Building Mobile Web or Hypermedia Applications is usually difficult since there is a myriad of issues to take into account. Moreover adding support for personalized or context-aware behaviors goes far beyond the possibilities of many kinds of organizations that intend to build this kind of software (museums, city halls, etc). In this article we present a novel approach to delegate part of the effort in building mobile Web software to developers outside those organizations or even to final users. We show that this approach is feasible, light and practical and present a set of experiments we developed to verify our claims.},
booktitle = {Revised Selected Papers of the ICWE 2013 International Workshops on Current Trends in Web Engineering - Volume 8295},
pages = {223–237},
numpages = {15},
keywords = {Mobile Web Applications, Mobile Hypermedia, Crowdsoursing, Client-Side Adaptation}
}

@inproceedings{10.1145/3167918.3167965,
author = {Kumar, Pinky P. and Rashid, Mahmood A.},
title = {Crowdsourcing based social awareness for taboo diseases like HIV/AIDS},
year = {2018},
isbn = {9781450354363},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3167918.3167965},
doi = {10.1145/3167918.3167965},
abstract = {Creating social awareness for sexually transmitted diseases, such as HIV/AIDS is critical. The 2015 UNAIDS statistics shows that newly infected cases of HIV/AIDS has fallen down by 35\% since 2000, worldwide. However, the statistics of Fiji tells a different story. The newly infected HIV/AIDS cases in Fiji has been increasing every year since 2000. Different HIV/AIDS awareness programs have been launched previously such as, workshops and seminars. Despite these efforts, there has been no reduction in the newly infected HIV/AIDS cases. Fiji's health sector is still using traditional approaches for building public awareness. Therefore, the primary purpose of this study is to explore the benefits of crowdsourcing in developing the social awareness on taboo diseases among Fijians. Data for this study were collected through questionnaire and experimental methods from the people living with HIV as well as from the young Fijians, such as secondary and tertiary students. The result clearly showed that crowdsourcing can be an effective means of assisting Fiji's public health by reaching out to the remote areas, reducing the program costs, and assisting thousands of people simultaneously.},
booktitle = {Proceedings of the Australasian Computer Science Week Multiconference},
articleno = {25},
numpages = {9},
keywords = {taboo disease, social awareness, public health, crowdsourcing, HIV/AIDS},
location = {Brisband, Queensland, Australia},
series = {ACSW '18}
}

@inproceedings{10.1145/3078714.3078746,
author = {Sethi, Ricky J.},
title = {Crowdsourcing the Verification of Fake News and Alternative Facts},
year = {2017},
isbn = {9781450347082},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3078714.3078746},
doi = {10.1145/3078714.3078746},
abstract = {Fake news and alternative facts have dominated the news cycle of late. In this paper, we present a prototype system that uses social argumentation to verify the validity of proposed alternative facts and help in the detection of fake news. We utilize fundamental argumentation ideas in a graph-theoretic framework that also incorporates semantic web and linked data principles. The argumentation structure is crowdsourced and mediated by expert moderators in a virtual community.},
booktitle = {Proceedings of the 28th ACM Conference on Hypertext and Social Media},
pages = {315–316},
numpages = {2},
keywords = {social argumentation, fake news, alternative facts},
location = {Prague, Czech Republic},
series = {HT '17}
}

@inproceedings{10.1145/3027063.3053356,
author = {Homan, Christopher Michael and Schull, Jon I. and Prabhu, Akshai},
title = {On the Genesis of an Assistive Technology Crowdsourcing Community},
year = {2017},
isbn = {9781450346566},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3027063.3053356},
doi = {10.1145/3027063.3053356},
abstract = {The e-NABLE movement is a global confederation that designs, builds, and distributes free, 3D-printed, upper limb assistive devices to children born without fingers and hands. It has been called one of the most inspiring philanthropic efforts of the 21st century. We use social network analysis and natural language processing on the original e-NABLE Google+ community to understand the challenges and opportunities in organizing a rapidly growing real-world social entrepreneurship venture via social media. Our results provide important lessons and benchmarks for similar communities.},
booktitle = {Proceedings of the 2017 CHI Conference Extended Abstracts on Human Factors in Computing Systems},
pages = {1003–1011},
numpages = {9},
keywords = {time-series analysis, social network analysis, social media, micromanufacturing, crowdsourcing, computational linguistics.},
location = {Denver, Colorado, USA},
series = {CHI EA '17}
}

@inproceedings{10.5555/2343776.2343793,
author = {Cavallo, Ruggiero and Jain, Shaili},
title = {Efficient crowdsourcing contests},
year = {2012},
isbn = {0981738125},
publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
address = {Richland, SC},
abstract = {A principal seeks production of a good within a limited timeframe with a hard deadline, after which any good procured has no value. There is inherent uncertainty in the production process, which in light of the deadline may warrant simultaneous production of multiple goods by multiple producers despite there being no marginal value for extra goods beyond the maximum quality good produced. This motivates a crowdsourcing model of procurement. We address efficient execution of such procurement from a social planner's perspective, taking account of and optimally balancing the value to the principal with the costs to producers (modeled as effort expenditure) while, crucially, contending with self-interest on the part of all players. A solution to this problem involves both an algorithmic aspect that determines an optimal effort level for each producer given the principal's value, and also an incentive mechanism that achieves equilibrium implementation of the socially optimal policy despite the principal privately observing his value, producers privately observing their skill levels and effort expenditure, and all acting selfishly to maximize their own individual welfare. In contrast to popular "winner take all" contests, the efficient mechanism we propose involves a payment to every producer that expends non-zero effort in the efficient policy.},
booktitle = {Proceedings of the 11th International Conference on Autonomous Agents and Multiagent Systems - Volume 2},
pages = {677–686},
numpages = {10},
keywords = {social welfare, mechanism design, crowdsourcing, contests},
location = {Valencia, Spain},
series = {AAMAS '12}
}

@inproceedings{10.1007/978-3-030-18576-3_16,
author = {Zhai, Dongjun and Liu, An and Chen, Shicheng and Li, Zhixu and Zhang, Xiangliang},
title = {SeqST-ResNet: A Sequential Spatial Temporal ResNet for Task Prediction in Spatial Crowdsourcing},
year = {2019},
isbn = {978-3-030-18575-6},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-18576-3_16},
doi = {10.1007/978-3-030-18576-3_16},
abstract = {Task appearance prediction has great potential to improve task assignment in spatial crowdsourcing platforms. The main challenge of this prediction problem is to model the spatial dependency among neighboring regions and the temporal dependency at different time scales (e.g., hourly, daily, and weekly). A recent model ST-ResNet predicts traffic flow by capturing the spatial and temporal dependencies in historical data. However, the data fragments are concatenated as one tensor fed to the deep neural networks, rather than learning the temporal dependencies in a sequential manner. We propose a novel deep learning model, called SeqST-ResNet, which well captures the temporal dependencies of historical task appearance in sequences at several time scales. We validate the effectiveness of our model via experiments on a real-world dataset. The experimental results show that our SeqST-ResNet model significantly outperforms ST-ResNet when predicting tasks at hourly intervals and also during weekday and weekends, more importantly, in regions with intensive task requests.},
booktitle = {Database Systems for Advanced Applications: 24th International Conference, DASFAA 2019, Chiang Mai, Thailand, April 22–25, 2019, Proceedings, Part I},
pages = {260–275},
numpages = {16},
keywords = {Task prediction, Spatial crowdsourcing, Deep neural network},
location = {Chiang Mai, Thailand}
}

@inproceedings{10.1145/3254783,
author = {Lane, Nic},
title = {Session details: Crowdsourcing II},
year = {2013},
isbn = {9781450317702},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3254783},
doi = {10.1145/3254783},
booktitle = {Proceedings of the 2013 ACM International Joint Conference on Pervasive and Ubiquitous Computing},
location = {Zurich, Switzerland},
series = {UbiComp '13}
}

@inproceedings{10.1007/978-3-319-96893-3_19,
author = {Dong, Zhaoan and Fan, Ju and Lu, Jiaheng and Du, Xiaoyong and Ling, Tok Wang},
title = {Using Crowdsourcing for Fine-Grained Entity Type Completion in Knowledge Bases},
year = {2018},
isbn = {978-3-319-96892-6},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-319-96893-3_19},
doi = {10.1007/978-3-319-96893-3_19},
abstract = {Recent years have witnessed the proliferation of large-scale Knowledge Bases (KBs). However, many entities in KBs have incomplete type information, and some are totally untyped. Even worse, fine-grained types (e.g., BasketballPlayer) containing rich semantic meanings are more likely to be incomplete, as they are more difficult to be obtained. Existing machine-based algorithms use predicates (e.g., birthPlace) of entities to infer their missing types, and they have limitations that the predicates may be insufficient to infer fine-grained types. In this paper, we utilize crowdsourcing to solve the problem, and address the challenge of controlling crowdsourcing cost. To this end, we propose a hybrid machine-crowdsourcing approach for fine-grained entity type completion. It firstly determines the types of some “representative” entities via crowdsourcing and then infers the types for remaining entities based on the crowdsourcing results. To support this approach, we first propose an embedding-based influence for type inference which considers not only the distance between entity embeddings but also the distances between entity and type embeddings. Second, we propose a new difficulty model for entity selection which can better capture the uncertainty of the machine algorithm when identifying the entity types. We demonstrate the effectiveness of our approach through experiments on real crowdsourcing platforms. The results show that our method outperforms the state-of-the-art algorithms by improving the effectiveness of fine-grained type completion at affordable crowdsourcing cost.},
booktitle = {Web and Big Data: Second International Joint Conference, APWeb-WAIM 2018, Macau, China, July 23-25, 2018, Proceedings, Part II},
pages = {248–263},
numpages = {16},
keywords = {Crowdsourcing, Entity type completion, Knowledge base},
location = {Macau, China}
}

@inproceedings{10.1145/3139958.3139973,
author = {Li, Wei and Chen, Haiquan and Ku, Wei-Shinn and Qin, Xiao},
title = {Scalable Spatiotemporal Crowdsourcing for Smart Cities based on Particle Filtering},
year = {2017},
isbn = {9781450354905},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3139958.3139973},
doi = {10.1145/3139958.3139973},
abstract = {In mobile crowdsourcing, workers are financially motivated to perform as many self-selected tasks as possible to maximize their revenue. Unfortunately, the existing task scheduling approaches in mobile crowdsourcing fail to consider task execution duration and do not scale for massive tasks and large geographic areas (e.g., a whole city). In this paper, we study on the geo-task scheduling problem (GTS) under the various spatial and temporal constraints in real-world mobile crowdsourcing applications, including task execution duration and task expiration time. Given the location of a worker, the goal of our study is to find an optimal task execution sequence that maximizes the number of tasks that could be finished. Since the exact solution to the maximum task scheduling is computationally intractable, we propose two sub-optimal approaches (LCPF and NUD-IC) based on the particle filtering and the DBSCAN clustering.},
booktitle = {Proceedings of the 25th ACM SIGSPATIAL International Conference on Advances in Geographic Information Systems},
articleno = {63},
numpages = {4},
keywords = {Particle Filtering, Mobile Crowdsourcing, DBSCAN},
location = {Redondo Beach, CA, USA},
series = {SIGSPATIAL '17}
}

@inproceedings{10.1145/3309700.3338418,
author = {Naruse, Kana and Takamichi, Shinnosuke and Tanikawa, Tomohiro and Yoshida, Shigeo and Narumi, Takuji and Hirose, Michitaka},
title = {Estimating confidence in voices using crowdsourcing for alleviating tension with altered auditory feedback},
year = {2020},
isbn = {9781450366793},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3309700.3338418},
doi = {10.1145/3309700.3338418},
abstract = {People tend to face difficulties while speaking, owing to excessive tension. To relieve this tension, we propose a method to alter their voice such that it appears more confident and feed this voice back to the speaker in real time. As determining the appropriate parameters for voice processing is difficult, we gathered data on the perception of confidence in voices through crowdsourcing and constructed a model for estimating confidence scores from voice-processing parameters. An analysis of the model showed that although the coefficient of determination was not considerably large, the inflection of speech tended to affect the perception of confidence.},
booktitle = {Proceedings of Asian CHI Symposium 2019: Emerging HCI Research Collection},
pages = {15–22},
numpages = {8},
keywords = {tension, speech, self-perception, emotion, crowdsourcing, confidence, auditory feedback},
location = {Glasgow, Scotland Uk},
series = {AsianHCI '19}
}

@inproceedings{10.1109/ICPADS.2015.42,
author = {Haijiang Xie and Li Lin and Zhiping Jiang and Wei Xi and Kun Zhao and Meiyong Ding and Jizhong Zhao},
title = {Accelerating Crowdsourcing Based Indoor Localization Using CSI},
year = {2015},
isbn = {9780769557854},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/ICPADS.2015.42},
doi = {10.1109/ICPADS.2015.42},
abstract = {Indoor localization is of importance for many applications. Crowdsourcing individual users' measurements can provide accurate localization without costly site-survey. However, crowdsourcing based approaches suffer from the cold start problem, in which at the beginning of system deployment, there are insufficient users to contribute their measurements, resulting in inaccurate and time-inefficient localization. In this paper, we propose a hybrid indoor localization method to solve such problem, called ACIL. We first employ the inertial navigation technique to localize some core positions or paths. To tackle the inaccuracy problem, we propose an effective method that utilizes the channel state information (CSI) of wireless signals for accurate distance estimation. This method is based on a new observation: there is a ripple-like fading pattern in wireless signals upon moving objects. Leveraging this observation, our system is capable of calculating the distance of human's movement and his/her direction. We also propose a graph-matching algorithm to setup the correlation between the trajectory and floor map. With those extra obtained location information, the impact of cold start issue will be significantly mitigated, while the LBS can be guaranteed with high localization accuracy. Extensive experiments show that the effectiveness in the human localization and movement detection. Extensive experiments validate the great performance of our protocol in case of various human locations and diverse channel conditions.},
booktitle = {Proceedings of the 2015 IEEE 21st International Conference on Parallel and Distributed Systems (ICPADS)},
pages = {274–281},
numpages = {8},
series = {ICPADS '15}
}

@inproceedings{10.1145/2691195.2691223,
author = {Zambrano, Ra\'{u}l and Eymann, Simone},
title = {Crowdsourcing and human development: the role of governments},
year = {2014},
isbn = {9781605586113},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2691195.2691223},
doi = {10.1145/2691195.2691223},
abstract = {This paper explores in which way crowdsourcing and other new technologies can help governments in developing countries work more closely with stakeholders to improve public policy making and allocate public resources in a more responsive fashion vis-a-vis people's needs and priorities. The paper first sets a general background to frame the issues followed by a short literature review of the latest research in this area. It then proposes a new analytical framework which is used to study several cases studies from which it draws conclusions and suggests areas for further research.},
booktitle = {Proceedings of the 8th International Conference on Theory and Practice of Electronic Governance},
pages = {170–177},
numpages = {8},
keywords = {state capacity, public policy making, public policy, institutional development, human development, empowerment, e-participation, e-government, e-governance, e-democracy, democratic governance, crowdsourcing, citizensourcing, ICT for development},
location = {Guimaraes, Portugal},
series = {ICEGOV '14}
}

@inproceedings{10.1145/2989238.2989243,
author = {Abhinav, Kumar and Dubey, Alpana and Virdi, Gurdeep and Kass, Alex},
title = {Analyzing on-boarding time in context of crowdsourcing},
year = {2016},
isbn = {9781450343954},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2989238.2989243},
doi = {10.1145/2989238.2989243},
abstract = {Crowdsourcing is an emerging area which leverages collective intelligence of the crowd. Although crowdsourcing provides several benefits, it also brings uncertainty in any project execution. The uncertainty may be because of the time taken in on-boarding workers and lack of confidence in workers. The On-boarding time specifically becomes important when tasks are of short duration as it is not worth spending too much of time in on-boarding a worker for short task. In this paper, we empirically analyze 59,597 tasks data from Upwork, an online marketplace, to understand major factors that impact On-boarding time. We identified that certain factors, such as Feedback, Hiring rate, Total hours spent, Length of requirement etc., affect the On-boarding time. We applied two predictive models to predict the On-boarding time. Our study provides insights for researchers, organizations, etc. who are looking to accomplish their tasks through crowdsourcing and helps them to better understand factors which influence the On-boarding time.},
booktitle = {Proceedings of the 2nd International Workshop on Software Analytics},
pages = {29–35},
numpages = {7},
keywords = {Flash Teams, Data Analytics, Crowdsourcing},
location = {Seattle, WA, USA},
series = {SWAN 2016}
}

@inproceedings{10.1145/2751957.2755504,
author = {Taylor, Joseph},
title = {Crowdsourcing IT Work: A Three-Fold Perspective from the Workers, Buyers, and Platform Providers},
year = {2015},
isbn = {9781450335577},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2751957.2755504},
doi = {10.1145/2751957.2755504},
abstract = {This paper will present a proposal for research in the area of crowdsourcing. The proposal will highlight the need for research in the area of crowdsourcing as a mechanism to enhance and expand the technology workforce. It does so by examining the technology crowdsourcing phenomenon from three perspectives: the worker (or labor supply), the buyer of technology services (or labor demand) and the marketplaces that facilitates the buyer-seller transaction. It will explore how workforce development and enterprise readiness theories can be applied in explaining how crowdsourcing can be applied to technology tasks. This dissertation will be structured in a three study format. Study one will explore the technology crowdsourcing phenomenon from a "crowdworker" perspective. This study will examine technology crowdwork from a career anchors perspective, and will highlight the potential role of crowdsourcing in expanding the technology workforce to additional sources of worker capacity. This study will establish the theories that describe the motivations and outcomes achieved by workers in crowdsourcing project engagements, and utilize Schein's Career Anchors (Schein 1990) to examine the motivations of workers technology enabled collaborative work environments. Study two will focus on the perceptions and readiness for crowdsourcing labor on the part of buyers of IT services. The research will collect survey data regarding enterprise readiness, and will examine the current state of enterprise readiness to adopt new development techniques. Study three will utilize a design science perspective to examine the ability of crowdsourcing marketplace platforms to meet the needs of IT service buyers and IT service workers as identified in Study's one and two.},
booktitle = {Proceedings of the 2015 ACM SIGMIS Conference on Computers and People Research},
pages = {1–2},
numpages = {2},
keywords = {it services, enterprise readiness, crowdsourcing, career anchors},
location = {Newport Beach, California, USA},
series = {SIGMIS-CPR '15}
}

@inproceedings{10.1109/JCDL.2019.00093,
author = {Han, Wenting and Song, Shijie and Zhao, Yuxiang and Zhu, Qinghua},
title = {The role of self-efficacy and familiarity in digital humanity crowdsourcing: a preliminary study from transcribe-sheng project},
year = {2020},
isbn = {9781728115474},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/JCDL.2019.00093},
doi = {10.1109/JCDL.2019.00093},
abstract = {In this work, we employed regression analysis based on Transcribe-Sheng Project, a typical Chinese crowdsourcing project in culture heritage, to explore the influencing effects of the two predicting factors---self-efficacy and familiarity, on the volunteers' task completion and task performance respectively. The preliminary study found that: 1) Familiarity (including familiarity of background knowledge and familiarity of transcription platform) was the main factor that influenced task completion; 2) Familiarity of background knowledge significantly influenced the task performance; 3) The effects of self-efficacy on both task completion and task performance were not significant.},
booktitle = {Proceedings of the 18th Joint Conference on Digital Libraries},
pages = {408–409},
numpages = {2},
keywords = {transcribe sheng project, self-efficacy, familiarity, digital humanity, crowdsourcing},
location = {Champaign, Illinois},
series = {JCDL '19}
}

@inproceedings{10.1145/2787394.2787397,
author = {Mandalari, Anna Maria and Bagnulo, Marcelo and Lutu, Andra},
title = {Informing Protocol Design Through Crowdsourcing: the Case of Pervasive Encryption},
year = {2015},
isbn = {9781450335393},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2787394.2787397},
doi = {10.1145/2787394.2787397},
abstract = {Middleboxes, such as proxies, firewalls and NATs play an important role in the modern Internet ecosystem. On one hand, they perform advanced functions, e.g. traffic shaping, security or enhancing application performance. On the other hand, they turn the Internet into a hostile ecosystem for innovation, as they limit the deviation from deployed protocols. It is therefore essential, when designing a new protocol, to first understand its interaction with the elements of the path. The emerging area of crowdsourcing solutions can help to shed light on this issue. Such approach allows us to reach large and different sets of users and also different types of devices and networks to perform Internet measurements. In this paper, we show how to make informed protocol design choices by using a crowdsourcing platform. We consider a specific use case, namely the case of pervasive encryption in the modern Internet. Given the latest public disclosures of the NSA global surveillance operations, the issue of privacy in the Internet became of paramount importance. Internet community efforts are thus underway to increase the adoption of encryption. Using a crowdsourcing approach, we perform large-scale TLS measurements to advance our understanding on whether wide adoption of encryption is possible in today's Internet.},
booktitle = {Proceedings of the 2015 ACM SIGCOMM Workshop on Crowdsourcing and Crowdsharing of Big (Internet) Data},
pages = {3–8},
numpages = {6},
keywords = {middleboxes, internet measurements, crowdsourcing, TLS},
location = {London, United Kingdom},
series = {C2B(1)D '15}
}

@inproceedings{10.1145/3152896.3152898,
author = {Nguyen, Q. N. and Frisiello, A. and Rossi, C.},
title = {Co-design of a crowdsourcing solution for disaster risk reduction},
year = {2017},
isbn = {9781450354240},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3152896.3152898},
doi = {10.1145/3152896.3152898},
abstract = {Disaster Risk Reduction (DRR) is a complex field in which a huge amount of data is used to plan preventive measures, get prepared to natural disasters, and effectively respond when they strike. This work focuses on the definition of a co-design methodology to integrate a crowdsourcing solution in the DRR processes. We define the proposed methodology, and implement it involving operators and experts in the DRR domain (crisis managers, technical services, first responders). We show how a participatory design approach helps in the design of a crowdsourcing solution that experts are willing to integrate into their DRR procedures.},
booktitle = {Proceedings of the First CoNEXT Workshop on ICT Tools for Emergency Networks and DisastEr Relief},
pages = {7–12},
numpages = {6},
keywords = {visual analysis, user interface, user experience, prototyping, human-centred design, disaster risk reduction, crowdsourcing, co-design},
location = {Incheon, Republic of Korea},
series = {I-TENDER '17}
}

@inproceedings{10.1145/2872427.2883035,
author = {Wilson, Shomir and Schaub, Florian and Ramanath, Rohan and Sadeh, Norman and Liu, Fei and Smith, Noah A. and Liu, Frederick},
title = {Crowdsourcing Annotations for Websites' Privacy Policies: Can It Really Work?},
year = {2016},
isbn = {9781450341431},
publisher = {International World Wide Web Conferences Steering Committee},
address = {Republic and Canton of Geneva, CHE},
url = {https://doi.org/10.1145/2872427.2883035},
doi = {10.1145/2872427.2883035},
abstract = {Website privacy policies are often long and difficult to understand. While research shows that Internet users care about their privacy, they do not have time to understand the policies of every website they visit, and most users hardly ever read privacy policies. Several recent efforts aim to crowdsource the interpretation of privacy policies and use the resulting annotations to build more effective user interfaces that provide users with salient policy summaries. However, very little attention has been devoted to studying the accuracy and scalability of crowdsourced privacy policy annotations, the types of questions crowdworkers can effectively answer, and the ways in which their productivity can be enhanced. Prior research indicates that most Internet users often have great difficulty understanding privacy policies, suggesting limits to the effectiveness of crowdsourcing approaches. In this paper, we assess the viability of crowdsourcing privacy policy annotations. Our results suggest that, if carefully deployed, crowdsourcing can indeed result in the generation of non-trivial annotations and can also help identify elements of ambiguity in policies. We further introduce and evaluate a method to improve the annotation process by predicting and highlighting paragraphs relevant to specific data practices.},
booktitle = {Proceedings of the 25th International Conference on World Wide Web},
pages = {133–143},
numpages = {11},
keywords = {privacy policies, privacy, machine learning, crowdsourcing, HCI},
location = {Montr\'{e}al, Qu\'{e}bec, Canada},
series = {WWW '16}
}

@inproceedings{10.1145/2968219.2968585,
author = {Ludwig, Thomas and Kotthaus, Christoph and Pipek, Volkmar},
title = {Situated and ubiquitous crowdsourcing with volunteers during disasters},
year = {2016},
isbn = {9781450344623},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2968219.2968585},
doi = {10.1145/2968219.2968585},
abstract = {Although emergency services have already recognized the importance of citizen-initiated activities during disasters, still questions with regard to the coordination of spontaneous volunteers and their activities arise. Situated and ubiquitous crowdsourcing seem to be appropriate concepts for supporting the management of voluntary activities during emergencies. Within this paper we present two tools that encompass both types of crowdsourcing mechanisms.},
booktitle = {Proceedings of the 2016 ACM International Joint Conference on Pervasive and Ubiquitous Computing: Adjunct},
pages = {1441–1447},
numpages = {7},
keywords = {volunteers, ubiquitous crowdsourcing, situated crowdsourcing, crisis management},
location = {Heidelberg, Germany},
series = {UbiComp '16}
}

@inproceedings{10.1109/WCNC45663.2020.9120841,
author = {Hao, Lifei and Jia, Bing and Liu, Jingbin and Huang, Baoqi and Li, Wuyungerile},
title = {VCG-QCP: A Reverse Pricing Mechanism Based on VCG and Quality All-pay for Collaborative Crowdsourcing},
year = {2020},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/WCNC45663.2020.9120841},
doi = {10.1109/WCNC45663.2020.9120841},
abstract = {With the rapid development of the Internet and combined with outsourcing, a new paradigm – crowdsourcing which shines brilliantly as a new labor mode. However, the existing pricing strategies for crowdsourcing tasks have several undesirable problems, e.g., no universal pricing model, not meeting the multiple requirements of users, pricing rely too much on decision makers, etc., which bring an unreasonable allocation of task rewards so as to make the pricing results subjective and uncontrollable. Therefore, this paper proposes a reverse pricing mechanism based on VCG and quality all-pay for collaborative crowdsourcing (VCG-QCP). The actual crowdsourcing scenario is considered with VCG mechanism, and the concept of quality all-pay is introduced to evaluate the work quality of workers who might perform the task. Then a general reverse pricing model is established by mathematical modeling, and the pricing algorithm is designed based on this model. Simulations show that the proposed method can achieve higher algorithm efficiency, higher task completion quality, a reasonable balance of benefits between employers and workers, and ensuring the truthfulness of workers’ bidding.},
booktitle = {2020 IEEE Wireless Communications and Networking Conference (WCNC)},
pages = {1–6},
numpages = {6},
location = {Seoul, Korea (South)}
}

@inproceedings{10.1109/GLOCOM.2017.8254121,
author = {Tang, Ming and Gao, Lin and Huang, Jianwei},
title = {A General Framework for Crowdsourcing Mobile Communication, Computation, and Caching},
year = {2017},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/GLOCOM.2017.8254121},
doi = {10.1109/GLOCOM.2017.8254121},
abstract = {Today's mobile devices are capable of tackling various complicated tasks that may require a large amount of communication, computation, and caching (3C) resources. Due to users' heterogeneous resources and service requirements, it is challenging for each user to always accomplish his task satisfactorily. To alleviate this issue, mobile users can exploit the heterogeneity and crowdsource their resources to enhance the task execution performance. In this paper, we propose a general 3C framework that enables mobile users to share all three types of resources through device- to-device connections. Such a framework generalizes many existing 1C/2C resource sharing models (that only shares one or two types of resources among users). To quantify the benefit of the proposed framework, we focus on an energy minimization problem, and show that the 3C framework always achieves a smaller total energy consumption, comparing with other 1C/2C models. Furthermore, we show that the energy reduction is maximized, when user connection probability and content caching ratio are neither too large nor too small. Our numerical results show that, when ignoring device-to-device transmission energy, the general 3C framework can reduce the total energy consumption by 82.98\%, comparing with the 1C/2C models.},
booktitle = {GLOBECOM 2017 - 2017 IEEE Global Communications Conference},
pages = {1–6},
numpages = {6},
location = {Singapore}
}

@inproceedings{10.1145/3256341,
author = {Ju, Wendy},
title = {Session details: Crowdsourcing},
year = {2012},
isbn = {9781450310864},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3256341},
doi = {10.1145/3256341},
booktitle = {Proceedings of the ACM 2012 Conference on Computer Supported Cooperative Work},
location = {Seattle, Washington, USA},
series = {CSCW '12}
}

@inproceedings{10.1145/1864431.1864504,
author = {Vukovic, Maja and Kumara, Soundar and Greenshpan, Ohad},
title = {Ubiquitous crowdsourcing},
year = {2010},
isbn = {9781450302838},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1864431.1864504},
doi = {10.1145/1864431.1864504},
abstract = {Web 2.0 provides the technological foundations upon which the crowdsourcing paradigm evolves and operates, enabling networked experts to work on various problem solving and data-intensive tasks. During the past decade crowdsourcing grew from a number of purpose-built initiatives, such as Wikipedia and Mechanical Turk, to a technique that today attracts and engages over 2 million people worldwide. As the computing systems are becoming more intimately embedded in physical and social contexts, promising truly ubiquitous computing, crowdsourcing takes new forms. Increasingly, crowds are engaged through mobile devices, to capture, share and validate sheer amount data (e.g. reporting security threats or capturing social events).This workshop challenges researchers and practitioners to think about three key aspects of ubiquitous crowdsourcing. Firstly, to establish technological foundations, what are the interaction models and protocols between the ubiquitous computing systems and the crowd? Secondly, how is crowdsourcing going to face the challenges in quality assurance, while providing valuable incentive frameworks that enable honest contributions? Finally, what are the novel applications of crowdsourcing enabled by ubiquitous computing systems?},
booktitle = {Proceedings of the 12th ACM International Conference Adjunct Papers on Ubiquitous Computing - Adjunct},
pages = {523–526},
numpages = {4},
keywords = {ubiquitous, mobile, crowdsourcing},
location = {Copenhagen, Denmark},
series = {UbiComp '10 Adjunct}
}

@inproceedings{10.1145/3021460.3021483,
author = {S., Lalit Mohan and Raman, Priya and Choppella, Venkatesh and Reddy, Y. R.},
title = {A Crowdsourcing Approach for Quality Enhancement of eLearning Systems},
year = {2017},
isbn = {9781450348560},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3021460.3021483},
doi = {10.1145/3021460.3021483},
abstract = {In India, a large number of engineering undergraduates are adopting eLearning as it provides access to best faculty and reduces concerns on inadequate physical infrastructure at colleges. Virtual Labs is a Government of India eLearning initiative containing simulation and remote triggered labs for engineering students. Virtual Labs developed over a period of 6 years is used by more than a million undergraduate students across nine engineering disciplines. The software used for developing these experiments requires substantial effort for maintenance due to deprecation, compatibility, etc. We propose a targeted crowdsourcing approach for maintenance of Virtual Labs with sustainable quality. The targeted crowdsourcing involves the large number of engineering students who are also the major stakeholders of these labs. Our quality enhancement using crowdsourcing approach was validated for 14 labs and would be extended to 191 labs based on encouraging results.},
booktitle = {Proceedings of the 10th Innovations in Software Engineering Conference},
pages = {188–194},
numpages = {7},
keywords = {eLearning Systems, Software Development, Quality, Crowdsourcing},
location = {Jaipur, India},
series = {ISEC '17}
}

@inproceedings{10.1145/2666539.2666569,
author = {Yan, Minzhi and Sun, Hailong and Liu, Xudong},
title = {iTest: testing software with mobile crowdsourcing},
year = {2014},
isbn = {9781450332248},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2666539.2666569},
doi = {10.1145/2666539.2666569},
abstract = {In recent years, a lot of crowdsourcing systems have emerged and lead to many successful crowdsourcing systems like Wiki-pedia, Amazon Mechanical Turk and Waze. In the field of software engineering, crowdtesting has acquired increased interest and adoption, especially among personal developers and smaller companies. In this paper, we present iTest which combines mobile crowdsourcing and software testing together to support the testing of mobile application and web services. iTest is a framework for software developers to submit their software and conveniently get the test results from the crowd testers. Firstly, we analyze the key problems need to be solved in a mobile crowdtesting platform; Secondly, we present the architecture of iTest framework; Thirdly, we introduce the workflow of testing web service in iTest and propose an algorithm for solving the tester selection problem mentioned in Section 2; Then the development kit to support testing mobile application is explained; Finally, we perform two experiments to illustrate that both the way to access network and tester's location influence the performance of web service.},
booktitle = {Proceedings of the 1st International Workshop on Crowd-Based Software Development Methods and Technologies},
pages = {19–24},
numpages = {6},
keywords = {web service, mobile crowdsourcing, mobile application, Software testing},
location = {Hong Kong, China},
series = {CrowdSoft 2014}
}

@inproceedings{10.1109/COASE.2018.8560512,
author = {Fadda, Edoardo and Perboli, Guido and Vallesio, Valerio and Mana, Dario},
title = {Sustainable mobility and user preferences by crowdsourcing data: the Open Agora project},
year = {2018},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/COASE.2018.8560512},
doi = {10.1109/COASE.2018.8560512},
abstract = {One application of network optimization is the study of the policies able to change people habits in transportation mode selection. The main strategy for achieving this objective is to develop a model describing the preferences of the people by considering the characteristics of the transportation modes (such as cost, travel duration) and then to develop policies in order to improve the characteristics of the target transportation mode. These models are called utility models and have a long story. Nevertheless, the data needed for their fitting are difficult to get. One of the main issues in this case is how to collect the data and how to tune a model that can be easily scaled and adapted to different settings. In this paper, we describe the results achieved during the Open Agora project where the utility model is tined with crowdsourcing data coming from mobile phone applications and collected indirectly by the users.},
booktitle = {2018 IEEE 14th International Conference on Automation Science and Engineering (CASE)},
pages = {1243–1248},
numpages = {6},
location = {Munich, Germany}
}

@inproceedings{10.1145/2556420.2556479,
author = {Pavlick, Ellie and Yan, Rui and Callison-Burch, Chris},
title = {Crowdsourcing for grammatical error correction},
year = {2014},
isbn = {9781450325417},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2556420.2556479},
doi = {10.1145/2556420.2556479},
abstract = {We discuss the problem of grammatical error correction, which has gained attention for its usefulness both in the development of tools for learners of foreign languages and as a component of statistical machine translation systems. We believe the task of suggesting grammar and style corrections in writing is well suited to a crowdsourcing solution but is currently hindered by the difficulty of automatic quality control. In this proposal, we motivate the problem of grammatical error correction and outline the challenges of ensuring quality in a setting where traditional methods of aggregation (e.g. majority vote) fail to produce the desired results. We then propose a design for quality control and present preliminary results indicating the potential of crowd workers to provide a scalable solution.},
booktitle = {Proceedings of the Companion Publication of the 17th ACM Conference on Computer Supported Cooperative Work \&amp; Social Computing},
pages = {209–212},
numpages = {4},
keywords = {postediting, esl, crowdsourcing},
location = {Baltimore, Maryland, USA},
series = {CSCW Companion '14}
}

@inproceedings{10.1109/CrowdRE.2015.7367586,
author = {Srivastava, Pratyoush K. and Sharma, Richa},
title = {Crowdsourcing to elicit requirements for MyERP application},
year = {2015},
isbn = {9781509001132},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/CrowdRE.2015.7367586},
doi = {10.1109/CrowdRE.2015.7367586},
abstract = {Crowdsourcing is an emerging method to collect requirements for software systems. Applications seeking global acceptance need to meet the expectations of a wide range of users. Collecting requirements and arriving at consensus with a wide range of users is difficult using traditional method of requirements elicitation. This paper presents crowdsourcing based approach for German medium-size software company MyERP that might help the company to get access to requirements from non-German customers. We present the tasks involved in the proposed solution that would help the company meet the goal of eliciting requirements at a fast pace with non-German customers.},
booktitle = {Proceedings of the 2015 IEEE 1st International Workshop on Crowd-Based Requirements Engineering (CrowdRE)},
pages = {31–35},
numpages = {5},
series = {CROWDRE '15}
}

@inproceedings{10.1145/3308558.3320096,
author = {Aroyo, Lora and Dumitrache, Anca and Inel, Oana and Szl\'{a}vik, Zolt\'{a}n and Timmermans, Benjamin and Welty, Chris},
title = {Crowdsourcing Inclusivity: Dealing with Diversity of Opinions, Perspectives and Ambiguity in Annotated Data},
year = {2019},
isbn = {9781450366755},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3308558.3320096},
doi = {10.1145/3308558.3320096},
abstract = {In this tutorial, we introduce a novel crowdsourcing methodology called CrowdTruth [1, 9]. The central characteristic of CrowdTruth is harnessing the diversity in human interpretation to capture the wide range of opinions and perspectives, and thus provide more reliable, realistic and inclusive real-world annotated data for training and evaluating machine learning components. Unlike other methods, we do not discard dissenting votes, but incorporate them into a richer and more continuous representation of truth. CrowdTruth is a widely used crowdsourcing methodology1 adopted by industrial partners and public organizations such as Google, IBM, New York Times, Cleveland Clinic, Crowdynews, Sound and Vision archive, Rijksmuseum, and in a multitude of domains such as AI, news, medicine, social media, cultural heritage, and social sciences. The goal of this tutorial is to introduce the audience to a novel approach to crowdsourcing that takes advantage of the diversity of opinions and perspectives that is inherent to the Web, as methods that deal with disagreement and diversity in crowdsourcing have become increasingly popular. Creating this more complex notion of truth contributes directly to the larger discussion on how to make the Web more reliable, diverse and inclusive.},
booktitle = {Companion Proceedings of The 2019 World Wide Web Conference},
pages = {1294–1295},
numpages = {2},
keywords = {Perspectives, Medical Text Annotation, Inter-annotator Disagreement, Ground Truth, Diversity, Digital Humanities, Crowdsourcing, Computational Social Sciences, Ambiguity},
location = {San Francisco, USA},
series = {WWW '19}
}

@inproceedings{10.1145/3308558.3313599,
author = {Yang, Jie and Smirnova, Alisa and Yang, Dingqi and Demartini, Gianluca and Lu, Yuan and Cudre-Mauroux, Philippe},
title = {Scalpel-CD: Leveraging Crowdsourcing and Deep Probabilistic Modeling for Debugging Noisy Training Data},
year = {2019},
isbn = {9781450366748},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3308558.3313599},
doi = {10.1145/3308558.3313599},
abstract = {This paper presents Scalpel-CD, a first-of-its-kind system that leverages both human and machine intelligence to debug noisy labels from the training data of machine learning systems. Our system identifies potentially wrong labels using a deep probabilistic model, which is able to infer the latent class of a high-dimensional data instance by exploiting data distributions in the underlying latent feature space. To minimize crowd efforts, it employs a data sampler which selects data instances that would benefit the most from being inspected by the crowd. The manually verified labels are then propagated to similar data instances in the original training data by exploiting the underlying data structure, thus scaling out the contribution from the crowd. Scalpel-CD is designed with a set of algorithmic solutions to automatically search for the optimal configurations for different types of training data, in terms of the underlying data structure, noise ratio, and noise types (random vs. structural). In a real deployment on multiple machine learning tasks, we demonstrate that Scalpel-CD is able to improve label quality by 12.9\% with only 2.8\% instances inspected by the crowd.},
booktitle = {The World Wide Web Conference},
pages = {2158–2168},
numpages = {11},
keywords = {deep probabilistic models, crowdsourcing, Debugging training data},
location = {San Francisco, CA, USA},
series = {WWW '19}
}

@inproceedings{10.1109/ACII.2015.7344618,
author = {McDuff, Daniel and el Kaliouby, Rana and Picard, Rosalind W.},
title = {Crowdsourcing facial responses to online videos: Extended abstract},
year = {2015},
isbn = {9781479999538},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/ACII.2015.7344618},
doi = {10.1109/ACII.2015.7344618},
abstract = {Traditional observational research methods required an experimenter's presence in order to record videos of participants, and limited the scalability of data collection to typically less than a few hundred people in a single location. In order to make a significant leap forward in affective expression data collection and the insights based on it, our work has created and validated a novel framework for collecting and analyzing facial responses over the Internet. The first experiment using this framework enabled 3,268 trackable face videos to be collected and analyzed in under two months. Each participant viewed one or more commercials while their facial response was recorded and analyzed. Our data showed significantly different intensity and dynamics patterns of smile responses between subgroups who reported liking the commercials versus those who did not. Since this framework appeared in 2011, we have collected over three million videos of facial responses in over 75 countries using this same methodology, enabling facial analytics to become significantly more accurate and validated across five continents. Many new insights have been discovered based on crowd-sourced facial data, enabling Internet-based measurement of facial responses to become reliable and proven. We are now able to provide large-scale evidence for gender, cultural and age differences in behaviors. Today such methods are used as part of standard practice in industry for copy-testing advertisements and are increasingly used for online media evaluations, distance learning, and mobile applications.},
booktitle = {Proceedings of the 2015 International Conference on Affective Computing and Intelligent Interaction (ACII)},
pages = {512–518},
numpages = {7},
series = {ACII '15}
}

@inproceedings{10.1007/978-3-030-21935-2_9,
author = {Aihara, Kenro and Bin, Piao and Imura, Hajime},
title = {On the Relationship Between Accuracy of Bus Position Estimated by Crowdsourcing and Participation Density},
year = {2019},
isbn = {978-3-030-21934-5},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-21935-2_9},
doi = {10.1007/978-3-030-21935-2_9},
abstract = {The authors proposed a methodology of bus location service by crowdsource. In the conventional bus location service, a GPS receiver or the like for positioning the bus vehicle and a communication line for transmitting the position information are necessary for each bus and it causes expensive cost, whereas in this methodology, the BLE beacon and the application of the service installed in users’ smartphone transmits the beacon signal and the position information of the smartphone using the communication line of users’ smartphone so as to reduce the cost of bus operators.In order to grasp the position of each bus at any time on the server, one of the following is required: (1) At least one service user always exists for each bus, and (2) It is possible to estimate the position with high accuracy where there is no service user in a section.If contributing to the realization of smart city by grasping the situation in the town by crowdsourcing, it is extremely important to clarify the precision with respect to density that may be given as a relation between the number of users who participate in the program for grasping this situation and the size of the town.In this paper, the authors examine and show the relationship between the density of data collection and the accuracy of interpolation for the section where data is missing, using the position information actually obtained using the proposed application.},
booktitle = {Distributed, Ambient and Pervasive Interactions: 7th International Conference, DAPI 2019, Held as Part of the 21st HCI International Conference, HCII 2019, Orlando, FL, USA, July 26–31, 2019, Proceedings},
pages = {101–112},
numpages = {12},
keywords = {Crowdsourcing, Crowdsensing, Smart and hybrid cities, Internet of Things},
location = {Orlando, FL, USA}
}

@inproceedings{10.1109/ATNAC.2015.7366785,
author = {Schwartz, Christian and Borchert, Kathrin and Hirth, Matthias and Tran-Gia, Phuoc},
title = {Modeling crowdsourcing platforms to enable workforce dimensioning},
year = {2015},
isbn = {9781467393485},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/ATNAC.2015.7366785},
doi = {10.1109/ATNAC.2015.7366785},
abstract = {Crowdsourcing platforms provide an easy and scalable access to human workforce that can, e.g., provide subjective judgements, tagging information, or even generate knowledge. In conjunction with machine clouds offering scalable access to computing resources, these human cloud providers offer numerous possibilities for creating new applications which would not have been possible a few years ago. However, in order to build sustainable services on top of this inter-cloud environment, scalability considerations have to be made. While cloud computing systems are already well studied in terms of dimensioning of the hardware resources, there still exists little work on the appropriate scaling of crowdsourcing platforms. This is especially challenging, as the complex interaction between all involved stakeholders, platform providers, workers and employers has to be considered. The contribution of this work is threefold. First, we develop a model for common crowdsourcing platforms and implement the model using a simulative approach, which is validated with a comparison to an analytic M[X]/M/c system. In a second step, we evaluate inter-arrival times as well as campaign size distributions based on a dataset of a large commercial crowdsourcing platform to derive realistic model parameters and illustrate the differences to the analytic approximation. Finally, we perform a parameter study using the simulation model to derive guidelines for dimensioning crowdsourcing platforms, while considering relevant parameters for the involved stakeholders, i.e., the delay before work on a task begins and the work load of the workers.},
booktitle = {Proceedings of the 2015 International Telecommunication Networks and Applications Conference (ITNAC)},
pages = {30–37},
numpages = {8},
series = {ITNAC '15}
}

@inproceedings{10.5555/2969033.2969105,
author = {Jagabathula, Srikanth and Subramanian, Lakshminarayanan and Venkataraman, Ashwin},
title = {Reputation-based Worker filtering in crowdsourcing},
year = {2014},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {In this paper, we study the problem of aggregating noisy labels from crowd workers to infer the underlying true labels of binary tasks. Unlike most prior work which has examined this problem under the random worker paradigm, we consider a much broader class of adversarial workers with no specific assumptions on their labeling strategy. Our key contribution is the design of a computationally efficient reputation algorithm to identify and filter out these adversarial workers in crowd-sourcing systems. Our algorithm uses the concept of optimal semi-matchings in conjunction with worker penalties based on label disagreements, to assign a reputation score for every worker. We provide strong theoretical guarantees for deterministic adversarial strategies as well as the extreme case of sophisticated adversaries where we analyze the worst-case behavior of our algorithm. Finally, we show that our reputation algorithm can significantly improve the accuracy of existing label aggregation algorithms in real-world crowdsourcing datasets.},
booktitle = {Proceedings of the 27th International Conference on Neural Information Processing Systems - Volume 2},
pages = {2492–2500},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'14}
}

@inproceedings{10.5555/2615731.2615808,
author = {Nath, Swaprava and Narayanaswamy, Balakrishnan (Murali)},
title = {Productive output in hierarchical crowdsourcing},
year = {2014},
isbn = {9781450327381},
publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
address = {Richland, SC},
abstract = {Organically grown crowdsourcing networks, which includes production firms and social network-based crowdsourcing applications, tend to have a hierarchical structure. Considering the entire crowdsourcing system as a consolidated organization, a primary goal of a designer is to maximize the net productive output of this hierarchy using reward sharing as an incentive tool. Every individual in a hierarchy has a limited amount of effort that they can split between production and communication. Productive effort yields an agent a direct payoff, while the communication effort of an agent improves the productivity of other agents in her subtree. To understand how the net output of the crowdsourcing network is influenced by these components, we develop a game theoretic model that helps explain how the individuals trade off these two components depending on their position in the hierarchy and their shares of reward. We provide a detailed analysis of the Nash equilibrium efforts and a design recipe of the reward sharing scheme that maximizes the net productive output. Our results show that even under strategic behavior of the agents, it is sometimes possible to achieve the optimal output and also provide bounds on the achievability when this is not the case.},
booktitle = {Proceedings of the 2014 International Conference on Autonomous Agents and Multi-Agent Systems},
pages = {469–476},
numpages = {8},
keywords = {social output, price of anarchy, nash equilibrium, hierarchies, crowdsourcing},
location = {Paris, France},
series = {AAMAS '14}
}

@inproceedings{10.1007/978-3-030-01391-2_4,
author = {Dong, Zhaoan and Tu, Jianhong and Fan, Ju and Lu, Jiaheng and Du, Xiaoyong and Ling, Tok Wang},
title = {Crowd-Type: A Crowdsourcing-Based Tool for Type Completion in Knowledge Bases},
year = {2018},
isbn = {978-3-030-01390-5},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-01391-2_4},
doi = {10.1007/978-3-030-01391-2_4},
abstract = {Entity type completion in Knowledge Bases (KBs) is an important and challenging problem. In our recent work, we have proposed a hybrid framework which combines the human intelligence of crowdsourcing with automatic algorithms to address the problem. In this demo, we have implemented the framework in a crowdsourcing-based system, named Crowd-Type, for fine-grained type completion in KBs. In particular, Crowd-Type firstly employs automatic algorithms to select the most representative entities and assigns them to human workers, who will verify the types for assigned entities. Then, the system infers and determines the correct types for all entities utilizing both the results of crowdsourcing and machine-based algorithms. Our system gives a vivid demonstration to show how crowdsourcing significantly improves the performance of automatic type completion algorithms.},
booktitle = {Advances in Conceptual Modeling: ER 2018 Workshops Emp-ER, MoBiD, MREBA, QMMQ, SCME, Xi’an, China, October 22-25, 2018, Proceedings},
pages = {17–21},
numpages = {5},
location = {Xi'an, China}
}

@inproceedings{10.1145/3345035.3345067,
author = {Chi, Aining and Ren, Nan},
title = {Research on the Impact of Task Feedback on the Performance of Creative Crowdsourcing Solvers},
year = {2019},
isbn = {9781450372190},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3345035.3345067},
doi = {10.1145/3345035.3345067},
abstract = {The key factor of crowdsourcing development is the solvers performance. The paper based on the customer perceived value theory to explore the influences of employer's task feedback on the performance of solvers in creative crowdsourcing. Employer's feedback in terms of quality, creativity, transaction price and service has different degrees of influence on the performance of solvers in creative crowdsourcing, while the value perception in terms of time and after-sales commitment has no obvious effect. The feedback of the employer in terms of favorable comment plays an mediating effect in the relationship between service quality and performance of solvers. And the relational behavior between the employer and the solver regulates the above mediating effect. Finally, according to the conclusions of the research, resource allocation suggestions of the strategic for the solvers are proposed.},
booktitle = {Proceedings of the 2019 10th International Conference on E-Business, Management and Economics},
pages = {101–105},
numpages = {5},
keywords = {task feedback, solvers performance, Crowdsourcing},
location = {Beijing, China},
series = {ICEME '19}
}

@inproceedings{10.1145/2872518.2891113,
author = {Suzuki, Yu and Nakamura, Satoshi},
title = {Assessing the Quality of Wikipedia Editors through Crowdsourcing},
year = {2016},
isbn = {9781450341448},
publisher = {International World Wide Web Conferences Steering Committee},
address = {Republic and Canton of Geneva, CHE},
url = {https://doi.org/10.1145/2872518.2891113},
doi = {10.1145/2872518.2891113},
abstract = {In this paper, we propose a method for assessing the quality of Wikipedia editors. By effectively determining whether the text meaning persists over time, we can determine the actual contribution by editors. This is used in this paper to detect vandal. However, the meaning of text does not always change if a term in the text is added or removed. Therefore, we cannot capture the changes of text meaning automatically, so we cannot detect whether the meaning of text survives or not. To solve this problem, we use crowdsourcing to manually detect changes of text meaning. In our experiment, we confirmed that our proposed method improves the accuracy of detecting vandals by about 5\%.},
booktitle = {Proceedings of the 25th International Conference Companion on World Wide Web},
pages = {1001–1006},
numpages = {6},
keywords = {wikipedia, vandalism, quality, crowdsourcing},
location = {Montr\'{e}al, Qu\'{e}bec, Canada},
series = {WWW '16 Companion}
}

@inproceedings{10.5555/2887007.2887188,
author = {Yu, Han and Miao, Chunyan and Shen, Zhiqi and Leung, Cyril and Chen, Yiqiang and Yang, Qiang},
title = {Efficient task sub-delegation for crowdsourcing},
year = {2015},
isbn = {0262511290},
publisher = {AAAI Press},
abstract = {Reputation-based approaches allow a crowdsourcing system to identify reliable workers to whom tasks can be delegated. In crowdsourcing systems that can be modeled as multi-agent trust networks consist of resource constrained trustee agents (i.e., workers), workers may need to further sub-delegate tasks to others if they determine that they cannot complete all pending tasks before the stipulated deadlines. Existing reputation-based decision-making models cannot help workers decide when and to whom to sub-delegate tasks. In this paper, we proposed a reputation aware task sub-delegation (RTS) approach to bridge this gap. By jointly considering a worker's reputation, workload, the price of its effort and its trust relationships with others, RTS can be implemented as an intelligent agent to help workers make sub-delegation decisions in a distributed manner. The resulting task allocation maximizes social welfare through efficient utilization of the collective capacity of a crowd, and provides provable performance guarantees. Experimental comparisons with state-of-the-art approaches based on the Epinions trust network demonstrate significant advantages of RTS under high workload conditions.},
booktitle = {Proceedings of the Twenty-Ninth AAAI Conference on Artificial Intelligence},
pages = {1305–1311},
numpages = {7},
location = {Austin, Texas},
series = {AAAI'15}
}

@inproceedings{10.5555/3020652.3020705,
author = {Lin, Christopher H. and Mausam and Weld, Daniel S.},
title = {Crowdsourcing control: moving beyond multiple choice},
year = {2012},
isbn = {9780974903989},
publisher = {AUAI Press},
address = {Arlington, Virginia, USA},
abstract = {To ensure quality results from crowdsourced tasks, requesters often aggregate worker responses and use one of a plethora of strategies to infer the correct answer from the set of noisy responses. However, all current models assume prior knowledge of all possible outcomes of the task. While not an unreasonable assumption for tasks that can be posited as multiple-choice questions (e.g. n-ary classification), we observe that many tasks do not naturally fit this paradigm, but instead demand a free-response formulation where the outcome space is of infinite size (e.g. audio transcription). We model such tasks with a novel probabilistic graphical model, and design and implement LAZYSUSAN, a decision-theoretic controller that dynamically requests responses as necessary in order to infer answers to these tasks. We also design an EM algorithm to jointly learn the parameters of our model while inferring the correct answers to multiple tasks at a time. Live experiments on Amazon Mechanical Turk demonstrate the superiority of LAZYSUSAN at solving SAT Math questions, eliminating 83.2\% of the error and achieving greater net utility compared to the state-of-the-art strategy, majority-voting. We also show in live experiments that our EM algorithm outperforms majority-voting on a visualization task that we design.},
booktitle = {Proceedings of the Twenty-Eighth Conference on Uncertainty in Artificial Intelligence},
pages = {491–500},
numpages = {10},
location = {Catalina Island, CA},
series = {UAI'12}
}

@inproceedings{10.1145/3170427.3188667,
author = {Korovina, Olga and Casati, Fabio and Nielek, Radoslaw and Baez, Marcos and Berestneva, Olga},
title = {Investigating Crowdsourcing as a Method to Collect Emotion Labels for Images},
year = {2018},
isbn = {9781450356213},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3170427.3188667},
doi = {10.1145/3170427.3188667},
abstract = {Labeling images is essential towards enabling the search and organization of digital media. This is true for both "factual", objective tags such as time, place and people, as well as for subjective labels, such as the emotion a picture generates. Indeed, the ability to associate emotions to images is one of the key functionality most image analysis services today strive to provide. In this paper we study how emotion labels for images can be crowdsourced and uncover limitations of the approach commonly used to gather training data today, that of harvesting images and tags from social media.},
booktitle = {Extended Abstracts of the 2018 CHI Conference on Human Factors in Computing Systems},
pages = {1–6},
numpages = {6},
keywords = {subjective tasks, image tagging, emotions, crowdsourcing},
location = {Montreal QC, Canada},
series = {CHI EA '18}
}

@inproceedings{10.1109/ICIP.2015.7351042,
author = {Nicholson, Bryce and Sheng, Victor S. and Zhang, Jing},
title = {Noise correction of image labeling in crowdsourcing},
year = {2015},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICIP.2015.7351042},
doi = {10.1109/ICIP.2015.7351042},
abstract = {We investigate the methods of improving data quality, in terms of label accuracy, in the context of image labeling in crowdsourcing. First, we look at three consensus methods for inferring a ground-truth label from the multiple noisy labels obtained from crowdsourcing, i.e., Majority Voting (MV), Dawid Skene (DS), and KOS. We then apply three noise correction methods to correct labels inferred by these consensus methods, i.e., Polishing Labels (PL), Self-Training Correction (STC), and Cluster Correction (CC). Our experimental results show that the noise correction methods improve the labeling quality significantly.},
booktitle = {2015 IEEE International Conference on Image Processing (ICIP)},
pages = {1458–1462},
numpages = {5},
location = {Quebec City, QC, Canada}
}

@inproceedings{10.1145/2858036.2858268,
author = {Doroudi, Shayan and Kamar, Ece and Brunskill, Emma and Horvitz, Eric},
title = {Toward a Learning Science for Complex Crowdsourcing Tasks},
year = {2016},
isbn = {9781450333627},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2858036.2858268},
doi = {10.1145/2858036.2858268},
abstract = {We explore how crowdworkers can be trained to tackle complex crowdsourcing tasks. We are particularly interested in training novice workers to perform well on solving tasks in situations where the space of strategies is large and workers need to discover and try different strategies to be successful. In a first experiment, we perform a comparison of five different training strategies. For complex web search challenges, we show that providing expert examples is an effective form of training, surpassing other forms of training in nearly all measures of interest. However, such training relies on access to domain expertise, which may be expensive or lacking. Therefore, in a second experiment we study the feasibility of training workers in the absence of domain expertise. We show that having workers validate the work of their peer workers can be even more effective than having them review expert examples if we only present solutions filtered by a threshold length. The results suggest that crowdsourced solutions of peer workers may be harnessed in an automated training pipeline.},
booktitle = {Proceedings of the 2016 CHI Conference on Human Factors in Computing Systems},
pages = {2623–2634},
numpages = {12},
keywords = {worker training, worked examples, web search, peer review, education, crowdsourcing},
location = {San Jose, California, USA},
series = {CHI '16}
}

@inproceedings{10.1145/3251625,
author = {Krishnamurthy, Balachander},
title = {Session details: Uses of Crowdsourcing for Networking},
year = {2015},
isbn = {9781450335393},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3251625},
doi = {10.1145/3251625},
booktitle = {Proceedings of the 2015 ACM SIGCOMM Workshop on Crowdsourcing and Crowdsharing of Big (Internet) Data},
location = {London, United Kingdom},
series = {C2B(1)D '15}
}

@inproceedings{10.1109/COMPSAC.2015.279,
author = {Sakamoto, Mizuki and Nakajima, Tatsuo and Akioka, Sayaka},
title = {Design Strategies for Building Mobile Crowdsourcing Services},
year = {2015},
isbn = {9781467365642},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/COMPSAC.2015.279},
doi = {10.1109/COMPSAC.2015.279},
abstract = {In this paper, we present an overview of three community-based mobile crowd sourcing services that we have developed as case studies. We then extract four lessons learned from our experiences to show that motivating people is an important factor in designing mobile crowd sourcing service. The extracted lessons are essential to successful design strategies for developing future crowd sourcing services.},
booktitle = {Proceedings of the 2015 IEEE 39th Annual Computer Software and Applications Conference - Volume 03},
pages = {234–239},
numpages = {6},
keywords = {Mobile Crowdsourcing, Human and Social Factors},
series = {COMPSAC '15}
}

@inproceedings{10.1109/GLOCOM.2016.7842167,
author = {Ying, Xuhang and Roy, Sumit and Poovendran, Radha},
title = {Pricing Mechanism for Quality-Based Radio Mapping via Crowdsourcing},
year = {2016},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/GLOCOM.2016.7842167},
doi = {10.1109/GLOCOM.2016.7842167},
abstract = {White Space (WS) Networking crucially relies on the active monitoring of spatio-temporal spectrum usage (to identify WS opportunities). To achieve this, one way is to gather spectrum data via wide-area sensor deployment and construct better Radio Environment Maps (REMs) with spatial models such as Kriging and Gaussian Process (GP). An economically viable alternative is via incentivized crowdsourcing, i.e., outsourcing sensing tasks to mobile users who have sensorized high-end client devices like tablets or smartphones, and providing proper incentives to compensate for users' sensing costs. In crowdsourced REM, features that impact REM performance and economic cost include user locations and the heterogeneity of user devices, which impact data quality and sensing costs. In this work, we emphasize the use of a hardware noise term in the GP model to account for data quality, and adopt mutual information to quantify sampling performance; we further design a pricing mechanism that allows the platform to maximize its expected utility at each stage and send optimal price offers to users sequentially, with joint consideration of sampling value, data quality and cost. We conduct simulations to evaluate the performance. Simulation results show that our mechanism outperforms two baseline mechanisms, and benefits from more users and less hardware noise (i.e., better data quality).},
booktitle = {2016 IEEE Global Communications Conference (GLOBECOM)},
pages = {1–6},
numpages = {6},
location = {Washington, DC, USA}
}

@inproceedings{10.1109/SAHCN.2017.7964933,
author = {Xu, Jia and Li, Hui and Li, Yanxu and Yang, Dejun and Li, Tao},
title = {Incentivizing the Biased Requesters: Truthful Task Assignment Mechanisms in Crowdsourcing},
year = {2017},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/SAHCN.2017.7964933},
doi = {10.1109/SAHCN.2017.7964933},
abstract = {Crowdsourcing has become an effective tool to utilize human intelligence to perform tasks that are challenging for machines. In the integrated crowdsourcing systems, the requesters are non- monopolistic and may show preferences over the workers. We are the first to design the incentive mechanisms, which consider the issue of stimulating the biased requesters in the competing crowdsourcing market. In this paper, we explore truthful task assignment mechanisms to maximize the total value of accomplished tasks for this new scenario. We present three models of crowdsourcing, which take the preferences of the requesters and the workload constraints of the workers into consideration. We design a task assignment mechanism, which follows the matching approach to solve the Valuation Maximizing Assignment (VMA) problem for each of the three models. Through both rigorous theoretical analyses and extensive simulations, we demonstrate that the proposed assignment mechanisms achieve computational efficiency, workload feasibility, preference (universal) truthfulness and constant approximation.},
booktitle = {2017 14th Annual IEEE International Conference on Sensing, Communication, and Networking (SECON)},
pages = {1–9},
numpages = {9},
location = {San Diego, CA, USA}
}

@inproceedings{10.5555/2887007.2887187,
author = {Tran-Thanh, Long and Huynh, Trung Dong and Rosenfeld, Avi and Ramchurn, Sarvapali D. and Jennings, Nicholas R.},
title = {Crowdsourcing complex workflows under budget constraints},
year = {2015},
isbn = {0262511290},
publisher = {AAAI Press},
abstract = {We consider the problem of task allocation in crowdsourcing systems with multiple complex workflows, each of which consists of a set of inter-dependent micro-tasks. We propose Budgeteer, an algorithm to solve this problem under a budget constraint. In particular, our algorithm first calculates an efficient way to allocate budget to each workflow. It then determines the number of inter-dependent micro-tasks and the price to pay for each task within each workflow, given the corresponding budget constraints. We empirically evaluate it on a well-known crowdsourcing-based text correction workflow using Amazon Mechanical Turk, and show that Budgeteer can achieve similar levels of accuracy to current benchmarks, but is on average 45\% cheaper.},
booktitle = {Proceedings of the Twenty-Ninth AAAI Conference on Artificial Intelligence},
pages = {1298–1304},
numpages = {7},
location = {Austin, Texas},
series = {AAAI'15}
}

@inproceedings{10.5555/2887007.2887182,
author = {Jyothi, Preethi and Hasegawa-Johnson, Mark},
title = {Acquiring speech transcriptions using mismatched crowdsourcing},
year = {2015},
isbn = {0262511290},
publisher = {AAAI Press},
abstract = {Transcribed speech is a critical resource for building statistical speech recognition systems. Recent work has looked towards soliciting transcriptions for large speech corpora from native speakers of the language using crowdsourcing techniques. However, native speakers of the target language may not be readily available for crowdsourcing. We examine the following question: can humans unfamiliar with the target language help transcribe? We follow an information-theoretic approach to this problem: (1) We learn the characteristics of a noisy channel that models the transcribers' systematic perception biases. (2) We use an error-correcting code, specifically a repetition code, to encode the inputs to this channel, in conjunction with a maximum-likelihood decoding rule. To demonstrate the feasibility of this approach, we transcribe isolated Hindi words with the help of Mechanical Turk workers unfamiliar with Hindi. We successfully recover Hindi words with an accuracy of over 85\% (and 94\% in a 4-best list) using a 15-fold repetition code. We also estimate the conditional entropy of the input to this channel (Hindi words) given the channel output (transcripts from crowdsourced workers) to be less than 2 bits; this serves as a theoretical estimate of the average number of bits of auxiliary information required for errorless recovery.},
booktitle = {Proceedings of the Twenty-Ninth AAAI Conference on Artificial Intelligence},
pages = {1263–1269},
numpages = {7},
location = {Austin, Texas},
series = {AAAI'15}
}

@inproceedings{10.5555/3061053.3061159,
author = {Segal, Avi and Gal, Ya'akov and Kamar, Ece and Horvitz, Eric and Bowyer, Alex and Miller, Grant},
title = {Intervention strategies for increasing engagement in crowdsourcing: platform, predictions, and experiments},
year = {2016},
isbn = {9781577357704},
publisher = {AAAI Press},
abstract = {Volunteer-based crowdsourcing depend critically on maintaining the engagement of participants. We explore a methodology for extending engagement in citizen science by combining machine learning with intervention design. We first present a platform for using real-time predictions about forthcoming disengagement to guide interventions. Then we discuss a set of experiments with delivering different messages to users based on the proximity to the predicted time of disengagement. The messages address motivational factors that were found in prior studies to influence users' engagements. We evaluate this approach on Galaxy Zoo, one of the largest citizen science application on the web, where we traced the behavior and contributions of thousands of users who received intervention messages over a period of a few months. We found sensitivity of the amount of user contributions to both the timing and nature of the message. Specifically, we found that a message emphasizing the helpfulness of individual users significantly increased users' contributions when delivered according to predicted times of disengagement, but not when delivered at random times. The influence of the message on users' contributions was more pronounced as additional user data was collected and made available to the classifier.},
booktitle = {Proceedings of the Twenty-Fifth International Joint Conference on Artificial Intelligence},
pages = {3861–3867},
numpages = {7},
location = {New York, New York, USA},
series = {IJCAI'16}
}

@inproceedings{10.5555/2874493.2874517,
author = {Euzenat, J\'{e}r\^{o}me},
title = {Uncertainty in crowdsourcing ontology matching},
year = {2013},
publisher = {CEUR-WS.org},
address = {Aachen, DEU},
booktitle = {Proceedings of the 8th International Conference on Ontology Matching - Volume 1111},
pages = {221–222},
numpages = {2},
location = {Sydney, Australia},
series = {OM'13}
}

@inproceedings{10.1145/3358695.3360920,
author = {Mauricio Yagui, Marcela Mayumi and Monsores Passos Maia, Lu\'{\i}s Fernando and Oliveira, Jonice and Vivacqua, Adriana},
title = {A Crowdsourcing Platform for Curating Cultural and Empirical Knowledge. A Study Applied to Botanical Collections},
year = {2019},
isbn = {9781450369886},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3358695.3360920},
doi = {10.1145/3358695.3360920},
abstract = {Empirical and Traditional knowledge is part of the concept of intangible cultural heritage, which represents immaterial cultural heritage items such as facts, stories, and traditions that are part of local culture. However, this knowledge might be lost over time. Encouraging public participation to record empirical knowledge is a way to preserve the culture of a region and make it known to others. This paper presents a web system that aims to record and to preserve empirical knowledge data. To achieve this goal, the tool uses three combined approaches: (i) crowdsourcing calls oriented to content creation by visitors; (ii) collaborative curation among experts for selection, evaluation, and publication of content; and (iii) interconnection between user contributions and data already available in open repositories on the web (in LOD format). The tool was applied in the Botany domain to curate empirical and cultural data about medicinal plants, where experts from the Rio de Janeiro Botanical Garden evaluated it.},
booktitle = {IEEE/WIC/ACM International Conference on Web Intelligence - Companion Volume},
pages = {322–326},
numpages = {5},
keywords = {Linked Open Data, Intangible Cultural Heritage., Cultural Heritage, Crowdsourcing, Content Curation, Collaborative Systems},
location = {Thessaloniki, Greece},
series = {WI '19 Companion}
}

@inproceedings{10.5555/2832249.2832403,
author = {Chen, Cen and Cheng, Shih-Fen and Lau, Hoong Chuin and Misra, Archan},
title = {Towards city-scale mobile crowdsourcing: task recommendations under trajectory uncertainties},
year = {2015},
isbn = {9781577357384},
publisher = {AAAI Press},
abstract = {In this work, we investigate the problem of largescale mobile crowdsourcing, where workers are financially motivated to perform location-based tasks physically. Unlike current industry practice that relies on workers to manually pick tasks to perform, we automatically make task recommendation based on workers' historical trajectories and desired time budgets. The challenge of predicting workers' trajectories is that it is faced with uncertainties, as a worker does not take same routes every day. In this work, we depart from deterministic modeling and study the stochastic task recommendation problem where each worker is associated with several predicted routine routes with probabilities. We formulate this problem as a stochastic integer linear program whose goal is to maximize the expected total utility achieved by all workers. We further exploit the separable structures of the formulation and apply the Lagrangian relaxation technique to scale up computation. Experiments have been performed over the instances generated using the real Singapore transportation network. The results show that we can find significantly better solutions than the deterministic formulation.},
booktitle = {Proceedings of the 24th International Conference on Artificial Intelligence},
pages = {1113–1119},
numpages = {7},
location = {Buenos Aires, Argentina},
series = {IJCAI'15}
}

@inproceedings{10.1109/GLOCOM.2016.7842248,
author = {Ni, Jianbing and Lin, Xiaodong and Zhang, Kuan and Yu, Yong},
title = {Secure and Deduplicated Spatial Crowdsourcing: A Fog-Based Approach},
year = {2016},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/GLOCOM.2016.7842248},
doi = {10.1109/GLOCOM.2016.7842248},
abstract = {With the proliferation of mobile devices, spatial crowdsourcing is rising as a new paradigm that enables individuals to participate in tasks related to some locations in the physical world. Nevertheless, how to allocate these tasks to proper mobile users and improve communication efficiency are critical in spatial crowdsourcing. In this paper, we propose Fo-DSC, a fog-based deduplicated spatial crowdsourcing framework to achieve precise task allocation and secure data deduplication. Specifically, by integrating fog computing, we design a two-step task allocation mechanism to improve the accuracy of tasks allocation in spatial crowdsourcing. The fog nodes can detect and erase the repeated data in crowdsensing reports without learning any information about the reports. Furthermore, Fo-DSC efficiently records the contributions of mobile users whose data are reduplicated and deleted. As a result, these users do not become discouraged. Finally, we demonstrate that Fo-DSC satisfies the properties of fog-based task allocation and secure data deduplication with low computational and communication overheads.},
booktitle = {2016 IEEE Global Communications Conference (GLOBECOM)},
pages = {1–6},
numpages = {6},
location = {Washington, DC, USA}
}

@inproceedings{10.5555/2999134.2999212,
author = {Liu, Qiang and Peng, Jian and Ihler, Alexander},
title = {Variational inference for crowdsourcing},
year = {2012},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Crowdsourcing has become a popular paradigm for labeling large datasets. However, it has given rise to the computational task of aggregating the crowdsourced labels provided by a collection of unreliable annotators. We approach this problem by transforming it into a standard inference problem in graphical models, and applying approximate variational methods, including belief propagation (BP) and mean field (MF). We show that our BP algorithm generalizes both majority voting and a recent algorithm by Karger et al. [1], while our MF method is closely related to a commonly used EM algorithm. In both cases, we find that the performance of the algorithms critically depends on the choice of a prior distribution on the workers' reliability; by choosing the prior properly, both BP and MF (and EM) perform surprisingly well on both simulated and real-world datasets, competitive with state-of-the-art algorithms based on more complicated modeling assumptions.},
booktitle = {Proceedings of the 25th International Conference on Neural Information Processing Systems - Volume 1},
pages = {692–700},
numpages = {9},
location = {Lake Tahoe, Nevada},
series = {NIPS'12}
}

@inproceedings{10.1145/3011141.3011173,
author = {Gushima, Kota and Sakamoto, Mizuki and Nakajima, Tatsuo},
title = {Community-based crowdsourcing to increase a community's well-being},
year = {2016},
isbn = {9781450348072},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3011141.3011173},
doi = {10.1145/3011141.3011173},
abstract = {The paper proposes a new way to encourage a community to increase the sustainability of their surroundings based on crowdsourcing. The most important characteristic in the approach is to automatically insert tasks that increase the community's well-being. Therefore, the more tasks the community's members perform that make their surroundings sustainable, the more they increase their well-being. The research is based on a microcrowdfunding crowdsourcing infrastructure. A new functionality to automatically add micro-tasks to increase a community's well-being is designed for enhancing the infrastructure. We also describe a preliminary user study to demonstrate the feasibility of our approach.},
booktitle = {Proceedings of the 18th International Conference on Information Integration and Web-Based Applications and Services},
pages = {1–6},
numpages = {6},
keywords = {positive psychology, human well-being, crowdsourcing},
location = {Singapore, Singapore},
series = {iiWAS '16}
}

@inproceedings{10.5555/2095116.2095185,
author = {Chawla, Shuchi and Hartline, Jason D. and Sivan, Balasubramanian},
title = {Optimal crowdsourcing contests},
year = {2012},
publisher = {Society for Industrial and Applied Mathematics},
address = {USA},
abstract = {We study the design and approximation of optimal crowdsourcing contests. Crowdsourcing contests can be modeled as all-pay auctions because entrants must exert effort up-front to enter. Unlike all-pay auctions where a usual design objective would be to maximize revenue, in crowdsourcing contests, the principal only benefits from the submission with the highest quality. We give a theory for optimal crowdsourcing contests that mirrors the theory of optimal auction design: the optimal crowdsourcing contest is a virtual valuation optimizer (the virtual valuation function depends on the distribution of contestant skills and the number of contestants). We also compare crowdsourcing contests with more conventional means of procurement. In this comparison, crowdsourcing contests are relatively disadvantaged because the effort of losing contestants is wasted. Nonetheless, we show that crowdsourcing contests are 2-approximations to conventional methods for a large family of "regular" distributions, and 4-approximations, otherwise.},
booktitle = {Proceedings of the Twenty-Third Annual ACM-SIAM Symposium on Discrete Algorithms},
pages = {856–868},
numpages = {13},
location = {Kyoto, Japan},
series = {SODA '12}
}

@inproceedings{10.1145/3014087.3014102,
author = {Aletdinova, Anna and Kravchenko, Maxim and Bakaev, Maxim},
title = {Crowdsourcing and the effectiveness of C2G interaction in Russia},
year = {2016},
isbn = {9781450348591},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3014087.3014102},
doi = {10.1145/3014087.3014102},
abstract = {E-government is a new form of interaction between the governmental and administrative bodies, as well as with organizations and citizens. Its further development is related to initiative of the society members in promoting their ideas and projects, which may be placed on dedicated e-government Internet portals or with respective commercial services. In our paper we analyze the conceptual foundations of crowdsourcing and its actual development in Russia, mostly in relation to socially important projects. Particularly, we specify the process models for managing different types of crowdsourcing projects and provide historical data on their funding and success rates. We also identify factors that are most significant for crowdsourcing projects to achieve their final goals. Finally, to increase the effectiveness of crowdsourcing within e-government infrastructure, we propose to create the unified system joining several individual platforms and e-receptions, to compensate for certain existing deficiencies in social projects promotion technologies.},
booktitle = {Proceedings of the International Conference on Electronic Governance and Open Society: Challenges in Eurasia},
pages = {202–211},
numpages = {10},
keywords = {projects classification, information society, e-governance effectiveness, crowdsourcing platforms, crowdsourcing development},
location = {St. Petersburg, Russia},
series = {EGOSE '16}
}

@inproceedings{10.1007/978-3-319-91806-8_6,
author = {Spitz, Rejane and Queiroz, Francisco and Pereira, Clorisval and Cardarelli Leite, Leonardo and Ferranti, Marcelo P. and Dam, Peter},
title = {Do You Eat This? Changing Behavior Through Gamification, Crowdsourcing and Civic Engagement},
year = {2018},
isbn = {978-3-319-91805-1},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-319-91806-8_6},
doi = {10.1007/978-3-319-91806-8_6},
abstract = {The current excessive use of artificial additives by the food industry, the side effects of these potentially harmful ingredients and their impact on public health should be more widely acknowledged by consumers and further disclosed and discussed by citizens. Governments should develop stricter regulations on food additives, promote better labeling, apply taxes on miscreant food and conduct tighter industry surveillance. In parallel, broader behavior change towards nutrition habits might also be fostered through social innovation and citizen participation. In this paper, we present the design process for creating Dyet (Do you eat this?), a gamified app devised for collecting data and informing on the presence of such additives in commercially available food products. We argue that information on food ingredients and artificial additives should not only be accessible and legible, but also intelligible and personally meaningful to citizens. Through gameplay, we expect to foster the habit of reading ingredients lists, encouraging users to better inform themselves about what they eat and drink. Our overall goal is to change consumer’s potentially unsafe eating habits by bringing visibility to the excessive intake of artificial additives and on harmful food industry practices, making it possible and easier for everyone to make healthier dietary choices.},
booktitle = {Design, User Experience, and Usability: Users, Contexts and Case Studies: 7th International Conference, DUXU 2018, Held as Part of HCI International 2018, Las Vegas, NV, USA, July 15–20, 2018, Proceedings, Part III},
pages = {67–79},
numpages = {13},
keywords = {Crowdsourcing, Gamification, Interface design},
location = {Las Vegas, NV, USA}
}

@inproceedings{10.1145/3152494.3167980,
author = {Mazumdar, Pramit and Patra, Bidyut Kr. and Babu, Korra Sathya},
title = {Handling cold-start scenarios in point-of-interest recommendations through crowdsourcing},
year = {2018},
isbn = {9781450363419},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3152494.3167980},
doi = {10.1145/3152494.3167980},
abstract = {Point-of-Interest (POI) recommender systems are used to suggest places or venues for a target user based on her preferences. The traditional recommender systems utilise the historical data to learn user preferences and then select places that relate to them. Therefore, knowledge of the features of a POI is important along with the preferences of the target users for a traditional recommender system. This technique faces a serious problem when a new POI emerges in a city. A 'new' POI has no historical data and hence a recommender system fails to learn about its features. This results in absence of the 'new' POIs in the recommended list. Such a scenario is popularly known as the POI cold-start scenario. Online social networks such as Yelp, TripAdvisor, Foursquare, etc. that provide POI recommendations as a service mostly face this particular problem. To address this issue, the proposed work gathers content on the cold-start POIs by crowdsourcing other online social networks and subsequently the dominating features at POIs are identified from the collected review and rating data. These features are utilized to address the cold-start problem. Finally, we develop a POI recommender system that can handle the POI cold-start scenario. We experimented on the real-world data provided by Yelp and the results are found to be significantly better than the state-of-art techniques in handling cold-start scenarios.},
booktitle = {Proceedings of the ACM India Joint International Conference on Data Science and Management of Data},
pages = {322–324},
numpages = {3},
keywords = {spam reviews, recommender systems, online social networks},
location = {Goa, India},
series = {CODS-COMAD '18}
}

@inproceedings{10.1145/2800835.2800971,
author = {Cvijikj, Irena Pletikosa and Kadar, Cristina and Ivan, Bogdan and Te, Yiea-Funk},
title = {Towards a crowdsourcing approach for crime prevention},
year = {2015},
isbn = {9781450335751},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2800835.2800971},
doi = {10.1145/2800835.2800971},
abstract = {With the rising level of criminal activities, crime is becoming one of the main problems of modern society. To address this issue, we implement a mobile application for crime prevention. We focus on the usage intention and motivations for content creation and consumption. Our results indicate that people are willing to use the app for acquiring and sharing crime-related information, but not on a daily basis. In addition, participation on the platform was found to be driven by affective and rational motivations, to contribute to the neighborhood safety and in return receive help for maintaining personal safety.},
booktitle = {Adjunct Proceedings of the 2015 ACM International Joint Conference on Pervasive and Ubiquitous Computing and Proceedings of the 2015 ACM International Symposium on Wearable Computers},
pages = {1367–1372},
numpages = {6},
keywords = {public good, participation, motivations, mobile app, crowdsourcing, crime prevention, crime mapping},
location = {Osaka, Japan},
series = {UbiComp/ISWC'15 Adjunct}
}

@inproceedings{10.1145/2835776.2835797,
author = {Li, Qi and Ma, Fenglong and Gao, Jing and Su, Lu and Quinn, Christopher J.},
title = {Crowdsourcing High Quality Labels with a Tight Budget},
year = {2016},
isbn = {9781450337168},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2835776.2835797},
doi = {10.1145/2835776.2835797},
abstract = {In the past decade, commercial crowdsourcing platforms have revolutionized the ways of classifying and annotating data, especially for large datasets. Obtaining labels for a single instance can be inexpensive, but for large datasets, it is important to allocate budgets wisely. With limited budgets, requesters must trade-off between the quantity of labeled instances and the quality of the final results. Existing budget allocation methods can achieve good quantity but cannot guarantee high quality of individual instances under a tight budget. However, in some scenarios, requesters may be willing to label fewer instances but of higher quality. Moreover, they may have different requirements on quality for different tasks. To address these challenges, we propose a flexible budget allocation framework called Requallo. Requallo allows requesters to set their specific requirements on the labeling quality and maximizes the number of labeled instances that achieve the quality requirement under a tight budget. The budget allocation problem is modeled as a Markov decision process and a sequential labeling policy is produced. The proposed policy greedily searches for the instance to query next as the one that can provide the maximum reward for the goal. The Requallo framework is further extended to consider worker reliability so that the budget can be better allocated. Experiments on two real-world crowdsourcing tasks as well as a simulated task demonstrate that when the budget is tight, the proposed Requallo framework outperforms existing state-of-the-art budget allocation methods from both quantity and quality aspects.},
booktitle = {Proceedings of the Ninth ACM International Conference on Web Search and Data Mining},
pages = {237–246},
numpages = {10},
keywords = {crowdsourcing, budget allocation},
location = {San Francisco, California, USA},
series = {WSDM '16}
}

@inproceedings{10.1007/978-3-642-41338-4_17,
author = {Acosta, Maribel and Zaveri, Amrapali and Simperl, Elena and Kontokostas, Dimitris and Auer, S\"{o}ren and Lehmann, Jens},
title = {Crowdsourcing Linked Data Quality Assessment},
year = {2013},
isbn = {9783642413377},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-642-41338-4_17},
doi = {10.1007/978-3-642-41338-4_17},
abstract = {In this paper we look into the use of crowdsourcing as a means to handle Linked Data quality problems that are challenging to be solved automatically. We analyzed the most common errors encountered in Linked Data sources and classified them according to the extent to which they are likely to be amenable to a specific form of crowdsourcing. Based on this analysis, we implemented a quality assessment methodology for Linked Data that leverages the wisdom of the crowds in different ways: (i) a contest targeting an expert crowd of researchers and Linked Data enthusiasts; complemented by (ii) paid microtasks published on Amazon Mechanical Turk.We empirically evaluated how this methodology could efficiently spot quality issues in DBpedia. We also investigated how the contributions of the two types of crowds could be optimally integrated into Linked Data curation processes. The results show that the two styles of crowdsourcing are complementary and that crowdsourcing-enabled quality assessment is a promising and affordable way to enhance the quality of Linked Data.},
booktitle = {Proceedings of the 12th International Semantic Web Conference - Part II},
pages = {260–276},
numpages = {17},
series = {ISWC '13}
}

@inproceedings{10.1109/UCC.2014.98,
author = {Hara, Tenshi and Springer, Thomas and Muthmann, Klemens and Schill, Alexander},
title = {Towards a Reusable Infrastructure for Crowdsourcing},
year = {2014},
isbn = {9781479978816},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/UCC.2014.98},
doi = {10.1109/UCC.2014.98},
abstract = {In the course of the last few years crowd sourcing has received growing research focus due to its conception of solving complex tasks with the help of a flexible group of contributors of whom each needs to only contribute a simpler task part. Hence, the crowd can contribute by collecting data from distributed locations, completing map information, or voting on product ideas, et cetera. However, even though it is a necessary conceptual feature, the participation of large numbers of users with heterogeneous devices, generic infrastructures for crowd sourcing can hardly be found. For example, the management of users, mobile devices and contributed data has to be repetitively implemented in new projects. To ease the development of crowd sourcing applications, in this paper we propose a generic platform for simplified crowd sourcing deployment while supporting diverse crowd sourcing scenarios, the ability to handle large numbers of users and the involvement of heterogeneous mobile devices. The focus therein is put on the deployment process. Hence, the evaluation is based on an actual deployment, namely the migration of Cyface, an existing crowd sourcing project build from scratch, into using our proposed infrastructure.},
booktitle = {Proceedings of the 2014 IEEE/ACM 7th International Conference on Utility and Cloud Computing},
pages = {618–623},
numpages = {6},
keywords = {infrastructure, distributed hash table, crowdsourcing, SANE, Cyface},
series = {UCC '14}
}

@inproceedings{10.5555/2667680.2667681,
author = {Birch, Kate E. and Heffernan, Kayla J.},
title = {Crowdsourcing for clinical research: an evaluation of maturity},
year = {2014},
isbn = {9781921770357},
publisher = {Australian Computer Society, Inc.},
address = {AUS},
abstract = {With the growth of the Internet and individuals using the Internet for person health research, crowdsourcing clinical research has the potential to become a powerful tool in surveilling and monitoring health outcomes. This paper evaluates the maturity of the emerging tool of crowdsourcing clinical research using two carefully selected and adapted evaluation models: Project Management Maturity Model (ProMMM) and National Infrastructure Maturity Model (NIMM). Two models were used in conjunction for the evaluation as ProMMM focuses on a professional's ability to utilise crowdsourcing for clinical research, while NIMM focuses on the maturity of crowdsourcing clinical research itself. To evaluate maturity, the authors reviewed available literature and conducted primary research in the form of interviews at the Melbourne Brain Centre at Royal Melbourne Hospital with Associate Professor Helmut Butzkueven, MS Neurologist and Researcher, and Dr Athina (Tina) Soulis, General Manager of Neuroscience Trials Australia. The tool of crowdsourcing for clinical research and the users and prospective users of the tool were found to be in immaturity. Despite immaturity, the future holds exciting applications for crowdsourcing clinical research with the potential to save costs, time, and recruit wider cohorts into clinical research.},
booktitle = {Proceedings of the Seventh Australasian Workshop on Health Informatics and Knowledge Management - Volume 153},
pages = {3–11},
numpages = {9},
keywords = {maturity, evaluation, crowdsourcing, clinical research},
location = {Auckland, New Zealand},
series = {HIKM '14}
}

@inproceedings{10.1007/978-3-319-91458-9_23,
author = {Chen, Zhao and Cheng, Peng and Zhang, Chen and Chen, Lei},
title = {Effective Solution for Labeling Candidates with a Proper Ration for&nbsp;Efficient Crowdsourcing},
year = {2018},
isbn = {978-3-319-91457-2},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-319-91458-9_23},
doi = {10.1007/978-3-319-91458-9_23},
abstract = {One of the core problems of crowdsourcing research is how to reduce the cost, in other words, how to get better results with a limited budget. To save budget, most researchers concentrate on internal steps of crowdsourcing while in this work we focus on the pre-processing stage: how to select the input for crowds to contribute. A straightforward application of this work is to help budget-limited machine learning researchers to get better balanced training data from crowd labeling. Specifically, we formulate the prior information based input manipulating procedure as the Candidate Selection Problem (CSP) and propose an end-squeezing algorithm for it. Our results show that a considerable cost reduction can be achieved by manipulating the input to the crowd with the help of some additional prior information. We verify the effectiveness and efficiency of these algorithms through extensive experiments.},
booktitle = {Database Systems for Advanced Applications: 23rd International Conference, DASFAA 2018, Gold Coast, QLD, Australia, May 21-24, 2018, Proceedings, Part II},
pages = {386–394},
numpages = {9},
location = {Gold Coast, QLD, Australia}
}

@inproceedings{10.1145/2983323.2983886,
author = {Huang, Chao and Wu, Xian and Wang, Dong},
title = {Crowdsourcing-based Urban Anomaly Prediction System for Smart Cities},
year = {2016},
isbn = {9781450340731},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2983323.2983886},
doi = {10.1145/2983323.2983886},
abstract = {Crowdsourcing has become an emerging data collection paradigm for smart city applications. A new category of crowdsourcing-based urban anomaly reporting systems have been developed to enable pervasive and real-time reporting of anomalies in cities (e.g., noise, illegal use of public facilities, urban infrastructure malfunctions). An interesting challenge in these applications is how to accurately predict an anomaly in a given region of the city before it happens. Prior works have made significant progress in anomaly detection. However, they can only detect anomalies after they happen, which may lead to significant information delay and lack of preparedness to handle the anomalies in an efficient way. In this paper, we develop a Crowdsourcing-based Urban Anomaly Prediction Scheme (CUAPS) to accurately predict the anomalies of a city by exploring both spatial and temporal information embedded in the crowdsourcing data. We evaluated the performance of our scheme and compared it to the state-of-the-art baselines using four real-world datasets collected from 311 service in the city of New York. The results showed that our scheme can predict different categories of anomalies in a city more accurately than the baselines.},
booktitle = {Proceedings of the 25th ACM International on Conference on Information and Knowledge Management},
pages = {1969–1972},
numpages = {4},
keywords = {smart cities, crowdsourcing, anomaly prediction, Bayesian inference},
location = {Indianapolis, Indiana, USA},
series = {CIKM '16}
}

@inproceedings{10.5555/2655780.2655900,
author = {Rubenstein, Ellen L.},
title = {Crowdsourcing health literacy: the case of an online community},
year = {2013},
isbn = {0877155453},
publisher = {American Society for Information Science},
address = {USA},
abstract = {Drawing on data from 31 semi-structured, in-depth interviews, participant observation, and online archives analysis, this paper examines the health information interactions that participants in an online breast cancer community experienced as they progressed through breast cancer and survivorship. The findings reveal... This research highlights patients' perceptions of information gaps, how patients navigated through their information gaps with the help of the community, and the significance of peer interaction in the comprehension of medical information and medical decision-making.},
booktitle = {Proceedings of the 76th ASIS&amp;T Annual Meeting: Beyond the Cloud: Rethinking Information Boundaries},
articleno = {120},
numpages = {5},
keywords = {online communities, health literacy, health information},
location = {Montreal, Quebec, Canada},
series = {ASIST '13}
}

@inproceedings{10.1145/2660114.2660128,
author = {Nguyen, Nhatvi},
title = {Microworkers Crowdsourcing Approach, Challenges and Solutions},
year = {2014},
isbn = {9781450331289},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2660114.2660128},
doi = {10.1145/2660114.2660128},
abstract = {Founded in May 2009, Microworkers.com is an international Crowdsourcing platform focusing on Microtasks. At present, more than 600,000 users from over 190 countries have already registered to our platform. This extensively diverse workforce is the key to the current success of Microworkers as it gives opportunity to our clients to draw widely varying experiences and knowledge from a large, heterogeneous audience in arriving at innovative solutions. With the explosion of social media, mobile apps and online digital technology, the communication channels on how modern-day workers and tech-savvy consumers have profoundly changed. With that, how businesses communicate with their consumers, and to their employees, have also greatly transformed. While innovation remains the hallmark of staying competitive, the power of crowdsourcing is becoming more widely recognized because of the broad participation that takes place at relatively minimal costs. This brilliant mass collaboration approach allows companies to generate solutions from freelance professionals who get paid only if you utilize their ideas. Crowdsourcing lets any business, of any size and nature, tap into the collective intelligence of global crowds in order to complete business related tasks that a company would normally either perform itself or outsource to a third-party provider. It has become more possible to optimize multimedia systems more rapidly and to address human factors more effectively. Not only it allows businesses to expand the size of their talent pool, it is also a time and resource-efficient method to gain deeper insight into what direct consumers really want.In crowdsourcing platforms, there is perfect meritocracy. Especially in systems like Microworkers; age, gender, race, education, and job history does not matter, as the quality of work is all that counts; and every task is available to Users of every imaginable background. If you are capable of completing the required Microtask, you've got the job. For the past five years, Microworkers have effortlessly given opportunities to countless individuals across the globe whom are either looking for a temporary source of income, supplemental income or in many instances, main source of livelihood. While making opportunities available to eager, talented Workers and at the same time providing cost-effective solutions to job providers, Microworkers creates a win-win structure to anyone who believes can take advantage of its system. Apart from serving as a platform that connects Workers Employers, over time Microworkers Users have formed communities that provide support and assistance to fellow Users. Though having a large diverse workforce is the framework for delivering solutions to our clients, the same pose challenges both on our underlying infrastructure, as well as on providing support. Many other challenges arise in crowdsourcing set ups due to the fact that a community of users (or Microworkers) is a complex and dynamic system highly sensitive to changes in the form and the parameterization of their activities. Microworkers' present approach in dealing with these challenges include identification of optimal crowd members, ensuring clear directions and requirements, designing incentive structures that are not conducive to cheating, among many others.},
booktitle = {Proceedings of the 2014 International ACM Workshop on Crowdsourcing for Multimedia},
pages = {1},
numpages = {1},
keywords = {microworkers, microtask, mass collaboration, crowdtask, crowdsourcing platforms, crowdsourcing, crowdsource},
location = {Orlando, Florida, USA},
series = {CrowdMM '14}
}

@inproceedings{10.1109/WI-IAT.2014.52,
author = {Ignatov, Dmitry I. and Kaminskaya, Alexandra and Konstantinova, Natalia and Konstantinov, Andrey},
title = {Recommender System for Crowdsourcing Platform Witology},
year = {2014},
isbn = {9781479941438},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/WI-IAT.2014.52},
doi = {10.1109/WI-IAT.2014.52},
abstract = {This paper discusses the recommender models and methods for crowd sourcing platforms. These models are based on modern methods of data analysis of object-attribute data, such as Formal Concept Analysis and biclustering. In particular, the paper is focused on the solution of two tasks - idea and antagonists recommendation - on the example of crowd sourcing platform Witology.},
booktitle = {Proceedings of the 2014 IEEE/WIC/ACM International Joint Conferences on Web Intelligence (WI) and Intelligent Agent Technologies (IAT) - Volume 01},
pages = {327–335},
numpages = {9},
keywords = {recommender systems, crowdsourcing, biclustering, Formal Concept Analysis},
series = {WI-IAT '14}
}

@inproceedings{10.1109/CSI-SE.2015.10,
author = {Xie, Tao and Bishop, Judith and Horspool, R. Nigel and Tillmann, Nikolai and Halleux, Jonathan de},
title = {Crowdsourcing Code and Process via Code Hunt},
year = {2015},
isbn = {9781467370400},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/CSI-SE.2015.10},
doi = {10.1109/CSI-SE.2015.10},
abstract = {Crowd sourcing programming relies on active participation. One way to get such participation is through an engaging game. Code Hunt (https://www.codehunt.com/) from Microsoft Research is a web-based serious gaming platform with the potential to be leveraged as a crowd sourcing system. In Code Hunt, players create programs by re-engineering against a changing set of test cases. The game has been played by over 100,000 players in world-wide contests, and to practice coding skills. The vast collected data of code modified by players and the process taken to succeed could be used by others for software construction, teaching, or learning. In this position paper, we discuss these existing crowd sourcing activities in Code Hunt and a future game type for crowd sourcing.},
booktitle = {Proceedings of the 2015 IEEE/ACM 2nd International Workshop on CrowdSourcing in Software Engineering},
pages = {15–16},
numpages = {2},
keywords = {programming contests, educational software engineering, educational gamification, crowdsourcing},
series = {CSI-SE '15}
}

@inproceedings{10.1145/2702123.2702338,
author = {Curmi, Franco and Ferrario, Maria Angela and Whittle, Jon and Mueller, Florian 'Floyd'},
title = {Crowdsourcing Synchronous Spectator Support: (go on, go on, you're the best)n-1},
year = {2015},
isbn = {9781450331456},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2702123.2702338},
doi = {10.1145/2702123.2702338},
abstract = {Many studies have shown that crowd-support, such as cheering during sport events, can have a positive impact on athletes' performance. However, up until recently this support was only possible if the supporters and the athletes were geographically co-located. Can cheering be done remotely and would this be effective? In this paper we investigate the effect and possibilities of live remote cheering on co-located athletes and online supporting crowds that have a weak social tie and no social tie with the athlete. We recruit 140 online spectators and 5 athletes for an ad-hoc 5km road race. Results indicate that crowds socially closer to the athletes are significantly more engaged in the support. The athletes were excited by live remote cheering from friendsourced spectators and cheering from unknown crowdsourced participants indicating that remote friends and outsourced spectators could be an important source of support.},
booktitle = {Proceedings of the 33rd Annual ACM Conference on Human Factors in Computing Systems},
pages = {757–766},
numpages = {10},
keywords = {sports, spectators, spectator support, social networks, human behavior, friendsourcing, crowdsourcing, cheering, broadcast},
location = {Seoul, Republic of Korea},
series = {CHI '15}
}

@inproceedings{10.1007/978-3-319-46295-0_49,
author = {Yu, Hao and Wang, Zhongjie and Chi, Xu and Xu, Xiaofei},
title = {Studying Social Collaboration Features and Patterns in Service Crowdsourcing},
year = {2016},
isbn = {978-3-319-46294-3},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-319-46295-0_49},
doi = {10.1007/978-3-319-46295-0_49},
abstract = {Service crowdsourcing follows typical social collaboration processes with stochastic and dynamic characteristics. In this paper, the “bug-fix” social collaboration on GitHub is used as a case scenario of crowdsourcing, and 53,475 issues in 10 OSS projects are collected to conduct an empirical study on features and patterns of service crowdsourcing. Seven collaboration features (CFs) are proposed to delineate social characteristics of crowdsourcing. In terms of these CFs, social collaboration processes are clustered and results show that these features have significant distinguishability. An extended Generalized Sequential Pattern (GSP) algorithm is put forward to identify two types of collaboration patterns called participant-oriented pattern (PP) and role-oriented pattern (RP), and the richness and individualized degree of collaboration patterns in different OSS projects are analyzed and compared.},
booktitle = {Service-Oriented Computing: 14th International Conference, ICSOC 2016, Banff, AB, Canada, October 10-13, 2016, Proceedings},
pages = {697–704},
numpages = {8},
keywords = {Open Source Software (OSS), Collaboration features, Collaboration pattern, Social collaboration process, Service crowdsourcing},
location = {Banff, Canada}
}

@inproceedings{10.1007/978-3-319-07782-6_70,
author = {Ye, Chen and Wang, Hongzhi},
title = {Capture Missing Values Based on Crowdsourcing},
year = {2014},
isbn = {9783319077819},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-319-07782-6_70},
doi = {10.1007/978-3-319-07782-6_70},
abstract = {Due to the unreliable environment in mobile could, attribute values or tuples may be missing or lost. Thus we should capture missing values to make data mining and analysis more accurate. Besides ignoring or setting to default values, many imputation methods have been proposed, but they also have their limitations. This paper proposes a human-machine hybrid workflow to study the missing value filling method with crowdsourcing. First we propose a missing value selection algorithm to select the missing values which are suitable to use crowdsourcing for filling. Then we propose three missing values filling methods according to different attribute types to select answers from crowdsourcing. Experimental results show that our algorithms could improve data quality significantly with low costs.},
booktitle = {Proceedings of the 9th International Conference on Wireless Algorithms, Systems, and Applications - Volume 8491},
pages = {783–792},
numpages = {10},
keywords = {missing values, data cleaning, crowdsourcing},
location = {Harbin, China},
series = {WASA 2014}
}

@inproceedings{10.5555/2820116.2820119,
author = {Xie, Tao and Bishop, Judith and Horspool, R. Nigel and Tillmann, Nikolai and de Halleux, Jonathan},
title = {Crowdsourcing code and process via code hunt},
year = {2015},
publisher = {IEEE Press},
abstract = {Crowdsourcing programming relies on active participation. One way to get such participation is through an engaging game. Code Hunt (https://www.codehunt.com/) from Microsoft Research is a web-based serious gaming platform with the potential to be leveraged as a crowdsourcing system. In Code Hunt, players create programs by re-engineering against a changing set of test cases. The game has been played by over 100,000 players in world-wide contests, and to practice coding skills. The vast collected data of code modified by players and the process taken to succeed could be used by others for software construction, teaching, or learning. In this position paper, we discuss these existing crowdsourcing activities in Code Hunt and a future game type for crowdsourcing.},
booktitle = {Proceedings of the Second International Workshop on CrowdSourcing in Software Engineering},
pages = {15–16},
numpages = {2},
location = {Florence, Italy},
series = {CSI-SE '15}
}

@inproceedings{10.1145/2370216.2370373,
author = {Tamilin, Andrei and Carreras, Iacopo and Ssebaggala, Emmanuel and Opira, Alfonse and Conci, Nicola},
title = {Context-aware mobile crowdsourcing},
year = {2012},
isbn = {9781450312240},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2370216.2370373},
doi = {10.1145/2370216.2370373},
abstract = {Ubiquity of internet-connected media-and sensor-equipped portable devices has emerged a range of opportunities for direct involvement of citizens into public decision making, leading to a new participatory format of public administration functioning. Intersecting the power of the crowdsourcing problem-solving paradigm by directly relying on human intelligence, with instantaneity and situation-awareness of mobile technologies, one gets a context-aware crowdsourcing approach for problem-solving in the right circumstances with the right people. In this paper, we present a prototype implementation of a context-aware mobile crowdsourcing system that enables the deployment and execution of crowdsourcing campaigns with users carrying mobile devices. The system is designed to maximize conditions for user participation, while minimizing the usage of energy. The paper describes the system architecture, defines an optimized sampling algorithm, and outlines a preliminary experimentation study carried out.},
booktitle = {Proceedings of the 2012 ACM Conference on Ubiquitous Computing},
pages = {717–720},
numpages = {4},
keywords = {mobile crowdsourcing, localization, energy-efficiency, context-aware systems},
location = {Pittsburgh, Pennsylvania},
series = {UbiComp '12}
}

@inproceedings{10.1145/2835776.2835835,
author = {Kazai, Gabriella and Zitouni, Imed},
title = {Quality Management in Crowdsourcing using Gold Judges Behavior},
year = {2016},
isbn = {9781450337168},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2835776.2835835},
doi = {10.1145/2835776.2835835},
abstract = {Crowdsourcing relevance labels has become an accepted practice for the evaluation of IR systems, where the task of constructing a test collection is distributed over large populations of unknown users with widely varied skills and motivations. Typical methods to check and ensure the quality of the crowd's output is to inject work tasks with known answers (gold tasks) on which workers' performance can be measured. However, gold tasks are expensive to create and have limited application. A more recent trend is to monitor the workers' interactions during a task and estimate their work quality based on their behavior. In this paper, we show that without gold behavior signals that reflect trusted interaction patterns, classifiers can perform poorly, especially for complex tasks, which can lead to high quality crowd workers getting blocked while poorly performing workers remain undetected. Through a series of crowdsourcing experiments, we compare the behaviors of trained professional judges and crowd workers and then use the trained judges' behavior signals as gold behavior to train a classifier to detect poorly performing crowd workers. Our experiments show that classification accuracy almost doubles in some tasks with the use of gold behavior data.},
booktitle = {Proceedings of the Ninth ACM International Conference on Web Search and Data Mining},
pages = {267–276},
numpages = {10},
keywords = {measurement, experimentation},
location = {San Francisco, California, USA},
series = {WSDM '16}
}

@inproceedings{10.5220/0005835004820489,
author = {Machado, Leticia and Kroll, Josiane and Prikladnicki, Rafael and Souza, Cleidson R. B. de and Carmel, Erran},
title = {Software Crowdsourcing Challenges in the Brazilian IT Industry},
year = {2016},
isbn = {9789897581878},
publisher = {SCITEPRESS - Science and Technology Publications, Lda},
address = {Setubal, PRT},
url = {https://doi.org/10.5220/0005835004820489},
doi = {10.5220/0005835004820489},
abstract = {Software crowdsourcing has been regarded as a new paradigm for the provision of crowd-labor in software development tasks. Companies around the world adopt this paradigm to identify collective solutions to solve problems, ways to accelerate time-to-market, increase the quality and reduce the software cost. Although this paradigm is a trend in the software engineering area, several challenges are behind software crowdsourcing. In this study, we explore how the software crowdsourcing has been developed in the Brazilian IT industry. We have conducted 20 interviews with Brazilians practitioners in order to identify the main challenges for software crowdsourcing in Brazil. Additionally, we identified and discussed enablers and blockers\^{a} factors, practice implications and directions for future research in the area. Our paper aims to provide an overview of the software crowdsourcing in Brazil and motivation for researchers to better understand challenges faced by the Brazilian IT industry.},
booktitle = {Proceedings of the 18th International Conference on Enterprise Information Systems},
pages = {482–489},
numpages = {8},
keywords = {Software Engineering, Software Development, Software Crowdsourcing, IT Industry., Challenges, Brazil},
location = {Rome, Italy},
series = {ICEIS 2016}
}

@inproceedings{10.1007/978-3-030-33702-5_10,
author = {Ye, Bin and Wang, Yan and Orgun, Mehmet and Sheng, Quan Z.},
title = {N2TM: A New Node to Trust Matrix Method for Spam Worker Defense in Crowdsourcing Environments},
year = {2019},
isbn = {978-3-030-33701-8},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-33702-5_10},
doi = {10.1007/978-3-030-33702-5_10},
abstract = {To defend against spam workers in crowdsourcing environments, the existing solutions overlook the fact that a spam worker with guises can easily bypass the defense. To alleviate this problem, in this paper, we propose a Node to Trust Matrix method (N2TM) that represents a worker node in a crowdsourcing network as an un-manipulable Worker Trust Matrix (WTM) for identifying the worker’s identity. In particular, we first present a crowdsourcing trust network consisting of requester nodes, worker nodes, and transaction-based edges. Then, we construct WTMs for workers based on the trust network. A WTM consists of trust indicators measuring the extent to which a worker is trusted by different requesters in different sub-networks. Moreover, we show the un-manipulable property and the usable property of a WTM that are crucial for identifying a worker’s identity. Furthermore, we leverage deep learning techniques to predict a worker’s identity with its WTM as input. Finally, we demonstrate the superior performance of our proposed N2TM in identifying spam workers with extensive experiments.},
booktitle = {Service-Oriented Computing: 17th International Conference, ICSOC 2019, Toulouse, France, October 28–31, 2019, Proceedings},
pages = {119–134},
numpages = {16},
keywords = {Crowdsourcing, Trust, Spam worker identification},
location = {Toulouse, France}
}

@inproceedings{10.1007/978-3-642-41924-9_46,
author = {Lukyanenko, Roman and Parsons, Jeffrey},
title = {Lightweight Conceptual Modeling for Crowdsourcing},
year = {2013},
isbn = {9783642419232},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-642-41924-9_46},
doi = {10.1007/978-3-642-41924-9_46},
abstract = {As more organizations rely on externally-produced information, an important issue is how to develop conceptual models for such data. Considering the limitations of traditional conceptual modeling, we propose a "lightweight" modeling alternative to traditional "class-based" conceptual modeling as typified by the E-R model. We demonstrate the approach using a real-world crowdsourcing project, NLNature.},
booktitle = {Proceedings of the 32nd International Conference on Conceptual Modeling - Volume 8217},
pages = {508–511},
numpages = {4},
keywords = {Ontology, Information Quality, Conceptual Modeling, Cognition},
location = {Hong-Kong, China},
series = {ER 2013}
}

@inproceedings{10.1145/2960811.2960815,
author = {Granell, Emilio and Mart\'{\i}nez-Hinarejos, Carlos-D.},
title = {A Multimodal Crowdsourcing Framework for Transcribing Historical Handwritten Documents},
year = {2016},
isbn = {9781450344388},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2960811.2960815},
doi = {10.1145/2960811.2960815},
abstract = {Transcription of handwritten historical documents is one of the main topics in document analysis systems, due to cultural reasons. State-of-the-art handwritten text recognition systems allow to speed up the transcription task. Currently, this automatic transcription is far from perfect, and human expert revision is required in order to obtain the actual transcription. In this context, crowdsourcing emerged as a powerful tool for massive transcription at a relatively low cost, since the supervision effort of professional transcribers may be dramatically reduced. However, current transcription crowdsourcing platforms are mainly limited to the use of non-mobile devices, since the use of keyboards in mobile devices is not friendly enough for most users. This work presents the alternative of using speech dictation of handwritten text lines as transcription source in a crowdsourcing platform. The experiments explore how an initial handwritten text recognition hypothesis can be improved by using the contribution of speech recognition from several speakers, providing as a final result a better hypothesis to be amended by a professional transcriber with less effort.},
booktitle = {Proceedings of the 2016 ACM Symposium on Document Engineering},
pages = {157–163},
numpages = {7},
keywords = {speech recognition, multimodal combination, historical handwritten transcription, crowdsourcing framework},
location = {Vienna, Austria},
series = {DocEng '16}
}

@inproceedings{10.1145/2591062.2591153,
author = {Chen, Zhenyu and Luo, Bin},
title = {Quasi-crowdsourcing testing for educational projects},
year = {2014},
isbn = {9781450327688},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2591062.2591153},
doi = {10.1145/2591062.2591153},
abstract = {The idea of crowdsourcing tasks in software engineering, especially software testing, has gained popularity in recent years. Crowdsourcing testing and educational projects are natural complementary. One of the challenges of crowdsourcing testing is to find a number of qualified workers with low cost. Students in software engineering are suitable candidates for crowdsourcing testing. On the other hand, practical projects play a key role in software engineering education. In order to enhance educational project outcomes and achieve industrial-strength training, we need to provide the opportunity for students to be exposed to commercial software development.  In this paper, we report a preliminary study on crowdsourcing testing for educational projects. We introduce three commercial software products as educational testing projects, which are crowdsourced by our teaching support system. We call this "Quasi-Crowdsourcing Test" (QCT) because the candidate workers are students, who have certain social relations. The investigation results are encouraging and show to be beneficial to both the students and industry in QCT projects.},
booktitle = {Companion Proceedings of the 36th International Conference on Software Engineering},
pages = {272–275},
numpages = {4},
keywords = {system testing, educational projects, Crowdsourcing test},
location = {Hyderabad, India},
series = {ICSE Companion 2014}
}

@inproceedings{10.1145/3306500.3313980,
author = {Guo, Jie and Wang, JiaWei and Yan, ZhouYao},
title = {Motivation and factors effecting the participation behavior in the urban crowdsourcing logistics: evidence from China},
year = {2019},
isbn = {9781450366021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3306500.3313980},
doi = {10.1145/3306500.3313980},
abstract = {Purpose- The booming development of the new business model of local e-commerce has challenged the distribution capability of traditional urban logistics company in China. Crowdsourcing logistics, as a new terminal logistics distribution mode, provide a new perspective to solve the local logistics bottleneck in E-commerce effectively. In order to apply and develop crowdsourcing logistics better and design an effective crowdsourcing logistics platform, a better understanding of the participation behavior to the crowdsourcing logistics is needed. The purpose of this paper is to use the Unified Theory of Acceptance and Use of Technology (UTAU) as a framework to develop a model to identify the effective factors of the participation behavior to the crowdsourcing logistics.Design/methodology/approach- To test the model, a survey of 296 respondents in China is undertaken. And to analyze the participation behavior to the crowdsourcing logistics from the survey data, the structural equation modeling(SEM) is used. Findings- The results indicated that performance expectancy and social influence positively affect the intention of participation; perceived risk negatively influence the intention of participation; the higher the intention of participation, the more participative behavior of crowdsourcing logistics; and facilitating conditions also an important factor that leads to more participative behavior.Research Limitations/implications-This research is limited by the young adults sample and the website questionnaire platform that might confine the generalizability of the study. Also, additional variables need to be examined in order to better explain crowdsourcing logistic behavior. The result of the research provides insights for company related take-out O2O and logistic to build successful crowdsourcing model, engage young employees who are familiar with network in urban crowdsourcing logistic, and increase involvement in sharing economy.Practical implications- The results of this research can help the management of the urban crowdsourcing logistics companies to understand the participative intention and behavior to their products and services of people, so that they can improve their business model and design an effective and attractive crowdsourcing logistics platform.Originality/value- This study developed the Unified Theory of Acceptance and Use of Technology to better explain the participation behavior in the crowdsourcing logistics. And the paper develops an understanding of how crowdsourcing logistics platform should be improved and design to appeal more people to take part in the new logistics model.},
booktitle = {Proceedings of the 10th International Conference on E-Education, E-Business, E-Management and E-Learning},
pages = {334–341},
numpages = {8},
keywords = {participant behavior, crowdsourcing logistics, UTAUT, SEM},
location = {Tokyo, Japan},
series = {IC4E '19}
}

@inproceedings{10.1145/2600428.2609479,
author = {Alonso, Omar and Stone, Maria},
title = {Building a query log via crowdsourcing},
year = {2014},
isbn = {9781450322577},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2600428.2609479},
doi = {10.1145/2600428.2609479},
abstract = {A query log is a key asset in a commercial search engine. Everyday millions of users rely on search engines to find information on the Web by entering a few keywords on a simple search interface. Those queries represent a subset of user behavioral data which is used to mine and discover search patterns for improving the overall end user experience. While queries are very useful, it is not always possible to capture precisely what the user was looking for when the intent is not that clear. We explore a different alternative based on human computation to gather a bit more information from users and show the type of query log that would be possible to construct.},
booktitle = {Proceedings of the 37th International ACM SIGIR Conference on Research \&amp; Development in Information Retrieval},
pages = {939–942},
numpages = {4},
keywords = {user studies, query logs, query annotation, crowdsourcing},
location = {Gold Coast, Queensland, Australia},
series = {SIGIR '14}
}

@inproceedings{10.1145/2702123.2702145,
author = {Cheng, Justin and Teevan, Jaime and Bernstein, Michael S.},
title = {Measuring Crowdsourcing Effort with Error-Time Curves},
year = {2015},
isbn = {9781450331456},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2702123.2702145},
doi = {10.1145/2702123.2702145},
abstract = {Crowdsourcing systems lack effective measures of the effort required to complete each task. Without knowing how much time workers need to execute a task well, requesters struggle to accurately structure and price their work. Objective measures of effort could better help workers identify tasks that are worth their time. We propose a data-driven effort metric, ETA (error-time area), that can be used to determine a task's fair price. It empirically models the relationship between time and error rate by manipulating the time that workers have to complete a task. ETA reports the area under the error-time curve as a continuous metric of worker effort. The curve's 10th percentile is also interpretable as the minimum time most workers require to complete the task without error, which can be used to price the task. We validate the ETA metric on ten common crowdsourcing tasks, including tagging, transcription, and search, and find that ETA closely tracks how workers would rank these tasks by effort. We also demonstrate how ETA allows requesters to rapidly iterate on task designs and measure whether the changes improve worker efficiency. Our findings can facilitate the process of designing, pricing, and allocating crowdsourcing tasks.},
booktitle = {Proceedings of the 33rd Annual ACM Conference on Human Factors in Computing Systems},
pages = {1365–1374},
numpages = {10},
keywords = {task effort, microtasks, crowdsourcing},
location = {Seoul, Republic of Korea},
series = {CHI '15}
}

@inproceedings{10.1109/SCC.2015.89,
author = {Balamurugan, Chithralekha and Kunde, Shruti and Gupta, Avantika and Chander, Deepthi and Dasgupta, Koustuv},
title = {Service Assurance Framework for Enterprise Task Crowdsourcing},
year = {2015},
isbn = {9781467372817},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/SCC.2015.89},
doi = {10.1109/SCC.2015.89},
abstract = {Crowd sourcing platforms enable enterprise requesters to leverage the online workforce to process voluminous enterprise tasks on a regular basis. Web services provided by these platforms facilitate requesters to post tasks, retrieve results and incentivize crowd workers. However, service assurance associated with task execution by crowd workers is not provided by these platforms. Owing to the flexible, uncommitted, discretionary working patterns of online crowd workers, service assurance for task execution is considered to be beyond the service assurance offerings of existing crowd sourcing platforms. Enterprises however, require these guarantees to be able to adopt crowd sourcing in a profound manner. In this paper, we propose a Service Assurance Framework as a crowd sourcing platform augmenting service, that provides service assurance for task execution by crowd workers. The framework helps enterprise requesters to identify and engage with workers who possess suitable service assurance attributes. To the best of our knowledge, this work is a first of its kind, in providing service assurance associated with task execution by crowd workers, with respect to enterprise tasks. We implemented the proposed framework and conducted a four-week long, large scale crowd sourcing experiment involving digitization of forms posted by an enterprise requester. Our results validate the efficacy of the proposed service assurance framework for enterprise crowd sourcing and advocate its adoption.},
booktitle = {Proceedings of the 2015 IEEE International Conference on Services Computing},
pages = {616–623},
numpages = {8},
keywords = {Worker Performance, Service Assurance-Guaranteed Crowdsourcing, Service Assurance, Enterprise Task Crowdsourcing},
series = {SCC '15}
}

@inproceedings{10.1145/3253092,
author = {Cox, Landon},
title = {Session details: Crowdsourcing},
year = {2011},
isbn = {9781450306430},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3253092},
doi = {10.1145/3253092},
booktitle = {Proceedings of the 9th International Conference on Mobile Systems, Applications, and Services},
location = {Bethesda, Maryland, USA},
series = {MobiSys '11}
}

@inproceedings{10.1145/2737095.2742931,
author = {Jain, Ayush and Raj, Saswat and Harshit and Misra, Rajiv and Baveja, B. M.},
title = {Road congestion sensing via crowdsourcing and MapReduce},
year = {2015},
isbn = {9781450334754},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2737095.2742931},
doi = {10.1145/2737095.2742931},
abstract = {Road congestion has become a major problem in cities in developing countries resulting in massive delays, wastage of fuel and road accidents. For proper handling it is essential to observe the road congestion patterns. Methods like on-road cameras, etc. require huge investments whereas crowdsourcing methods generate large amount of redundant data. This paper presents a new approach using event sensing to capture relevant crowdsourced data to estimate road traffic congestion and utilizes MapReduce for generating analytics efficiently.},
booktitle = {Proceedings of the 14th International Conference on Information Processing in Sensor Networks},
pages = {356–357},
numpages = {2},
location = {Seattle, Washington},
series = {IPSN '15}
}

@inproceedings{10.1145/2390034.2390036,
author = {Guy, Ido},
title = {Crowdsourcing in the enterprise},
year = {2012},
isbn = {9781450317153},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2390034.2390036},
doi = {10.1145/2390034.2390036},
abstract = {This talk reviews several of the recent studies conducted by the Social Technologies group at IBM Research-Haifa, which demonstrate the use of social analytics tools to extract value of enterprise social media. From recommender systems, through activity stream filtering and analysis, to crowdsourcing games in the enterprise, the voice of the employees can now be heard and utilized better than ever within the newly formed social business.},
booktitle = {Proceedings of the 1st International Workshop on Multimodal Crowd Sensing},
pages = {1–2},
numpages = {2},
keywords = {social technologies, social media, social business, social analytics, enterprise, crowdsourcing, crowd sensing, crowd computing},
location = {Maui, Hawaii, USA},
series = {CrowdSens '12}
}

@inproceedings{10.1145/2642918.2647362,
author = {Hosio, Simo and Goncalves, Jorge and Lehdonvirta, Vili and Ferreira, Denzil and Kostakos, Vassilis},
title = {Situated crowdsourcing using a market model},
year = {2014},
isbn = {9781450330695},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2642918.2647362},
doi = {10.1145/2642918.2647362},
abstract = {Research is increasingly highlighting the potential for situated crowdsourcing to overcome some crucial limitations of online crowdsourcing. However, it remains unclear whether a situated crowdsourcing market can be sustained, and whether worker supply responds to price-setting in such a market. Our work is the first to systematically investigate workers' behaviour and response to economic incentives in a situated crowdsourcing market. We show that the market-based model is a sustainable approach to recruiting workers and obtaining situated crowdsourcing contributions. We also show that the price mechanism is a very effective tool for adjusting the supply of labour in a situated crowdsourcing market. Our work advances the body of work investigating situated crowdsourcing.},
booktitle = {Proceedings of the 27th Annual ACM Symposium on User Interface Software and Technology},
pages = {55–64},
numpages = {10},
keywords = {virtual currency, situated technologies, market, crowdsourcing},
location = {Honolulu, Hawaii, USA},
series = {UIST '14}
}

@inproceedings{10.1145/3249011,
author = {Kittur, Niki},
title = {Session details: Crowdsourcing},
year = {2011},
isbn = {9781450302289},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3249011},
doi = {10.1145/3249011},
booktitle = {Proceedings of the SIGCHI Conference on Human Factors in Computing Systems},
location = {Vancouver, BC, Canada},
series = {CHI '11}
}

@inproceedings{10.5555/2540128.2540429,
author = {Han, Jun and Fan, Ju and Zhou, Lizhu},
title = {Crowdsourcing-assisted query structure interpretation},
year = {2013},
isbn = {9781577356332},
publisher = {AAAI Press},
abstract = {Structured Web search incorporating data from structured sources into search engine results has attracted much attention from both academic and industrial communities. To understand user's intent, query structure interpretation is proposed to analyze the structure of queries in a query log and map query terms to the semantically relevant attributes of data sources in a target domain. Existing methods assume all queries should be classified to the target domain, and thus they are limited when interpreting queries from different domains in real query logs. To address the problem, we introduce a human-machine hybrid method by utilizing crowdsourcing platforms. Our method selects a small number of query terms and asks the crowdsourcing workers to interpret them, and then infers the interpretations based on the crowdsourcing results. To improve the performance, we propose an iterative probabilistic inference method based on a similarity graph of query terms, and select the most useful query terms for crowdsourcing by considering their domain-relevance and gained benefit. We evaluate our method on a real query log, and the experimental results show that our method outperforms the state-of-the-art method.},
booktitle = {Proceedings of the Twenty-Third International Joint Conference on Artificial Intelligence},
pages = {2092–2098},
numpages = {7},
location = {Beijing, China},
series = {IJCAI '13}
}

@inproceedings{10.1145/3244832,
author = {schraefel, mc},
title = {Session details: Crowdsourcing},
year = {2011},
isbn = {9781450307161},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3244832},
doi = {10.1145/3244832},
booktitle = {Proceedings of the 24th Annual ACM Symposium on User Interface Software and Technology},
location = {Santa Barbara, California, USA},
series = {UIST '11}
}

@inproceedings{10.1145/3308560.3317084,
author = {Zequeira Jim\'{e}nez, Rafael and Llagostera, Anna and Naderi, Babak and M\"{o}ller, Sebastian and Berger, Jens},
title = {Intra- and Inter-rater Agreement in a Subjective Speech Quality Assessment Task in Crowdsourcing},
year = {2019},
isbn = {9781450366755},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3308560.3317084},
doi = {10.1145/3308560.3317084},
abstract = {Crowdsourcing is a great tool for conducting subjective user studies with large amounts of users. Collecting reliable annotations about the quality of speech stimuli is challenging. The task itself is of high subjectivity and users in crowdsourcing work without supervision. This work investigates the intra- and inter-listener agreement withing a subjective speech quality assessment task. To this end, a study has been conducted in the laboratory and in crowdsourcing in which listeners were requested to rate speech stimuli with respect to their overall quality. Ratings were collected on a 5-point scale in accordance with the ITU-T Rec. P.800 and P.808, respectively. The speech samples were taken from the database ITU-T Rec. P.501 Annex D, and were presented four times to the listeners. Finally, the crowdsourcing results were contrasted to the ratings collected in the laboratory. Strong and significant Spearman’s correlation was achieved when contrasting the ratings collected in both environments. Our analysis show that while the inter-rater agreement increased the more the listeners conducted the assessment task, the intra-rater reliability remained constant. Our study setup helped to overcome the subjectivity of the task and we found that disagreement can represent a source of information to some extent.},
booktitle = {Companion Proceedings of The 2019 World Wide Web Conference},
pages = {1138–1143},
numpages = {6},
keywords = {subjectivity in crowdsourcing, speech quality assessment, listeners’ agreement, inter-rater reliability, crowdsourcing},
location = {San Francisco, USA},
series = {WWW '19}
}

@inproceedings{10.1145/3041021.3051693,
author = {Tinati, Ramine and Madaan, Aastha and Hall, Wendy},
title = {The Role of Crowdsourcing in the Emerging Internet-Of-Things},
year = {2017},
isbn = {9781450349147},
publisher = {International World Wide Web Conferences Steering Committee},
address = {Republic and Canton of Geneva, CHE},
url = {https://doi.org/10.1145/3041021.3051693},
doi = {10.1145/3041021.3051693},
abstract = {In this position paper we wish to propose and discuss several open research questions associated with the IoT. In particular, we wish to consider how crowdsourcing can be used as a scalable, reliable, and sustainable approach to support various computationally difficult and ambiguous tasks recognised in IoT research. We illustrate our work by examining a number of use cases related to healthcare and smart cities, and finally consider the future development of the IoT eco-system with respect to the socio-technical philosophy and implementation of the Web Observatory.},
booktitle = {Proceedings of the 26th International Conference on World Wide Web Companion},
pages = {1669–1672},
numpages = {4},
keywords = {web observatory, iot, internet-of-things, crowdsourcing},
location = {Perth, Australia},
series = {WWW '17 Companion}
}

@inproceedings{10.1145/3157754.3157764,
author = {Barashev, Andrey and Li, Guoxin},
title = {Personal Trait Predicting Work Engagement in Crowdsourcing through Achievement Goals: Mediation Analyses},
year = {2017},
isbn = {9781450353670},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3157754.3157764},
doi = {10.1145/3157754.3157764},
abstract = {The purpose of this research is to investigate how personal traits influence work engagement on crowdsourcing platform through achievement goals. This study uses Reinforcement Sensitivity Theory (RST) in order to explain initial energization of behavior as well as Achievement goal theory's constructs to explain direction of this energy. The indirect effects of Fight-Flight-Freeze system (FFFS) through performance approach and performance avoidance goals on work engagement has been studied.The results of current study show that avoidance based personal trait FFFS indirectly relates to work engagement through performance approach and performance avoidance goals, such as FFFS positively predicts work engagement through performance approach goals and negatively predict work engagement through performance avoidance goals. In addition, this study proves that FFFS positively predict adoption of both performance approach and performance avoidance goals. Hence, promoting adoption of performance approach goals is a good way for improving workers' work engagement.These results are important for crowdsourcing platform managers and could help them to improve work engagement of workers without providing additional reward.},
booktitle = {Proceedings of the 8th International Conference on E-Business, Management and Economics},
pages = {28–32},
numpages = {5},
keywords = {Work Engagement, Personal traits, Motivation, Crowdsourcing, Achievement Goals},
location = {Birmingham, United Kingdom},
series = {ICEME '17}
}

@inproceedings{10.5555/2390665.2390704,
author = {Zeichner, Naomi and Berant, Jonathan and Dagan, Ido},
title = {Crowdsourcing inference-rule evaluation},
year = {2012},
publisher = {Association for Computational Linguistics},
address = {USA},
abstract = {The importance of inference rules to semantic applications has long been recognized and extensive work has been carried out to automatically acquire inference-rule resources. However, evaluating such resources has turned out to be a non-trivial task, slowing progress in the field. In this paper, we suggest a framework for evaluating inference-rule resources. Our framework simplifies a previously proposed "instance-based evaluation" method that involved substantial annotator training, making it suitable for crowdsourcing. We show that our method produces a large amount of annotations with high inter-annotator agreement for a low cost at a short period of time, without requiring training expert annotators.},
booktitle = {Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Short Papers - Volume 2},
pages = {156–160},
numpages = {5},
location = {Jeju Island, Korea},
series = {ACL '12}
}

@inproceedings{10.1145/2989238.2989245,
author = {Alelyani, Turki and Yang, Ye},
title = {Software crowdsourcing reliability: an empirical study on developers behavior},
year = {2016},
isbn = {9781450343954},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2989238.2989245},
doi = {10.1145/2989238.2989245},
abstract = {Crowdsourcing has become an emergent paradigm for software production in recent decades. Its open-call format attracts the participation of hundreds of thousands of developers. To ensure the success of software crowdsourcing, we must accurately measure and monitor the reliability of participating crowd workers, which, surprisingly, has rarely been done. To that end, this paper aims to examine the dependability of crowd workers in selecting tasks for software crowdsourcing. Empirical analysis of worker behaviors will investigate the following: (1) workers’ behavior when registering and carrying out the announced tasks; (2) the relationship between rewards and performance; (3) the effects of development type among different groups; and (4) the evolution of workers’ behavior according to the skills they have adopted. This study’s findings include: (1) On average, most reliable crowdsourcing group responds to a task call within 10\% of their allotted time and completes the assigned work in less than 5\% of that time. (2) Crowd workers tend to focus on tasks according to specific ranges of rewards and types of challenges, based on their skill levels. (3) Crowd skills spread evenly across the entire range of groups. In summary, our results can guide future research into crowdsourcing service design and can inform ideas for crowdsourcing strategy conception according to time, reward, development type, and other aspects of crowdsourcing.},
booktitle = {Proceedings of the 2nd International Workshop on Software Analytics},
pages = {36–42},
numpages = {7},
keywords = {workers, software, reliability, TopCoder, Crowdsourcing},
location = {Seattle, WA, USA},
series = {SWAN 2016}
}

@inproceedings{10.1145/3184558.3191519,
author = {Kandappu, Thivya and Misra, Archan and Koh, Desmond and Tandriansyah, Randy Daratan and Jaiman, Nikita},
title = {A Feasibility Study on Crowdsourcing to Monitor Municipal Resources in Smart Cities},
year = {2018},
isbn = {9781450356404},
publisher = {International World Wide Web Conferences Steering Committee},
address = {Republic and Canton of Geneva, CHE},
url = {https://doi.org/10.1145/3184558.3191519},
doi = {10.1145/3184558.3191519},
abstract = {Active citizenry, whereby citizens actively participate in reporting and addressing challenges in urban service delivery is a strategic goal of smart cities such as Singapore. In spite of the promise, we believe that the success of such large-scale nation-wide crowdsourcing deployments depend on the real-word user preferences and behavioral characteristics of citizens. In this paper, we first present our findings on behavioral preferences and key concerns of citizens regarding smart-city services via an opinion survey conducted with 1300 participants. We then propose a "citizen-controlled" urban services reporting platform where citizens actively report on the status of various municipal resources. We advocate the importance of matching user mobility patterns against task locations to make the platform more efficient (i.e., higher task completion rate and lower detour overhead).},
booktitle = {Companion Proceedings of the The Web Conference 2018},
pages = {919–925},
numpages = {7},
keywords = {citizens, mobile crowdsourcing, municipal monitoring, smart cities},
location = {Lyon, France},
series = {WWW '18}
}

@inproceedings{10.1109/WCNC.2018.8377405,
author = {Enami, Rita and Rajan, Dinesh and Camp, Joseph},
title = {RAIK: Regional analysis with geodata and crowdsourcing to infer key performance indicators},
year = {2018},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/WCNC.2018.8377405},
doi = {10.1109/WCNC.2018.8377405},
abstract = {Key Performance Indicators (KPIs) are important measures of the quality of service in cellular networks. There are multiple efforts by cellular carriers and 5G standardization to leverage the KPIs to minimize drive tests (MDT) and self-organize the network for optimal performance via user feedback. Such an approach accounts for user devices in the field of their operation according to their normal usage and circumvents a number of costs (e.g., manpower, equipment) traditionally covered by the carrier, either directly or through a third party. In this paper, we build a Regional Analysis to Infer KPIs (RAIK) framework to establish a relationship between geographical data and user data using crowdsourced measurements. To do so, we use a neural network and crowdsourced data obtained by user equipment (UE) to predict the KPIs in terms of the reference signal's received power (RSRP) and path loss estimation. Since these KPIs are a function of terrain type, we provide a two-layer coverage map by overlaying a performance layer on a 3-dimensional geographical map. As a result, we can efficiently use crowdsourced data (to not overextend user bandwidth and battery) and infer KPIs in areas where measurements have not or can not be performed. For example, we show that RAIK can use only geographical information to predict the KPIs in areas that lack signal quality data with a negligible mean squared error, a seven-fold reduction in error from state-of-the-art solutions.},
booktitle = {2018 IEEE Wireless Communications and Networking Conference (WCNC)},
pages = {1–6},
numpages = {6},
location = {Barcelona, Spain}
}

@inproceedings{10.1145/2837126.2837181,
author = {Fonteles, Andr\'{e} Sales and Bouveret, Sylvain and Gensel, J\'{e}r\^{o}me},
title = {Heuristics for Task Recommendation in Spatiotemporal Crowdsourcing Systems},
year = {2015},
isbn = {9781450334938},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2837126.2837181},
doi = {10.1145/2837126.2837181},
abstract = {Crowdsourcing systems (CS) are platforms that enable a system or a user to publish tasks in order to be accomplished by others. Typically, a CS is a system where users, called workers, perform tasks using desktop computers. Recently, some CS have appeared with spatiotemporal tasks. Such tasks require a worker to be in a given location within a specific time-window to be accomplished. We propose and study here the usage of five heuristics for solving the NP-hard trajectory recommendation problem (TRP). In a TRP, the system recommends a trajectory to a worker that allows him to accomplish spatiotemporal tasks he has skill and/or affinity with, without exceeding his available time. Our experiments show that some of our heuristics are efficient alternatives for a heavy optimal approach providing trajectories with an average utility of about 60\% of the optimal ones.},
booktitle = {Proceedings of the 13th International Conference on Advances in Mobile Computing and Multimedia},
pages = {1–5},
numpages = {5},
keywords = {task recommendation, task assignment, spatial crowdsourcing, crowdsensing},
location = {Brussels, Belgium},
series = {MoMM 2015}
}

@inproceedings{10.1007/978-3-319-27974-9_23,
author = {Packham, Sean and Suleman, Hussein},
title = {Crowdsourcing a Text Corpus is not a Game},
year = {2015},
isbn = {9783319279732},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-319-27974-9_23},
doi = {10.1007/978-3-319-27974-9_23},
abstract = {Building language corpora for low resource languages such as South Africa's isiXhosa is challenging because of limited digitized texts. Language corpora are needed for building information retrieval services such as search and translation and to support further online content creation. A novel solution was proposed to source original and relevant multilingual content by crowdsourcing translations via an online competitive game where participants would be paid for their contributions. Four experiments were conducted and the results support the idea that gamification by itself does not yield the widely expected benefits of increased motivation and engagement. We found that people do not volunteer without financial incentives, the form of payment does not matter, they would not continue contributing if the money is taken away and people preferred direct incentives and the possibility of incentives was not as strong a motivator.},
booktitle = {Proceedings of the 17th International Conference on Asia-Pacific Digital Libraries - Volume 9469},
pages = {225–234},
numpages = {10},
keywords = {Translation, Language corpora, Information retrieval, Gamification, Crowdsourcing}
}

@inproceedings{10.1145/2684822.2685308,
author = {Niu, Shuzi and Lan, Yanyan and Guo, Jiafeng and Cheng, Xueqi and Yu, Lei and Long, Guoping},
title = {Listwise Approach for Rank Aggregation in Crowdsourcing},
year = {2015},
isbn = {9781450333177},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2684822.2685308},
doi = {10.1145/2684822.2685308},
abstract = {Inferring a gold-standard ranking over a set of objects, such as documents or images, is a key task to build test collections for various applications like Web search and recommender systems. Crowdsourcing services provide an efficient and inexpensive way to collect judgments via labeling by sets of annotators. We thus study the problem of finding a consensus ranking from crowdsourced judgments. In contrast to conventional rank aggregation methods which minimize the distance between predicted ranking and input judgments from either pointwise or pairwise perspective, we argue that it is critical to consider the distance in a listwise way to emphasize the position importance in ranking. Therefore, we introduce a new listwise approach in this paper, where ranking measure based objective functions are utilized for optimization. In addition, we also incorporate the annotator quality into our model since the reliability of annotators can vary significantly in crowdsourcing. For optimization, we transform the optimization problem to the Linear Sum Assignment Problem, and then solve it by a very efficient algorithm named CrowdAgg guaranteeing the optimal solution. Experimental results on two benchmark data sets from different crowdsourcing tasks show that our algorithm is much more effective, efficient and robust than traditional methods.},
booktitle = {Proceedings of the Eighth ACM International Conference on Web Search and Data Mining},
pages = {253–262},
numpages = {10},
keywords = {rank aggregation, evaluation measures, crowdsourced labeling},
location = {Shanghai, China},
series = {WSDM '15}
}

@inproceedings{10.5555/2892753.2892959,
author = {Aydin, Bahadir Ismail and Yilmaz, Yavuz Selim and Li, Yaliang and Li, Qi and Gao, Jing and Demirbas, Murat},
title = {Crowdsourcing for multiple-choice question answering},
year = {2014},
publisher = {AAAI Press},
abstract = {We leverage crowd wisdom for multiple-choice question answering, and employ lightweight machine learning techniques to improve the aggregation accuracy of crowdsourced answers to these questions. In order to develop more effective aggregation methods and evaluate them empirically, we developed and deployed a crowdsourced system for playing the "Who wants to be a millionaire?" quiz show. Analyzing our data (which consist of more than 200,000 answers), we find that by just going with the most selected answer in the aggregation, we can answer over 90\% of the questions correctly, but the success rate of this technique plunges to 60\% for the later/harder questions in the quiz show. To improve the success rates of these later/harder questions, we investigate novel weighted aggregation schemes for aggregating the answers obtained from the crowd. By using weights optimized for reliability of participants (derived from the participants' confidence), we show that we can pull up the accuracy rate for the harder questions by 15\%, and to overall 95\% average accuracy. Our results provide a good case for the benefits of applying machine learning techniques for building more accurate crowdsourced question answering systems.},
booktitle = {Proceedings of the Twenty-Eighth AAAI Conference on Artificial Intelligence},
pages = {2946–2953},
numpages = {8},
location = {Qu\'{e}bec City, Qu\'{e}bec, Canada},
series = {AAAI'14}
}

@inproceedings{10.1145/2740908.2743972,
author = {Brambilla, Marco and Ceri, Stefano and Mauri, Andrea and Volonterio, Riccardo},
title = {An Explorative Approach for Crowdsourcing Tasks Design},
year = {2015},
isbn = {9781450334730},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2740908.2743972},
doi = {10.1145/2740908.2743972},
abstract = {Crowdsourcing applications are becoming widespread; they cover very different scenarios, including opinion mining, multimedia data annotation, localised information gathering, marketing campaigns, expert response gathering, and so on. The quality of the outcome of these applications depends on different design parameters and constraints, and it is very hard to judge about their combined effects without doing some experiments; on the other hand, there are no experiences or guidelines that tell how to conduct experiments, and thus these are often conducted in an ad-hoc manner, typically through adjustments of an initial strategy that may converge to a parameter setting which is quite different from the best possible one. In this paper we propose a comparative, explorative approach for designing crowdsourcing tasks. The method consists of defining a representative set of execution strategies, then execute them on a small dataset, then collect quality measures for each candidate strategy, and finally decide the strategy to be used with the complete dataset.},
booktitle = {Proceedings of the 24th International Conference on World Wide Web},
pages = {1125–1130},
numpages = {6},
keywords = {social computation, empirical method, crowdsourcing},
location = {Florence, Italy},
series = {WWW '15 Companion}
}

@inproceedings{10.5555/2615731.2615807,
author = {Xu, Haifeng and Larson, Kate},
title = {Improving the efficiency of crowdsourcing contests},
year = {2014},
isbn = {9781450327381},
publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
address = {Richland, SC},
abstract = {While winner-take-all crowdsourcing contests are wide spread in practice, several researchers have observed that their social welfare can be poor due to effort exerted by contestants who are never rewarded. In this paper we study the problem of efficiency in winner-take-all crowdsourcing contests. Using a discrete choice model to capture contestants' production qualities, we introduce a mechanism which filters out low-expertise contestants, before they are asked to produce a solution. We show that under a set of natural assumptions, such a mechanism has desirable incentive properties, attracts high-quality contestants and can improve social welfare. We also provide insights into the problem of prize setting for such contests.},
booktitle = {Proceedings of the 2014 International Conference on Autonomous Agents and Multi-Agent Systems},
pages = {461–468},
numpages = {8},
keywords = {winner-take-all crowdsourcing, prize setting, mechanims design, incentive compatible},
location = {Paris, France},
series = {AAMAS '14}
}

@inproceedings{10.1145/2818052.2874356,
author = {Girotto, Victor},
title = {Collective Creativity through a Micro-Tasks Crowdsourcing Approach},
year = {2016},
isbn = {9781450339506},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2818052.2874356},
doi = {10.1145/2818052.2874356},
abstract = {Research and commerce activity has been expanding the potential of micro-task markets. Initially used for simple, disconnected tasks, they have now been able to achieve impressive results in creative domains such as writing and design. The goals for this research are to further explore the possibilities of micro-task markets for performing creative work by defining a set of tasks and processes for such a synergistic creative collaboration, as well as expanding this micro-task approach beyond traditional markets such as Mechanical Turk to skilled and motivated communities.},
booktitle = {Proceedings of the 19th ACM Conference on Computer Supported Cooperative Work and Social Computing Companion},
pages = {143–146},
numpages = {4},
keywords = {crowdsourcing, Creativity, CSCW},
location = {San Francisco, California, USA},
series = {CSCW '16 Companion}
}

@inproceedings{10.1007/978-3-662-48616-0_34,
author = {Liang, Tingting and Chen, Liang and Xie, Zhining and Yang, Wei and Wu, Jian},
title = {CASE: A Platform for Crowdsourcing Based API Search},
year = {2015},
isbn = {978-3-662-48615-3},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-662-48616-0_34},
doi = {10.1007/978-3-662-48616-0_34},
abstract = {With the rapid growth of Web APIs on the Internet, searching appropriate APIs is becoming a challenging problem. General API search systems (e.g., ProgrammableWeb) implement API search through simple keywords matching leading to unsatisfactory search results. In this paper, we presents a crowdsourcing based API search engine CASE. Specifically, the API search engine leverages social information, Twitter List, a tool used by individual users to organize accounts that interest them on semantics. Based on the lists information, Latent Semantic Indexing (LSI) model is employed to compute the semantic similarity between the APIs and queries. Furthermore, the popularity of APIs inferred from the lists number is integrated with the semantic similarity to generate the final search result.},
booktitle = {Service-Oriented Computing: 13th International Conference, ICSOC 2015, Goa, India, November 16-19, 2015, Proceedings},
pages = {482–485},
numpages = {4},
keywords = {Latent Semantic Indexing (LSI), ProgrammableWeb (PW), Final Search Results, Twitter Lists, Application Programming Interface (API)},
location = {Goa, India}
}

@inproceedings{10.1145/2486001.2491719,
author = {Wang, Tianyi and Wang, Gang and Li, Xing and Zheng, Haitao and Zhao, Ben Y.},
title = {Characterizing and detecting malicious crowdsourcing},
year = {2013},
isbn = {9781450320566},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2486001.2491719},
doi = {10.1145/2486001.2491719},
abstract = {Popular Internet services in recent years have shown that remarkable things can be achieved by harnessing the power of the masses. However, crowd-sourcing systems also pose a real challenge to existing security mechanisms deployed to protect Internet services, particularly those tools that identify malicious activity by detecting activities of automated programs such as CAPTCHAs.In this work, we leverage access to two large crowdturfing sites to gather a large corpus of ground-truth data generated by crowdturfing campaigns. We compare and contrast this data with "organic" content generated by normal users to identify unique characteristics and potential signatures for use in real-time detectors. This poster describes first steps taken focused on crowdturfing campaigns targeting the Sina Weibo microblogging system. We describe our methodology, our data (over 290K campaigns, 34K worker accounts, 61 million tweets...), and some initial results.},
booktitle = {Proceedings of the ACM SIGCOMM 2013 Conference on SIGCOMM},
pages = {537–538},
numpages = {2},
keywords = {user behavior, malicious crowdsourcing, crowdturfing},
location = {Hong Kong, China},
series = {SIGCOMM '13}
}

@article{10.1145/2534169.2491719,
author = {Wang, Tianyi and Wang, Gang and Li, Xing and Zheng, Haitao and Zhao, Ben Y.},
title = {Characterizing and detecting malicious crowdsourcing},
year = {2013},
issue_date = {October 2013},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {43},
number = {4},
issn = {0146-4833},
url = {https://doi.org/10.1145/2534169.2491719},
doi = {10.1145/2534169.2491719},
abstract = {Popular Internet services in recent years have shown that remarkable things can be achieved by harnessing the power of the masses. However, crowd-sourcing systems also pose a real challenge to existing security mechanisms deployed to protect Internet services, particularly those tools that identify malicious activity by detecting activities of automated programs such as CAPTCHAs.In this work, we leverage access to two large crowdturfing sites to gather a large corpus of ground-truth data generated by crowdturfing campaigns. We compare and contrast this data with "organic" content generated by normal users to identify unique characteristics and potential signatures for use in real-time detectors. This poster describes first steps taken focused on crowdturfing campaigns targeting the Sina Weibo microblogging system. We describe our methodology, our data (over 290K campaigns, 34K worker accounts, 61 million tweets...), and some initial results.},
journal = {SIGCOMM Comput. Commun. Rev.},
month = aug,
pages = {537–538},
numpages = {2},
keywords = {user behavior, malicious crowdsourcing, crowdturfing}
}

@inproceedings{10.1007/978-3-319-26135-5_7,
author = {Thuan, Nguyen Hoang and Antunes, Pedro and Johnstone, David and Duy, Nguyen Huynh},
title = {Establishing a Decision Tool for Business Process Crowdsourcing},
year = {2015},
isbn = {9783319261348},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-319-26135-5_7},
doi = {10.1007/978-3-319-26135-5_7},
abstract = {The integration of crowdsourcing in organisations fosters new managerial and business capabilities, especially regarding flexibility and agility of external human resources. However, a crowdsourcing project involves considering multiple contextual factors and choices and dealing with the novelty of the strategy, which makes managerial decisions difficult. This research addresses the problem by proposing a tool supporting business decision-makers in the establishment of crowdsourcing projects. The proposed tool is based on an extensive review of prior research in crowdsourcing and an ontology that standardises the fundamental crowdsourcing concepts, processes, dependencies, constraints, and managerial decisions. In particular, we discuss the architecture of the proposed tool and present two prototypes, one supporting what-if analysis and the other supporting detailed establishment of crowdsourcing processes.},
booktitle = {Proceedings of the Second International Conference on Future Data and Security Engineering - Volume 9446},
pages = {85–97},
numpages = {13},
keywords = {Ontology, Design science, Decision support system, Crowdsourcing, Business process crowdsourcing},
location = {Ho Chi Minh City, Vietnam},
series = {FDSE 2015}
}

@inproceedings{10.5555/2772879.2773290,
author = {Dayama, Pankaj and Narayanaswamy, Balakrishnan and Garg, Dinesh and Narahari, Y.},
title = {Truthful Interval Cover Mechanisms for Crowdsourcing Applications},
year = {2015},
isbn = {9781450334136},
publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
address = {Richland, SC},
abstract = {The varying nature of qualities and costs of the crowdworkers makes task allocation a non-trivial problem in almost all crowdsourcing applications. If crowdworkers are strategic about their costs, the problem becomes even more challenging. Interestingly, in several crowdsourcing applications, for example, traffic monitoring, air pollution monitoring, digital epidemiology, smart grids operations, etc., the structure of the tasks in space or time exhibits a natural linear ordering. Motivated by the above observation, we model the problem of task allocation to strategic crowdworkers as an interval cover mechanism design problem. In this mechanism, a planner (or task requester) needs to crowdsource labels for a set of tasks in a cost effective manner and make a high quality inference. We consider two different scenarios in this problem: homogeneous and heterogeneous, based on the qualities of crowdworkers. We show that the task allocation problem is polynomial time solvable in the homogeneous case while it is NP-hard in the heterogeneous case. When the crowdworkers are strategic about their costs, we design truthful mechanisms for both the scenarios. In particular, for the heterogeneous case, we propose a novel approximation algorithm that is monotone, leading to a truthful interval cover mechanism via appropriate payments.},
booktitle = {Proceedings of the 2015 International Conference on Autonomous Agents and Multiagent Systems},
pages = {1091–1099},
numpages = {9},
keywords = {truthful mechanisms, task allocation, mechanism design, crowd sourcing},
location = {Istanbul, Turkey},
series = {AAMAS '15}
}

@inproceedings{10.1145/2875913.2875926,
author = {Yan, Minzhi and Sun, Hailong and Liu, Xudong},
title = {Efficient Testing of Web Services with Mobile Crowdsourcing},
year = {2015},
isbn = {9781450336413},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2875913.2875926},
doi = {10.1145/2875913.2875926},
abstract = {Nowadays, online Internet services are pervasive and can be invoked from diverse locations in anytime with multitudinous devices. Conventional testing approaches for online services like Web services are conducted by professional tester or developers and cannot simulate the real world running environment of a service. Fortunately, crowdtesting technology brings us promising hope and has acquired increasing interests and adoption because it can recruit plenty of end users to test services under real world environment with low cost. Meanwhile, improved mobile network techniques make crowdsourcing happen anywhere and anytime. In this paper, we present iTest which combines mobile crowdsourcing and web service testing together to support the performance testing of web services. iTest is a framework for service developers to submit their web services and conveniently get the test results from the crowd testers. Firstly, we analyze the key problems need to be solved in a mobile crowdtesting platform; secondly, the architecture of iTest framework and the workflow in it are presented; Thirdly, we perform experiments to illustrate that both the way to access network and tester's location influence the performance of web service, and formulate the tester selection problem as a Set Cover Problem and propose a greedy algorithm for solving this problem; Next, experimental evaluation of the tester selection algorithm is performed to illustrate its efficiency. Finally, we conclude our work and provide the directions for future work.},
booktitle = {Proceedings of the 7th Asia-Pacific Symposium on Internetware},
pages = {157–165},
numpages = {9},
keywords = {web service, set cover, mobile crowdsourcing, crowd testing, Web service testing},
location = {Wuhan, China},
series = {Internetware '15}
}

@inproceedings{10.1109/GLOCOM.2017.8254430,
author = {Wu, Shuang and Gao, Xiaofeng and Wu, Fan and Chen, Guihai},
title = {A Constant-Factor Approximation for Bounded Task Allocation Problem in Crowdsourcing},
year = {2017},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/GLOCOM.2017.8254430},
doi = {10.1109/GLOCOM.2017.8254430},
abstract = {As the technology advances, there are more and more mobile crowdsensing (MCS) platforms that try to leverage these devices to improve the quality of our life. In this paper, we consider a bounded task allocation problem (BTAP) in MCS platforms that involve the time-sensitive and location-dependent tasks. We ﬁrst formulate the bounded task allocation problem as an integer programing problem and prove its NP-hardness. Then we propose an approximated algorithm to solve the problem with (2+ε)-approximated ratio and show that it is a tight bound. So far as we know, we are the first to give a constant approximated ratio for such task allocation problems. Finally, we make some simulations to show the performance of our scheme comparing with other two benchmarks.},
booktitle = {GLOBECOM 2017 - 2017 IEEE Global Communications Conference},
pages = {1–6},
numpages = {6},
location = {Singapore}
}

@inproceedings{10.1109/COMM48946.2020.9141972,
author = {Enache, Florin and Greu, Victor and Ciot\^{\i}rnae, Petric\u{a} and Popescu, Florin},
title = {Model and Algorithms for Optimizing a Human Computing System Oriented to Knowledge Extraction by Use of Crowdsourcing},
year = {2020},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/COMM48946.2020.9141972},
doi = {10.1109/COMM48946.2020.9141972},
abstract = {The paper is addressing the actual context of Data Deluge, where the need and also premises to extract more knowledge are increasing, along with the increase of our expectations about performances. Besides, improving artificial intelligence (AI), by machine learning (ML), deep learning (DL) or cognitive learning (CL) performance/potential, when adding human contributions where necessary, is an important and promising research area. Consequently, our model, algorithms (ALG1; ALG2) and soft programs provide useful new instruments for implementing and optimizing the workflow based on crowdsourcing, when using human potential in a human computing system. We aim to increase AI quality adding multiple human outputs for every AI task and leveraging learning rules to be then extended to larger sets of tasks. This way, such hybrid system could be oriented to more knowledge extraction, by the generalization of images/ captions/labels toward more complex tasks, like providing content essential or question answering. Our instruments include features of ranking workers and tasks profiles, which will support the main original process of knowledge extraction, but also the inference elements, by small amounts of learning data (regarding the workers skills and tasks efficiency) to be transferred to AI/ML/DL/CL, which then could be used for processing larger volumes of similar data. Among the results conclusions is that using progressive optimization, structuring the data/tasks in variable (progressive) sets and potential (skill/number) of workers, is both efficacious and efficient, allowing a flexible control of the system and workflow for matching a diversity of tasks complexity/ difficulty/volume and leveraging knowledge extraction.},
booktitle = {2020 13th International Conference on Communications (COMM)},
pages = {297–302},
numpages = {6},
location = {Bucharest, Romania}
}

@inproceedings{10.1145/3318299.3318314,
author = {Jianwei, Xiang and Shuang, Liu and Han, Xu},
title = {A Monte-Carlo Approach to the Value of Information in Crowdsourcing Quality Control Tasks},
year = {2019},
isbn = {9781450366007},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3318299.3318314},
doi = {10.1145/3318299.3318314},
abstract = {In the process of decision-making, the purpose of computing value of information (VOI) is to guide information collection process under uncertain environment, improve the quality of decision-making, and ultimately achieve the optimal decision. In the field of artificial intelligence, MDP is a basic theoretical model for modeling and planning decision problems, and also a major research area of sequential decision-making. In this paper, we presents a novel framework at a specific type of optimal uncertain sequential decision problems that need achieve the best trade-off between decision qualities and cost. We apply it to quality control in crowdsourcing task. Because of the combinatorial challenge of the state space when calculating the optimal policy of the general Markov decision model, this paper considers a more efficient approximation method: A Monte-Carlo Tree method computing the value of information (BMCT) based on belief states.},
booktitle = {Proceedings of the 2019 11th International Conference on Machine Learning and Computing},
pages = {119–123},
numpages = {5},
keywords = {monte-carlo sampling, crowdsourcing, Value of information, Markov decision process},
location = {Zhuhai, China},
series = {ICMLC '19}
}

@inproceedings{10.1109/IPDPS.2013.84,
author = {Boutsis, Ioannis and Kalogeraki, Vana},
title = {Crowdsourcing under Real-Time Constraints},
year = {2013},
isbn = {9780769549712},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/IPDPS.2013.84},
doi = {10.1109/IPDPS.2013.84},
abstract = {In the recent years we are experiencing the rapid growth of crowd sourcing systems, in which "human workers" are enlisted to perform tasks more effectively than computers, and get compensated for the work they provide. The common belief is that the wisdom of the "human crowd" can greatly complement many computer tasks which are assigned to machines. A significant challenge facing these systems is determining the most efficient allocation of tasks to workers to achieve successful completion of the tasks under real-time constraints. This paper presents REACT, a crowd sourcing system that seeks to address this challenge and proposes algorithms that aim to stimulate user participation and handle dynamic task assignment and execution in the crowd sourcing system. The goal is to determine the most appropriate workers to assign incoming tasks, in such a way so that the real-time demands are met and high quality results are returned. We empirically evaluate our approach and show that REACT meets the requested real-time demands, achieves good accuracy, is efficient, and improves the amount of successful tasks that meet their deadlines up to 61\% compared to traditional approaches like AMT.},
booktitle = {Proceedings of the 2013 IEEE 27th International Symposium on Parallel and Distributed Processing},
pages = {753–764},
numpages = {12},
keywords = {real-time, distributed systems, crowdsourcing},
series = {IPDPS '13}
}

@inproceedings{10.1145/2740908.2741994,
author = {Amer-Yahia, Sihem and Basu Roy, Senjuti},
title = {From Complex Object Exploration to Complex Crowdsourcing.},
year = {2015},
isbn = {9781450334730},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2740908.2741994},
doi = {10.1145/2740908.2741994},
abstract = {Forming and exploring complex objects is at the heart of a variety of emerging web applications. Historically, existing work on complex objects has been developed in two separate areas: composite item retrieval and team formation. At the same time, emerging applications that harness the wisdom of crowd workers, such as, document editing by workers, sentence translation by fans (or fan-subbing), innovative design, citizen science or journalism, represent complex crowdsourcing, in which an object may represent a complex task formed by a set of sub-tasks or a team of workers who work together to solve the task. The goal of this tutorial is to bridge the gap between composite item retrieval and team formation and define new research directions for complex crowdsourcing applications},
booktitle = {Proceedings of the 24th International Conference on World Wide Web},
pages = {1531–1532},
numpages = {2},
keywords = {optimization algorithms, complex object exploration, complex crowdsourcing},
location = {Florence, Italy},
series = {WWW '15 Companion}
}

@inproceedings{10.1145/2488388.2488489,
author = {Singer, Yaron and Mittal, Manas},
title = {Pricing mechanisms for crowdsourcing markets},
year = {2013},
isbn = {9781450320351},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2488388.2488489},
doi = {10.1145/2488388.2488489},
abstract = {Every day millions of crowdsourcing tasks are performed in exchange for payments. Despite the important role pricing plays in crowdsourcing campaigns and the complexity of the market, most platforms do not provide requesters appropriate tools for effective pricing and allocation of tasks.In this paper, we introduce a framework for designing mechanisms with provable guarantees in crowdsourcing markets. The framework enables automating the process of pricing and allocation of tasks for requesters in complex markets like Amazon's Mechanical Turk where workers arrive in an online fashion and requesters face budget constraints and task completion deadlines. We present constant-competitive incentive compatible mechanisms for maximizing the number of tasks under a budget, and for minimizing payments given a fixed number of tasks to complete. To demonstrate the effectiveness of this framework we created a platform that enables applying pricing mechanisms in markets like Mechanical Turk. The platform allows us to show that the mechanisms we present here work well in practice, as well as to give experimental evidence to workers' strategic behavior in absence of appropriate incentive schemes.},
booktitle = {Proceedings of the 22nd International Conference on World Wide Web},
pages = {1157–1166},
numpages = {10},
keywords = {mechanism design, mechanical turk, mechanical perk, human computation, crowdsourcing},
location = {Rio de Janeiro, Brazil},
series = {WWW '13}
}

@inproceedings{10.1145/2487788.2488125,
author = {Celis, L. Elisa and Dasgupta, Koustuv and Rajan, Vaibhav},
title = {Adaptive crowdsourcing for temporal crowds},
year = {2013},
isbn = {9781450320382},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2487788.2488125},
doi = {10.1145/2487788.2488125},
abstract = {Crowdsourcing is rapidly emerging as a computing paradigm that can employ the collective intelligence of a distributed human population to solve a wide variety of tasks. However, unlike organizational environments where workers have set work hours, known skill sets and performance indicators that can be monitored and controlled, most crowdsourcing platforms leverage the capabilities of fleeting workers who exhibit changing work patterns, expertise, and quality of work. Consequently, platforms exhibit significant variability in terms of performance characteristics (like response time, accuracy, and completion rate). While this variability has been folklore in the crowdsourcing community, we are the first to show data that displays this kind of changing behavior. Notably, these changes are not due to a distribution with high variance; rather, the distribution itself is changing over time.Deciding which platform is most suitable given the requirements of a task is of critical importance in order to optimize performance; further, making the decision(s) adaptively to accommodate the dynamically changing crowd characteristics is a problem that has largely been ignored. In this paper, we address the changing crowds problem and, specifically, propose a multi-armed bandit based framework. We introduce the simple epsilon-smart algorithm that performs robustly. Counterfactual results based on real-life data from two popular crowd platforms demonstrate the efficacy of the proposed approach. Further simulations using a random-walk model for crowd performance demonstrate its scalability and adaptability to more general scenarios.},
booktitle = {Proceedings of the 22nd International Conference on World Wide Web},
pages = {1093–1100},
numpages = {8},
keywords = {temporal behavior, online learning, multi-armed bandit, crowdsourcing},
location = {Rio de Janeiro, Brazil},
series = {WWW '13 Companion}
}

@inproceedings{10.1145/2461121.2461135,
author = {Takagi, Hironobu and Kosugi, Akihiro and Saito, Shin and Teraguchi, Masayoshi},
title = {Crowdsourcing platform for workplace accessibility},
year = {2013},
isbn = {9781450318440},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2461121.2461135},
doi = {10.1145/2461121.2461135},
abstract = {Our modern workplace is filled with information sources such as the Web, videos, documents and images. Each employee is required to learn from these sources to work effectively and to contribute to the company's business. Crowdsourcing services have a great potential to improve workplace accessibility by providing captions for meeting videos, describing key diagrams, and converting scanned materials into text files. However, it is risky to expose confidential materials in a public crowd. Crowdsourcing to employees who have knowledge and expertise may solve the issue of confidentiality, but it is difficult to reach them to access their "niche" spare time for tasks given their busy work schedules. Therefore, we propose a crowdsourcing platform to securely disseminate tasks to employees by inviting them to do microtasks. The basic strategy is akin to Web advertising. The system automatically suggests tasks to employees as a part of intranet webpages and in e-mail clients by considering work contexts, employee interests and expertise, and the security of the materials. We will first discuss the pros and cons of intraorganizational crowdsourcing and then propose the new crowdsourcing platform.},
booktitle = {Proceedings of the 10th International Cross-Disciplinary Conference on Web Accessibility},
articleno = {28},
numpages = {4},
keywords = {workplace, intranet, images, crowdsourcing, accessibility},
location = {Rio de Janeiro, Brazil},
series = {W4A '13}
}

@inproceedings{10.1145/2567948.2577371,
author = {Spirin, Nikita and Eslami, Motahhare and Ding, Jie and Jain, Pooja and Bailey, Brian and Karahalios, Karrie},
title = {Searching for design examples with crowdsourcing},
year = {2014},
isbn = {9781450327459},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2567948.2577371},
doi = {10.1145/2567948.2577371},
abstract = {Examples are very important in design, but existing tools for design example search still do not cover many cases. For instance, long tail queries containing subtle and subjective design concepts, like "calm and quiet", "elegant", "dark background with a hint of color to make it less boring", are poorly supported. This is mainly due to the inherent complexity of the task, which so far has been tackled only algorithmically using general image search techniques. We propose a powerful new approach based on crowdsourcing, which complements existing algorithmic approaches and addresses their shortcomings. Out of many explored crowdsourcing configurations we found that (1) a design need should be represented via several query images and (2) AMT crowd workers should assess a query-specific relevance of a candidate example from a pre-built design collection. To test the utility of our approach, we compared it with Google Images in a query-by-example mode. Based on feedback from expert designers, the crowd selects more relevant design examples.},
booktitle = {Proceedings of the 23rd International Conference on World Wide Web},
pages = {381–382},
numpages = {2},
keywords = {query-by-example, design search, crowdsourcing, amturk},
location = {Seoul, Korea},
series = {WWW '14 Companion}
}

@inproceedings{10.1145/3148456.3148491,
author = {J\'{u}nior, Paulo Sim\~{o}es and Novais, Renato and Vieira, Vaninha and Pedraza, Laia G. and Mendon\c{c}a, Manoel and Villela, Karina},
title = {Visualization mechanisms for crowdsourcing information in emergency coordination},
year = {2015},
isbn = {9781450353625},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3148456.3148491},
doi = {10.1145/3148456.3148491},
abstract = {To perform emergency coordination, people in a command centre need to process a large amount of data about the incident to make decisions, generally, under time pressure. A main challenge is to quickly obtain contextual information about the situation, which can be obtained from people in the place of the incident, in a crowdsourcing manner. This paper presents our investigation about visualization mechanisms to support command centres on analysing crowdsourcing information regarding emergency situations. As contributions, we highlight: 1) discussion of existing visualization mechanisms and their support on emergency management; 2) prototype of the Emergency Response Toolkit (ERTK), a set of tools to support command centres on using information from the crowd, e.g. in large-scale events; and 3) evaluation of ERTK and its visualization mechanisms with 11 emergency experts, in Brazil, Austria and Spain, collecting feedback to improve information visualization for emergency management.},
booktitle = {Proceedings of the 14th Brazilian Symposium on Human Factors in Computing Systems},
articleno = {35},
numpages = {8},
keywords = {information visualization, emergency management, emergency coordination, crowdsourcing},
location = {Salvador, Brazil},
series = {IHC '15}
}

@inproceedings{10.1145/3106426.3106436,
author = {Saberi, Morteza and Hussain, Omar K. and Chang, Elizabeth},
title = {An online statistical quality control framework for performance management in crowdsourcing},
year = {2017},
isbn = {9781450349512},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3106426.3106436},
doi = {10.1145/3106426.3106436},
abstract = {The big data research topic has grown rapidly for the past decade due to the advent of the "data deluge". Recent advancements in the literature leverage human computing power known as crowdsourcing to manage and harness big data for various applications. However, human involvement in the completion of crowdsourcing tasks is an error-prone process that affects the overall performance of the crowd. Thus, controlling the quality of workers is an essential step for crowdsourcing systems, which due to unavailability of ground-truth data for any task at hand becomes increasingly challenging. To propose a solution to this problem, in this study, we propose OSQC (Online Statistical Quality Control Framework) for managing the performance of workers in crowdsourcing. OSQC ascertains the worker's performance by using a statistical model and then leverages the traditional statistical control techniques to decide whether to retain a worker for crowdsourcing or to evict him. We evaluate our proposed framework on a real dataset and demonstrate how OSQC assists crowdsourcing to maintain its accuracy.},
booktitle = {Proceedings of the International Conference on Web Intelligence},
pages = {476–482},
numpages = {7},
keywords = {statistical quality control, multiple choice HIT, crowdsourcing management, crowd workers},
location = {Leipzig, Germany},
series = {WI '17}
}

@inproceedings{10.1145/3255760,
author = {Sellis, Timos},
title = {Session details: Research session 10: crowdsourcing},
year = {2014},
isbn = {9781450323765},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3255760},
doi = {10.1145/3255760},
booktitle = {Proceedings of the 2014 ACM SIGMOD International Conference on Management of Data},
location = {Snowbird, Utah, USA},
series = {SIGMOD '14}
}

@inproceedings{10.1109/ICSS.2015.14,
author = {Liu, Kaixu and Motta, Gianmario and You, Linlin and Ma, Tianyi},
title = {A Threefold Similarity Analysis of Crowdsourcing Feeds},
year = {2015},
isbn = {9781479999477},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/ICSS.2015.14},
doi = {10.1109/ICSS.2015.14},
abstract = {Crowdsourcing is a valuable social sensing for the smarter city. We present an approach for classifying crowd sourced feeds from a threefold point of view, namely image, text, and geography. The main idea is to extract feeds within a specific geographic range, and then analyze similarity of image color and text semantic. The approach enables to identify feeds that report the same issue, hence filtering redundant information. Based on proved methods and algorithms, such approach has been implemented in a software application, called CITY FEED, that is used by the Municipality of Pavia.},
booktitle = {Proceedings of the 2015 International Conference on Service Science},
pages = {93–98},
numpages = {6},
keywords = {Text similarity analysis, Smart city, Image similarity analysis, Crowdsourcing},
series = {ICSS '15}
}

@inproceedings{10.1145/2212776.2212801,
author = {Hughes, Lucy and Atkinson, Douglas and Berthouze, Nadia and Baurley, Sharon},
title = {Crowdsourcing an emotional wardrobe},
year = {2012},
isbn = {9781450310161},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2212776.2212801},
doi = {10.1145/2212776.2212801},
abstract = {Selecting clothing online requires decision-making about sensorial experiences, but online environments provide only limited sensorial information. Inferences are therefore made on the basis of product pictures and their textual description. This is often unreliable as it is either based on the designer's understanding of the product or deprived of perceptual content due to the difficulty of expressing such experiences. Using a purpose built website that combines and cross references multi-modal descriptive media, this study aims at investigating the possibility of using crowdsourcing mechanisms and multi-modal language to engage consumers in providing enriched descriptions of their tactile experiences of garments.},
booktitle = {CHI '12 Extended Abstracts on Human Factors in Computing Systems},
pages = {231–240},
numpages = {10},
keywords = {design research, crowdsourcing, affective computing},
location = {Austin, Texas, USA},
series = {CHI EA '12}
}

@inproceedings{10.1109/MASS.2015.40,
author = {Miao, Xin and Liu, Kebin and Chen, Lei and Liu, Yunhao},
title = {Quality-Aware Online Task Assignment in Mobile Crowdsourcing},
year = {2015},
isbn = {9781467391016},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/MASS.2015.40},
doi = {10.1109/MASS.2015.40},
abstract = {Mobile crowd sourcing (MCS) has grown to be a powerful computation paradigm to harness human power to solve real-world problems. Many commercial MCS platforms have arisen, enabling various novel applications. As crowd workers can be unreliable, a critical issue of these platforms is quality control. Many task assignment approaches have been proposed to increase the quality of crowd sourced tasks by matching workers and tasks in a bipartite graph. However, they fail to apply to MCS platforms where tasks are bound with locations. This paper considers the quality-aware online task assignment problem with location-based tasks. The goal is to optimize tasks' overall quality by assigning appropriate sets of tasks to workers in an online manner. To solve this problem, we propose a probabilistic quality measurement model and a hitchhiking model to characterize workers' behavior. Then we design a polynomial-time online assignment algorithm and prove that the proposed algorithm approximates the offline optimal solution with a competitive ratio of 10/7. Through extensive simulations, we demonstrate the efficiency and effectiveness of our solution.},
booktitle = {Proceedings of the 2015 IEEE 12th International Conference on Mobile Ad Hoc and Sensor Systems (MASS)},
pages = {127–135},
numpages = {9},
series = {MASS '15}
}

@inproceedings{10.1145/3041021.3055128,
author = {Goncalves, Jorge and Feldman, Michael and Hu, Subingqian and Kostakos, Vassilis and Bernstein, Abraham},
title = {Task Routing and Assignment in Crowdsourcing based on Cognitive Abilities},
year = {2017},
isbn = {9781450349147},
publisher = {International World Wide Web Conferences Steering Committee},
address = {Republic and Canton of Geneva, CHE},
url = {https://doi.org/10.1145/3041021.3055128},
doi = {10.1145/3041021.3055128},
abstract = {Appropriate task routing and assignment is an important, but often overlooked, element in crowdsourcing research and practice. In this paper, we explore and evaluate a mechanism that can enable matching crowdsourcing tasks to suitable crowd-workers based on their cognitive abilities. We measure participants' visual and fluency cognitive abilities with the well-established Kit of Factor-Referenced Cognitive Test, and measure crowdsourcing performance with our own set of developed tasks. Our results indicate that participants' cognitive abilities correlate well with their crowdsourcing performance. We also built two predictive models (beta and linear regression) for crowdsourcing task performance based on the performance on cognitive tests as explanatory variables. The model results suggest that it is feasible to predict crowdsourcing performance based on cognitive abilities. Finally, we discuss the benefits and challenges of leveraging workers' cognitive abilities to improve task routing and assignment in crowdsourcing environments.},
booktitle = {Proceedings of the 26th International Conference on World Wide Web Companion},
pages = {1023–1031},
numpages = {9},
keywords = {worker performance, visual tasks, task routing, task assignment, kit of factor-referenced cognitive tests., fluency tasks, crowdsourcing, cognitive abilities},
location = {Perth, Australia},
series = {WWW '17 Companion}
}

@inproceedings{10.1145/2806416.2806627,
author = {Zhang, Jing and Sheng, Victor S. and Wu, Jian and Fu, Xiaoqin and Wu, Xindong},
title = {Improving Label Quality in Crowdsourcing Using Noise Correction},
year = {2015},
isbn = {9781450337946},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2806416.2806627},
doi = {10.1145/2806416.2806627},
abstract = {This paper proposes a novel framework that introduces noise correction techniques to further improve label quality after ground truth inference in crowdsourcing. In the framework, an adaptive voting noise correction algorithm (AVNC) is proposed to identify and correct the most likely noises with the help of estimated qualities of labelers provided by the ground truth inference. The experimental results on two real-world datasets show that (1) the framework can improve label quality regardless of inference algorithms, especially under the circumstance that each example has a few noisy labels; and (2) since the algorithm AVNC considers both the number of and the probability of potential noises, it outperforms a baseline noise correction algorithm.},
booktitle = {Proceedings of the 24th ACM International on Conference on Information and Knowledge Management},
pages = {1931–1934},
numpages = {4},
keywords = {noise correction, label quality, inference, crowdsourcing},
location = {Melbourne, Australia},
series = {CIKM '15}
}

@inproceedings{10.1007/978-3-319-68204-4_24,
author = {Gil, Yolanda and Garijo, Daniel and Ratnakar, Varun and Khider, Deborah and Emile-Geay, Julien and McKay, Nicholas},
title = {A Controlled Crowdsourcing Approach for Practical Ontology Extensions and Metadata Annotations},
year = {2017},
isbn = {978-3-319-68203-7},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-319-68204-4_24},
doi = {10.1007/978-3-319-68204-4_24},
abstract = {Traditional approaches to ontology development have a large lapse between the time when a user using the ontology has found a need to extend it and the time when it does get extended. For scientists, this delay can be weeks or months and can be a significant barrier for adoption. We present a new approach to ontology development and data annotation enabling users to add new metadata properties on the fly as they describe their datasets, creating terms that can be immediately adopted by others and eventually become standardized. This approach combines a traditional, consensus-based approach to ontology development, and a crowdsourced approach where expert users (the crowd) can dynamically add terms as needed to support their work. We have implemented this approach as a socio-technical system that includes: (1) a crowdsourcing platform to support metadata annotation and addition of new terms, (2) a range of social editorial processes to make standardization decisions for those new terms, and (3) a framework for ontology revision and updates to the metadata created with the previous version of the ontology. We present a prototype implementation for the Paleoclimate community, the Linked Earth Framework, currently containing 700 datasets and engaging over 50 active contributors. Users exploit the platform to do science while extending the metadata vocabulary, thereby producing useful and practical metadata.},
booktitle = {The Semantic Web – ISWC 2017: 16th International Semantic Web Conference, Vienna, Austria, October 21-25, 2017, Proceedings, Part II},
pages = {231–246},
numpages = {16},
keywords = {Incremental vocabulary development, Semantic science, Collaborative ontology engineering, Semantic wiki, Crowdsourcing, Metadata},
location = {Vienna, Austria}
}

@inproceedings{10.5555/2891460.2891721,
author = {Lasecki, Walter S.},
title = {Crowdsourcing for deployable intelligent systems},
year = {2013},
publisher = {AAAI Press},
abstract = {My work aims to create a scaffold for deployable intelligent systems using crowdsourcing. Current approaches in artificial intelligence (AI) typically focus on solving a narrow subset of problems in a given space - for example: automatic speech recognition as part of a conversational assistant, machine vision as part of a question answering service for blind people, or planning as part of a home assistive robot. This approach is necessary to scope the solution, but often results in a large number of systems that are rarely deployed in real-world setting, but instead operate in toy domains, or in situations where other parts of the problem are assumed to be solved.The framework I have developed aims to use the crowd to help in two ways: (i) make it possible to use human intelligence to power parts of a system that automated approaches cannot or do not yet handle, and (ii) provide a means of enabling more effective deployable systems by people to provide reliable training data on-demand. This summary begins with a brief review of prior work, then outlines a number of different system that I have developed to demonstrate the capabilities of this framework, and concludes with future work to be completed as part of my thesis.},
booktitle = {Proceedings of the Twenty-Seventh AAAI Conference on Artificial Intelligence},
pages = {1670–1671},
numpages = {2},
location = {Bellevue, Washington},
series = {AAAI'13}
}

@inproceedings{10.1145/2806416.2806451,
author = {Asudeh, Abolfazl and Zhang, Gensheng and Hassan, Naeemul and Li, Chengkai and Zaruba, Gergely V.},
title = {Crowdsourcing Pareto-Optimal Object Finding By Pairwise Comparisons},
year = {2015},
isbn = {9781450337946},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2806416.2806451},
doi = {10.1145/2806416.2806451},
abstract = {This is the first study of crowdsourcing Pareto-optimal object finding over partial orders and by pairwise comparisons, which has applications in public opinion collection, group decision making, and information exploration. Departing from prior studies on crowdsourcing skyline and ranking queries, it considers the case where objects do not have explicit attributes and preference relations on objects are strict partial orders. The partial orders are derived by aggregating crowdsourcers' responses to pairwise comparison questions. The goal is to find all Pareto-optimal objects by the fewest possible questions. It employs an iterative question-selection framework. Guided by the principle of eagerly identifying non-Pareto optimal objects, the framework only chooses candidate questions which must satisfy three conditions. This design is both sufficient and efficient, as it is proven to find a short terminal question sequence. The framework is further steered by two ideas---macro-ordering and micro-ordering. By different micro-ordering heuristics, the framework is instantiated into several algorithms with varying power in pruning questions. Experiment results using both real crowdsourcing marketplace and simulations exhibited not only orders of magnitude reductions in questions when compared with a brute-force approach, but also close-to-optimal performance from the most efficient instantiation.},
booktitle = {Proceedings of the 24th ACM International on Conference on Information and Knowledge Management},
pages = {753–762},
numpages = {10},
keywords = {skyline query, partial order, pareto-optimal object, pairwise comparison, opinion collection, information exploration, human computation, decision making, crowdsourcing},
location = {Melbourne, Australia},
series = {CIKM '15}
}

@inproceedings{10.1145/2598153.2602248,
author = {Rahmanian, Bahareh and Davis, Joseph G.},
title = {User interface design for crowdsourcing systems},
year = {2014},
isbn = {9781450327756},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2598153.2602248},
doi = {10.1145/2598153.2602248},
abstract = {Harnessing human computation through crowdsourcing offers an alternative approach to solving complex problems, especially those that are relatively easy for humans but difficult for computers. Micro-tasking platforms such as Amazon Mechanical Turk have attracted large, on-demand work force of millions of workers as well as hundreds of thousands of job requesters. Achieving high quality results by putting humans in the loop is one of the main goals of these crowdsourcing systems. We study the effects of different user interface designs on the performance of crowdsourcing systems. Our results indicate that user interface design choices have a significant effect on crowdsourced worker performance.},
booktitle = {Proceedings of the 2014 International Working Conference on Advanced Visual Interfaces},
pages = {405–408},
numpages = {4},
keywords = {user interface, crowdsourcing, cognitive load},
location = {Como, Italy},
series = {AVI '14}
}

@inproceedings{10.1145/2598153.2602249,
author = {Kucherbaev, Pavel and Daniel, Florian and Marchese, Maurizio and Casati, Fabio and Reavey, Brian},
title = {Toward effective tasks navigation in crowdsourcing},
year = {2014},
isbn = {9781450327756},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2598153.2602249},
doi = {10.1145/2598153.2602249},
abstract = {Crowdsourcing platforms are changing the way people can work and earn money. The population of workers on crowdsourcing platforms already counts millions and keeps growing. Workers on these platforms face several usability challenges, which we identify in this work by running two surveys on the CrowdFlower platform. Our surveys show that the majority of workers spend more than 25\% of their time on searching tasks to work on. Limitations in the current user interface of the task listing page prevent workers from focusing more on the execution. In this work we present an attempt to design and implement a specific user interface for task listing aimed to help workers spend less time searching for tasks and thus navigate among them more easily.},
booktitle = {Proceedings of the 2014 International Working Conference on Advanced Visual Interfaces},
pages = {401–404},
numpages = {4},
keywords = {user interfaces, search, crowdsourcing},
location = {Como, Italy},
series = {AVI '14}
}

@inproceedings{10.5555/3157382.3157663,
author = {Lahouti, Farshad and Hassibi, Babak},
title = {Fundamental limits of budget-fidelity trade-off in label crowdsourcing},
year = {2016},
isbn = {9781510838819},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Digital crowdsourcing (CS) is a modern approach to perform certain large projects using small contributions of a large crowd. In CS, a taskmaster typically breaks down the project into small batches of tasks and assigns them to so-called workers with imperfect skill levels. The crowdsourcer then collects and analyzes the results for inference and serving the purpose of the project. In this work, the CS problem, as a human-in-the-loop computation problem, is modeled and analyzed in an information theoretic rate-distortion framework. The purpose is to identify the ultimate fidelity that one can achieve by any form of query from the crowd and any decoding (inference) algorithm with a given budget. The results are established by a joint source channel (de)coding scheme, which represent the query scheme and inference, over parallel noisy channels, which model workers with imperfect skill levels. We also present and analyze a query scheme dubbed k-ary incidence coding and study optimized query pricing in this setting.},
booktitle = {Proceedings of the 30th International Conference on Neural Information Processing Systems},
pages = {5065–5073},
numpages = {9},
location = {Barcelona, Spain},
series = {NIPS'16}
}

@inproceedings{10.1007/978-3-319-24258-3_8,
author = {Gadiraju, Ujwal and Fetahu, Besnik and Kawase, Ricardo},
title = {Training Workers for Improving Performance in Crowdsourcing Microtasks},
year = {2015},
isbn = {978-3-319-24257-6},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-319-24258-3_8},
doi = {10.1007/978-3-319-24258-3_8},
abstract = {With the advent and growing use of crowdsourcing labor markets for a variety of applications, optimizing the quality of results produced is of prime importance. The quality of the results produced is typically a function of the performance of crowd workers. In this paper, we investigate the notion of treating crowd workers as ‘learners’ in a novel learning environment. This learning context is characterized by a short-lived learning phase and immediate application of learned concepts. We draw motivation from the desire of crowd workers to perform well in order to maintain a good reputation, while attaining monetary rewards successfully. Thus, we delve into training workers in specific microtasks of different types. We exploit (i) implicit training, where workers are provided training when they provide erraneous responses to questions with priorly known answers, and (ii) explicit training, where workers are required to go through a training phase before they attempt to work on the task itself. We evaluated our approach in 4 different types of microtasks with a total of 1200 workers, who were subjected to either one of the proposed training strategies or baseline case of no training. The results show that workers who undergo training depict an improvement in performance upto 5&nbsp;\%, and a reduction in the task completion time upto 41&nbsp;\%. Additionally, crowd training led to the elimination of malicious workers and a costs-benefit gain upto nearly 15&nbsp;\%.},
booktitle = {Design for Teaching and Learning in a Networked World: 10th European Conference on Technology Enhanced Learning, EC-TEL 2015, Toledo, Spain, September 15-18, 2015, Proceedings},
pages = {100–114},
numpages = {15},
keywords = {Performance, Microtask, Learning, Training, Workers, Crowdsourcing},
location = {Toledo, Spain}
}

@inproceedings{10.1109/MDM.2013.109,
author = {Chatzimilioudis, Georgios and Zeinalipour-Yazti, Demetrios},
title = {Crowdsourcing for Mobile Data Management},
year = {2013},
isbn = {9780769549736},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/MDM.2013.109},
doi = {10.1109/MDM.2013.109},
abstract = {Crowdsourcing refers to a distributed problemsolving model in which a crowd of undefined size is engaged in the task of solving a complex problem through an open call. This novel problem-solving model found its way into numerous applications on the web for voting, fund-raising, micro-works and wisdom-of-the-crowd scenarios. On the other hand, the shift of desktop users to mobile platforms in the post-PC era, along with the unique multi-sensing capabilities of modern mobile devices are expected to eventually unfold the full potential of Crowdsourcing. Smartphones offer a great platform for extending and diversifying web-based crowdsourcing applications to a larger contributing crowd, making contribution easier and omni-present. This advanced seminar presents the fundamental concepts behind crowdsourcing and its applications to mobile data management. In the first part of the seminar, we will overview the crowdsourcing research landscape from a variety of perspectives, with a particular emphasis on the latest data management trends. In the second and more extended part of the seminar, we will focus on an in-depth coverage of emerging mobile crowdsourcing architectures and systems, through a multi-dimensional taxonomy that will address location, sensing, power, performance, big-data and privacy among others. Furthermore, we will overview a number of in-house crowdsourcing prototypes we have developed and deployed over the last few years. The seminar concludes with challenges, opportunities and new directions in the field.},
booktitle = {Proceedings of the 2013 IEEE 14th International Conference on Mobile Data Management - Volume 02},
pages = {3–4},
numpages = {2},
series = {MDM '13}
}

@inproceedings{10.5555/2693848.2693910,
author = {Zou, Guangyu and Gil, Alvaro and Tharayil, Marina},
title = {An agent-based model for crowdsourcing systems},
year = {2014},
publisher = {IEEE Press},
abstract = {Crowdsourcing is a complex system composed of many interactive distributed agents whom we have little information about. Agent-based modeling (ABM) is a natural way to study complex systems since they share common properties, such as the global behavior emerging on the basis of local interactions between elements. Although significant attention has been given to dynamics of crowdsourcing systems, relatively little is known about how workers react to varying configurations of tasks. In addition, existing ABMs for crowdsourcing systems are theoretical, and not based on data from real crowdsourcing platforms. The focus of this paper is on capturing the relationships among properties of tasks, characteristics of workers, and performance metrics via an ABM. This approach is validated by running experiments on Amazon Mechanical Turk (AMT).},
booktitle = {Proceedings of the 2014 Winter Simulation Conference},
pages = {407–418},
numpages = {12},
location = {Savannah, Georgia},
series = {WSC '14}
}

@inproceedings{10.24963/ijcai.2023/333,
author = {Fang, Yili and Shen, Chaojie and Gu, Huamao and Han, Tao and Ding, Xinyi},
title = {TDG4Crowd: test data generation for evaluation of aggregation algorithms in crowdsourcing},
year = {2023},
isbn = {978-1-956792-03-4},
url = {https://doi.org/10.24963/ijcai.2023/333},
doi = {10.24963/ijcai.2023/333},
abstract = {In crowdsourcing, existing efforts mainly use real datasets collected from crowdsourcing as test datasets to evaluate the effectiveness of aggregation algorithms. However, these work ignore the fact that the datasets obtained by crowdsourcing are usually sparse and imbalanced due to limited budget. As a result, applying the same aggregation algorithm on different datasets often show contradicting conclusions. For example, on the RTE dataset, Dawid and Skene model performs significantly better than Majority Voting, while on the LableMe dataset, the experiments give the opposite conclusion. It is challenging to obtain comprehensive and balanced datasets at a low cost. To our best knowledge, little effort have been made to the fair evaluation of aggregation algorithms. To fill in this gap, we propose a novel method named TDG4Crowd that can automatically generate comprehensive and balanced datasets. Using Kullback Leibler divergence and Kolmogorov-Smirnov test, the experiment results show the superior of our method compared with others. Aggregation algorithms also perform more consistently on the synthetic datasets generated using our method.},
booktitle = {Proceedings of the Thirty-Second International Joint Conference on Artificial Intelligence},
articleno = {333},
numpages = {9},
location = {Macao, P.R.China},
series = {IJCAI '23}
}

@inproceedings{10.5555/3237383.3237900,
author = {Rangi, Anshuka and Franceschetti, Massimo},
title = {Multi-Armed Bandit Algorithms for Crowdsourcing Systems with Online Estimation of Workers' Ability},
year = {2018},
publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
address = {Richland, SC},
abstract = {Crowdsourcing systems have become a valuable solution for various organizations to outsource work on a temporary basis. Quality assurance in these systems remains a key issue due to the distributed setup of the crowdsourcing platforms and the absence of a priori information about the workers. Our work develops a notion of Limited-information Crowdsourcing Systems (LCS), where the task master can assign the work based on some knowledge of the workers' ability acquired over time. The key challenges in this new setup are determining an efficient workers' selection policy and estimating the abilities of the workers. To address the first challenge, we reduce the problem to an arm-limited, budget limited, multi-armed bandit (MAB) set-up, and use the simplified bounded KUBE (B-KUBE) algorithm as a solution. This algorithm has previously only been experimentally evaluated, and we provide provable performance guarantees, showing that it is order optimal, namely the expected regret of B-KUBE is $O(\l{}og(B))$ where B is the total budget of the task master. The second challenge is solved by formalizing the notion of workers' ability mathematically, and proposing a strategy for its estimation. We experimentally evaluate B-KUBE in conjunction with this strategy, showing that it outperforms other state-of- the-art MAB algorithms when applied in the same setting.},
booktitle = {Proceedings of the 17th International Conference on Autonomous Agents and MultiAgent Systems},
pages = {1345–1352},
numpages = {8},
keywords = {multi-armed bandits, crowdsourcing systems, bounded knapsack problem.},
location = {Stockholm, Sweden},
series = {AAMAS '18}
}

@inproceedings{10.1109/ICPP.2015.67,
author = {Sun, Wei and Zhu, Yanmin and Ni, Lionel M. and Li, Bo},
title = {Crowdsourcing Sensing Workloads of Heterogeneous Tasks: A Distributed Fairness-Aware Approach},
year = {2015},
isbn = {9781467375870},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/ICPP.2015.67},
doi = {10.1109/ICPP.2015.67},
abstract = {Crowd sourced sensing over smartphones presents a new paradigm for collecting sensing data over a vast area for real-time monitoring applications. A monitoring application may require different types of sensing data, while under a budget constraint. This paper explores the crucial problem of maximizing the aggregate data utility of heterogeneous sensing tasks while maintaining utility-centric fairness across different tasks under a budget constraint. In particular, we take the redundancy of sensing data into account. This problem is highly challenging given its unique characteristics including the intrinsic trade off between aggregate data utility and fairness, and the large number of smartphones. We propose a fairness-aware distributed approach to solving this problem. To overcome the intractability of the problem, we decompose it to two sub problems of recruiting smartphones under a budget constraint and allocating workloads of sensing tasks. For the first sub problem, we propose an efficient greedy algorithm which has a constant approximation ratio of two. For the second problem, we apply dual based decomposition based on which we design a distributed algorithm for determining the workloads of different tasks on each recruited smartphone. We have implemented our distributed algorithm on a windows-based server and Android-based smartphones. With extensive simulations we demonstrate that our approach achieves high aggregate data utility while maintaining good utility-centric fairness across sensing tasks.},
booktitle = {Proceedings of the 2015 44th International Conference on Parallel Processing (ICPP)},
pages = {580–589},
numpages = {10},
series = {ICPP '15}
}

@inproceedings{10.1145/2872518.2889409,
author = {Basharat, Amna and Arpinar, I. Budak and Rasheed, Khaled},
title = {Leveraging Crowdsourcing for the Thematic Annotation of the Qur'an},
year = {2016},
isbn = {9781450341448},
publisher = {International World Wide Web Conferences Steering Committee},
address = {Republic and Canton of Geneva, CHE},
url = {https://doi.org/10.1145/2872518.2889409},
doi = {10.1145/2872518.2889409},
abstract = {In this paper, we illustrate how we leverage crowdsourcing to create workflows for knowledge engineering in specialized and knowledge intensive domains. We undertake the special case of the Arabic script of the Qur'an, a widely studied manuscript, and attempt to employ crowdsourcing methods for its thematic annotation at the sub-verse level, for which, there is no standardized knowledge model available to date. We demonstrate that our proposed method presents feasibility to achieve reliable annotations in an efficient and scalable manner. The proposed methodology and framework is meant to be generalizable to other knowledge intensive and specialized domains.},
booktitle = {Proceedings of the 25th International Conference Companion on World Wide Web},
pages = {13–14},
numpages = {2},
keywords = {thematic annotation, semantic web, qur'an, ontology, knowledge engineering, disambiguation, crowdsourcing},
location = {Montr\'{e}al, Qu\'{e}bec, Canada},
series = {WWW '16 Companion}
}

@inproceedings{10.5555/2964060.2964109,
author = {Hu, Qinmin and Huang, Xiangji},
title = {Bringing Information Retrieval into Crowdsourcing: A Case Study},
year = {2014},
isbn = {9783319060279},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {We propose a novel and economic framework for bringing information retrieval into crowdsourcing. Both crowdsourcing and information retrieval achieve mutual benefits, which result in 1 workers' quality control by using the query-oriented training; 2 cost savings in money and time; and 3 better qualified feedback information. In our case study, the costs of crowdsourcing for 18,260 jobs are as low as $47.25 and as short as 5 hours in total. Furthermore, the experimental results show that information retrieval techniques greatly reduce the workloads of crowdsourcing, which is only 5\% of the original work. At the other hand, crowdsourcing improves the accuracy of the information retrieval system through providing qualified feedback information.},
booktitle = {Proceedings of the 36th European Conference on IR Research on Advances in Information Retrieval - Volume 8416},
pages = {631–637},
numpages = {7},
location = {Amsterdam, The Netherlands},
series = {ECIR 2014}
}

@inproceedings{10.1109/MDM.2014.69,
author = {Phuttharak, Jurairat and Loke, Seng W.},
title = {Towards Declarative Programming for Mobile Crowdsourcing: P2P Aspects},
year = {2014},
isbn = {9781479957057},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/MDM.2014.69},
doi = {10.1109/MDM.2014.69},
abstract = {Peer-to-Peer technologies have been widely used in networks which manage vast amount of data daily. The proliferation of mobile devices strongly motivates mobile peer-to-peer network (M-P2P) applications, with benefits from network effects. We argue that logic programming for crowd sourcing can be useful in peer-to-peer computing for querying and multicasting tasks shared over peer networks. We introduce a declarative crowd sourcing platform for mobile applications, which combines conventional machine computation and the power of the crowd in social networking, particularly in M-P2P networks. This paper discusses a simple extension of Prolog, which we call Logic Crowd, focusing on enabling goal evaluation over peers in mobile peer networks. Additionally, we demonstrate that logic programming for crowd sourcing can be useful in peer-to-peer computing for querying and P2P style of task sharing over short-range networks. In this paper, we illustrate the potential of our approach via programming idioms, a prototype implementation and scenarios.},
booktitle = {Proceedings of the 2014 IEEE 15th International Conference on Mobile Data Management - Volume 02},
pages = {61–66},
numpages = {6},
keywords = {peer-to-peer network, mobile crowdsourcing, mobile application, declarative programming language},
series = {MDM '14}
}

@inproceedings{10.5555/2936924.2937143,
author = {Wang, Wanyuan and He, Zhanpeng and Shi, Peng and Wu, Weiwei and Jiang, Yichuan},
title = {Truthful Team Formation for Crowdsourcing in Social Networks: (Extended Abstract)},
year = {2016},
isbn = {9781450342391},
publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
address = {Richland, SC},
abstract = {This paper studies complex task crowdsourcing by team formation in social networks (SNs), where the requester wishes to hire a group of socially connected workers that can work together as a team. Previous social team crowdsourcing approaches mainly focus on the algorithmic part for social welfare maximization, however, ignore the strategic behavior of workers. In practical crowdsourcing markets, workers are selfish for maximizing their own profit. Within the traditional researches, these selfish workers can be encouraged to manipulate the crowdsourcing system. This untruthful behavior will discourage other workers from participations and is unprofitable for the requester. Thus, a truthful mechanism, guaranteeing that each worker's profit is optimized by behaving honestly, is essential to the success of a crowdsourcing system. Towards this end, in this paper, we develop two efficient truthful mechanisms for the small-scale and large-scale social team crowdsourcing applications, respectively. The experimental results on a real dataset show that compared to the benchmark optimal mechanism, the proposed mechanisms perform well for various scale applications on social welfare maximization.},
booktitle = {Proceedings of the 2016 International Conference on Autonomous Agents \&amp; Multiagent Systems},
pages = {1327–1328},
numpages = {2},
keywords = {team formation, social networks, mechanism design, crowdsourcing},
location = {Singapore, Singapore},
series = {AAMAS '16}
}

@inproceedings{10.1145/2942358.2942402,
author = {Hu, Qin and Wang, Shengling and Ma, Liran and Cheng, Xiuzhen and Bie, Rongfang},
title = {Solving the crowdsourcing dilemma using the zero-determinant strategy: poster},
year = {2016},
isbn = {9781450341844},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2942358.2942402},
doi = {10.1145/2942358.2942402},
abstract = {As a promising technology, crowdsourcing aims to accomplish a complex task via eliciting services from a large group of workers. However, recent observations indicate that the success of crowdsourcing is being hindered by the malicious behaviors of the workers. In this paper, we analyze the attack problem using an iterated prisoner's dilemma (IPD) game and propose an zero-determinant (ZD) strategy based algorithm. Simulation results demonstrate that the requestor can incentivize the worker to keep on cooperating.},
booktitle = {Proceedings of the 17th ACM International Symposium on Mobile Ad Hoc Networking and Computing},
pages = {373–374},
numpages = {2},
keywords = {zero-determinant strategy, game theory, crowdsourcing},
location = {Paderborn, Germany},
series = {MobiHoc '16}
}

@inproceedings{10.1109/HICSS.2014.181,
author = {Tajedin, Hamed and Nevo, Dorti},
title = {Value-Adding Intermediaries in Software Crowdsourcing},
year = {2014},
isbn = {9781479925049},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/HICSS.2014.181},
doi = {10.1109/HICSS.2014.181},
abstract = {The information systems (IS) discipline has been fertile ground for research that delineates the role of technology in transforming organizations. Crowd sourcing counts as one such phenomenon, but our empirical understanding of it is nascent at best. This paper presents a preliminary theoretical justification for the emergence of crowd sourcing intermediaries by describing how they add value to this new sourcing arrangement. We report findings of a case study as initial evidence confirming two sets of value-adding activities taking place in a crowd sourcing platform: those at the market (macro) level and those at the transaction (micro) level.},
booktitle = {Proceedings of the 2014 47th Hawaii International Conference on System Sciences},
pages = {1396–1405},
numpages = {10},
keywords = {software development, intermediation, Crowdsourcing},
series = {HICSS '14}
}

