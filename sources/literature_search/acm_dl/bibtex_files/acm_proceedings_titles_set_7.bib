@inproceedings{10.1145/3003965.3003973,
author = {Bock, Fabian and Martino, Sergio Di and Sester, Monika},
title = {What are the potentialities of crowdsourcing for dynamic maps of on-street parking spaces?},
year = {2016},
isbn = {9781450345774},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3003965.3003973},
doi = {10.1145/3003965.3003973},
abstract = {Finding a parking space is a crucial mobility problem, which could be mitigated by dynamic maps of parking availability. The creation of these maps requires current information on the state of the parking stalls, which could be obtained by (I) instrumenting the road infrastructure with sensors, (II) using probe vehicles, or (III) using mobile apps.In this paper, we investigate the potential predictive performances of a random forest binary classifier, comparing these three data collection strategies. As for the dataset, we used real infrastructure measurements in San Francisco for solution I. We simulated the crowdsourcing solutions II and III by downsampling that dataset, based on different assumptions.Evaluations show that the instrumented solution is clearly superior over the two crowdsourcing strategies, but with remarkably small differences to the probe vehicle scenario. On the other hand, a mobile app would require a very high penetration rate in order to be used for meaningful predictions.},
booktitle = {Proceedings of the 9th ACM SIGSPATIAL International Workshop on Computational Transportation Science},
pages = {19–24},
numpages = {6},
keywords = {spatial information and society, spatio-temporal data analysis, traffic telematics, transportation},
location = {Burlingame, California},
series = {IWCTS '16}
}

@inproceedings{10.1109/SocialCom.2013.155,
author = {Machedon, Radu and Rand, William and Joshi, Yogesh},
title = {Automatic Crowdsourcing-Based Classification of Marketing Messaging on Twitter},
year = {2013},
isbn = {9780769551371},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/SocialCom.2013.155},
doi = {10.1109/SocialCom.2013.155},
abstract = {As the volume of social media communications grow, many different stakeholders have sought to apply tools and methods for automatic identification of sentiment and topic in social network communications. In the domain of social media marketing it would be useful to automatically classify social media messaging into the classic framework of informative, persuasive and transformative advertising. In this paper we develop and present the construction and evaluation of supervised machine-learning classifiers for these concepts, drawing upon established procedures from the domains of sentiment analysis and crowd sourced text classification. We demonstrate that a reasonably effective classifier can be created to identify the informative nature of Tweets based on crowd sourced training data, we also present results for identifying persuasive and transformative content. We finish by summarizing our findings regarding applying these methods and by discussing recommendations for future work in the area of classifying the marketing content of Tweets.},
booktitle = {Proceedings of the 2013 International Conference on Social Computing},
pages = {975–978},
numpages = {4},
keywords = {crowdsourcing, machine learning, marketing, advertising, classification},
series = {SOCIALCOM '13}
}

@inproceedings{10.4108/icst.collaboratecom.2012.250434,
author = {Khazankin, R. and Satzger, B. and Dustdar, S.},
title = {Optimized execution of business processes on crowdsourcing platforms},
year = {2012},
isbn = {9781467327404},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.4108/icst.collaboratecom.2012.250434},
doi = {10.4108/icst.collaboratecom.2012.250434},
abstract = {Crowdsourcing in enterprises is a promising approach for organizing a flexible workforce. Recent developments show that the idea gains additional momentum. However, an obstacle for widespread adoption is the lack of an integrated way to execute business processes based on a crowdsourcing platform. The main difference compared to traditional approaches in business process execution is that tasks or activities cannot be directly assigned but are posted to the crowdsourcing platform, while people can choose deliberately which tasks to book and work on. In this paper we propose a framework for adaptive execution of business processes on top of a crowdsourcing platform. Based on historical data gathered by the platform we mine the booking behavior of people based on the nature and incentive of the crowdsourced tasks. Using the learned behavior model we derive an incentive management approach based on mathematical optimization that executes business processes in a cost-optimal way considering their deadlines. We evaluate our approach through simulations to prove the feasibility and effectiveness. The experiments verify our assumptions regarding the necessary ingredients of the approach and show the advantage of taking the booking behavior into account compared to the case when it is partially of fully neglected.},
booktitle = {Proceedings of the 2012 8th International Conference on Collaborative Computing: Networking, Applications and Worksharing (CollaborateCom 2012)},
pages = {443–451},
numpages = {9},
series = {COLLABORATECOM '12}
}

@inproceedings{10.1007/978-3-319-02786-9_9,
author = {Kurve, Aditya and Miller, David J. and Kesidis, George},
title = {Defeating Tyranny of the Masses in Crowdsourcing: Accounting for Low-Skilled and Adversarial Workers},
year = {2013},
isbn = {9783319027852},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-319-02786-9_9},
doi = {10.1007/978-3-319-02786-9_9},
abstract = {Crowdsourcing has emerged as a useful learning paradigm which allows us to instantly recruit workers on the web to solve large scale problems, such as quick annotation of image, web page, or document databases. Automated inference engines that fuse the answers or opinions from the crowd to make critical decisions are susceptible to unreliable, low-skilled and malicious workers who tend to mislead the system towards inaccurate inferences. We present a probabilistic generative framework to model worker responses for multicategory crowdsourcing tasks based on two novel paradigms. First, we decompose worker reliability into skill level and intention. Second, we introduce a stochastic model for answer generation that plausibly captures the interplay between worker skills, intentions, and task difficulties. This framework allows us to model and estimate a broad range of worker "types". A generalized Expectation Maximization algorithm is presented to jointly estimate the unknown ground truth answers along with worker and task parameters. As supported experimentally, the proposed scheme de-emphasizes answers from low skilled workers and leverages malicious workers to, in fact, improve crowd aggregation. Moreover, our approach is especially advantageous when there is an (a priori unknown) majority of low-skilled and/or malicious workers in the crowd.},
booktitle = {4th International Conference on Decision and Game Theory for Security - Volume 8252},
pages = {140–153},
numpages = {14},
keywords = {crowd aggregation, information fusion, malicious workers, probabilistic modeling},
location = {Fort Worth, TX, USA},
series = {GameSec 2013}
}

@inproceedings{10.1145/2525194.2525294,
author = {Balamurugan, Chithralekha and Roy, Shourya},
title = {Human computer interaction paradigm for business process task crowdsourcing},
year = {2013},
isbn = {9781450322539},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2525194.2525294},
doi = {10.1145/2525194.2525294},
abstract = {In conventional systems like the web-based systems, device-based systems, or desktop systems, the Human Computer Interaction (HCI) spectrum encompasses of user interaction methods, interaction modes, user interface design, etc. that strive to facilitate a user to seamlessly and intuitively interact with the system to accomplish his system goals. This spectrum usually considers aspects pertaining to bringing the user to initiate interaction with the system and retaining the user's interest to keep interacting with the system as an outcome of the usability/user experience that a user obtains while interacting with the system. In the context of crowdsourcing, these aspects have individual significance and not completely user experience based. Hence, it is essential to widen the spectrum of HCI for crowdsourcing to embrace these additional aspects explicitly and provide for a comprehensive HCI paradigm for crowdsourcingIn this paper, the conceptualization and definition of HCI paradigm for crowdsourcing has been described. The application of every aspect of the proposed paradigm is illustrated with respect to crowdsourcing of a typical form digitization task. The defined aspects could also serve as HCI evaluation parameters for business process task crowdsourcing},
booktitle = {Proceedings of the 11th Asia Pacific Conference on Computer Human Interaction},
pages = {264–273},
numpages = {10},
keywords = {HCI for crowdsourcing, HCI model for crowdsourcing, business process crowdsourcing, crowdsourcing, crowdsourcing HCI paradigm},
location = {Bangalore, India},
series = {APCHI '13}
}

@inproceedings{10.1145/1629255.1629296,
author = {Jagadeesan, A. P. and Lynn, A. and Corney, J. R. and Yan, X. T. and Wenzel, J. and Sherlock, A. and Regli, W.},
title = {Geometric reasoning via internet CrowdSourcing},
year = {2009},
isbn = {9781605587110},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1629255.1629296},
doi = {10.1145/1629255.1629296},
abstract = {The ability to interpret and reason about shapes is a peculiarly human capability that has proven difficult to reproduce algorithmically. So despite the fact that geometric modeling technology has made significant advances in the representation, display and modification of shapes, there have only been incremental advances in geometric reasoning. For example, although today's CAD systems can confidently identify isolated cylindrical holes, they struggle with more ambiguous tasks such as the identification of partial symmetries or similarities in arbitrary geometries. Even well defined problems such as 2D shape nesting or 3D packing generally resist elegant solution and rely instead on brute force explorations of a subset of the many possible solutions.Identifying economic ways to solving such problems would result in significant productivity gains across a wide range of industrial applications. The authors hypothesize that Internet Crowdsourcing might provide a pragmatic way of removing many geometric reasoning bottlenecks.This paper reports the results of experiments conducted with Amazon's mTurk site and designed to determine the feasibility of using Internet Crowdsourcing to carry out geometric reasoning tasks as well as establish some benchmark data for the quality, speed and costs of using this approach.After describing the general architecture and terminology of the mTurk Crowdsourcing system, the paper details the implementation and results of the following three investigations; 1) the identification of "Canonical" viewpoints for individual shapes, 2) the quantification of "similarity" relationships with-in collections of 3D models and 3) the efficient packing of 2D Strips into rectangular areas. The paper concludes with a discussion of the possibilities and limitations of the approach.},
booktitle = {2009 SIAM/ACM Joint Conference on Geometric and Physical Modeling},
pages = {313–318},
numpages = {6},
keywords = {2D strip packing, 3D similarity, canonical view, crowdsourcing, geometric reasoning, mTurk, micro-outsourcing},
location = {San Francisco, California},
series = {SPM '09}
}

@inproceedings{10.1109/ICGSE.2014.26,
author = {Wang, Hao and Wang, Yasha and Wang, Jiangtao},
title = {A Participant Recruitment Framework for Crowdsourcing Based Software Requirement Acquisition},
year = {2014},
isbn = {9781479943609},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/ICGSE.2014.26},
doi = {10.1109/ICGSE.2014.26},
abstract = {The opportunity to leverage crowd sourcing-based model to facilitate software requirements acquisition has been recognized to maximize the advantages of the diversity of talents and expertise available within the crowd. Identifying well-suited participants is a common issue in crowd sourcing system. Requirements acquisition tasks call for participants with particular kind of domain knowledge. However, current crowd sourcing system failed to provide such kind of identification among participants. We observed that participants with a particular kind of domain knowledge often have the opportunity to cluster in particular spatiotemporal spaces. Based on this observation, we propose a novel opportunistic participant recruitment framework to enable organizers to recruit participants with desired kind of domain knowledge in a more efficient way. We analyzed the feasibility of our opportunistic approach through both theoretic study on analytical model and simulated experiment on real world mobility model. The results showed the feasibility of our approach.},
booktitle = {Proceedings of the 2014 IEEE 9th International Conference on Global Software Engineering},
pages = {65–73},
numpages = {9},
keywords = {Crowdsourcing, Opportunistic Network},
series = {ICGSE '14}
}

@inproceedings{10.1145/1693042.1693093,
author = {Shah, Neeta and Dhanesha, Ashutosh and Seetharam, Dravida},
title = {Crowdsourcing for e-governance: case study},
year = {2009},
isbn = {9781605586632},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1693042.1693093},
doi = {10.1145/1693042.1693093},
abstract = {In this case study, we share our experience with creating and running a large scale initiative in the state of Gujarat in India wherein Crowdsourcing [1] was used to develop e-Governance applications. The case study provides details on the methodology used, lessons learnt, key success factors, challenges faced, and recommendations for future usage of crowdsourcing for building e-Governance applications.In this initiative, named INVITE, students were engaged in e-Governance application design and development for real project scenarios of various government departments. In a span of 2 years over 5000 students pursuing undergraduate and graduate courses from nearly 100 colleges and universities across the state participated. Over 500 faculty members also participated as project guides for the students who worked in teams of 3 or 4 members. Use of open source software tools was made mandatory. This community e-Governance initiative was supported by the three main pillars of the IT ecosystem - Government, Industry and Academia.A nodal IT agency of the state government documented 27 real world e-Governance project scenarios for various departments for the INVITE crowdsourcing experiment. Village kiosks for farmers, land records, animal husbandry, and museum administration are some of the areas for which the students developed applications. A programming contest was announced for attracting students to participate in the initiative.The state department of technical education provided impetus to INVITE by sending official circulars to heads of universities and colleges to encourage the students to take up the projects as part of their course requirements.An industry sponsor funded the basic infrastructure of a website for team registration and project submission, program management agency for mass communications, posters, and prizes for best applications for each project scenario. Provision was made to reward the faculty guides of winning teams and their institutes as well. As the student teams competed, the state government benefited by getting very good e-Governance applications developed for free by the crowdsourcing model.Various government departments nominated "resource persons" who interacted with the project teams to answer their queries about the prevailing manual systems and their vision of e-Governance for those functions.In summary, INVITE became a great platform for crowdsourcing resulting in "e-Governance for the people, by the people".},
booktitle = {Proceedings of the 3rd International Conference on Theory and Practice of Electronic Governance},
pages = {253–258},
numpages = {6},
keywords = {community, crowdsourcing, ecosystem, students},
location = {Bogota, Colombia},
series = {ICEGOV '09}
}

@inproceedings{10.1109/SOSE.2015.52,
author = {Assemi, Behrang and Schlagwein, Daniel and Safi, Hamid and Mesbah, Mahmoud},
title = {Crowdsourcing as a Method for the Collection of Revealed Preferences Data},
year = {2015},
isbn = {9781479983568},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/SOSE.2015.52},
doi = {10.1109/SOSE.2015.52},
abstract = {Crowdsourcing has been used widely for the collection of stated preference data (e.g., responses in a survey) by researchers. However, the use of crowdsourcing for collection of revealed preference data (e.g., real-life data collected in natural experiments) is much less common. The study reported in this short (research-in-progress) paper shows how crowdsourcing can be used as a method for the collection of revealed preference data in the context of transport studies. In transport studies, data is traditionally collected through surveys, diaries or simulations. Here, crowdsourcing could provide an alternative method that provides real-life data very fast and very cheap to researchers. To generate insights on crowdsourcing as an alternative data collection method, we use an open call on a crowdsourcing platform (Amazon Mechanical Turk - AMT), a mobile application (Advanced Travel Logging Application for Smartphones II - ATLAS II) and a participant survey to practically perform such a crowdsourced data collection and evaluate the effectiveness of the method. While the full study is still in progress, the initial results reported in this paper are promising and support the idea that crowdsourcing can indeed be used as an effective method for the collection of revealed preference data.},
booktitle = {Proceedings of the 2015 IEEE Symposium on Service-Oriented System Engineering},
pages = {378–382},
numpages = {5},
keywords = {Advanced Travel Logging Application for Smartphones II (ATLAS II), Amazon Mechanical Turk (AMT), Crowdsourcing, data collection, revealed preference data, transport studies},
series = {SOSE '15}
}

@inproceedings{10.1109/NBiS.2012.130,
author = {Korthaus, Axel and Dai, Wei},
title = {Crowdsourcing in Heterogeneous Networked Environments - Opportunities and Challenges},
year = {2012},
isbn = {9780769547794},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/NBiS.2012.130},
doi = {10.1109/NBiS.2012.130},
abstract = {Crowd sourcing has become a term used to describe a broad variety of effective approaches that engage potentially large and open crowds of participants for the undertaking of a task. The emergence of Heterogeneous Networks (Het Nets) that brings about a significant expansion of existing mobile network capacity will enable and accelerate new forms of mobile and ubiquitous crowd sourcing. This paper aims to explore some of the opportunities and challenges that will face crowd sourcing-based approaches in emerging heterogeneous networked environments. A conceptual architecture for a context-aware mobile crowd sourcing platform is proposed.},
booktitle = {Proceedings of the 2012 15th International Conference on Network-Based Information Systems},
pages = {483–488},
numpages = {6},
keywords = {crowdsourcing, crowdsourcing platform, heterogeneous networks, mobile crowdsourcing},
series = {NBIS '12}
}

@inproceedings{10.1109/WCNC.2016.7564930,
author = {Lauridsen, Mads and Rodriguez, Ignacio and Mikkelsen, Lars M⊘ller and Gimenez, Lucas Chavarria and Mogensen, Preben},
title = {Verification of 3G and 4G received power measurements in a crowdsourcing Android app},
year = {2016},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/WCNC.2016.7564930},
doi = {10.1109/WCNC.2016.7564930},
abstract = {Many crowdsourcing Android applications are available for measuring network Key Performance Indicators such as received power, latency, and throughput. The data is useful for end-users, researchers, and Mobile Network Operators, but unfortunately the applications' accuracy are rarely verified. In this paper we verify the crowdsourcing Android application NetMap's ability to measure LTE Reference Signal Received Power by analyzing the Root Mean Squared Error, being 2–3 dB, and cross-correlation coefficient, being above 0.8, with measurements obtained by use of a professional radio network scanner and measurement phones. In addition, the application is applicable, but less accurate, for 3G Received Signal Code Power measurements. The studies are made for various device speeds and in different scenarios including indoor, urban, and highway, where the NetMap application is showed to perform well.},
booktitle = {2016 IEEE Wireless Communications and Networking Conference},
pages = {1–6},
numpages = {6},
location = {Doha, Qatar}
}

@inproceedings{10.1145/2567948.2567951,
author = {Jung, Hyun Joon},
title = {Quality assurance in crowdsourcing via matrix factorization based task routing},
year = {2014},
isbn = {9781450327459},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2567948.2567951},
doi = {10.1145/2567948.2567951},
abstract = {We investigate a method of crowdsourced task routing based on matrix factorization. From a preliminary analysis of a real crowdsourced data, we begin an exploration of how to route crowdsourcing task via Matrix factorization (MF) which efficiently estimate missing values in a worker-task matrix. Our preliminary results show the benefits of task routing over random assignment, the strength of probabilistic MF over baseline methods.},
booktitle = {Proceedings of the 23rd International Conference on World Wide Web},
pages = {3–8},
numpages = {6},
keywords = {collaborative filtering, crowdsourcing, matrix factorization, quality assurance, recommendation, task routing},
location = {Seoul, Korea},
series = {WWW '14 Companion}
}

@inproceedings{10.1145/3240431.3240445,
author = {Alvarez, Julian},
title = {Datagame: Crowdsourcing, Metrics \&amp; Traces},
year = {2018},
isbn = {9781450364386},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3240431.3240445},
doi = {10.1145/3240431.3240445},
abstract = {This paper aims to present the concept of Datagame, a category of Serious Game associated with data exchange. After having exposed the concept of Datagame, presented subcategories and associated titles, reviewed the notion of crowdsourcing, metrics and traces, we will explore if such games relativize the unproductive criteria of Caillois and if direct benefits could be addressed to players.},
booktitle = {Proceedings of the 2nd International Conference on Web Studies},
pages = {72–76},
numpages = {5},
keywords = {Digital tools, digital methodology, tool criticism},
location = {Paris, France},
series = {WS.2 2018}
}

@inproceedings{10.1145/2030100.2030103,
author = {Mashhadi, Afra J. and Capra, Licia},
title = {Quality control for real-time ubiquitous crowdsourcing},
year = {2011},
isbn = {9781450309271},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2030100.2030103},
doi = {10.1145/2030100.2030103},
abstract = {Crowdsourcing has become a successful paradigm in the past decade, as Web 2.0 users have taken a more active role in producing content as well as consuming it. Recently this paradigm has broadened to incorporate ubiquitous applications, in which the smart-phone users contribute information about their surrounding, thus providing a collective knowledge about the physical world. However the acceptance and openness of such applications has made it easy to contribute poor quality content. Various solutions have been proposed for the Web-based domain, to assist with monitoring and filtering poor quality content, but these methods fall short when applied to ubiquitous crowdsourcing, where the task of collecting information has to be performed continuously and in real-time, by an always changing crowd. In this paper we discuss the challenges for quality control in ubiquitous crowdsorucing and propose a novel technique that reasons on users mobility patterns and quality of their past contributions to estimate user's credibility.},
booktitle = {Proceedings of the 2nd International Workshop on Ubiquitous Crowdsouring},
pages = {5–8},
numpages = {4},
keywords = {crowdsourcing, participation, quality assurance},
location = {Beijing, China},
series = {UbiCrowd '11}
}

@inproceedings{10.1145/3014087.3014112,
author = {Nikiforov, Alexander and Singireja, Anastasija},
title = {Open data and crowdsourcing perspectives for smart city in the United States and Russia},
year = {2016},
isbn = {9781450348591},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3014087.3014112},
doi = {10.1145/3014087.3014112},
abstract = {In this research paper we describe the transformation of open data strategy and implementation of crowdsourcing technologies for the city E-government services. Analysis of smart city projects provides the role of open data and crowdsourcing for smart city vision in United States and Russia. We define challenges and perspectives for collaboration of open data and crowdsourcing in smart city projects.},
booktitle = {Proceedings of the International Conference on Electronic Governance and Open Society: Challenges in Eurasia},
pages = {171–177},
numpages = {7},
keywords = {civic issue tracker, crowdsourcing, e-government, government 2.0, open data, open innovations, smart city},
location = {St. Petersburg, Russia},
series = {EGOSE '16}
}

@inproceedings{10.1145/1935826.1935828,
author = {Carvalho, Vitor R. and Lease, Matthew and Yilmaz, Emine},
title = {Crowdsourcing for search and data mining},
year = {2011},
isbn = {9781450304931},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1935826.1935828},
doi = {10.1145/1935826.1935828},
abstract = {The advent of crowdsourcing is revolutionizing data annotation, evaluation, and other traditionally manual-labor intensive processes by dramatically reducing the time, cost, and effort involved. This in turn is driving a disruptive shift in search and data mining methodology in areas such as: Evaluation: the Cranfield paradigm for search evaluation requires manually assessing document relevance to search queries. Recent work on stochastic evaluation has reduced but not removed this need for manual assessment.Supervised Learning: while traditional costs associated with data annotation have driven recent machine learning work (e.g. Learning to Rank) toward greater use of unsupervised and semi-supervised methods, the emergence of crowdsourcing has made labeled data far easier to acquire, thereby driving a potential resurgence in fully-supervised methods.Applications: Crowdsourcing has introduced exciting new opportunities to integrate human labor into automated systems: handling difficult cases where automation fails, exploiting the breadth of backgrounds, geographic dispersion, real-time response, etc.},
booktitle = {Proceedings of the Fourth ACM International Conference on Web Search and Data Mining},
pages = {5–6},
numpages = {2},
keywords = {crowdsourcing, data mining, search},
location = {Hong Kong, China},
series = {WSDM '11}
}

@inproceedings{10.1145/2018602.2018617,
author = {Bischof, Zachary S. and Otto, John S. and S\'{a}nchez, Mario A. and Rula, John P. and Choffnes, David R. and Bustamante, Fabi\'{a}n E.},
title = {Crowdsourcing ISP characterization to the network edge},
year = {2011},
isbn = {9781450308007},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2018602.2018617},
doi = {10.1145/2018602.2018617},
abstract = {Evaluating and characterizing Internet Service Providers (ISPs) is critical to subscribers shopping for alternative ISPs, companies providing reliable Internet services, and governments surveying the coverage of broadband services to its citizens. Ideally, ISP characterization should be done at scale, continuously, and from end users. While there has been significant progress toward this end, current approaches exhibit apparently unavoidable tradeoffs between coverage, continuous monitoring and capturing user-perceived performance.In this paper, we argue that network-intensive applications running on end systems avoid these tradeoffs, thereby offering an ideal platform for ISP characterization. Based on data collected from 500,000 peer-to-peer BitTorrent users across 3,150 networks, together with the reported results from the U.K. Ofcom/SamKnows studies, we show the feasibility of this approach to characterize the service that subscribers can expect from a particular ISP. We discuss remaining research challenges and design requirements for a solution that enables efficient and accurate ISP characterization at an Internet scale.},
booktitle = {Proceedings of the First ACM SIGCOMM Workshop on Measurements up the Stack},
pages = {61–66},
numpages = {6},
keywords = {broadband access networks, characterization, isp},
location = {Toronto, Ontario, Canada},
series = {W-MUST '11}
}

@inproceedings{10.1145/2348283.2348530,
author = {Lease, Matthew and Alonso, Omar},
title = {Crowdsourcing for search evaluation and social-algorithmic search},
year = {2012},
isbn = {9781450314725},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2348283.2348530},
doi = {10.1145/2348283.2348530},
abstract = {The first computers were people. Today, Internet-based access to 24/7 online human crowds has led to a renaissance of research in human computation and the advent of crowdsourcing. These new opportunities have brought a disruptive shift to research and practice for how we build intelligent systems today. Not only can labeled data for training and evaluation be collected faster, cheaper, and easier than ever before, but we now see human computation being integrated into the systems themselves, operating in concert with automation. This tutorial introduces opportunities and challenges of human computation and crowdsourcing, particularly for search evaluation and developing hybrid search solutions that integrate human computation with traditional forms of automated search. We review methodology and findings of recent research and survey current generation crowdsourcing platforms now available, analyzing methods, potential, and limitations across platforms.},
booktitle = {Proceedings of the 35th International ACM SIGIR Conference on Research and Development in Information Retrieval},
pages = {1180},
numpages = {1},
keywords = {crowdsourcing, human computation},
location = {Portland, Oregon, USA},
series = {SIGIR '12}
}

@inproceedings{10.1145/2676440.2676441,
author = {Chang, Kyungmin and Han, Dongsoo},
title = {Crowdsourcing-based radio map update automation for wi-fi positioning systems},
year = {2014},
isbn = {9781450331333},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2676440.2676441},
doi = {10.1145/2676440.2676441},
abstract = {With the popularization of smartphones, the needs for indoor location information are rapidly growing these days. Wi-Fi based positioning technique has been widely used to provide positioning information indoors. In particular, fingerprint-based localization is preferred because of its advantage in accuracy. However, the accuracy of localization gradually degrades as the Wi-Fi environment changes. In order to prevent the accuracy degradation, a radio map, which stores the Wi-Fi environment information, should be updated to accommodate the changes. Recalibration is commonly used to update a radio map, but it usually requires considerable time and effort. In this paper, we propose a method that can update the radio map automatically by using fingerprints collected from numerous users. In order to tag the locations of the collected fingerprints more accurately, the data from various sensors such as accelerometer and gyroscope are used. The proposed method also uses optimization algorithms along with a filtering method to remove erroneous data. The evaluation results showed that the accuracy achieved by the method was comparable to that of manual calibration in spite of using user feedback data. This indicates that without recalibration, the fingerprint-based indoor positioning system can keep up its accuracy if it is used by many users.},
booktitle = {Proceedings of the 3rd ACM SIGSPATIAL International Workshop on Crowdsourced and Volunteered Geographic Information},
pages = {24–31},
numpages = {8},
keywords = {auto-update, crowdsourcing, fingerprinting, radio map, wi-fi-based positioning system},
location = {Dallas, Texas},
series = {GeoCrowd '14}
}

@inproceedings{10.5555/2634433.2635066,
author = {Tsai, Wei-Tek and Qi, Guanqiu},
title = {A Cloud-Based Platform for Crowdsourcing and Self-Organizing Learning},
year = {2014},
isbn = {9781479936168},
publisher = {IEEE Computer Society},
address = {USA},
abstract = {This paper explores the application of Self-Organizing Learning (SOL) to software crowdsourcing so that people can learn software design. SOL principles include communication, reflection, collaboration, community, creative tools, and amplification. Based on these principles, this project proposed a cloud-based environment to support people to learn software design based on crowdsourcing including crowdsourcing competitions.},
booktitle = {Proceedings of the 2014 IEEE 8th International Symposium on Service Oriented System Engineering},
pages = {454–458},
numpages = {5},
keywords = {Cloud, Crowdsourcing, SOL},
series = {SOSE '14}
}

@inproceedings{10.1007/978-3-642-40495-5_10,
author = {Lykourentzou, Ioanna and Vergados, Dimitrios J. and Papadaki, Katerina and Naudet, Yannick},
title = {Guided Crowdsourcing for Collective Work Coordination in Corporate Environments},
year = {2013},
isbn = {9783642404948},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-642-40495-5_10},
doi = {10.1007/978-3-642-40495-5_10},
abstract = {Crowdsourcing is increasingly gaining attention as one of the most promising forms of large-scale dynamic collective work. However current crowdsourcing approaches do not offer guarantees often demanded by consumers, for example regarding minimum quality, maximum cost or job accomplishment time. The problem appears to have a greater impact in corporate environments because in this case the above-mentioned performance guarantees directly affect its viability against competition. Guided crowdsourcing can be an alternative to overcome these issues. Guided crowdsourcing refers to the use of Artificial Intelligence methods to coordinate workers in crowdsourcing settings, in order to ensure collective performance goals such as quality, cost or time. In this paper, we investigate its potential and examine it on an evaluation setting tailored for intra and inter-corporate environments.},
booktitle = {Proceedings of the 5th International Conference on Computational Collective Intelligence. Technologies and Applications - Volume 8083},
pages = {90–99},
numpages = {10},
keywords = {crowd coordination, crowdsourcing, resource allocation},
location = {Craiova, Romania},
series = {ICCCI 2013}
}

@inproceedings{10.1109/IPDPSW.2013.133,
author = {Sistla, AnilKumar and Parde, Natalie and Patel, Krunalkumar and Mehta, Gayatri},
title = {Cross-Architectural Study of Custom Reconfigurable Devices Using Crowdsourcing},
year = {2013},
isbn = {9780769549798},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/IPDPSW.2013.133},
doi = {10.1109/IPDPSW.2013.133},
abstract = {Coarse grained reconfigurable architectures (CGRAs) are promising due to the ability to highly customize such architectures to an application domain. However, good tools and good algorithms to map benchmarks onto these architectures are needed to support design space exploration for CGRAs. In particular, the mapping problem has been difficult to solve in a satisfying and general way. In this paper, we present an architectural design flow using crowd sourcing to provide mappings of benchmarks onto new architectures. We show that the crowd can provide high quality, reliable mappings, outperforming our custom Simulated Annealing algorithm in 37 of 42 trials. We further show that the crowd can provide other types of feedback that are difficult to obtain from an automatic mapping algorithm. Our proof of concept cross-architectural study concludes that a mesh architecture with 8Way connectivity outperforms the other interconnection options tested. A stripe architecture with dedicated vertical routes (StripeDR) performs competitively as well.},
booktitle = {Proceedings of the 2013 IEEE 27th International Symposium on Parallel and Distributed Processing Workshops and PhD Forum},
pages = {222–230},
numpages = {9},
keywords = {coarse grained reconfigurable architectures, cross-architecture, crowdsourcing, design space exploration, domain specific reconfigurable architectures},
series = {IPDPSW '13}
}

@inproceedings{10.1145/1873951.1874278,
author = {Snoek, Cees G.M. and Freiburg, Bauke and Oomen, Johan and Ordelman, Roeland},
title = {Crowdsourcing rock n' roll multimedia retrieval},
year = {2010},
isbn = {9781605589336},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1873951.1874278},
doi = {10.1145/1873951.1874278},
abstract = {In this technical demonstration, we showcase a multimedia search engine that facilitates semantic access to archival rock n' roll concert video. The key novelty is the crowdsourcing mechanism, which relies on online users to improve, extend, and share, automatically detected results in video fragments using an advanced timeline-based video player. The user-feedback serves as valuable input to further improve automated multimedia retrieval results, such as automatically detected concepts and automatically transcribed interviews. The search engine has been operational online to harvest valuable feedback from rock n' roll enthusiasts.},
booktitle = {Proceedings of the 18th ACM International Conference on Multimedia},
pages = {1535–1538},
numpages = {4},
keywords = {information visualization, semantic indexing, video retrieval},
location = {Firenze, Italy},
series = {MM '10}
}

@inproceedings{10.1145/2488388.2488490,
author = {Singla, Adish and Krause, Andreas},
title = {Truthful incentives in crowdsourcing tasks using regret minimization mechanisms},
year = {2013},
isbn = {9781450320351},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2488388.2488490},
doi = {10.1145/2488388.2488490},
abstract = {What price should be offered to a worker for a task in an online labor market? How can one enable workers to express the amount they desire to receive for the task completion? Designing optimal pricing policies and determining the right monetary incentives is central to maximizing requester's utility and workers' profits. Yet, current crowdsourcing platforms only offer a limited capability to the requester in designing the pricing policies and often rules of thumb are used to price tasks. This limitation could result in inefficient use of the requester's budget or workers becoming disinterested in the task.In this paper, we address these questions and present mechanisms using the approach of regret minimization in online learning. We exploit a link between procurement auctions and multi-armed bandits to design mechanisms that are budget feasible, achieve near-optimal utility for the requester, are incentive compatible (truthful) for workers and make minimal assumptions about the distribution of workers' true costs. Our main contribution is a novel, no-regret posted price mechanism, BP-UCB, for budgeted procurement in stochastic online settings. We prove strong theoretical guarantees about our mechanism, and extensively evaluate it in simulations as well as on real data from the Mechanical Turk platform. Compared to the state of the art, our approach leads to a 180\% increase in utility.},
booktitle = {Proceedings of the 22nd International Conference on World Wide Web},
pages = {1167–1178},
numpages = {12},
keywords = {crowdsourcing, incentive compatible mechanisms, multi-armed bandits, posted prices, procurement auctions, regret minimization},
location = {Rio de Janeiro, Brazil},
series = {WWW '13}
}

@inproceedings{10.1109/WI-IAT.2012.104,
author = {Yu, Han and Shen, Zhiqi and Miao, Chunyan and An, Bo},
title = {Challenges and Opportunities for Trust Management in Crowdsourcing},
year = {2012},
isbn = {9780769548807},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/WI-IAT.2012.104},
doi = {10.1109/WI-IAT.2012.104},
abstract = {Crowd sourcing (CS) systems offer a new way for businesses and individuals to leverage on the power of mass collaboration to accomplish complex tasks in a divide-and-conquer manner. In existing CS systems, no facility has been provided for analyzing the trustworthiness of workers and providing decision support for allocating tasks to workers, which leads to high dependency of the quality of work on the behavior of workers in CS systems as shown in this paper. To address this problem, trust management mechanisms are urgently needed. Traditional trust management techniques are focused on identifying the most trustworthy service providers (SPs) as accurately as possible. Little thoughts were given to the question of how to utilize these SPs due to two common assumptions: 1) an SP can serve an unlimited number of requests in one time unit, and 2) a service consumer (SC) only needs to select one SP for interaction to complete a task. However, in CS systems, these two assumptions are no longer valid. Thus, existing models cannot be directly used for trust management in CS systems. This paper takes the first step towards a systematic investigation of trust management in CS systems by extending existing trust management models for CS trust management and conducting extensive experiments to study and analyze the performance of various trust management models in crowd sourcing. In this paper, the following key contributions are made. We 1) propose extensions to existing trust management approaches to enable them to operate in CS systems, 2) design a simulation test-bed based on the system characteristics of Amazon's Mechanical Turk (AMT) to make evaluation close to practical CS systems, 3) discuss the effect of incorporating trust management into CS system on the overall social welfare, and 4) identify the challenges and opportunities for future trust management research in CS systems.},
booktitle = {Proceedings of the The 2012 IEEE/WIC/ACM International Joint Conferences on Web Intelligence and Intelligent Agent Technology - Volume 02},
pages = {486–493},
numpages = {8},
keywords = {crowdsourcing, interaction decision making, reputation, trust},
series = {WI-IAT '12}
}

@inproceedings{10.1145/2141512.2141572,
author = {Noble, Jennifer A.},
title = {Minority voices of crowdsourcing: why we should pay attention to every member of the crowd},
year = {2012},
isbn = {9781450310512},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2141512.2141572},
doi = {10.1145/2141512.2141572},
abstract = {In this paper I look at the dynamics of human behavior in crowds, focusing the role of non-normative voices in current crowdsourcing initiatives. I reiterate the idea that the power of the crowd lies not in the majority but in the collective. Many popular crowdsourcing platforms are designed to disregard outliers and only reward answers that agree with the masses. I use the example of the Long Tail in order to challenge developers to design more crowdsourcing tasks that take advantage of wide variance.},
booktitle = {Proceedings of the ACM 2012 Conference on Computer Supported Cooperative Work Companion},
pages = {179–182},
numpages = {4},
keywords = {business, crowd motivation, crowdsourcing, literature review, social science, the long tail, theory},
location = {Seattle, Washington, USA},
series = {CSCW '12}
}

@inproceedings{10.1109/HICSS.2014.179,
author = {Fichman, Pnina and Hara, Noriko and Rosenbaum, Howard},
title = {Introduction to Crowdsourcing Content Production and Online Knowledge Repositories Minitrack},
year = {2014},
isbn = {9781479925049},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/HICSS.2014.179},
doi = {10.1109/HICSS.2014.179},
abstract = {As various forms of collaboration are enabled (and constrained) by the affordances available in social media, researchers are investigating a range of issues including: 1) the diverse ways in which people collaborate to create, manage, curate and manipulate online content and how these activities affect digital repositories, 2) how those who manage these repositories are responding to the co-creation of online content 3) the dynamics of crowd sourced online collaborations and online communities of practice, and 4) the ways in which we can best describe the socio-technical interaction networks that facilitate and inhibit mass knowledge production. This minitrack focuses on online interactions for knowledge production on crowd sourced sites.},
booktitle = {Proceedings of the 2014 47th Hawaii International Conference on System Sciences},
pages = {1385},
series = {HICSS '14}
}

@inproceedings{10.1145/2393347.2396539,
author = {Chen, Kuan-Ta and Chu, Wei-Ta and Larson, Martha and Ooi, Wei Tsang},
title = {ACM multimedia 2012 workshop on crowdsourcing for multimedia},
year = {2012},
isbn = {9781450310895},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2393347.2396539},
doi = {10.1145/2393347.2396539},
abstract = {Crowdsourcing for multimedia involves exploiting both human intelligence and the combination of a large number of individual human contributions (i.e., the 'wisdom of the crowd') to develop techniques, systems and data sets that advance the state of the art. The ACM Multimedia 2012 Workshop on Crowdsourcing for Multimedia (CrowdMM 2012) provides a forum presenting crowdsourcing techniques for multimedia, as well as innovative ideas exemplifying how multimedia research can benefit from crowdsourcing. Through presented papers, invited talks and a panel, the workshop will promote interactive discussion on the scope and research potentials of crowdsourcing. The goal is to provide information to the multimedia research community on the principles of crowdsourcing and to inspire researchers to address the limitations of current studies by innovative use of human computation and collective intelligence. The workshop views crowdsourcing in the broad sense: it encompasses both unsolicited human contributions, e.g., tags assigned by users to images, and also solicited contributions, e.g., annotations gathered by making use of crowdsourcing platforms that micro-outsource tasks to a large pool of human workers.},
booktitle = {Proceedings of the 20th ACM International Conference on Multimedia},
pages = {1505–1506},
numpages = {2},
keywords = {crowdsourcing, human computing, multimedia system},
location = {Nara, Japan},
series = {MM '12}
}

@inproceedings{10.5555/2986459.2986749,
author = {Abernethy, Jacob and Frongillo, Rafael M.},
title = {A collaborative mechanism for crowdsourcing prediction problems},
year = {2011},
isbn = {9781618395993},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Machine Learning competitions such as the Netflix Prize have proven reasonably successful as a method of "crowdsourcing" prediction tasks. But these competitions have a number of weaknesses, particularly in the incentive structure they create for the participants. We propose a new approach, called a Crowdsourced Learning Mechanism, in which participants collaboratively "learn" a hypothesis for a given prediction task. The approach draws heavily from the concept of a prediction market, where traders bet on the likelihood of a future event. In our framework, the mechanism continues to publish the current hypothesis, and participants can modify this hypothesis by wagering on an update. The critical incentive property is that a participant will profit an amount that scales according to how much her update improves performance on a released test set.},
booktitle = {Proceedings of the 24th International Conference on Neural Information Processing Systems},
pages = {2600–2608},
numpages = {9},
location = {Granada, Spain},
series = {NIPS'11}
}

@proceedings{10.1145/2660114,
title = {CrowdMM '14: Proceedings of the 2014 International ACM Workshop on Crowdsourcing for Multimedia},
year = {2014},
isbn = {9781450331289},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {The power of crowds, leveraging a large number of human contributors and the capabilities of human computation, has enormous potential to address key challenges in the area of multimedia research. This power is, however, of difficult exploitation: challenges arise from the fact that a community of users or workers is a complex and dynamic system highly sensitive to changes in the form and the parameterization of their activities. Since 2012, the International ACM Workshop on Crowdsourcing for Multimedia, CrowdMM, has been the venue for collecting new insights on the effective deployment of crowdsourcing towards boosting Multimedia research.In its third edition, CrowdMM14 especially focuses on contributions that propose solutions for the key challenges that face widespread adoption of crowdsourcing paradigms in the multimedia research community. These include: identification of optimal crowd members (e.g., user expertise, worker reliability), providing effective explanations (i.e., good task design), controlling noise and quality in the results, designing incentive structures that do not breed cheating, adversarial environments, gathering necessary background information about crowd members without violating privacy, controlling descriptions of task.The call for papers attracted 26 international submissions (62\% increase with respect to the 2013 edition), three of which were short paper submissions. Of these, 8 were accepted as oral presentations and 5 as poster presentations. All papers received at least three double blind reviews, and 3.4 reviews on average.For a keynote talk, Nhatvi Nguyen (CEO of Microworkers) talks about Crowdsourcing Challenges from Platform Provider's Point of View. In addition to that CrowdMM features this year a crowd-sourced keynote, during which all CrowdMM14 authors give their view on the future and the Challenges that Crowdsourcing has still ahead.},
location = {Orlando, Florida, USA}
}

@inproceedings{10.1145/1816123.1816143,
author = {Eckert, Kai and Niepert, Mathias and Niemann, Christof and Buckner, Cameron and Allen, Colin and Stuckenschmidt, Heiner},
title = {Crowdsourcing the assembly of concept hierarchies},
year = {2010},
isbn = {9781450300858},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1816123.1816143},
doi = {10.1145/1816123.1816143},
abstract = {The "wisdom of crowds" is accomplishing tasks that are cumbersome for individuals yet cannot be fully automated by means of specialized computer algorithms. One such task is the construction of thesauri and other types of concept hierarchies. Human expert feedback on the relatedness and relative generality of terms, however, can be aggregated to dynamically construct evolving concept hierarchies. The InPhO (Indiana Philosophy Ontology) project bootstraps feedback from volunteer users unskilled in ontology design into a precise representation of a specific domain. The approach combines statistical text processing methods with expert feedback and logic programming to create a dynamic semantic representation of the discipline of philosophy.In this paper, we show that results of comparable quality can be achieved by leveraging the workforce of crowdsourcing services such as the Amazon Mechanical Turk (AMT). In an extensive empirical study, we compare the feedback obtained from AMT's workers with that from the InPhO volunteer users providing an insight into qualitative differences of the two groups. Furthermore, we present a set of strategies for assessing the quality of different users when gold standards are missing. We finally use these methods to construct a concept hierarchy based on the feedback acquired from AMT workers.},
booktitle = {Proceedings of the 10th Annual Joint Conference on Digital Libraries},
pages = {139–148},
numpages = {10},
keywords = {crowdsourcing, similarity, thesaurus learning},
location = {Gold Coast, Queensland, Australia},
series = {JCDL '10}
}

@inproceedings{10.4108/icst.collaboratecom.2012.250412,
author = {Westerski, A. and Iglesias, C. A. and Garcia, J. E.},
title = {Idea relationship analysis in open innovation crowdsourcing systems},
year = {2012},
isbn = {9781467327404},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.4108/icst.collaboratecom.2012.250412},
doi = {10.4108/icst.collaboratecom.2012.250412},
abstract = {Idea Management Systems are an implementation of open innovation notion in the Web environment with the use of crowdsourcing techniques. In this area, one of the popular methods for coping with large amounts of data is duplicate detection. With our research, we answer a question if there is room to introduce more relationship types and in what degree would this change affect the amount of idea metadata and its diversity. Furthermore, based on hierarchical dependencies between idea relationships and relationship transitivity we propose a number of methods for dataset summarization. To evaluate our hypotheses we annotate idea datasets with new relationships using the contemporary methods of Idea Management Systems to detect idea similarity. Having datasets with relationship annotations at our disposal, we determine if idea features not related to idea topic (e.g. innovation size) have any relation to how annotators perceive types of idea similarity or dissimilarity.},
booktitle = {Proceedings of the 2012 8th International Conference on Collaborative Computing: Networking, Applications and Worksharing (CollaborateCom 2012)},
pages = {289–296},
numpages = {8},
series = {COLLABORATECOM '12}
}

@inproceedings{10.1145/2393132.2393159,
author = {K\"{a}rkk\"{a}inen, Hannu and Jussila, Jari and Multasuo, Jani},
title = {Can crowdsourcing really be used in B2B innovation?},
year = {2012},
isbn = {9781450316378},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2393132.2393159},
doi = {10.1145/2393132.2393159},
abstract = {The aim of this research is to explore the use of crowdsourcing especially from business-to-business companies' innovation perspective, and to create a more comprehensive picture of the possibilities of crowdsourcing for companies operating in business-to-business markets. Business-to-business context was chosen because it is in many ways a very different environment for crowdsourcing than business-to-consumer context, and we found no academic studies on the topic. A systematic literature review was performed to gain an understanding of the state-of-the-art, and to create a research framework on the concept of crowdsourcing in innovation covering e.g. the type of crowdsourcing used, which crowds were used and in what innovation process phase crowds were utilized. Concerning the current ways of using 'crowds' and crowdsourcing in B2B innovation process, we found evidence of using crowdsourcing in B2B's in all the three innovation process phases: front-end, product development, and commercialization. Furthermore, evidence was found for crowdsourcing to be used in innovation mainly in the manner of crowd creation, crowd wisdom and crowd funding. We found that the role of social media was quite essential in all the found B2B crowdsourcing examples.},
booktitle = {Proceeding of the 16th International Academic MindTrek Conference},
pages = {134–141},
numpages = {8},
keywords = {business-to-business, crowdsourcing, innovation, social media},
location = {Tampere, Finland},
series = {MindTrek '12}
}

@inproceedings{10.1007/978-3-642-25535-9_20,
author = {Khazankin, Roman and Psaier, Harald and Schall, Daniel and Dustdar, Schahram},
title = {QoS-Based task scheduling in crowdsourcing environments},
year = {2011},
isbn = {9783642255342},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-642-25535-9_20},
doi = {10.1007/978-3-642-25535-9_20},
abstract = {Crowdsourcing has emerged as an important paradigm in human-problem solving techniques on the Web. One application of crowdsourcing is to outsource certain tasks to the crowd that are difficult to implement as solutions based on software services only. Another benefit of crowdsourcing is the on-demand allocation of a flexible workforce. Businesses may outsource certain tasks to the crowd based on workload variations. The paper addresses the monitoring of crowd members' characteristics and the effective use of monitored data to improve the quality of work. Here we propose the extensions of standards such as Web Service Level Agreement (WSLA) to settle quality guarantees between crowd consumers and the crowdsourcing platform. Based on negotiated agreements, we provide a skill-based crowd scheduling algorithm. We evaluate our approach through simulations.},
booktitle = {Proceedings of the 9th International Conference on Service-Oriented Computing},
pages = {297–311},
numpages = {15},
keywords = {QoS agreements, crowdsourcing, scheduling, skill monitoring},
location = {Paphos, Cyprus},
series = {ICSOC'11}
}

@inproceedings{10.1145/2461121.2461129,
author = {Cardonha, Carlos and Gallo, Diego and Avegliano, Priscilla and Herrmann, Ricardo and Koch, Fernando and Borger, Sergio},
title = {A crowdsourcing platform for the construction of accessibility maps},
year = {2013},
isbn = {9781450318440},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2461121.2461129},
doi = {10.1145/2461121.2461129},
abstract = {We present in this article a crowdsourcing platform that enables the collaborative creation of accessibility maps. The platform provides means for integration of different kind of data, collected automatically or with user intervention, to augment standard maps with accessibility information. The article shows the architecture of the platform, dedicating special attention to the smartphone applications we developed for data collection. The article also describes a preliminar experiment conducted on field, showing how the analysis of data produced by our solution can bring novel insights in accessibility challenges that can be found in cities.},
booktitle = {Proceedings of the 10th International Cross-Disciplinary Conference on Web Accessibility},
articleno = {26},
numpages = {4},
keywords = {accessibility, breadcrumb, citizen sensing, crowdsourcing, data collection, mobile, smarter cities},
location = {Rio de Janeiro, Brazil},
series = {W4A '13}
}

@inproceedings{10.1145/2818346.2820748,
author = {Misu, Teruhisa},
title = {Visual Saliency and Crowdsourcing-based Priors for an In-car Situated Dialog System},
year = {2015},
isbn = {9781450339124},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2818346.2820748},
doi = {10.1145/2818346.2820748},
abstract = {This paper addresses issues in situated language understanding in a moving car. We propose a reference resolution method to identify user queries about specific target objects in their surroundings. We investigate methods of predicting which target object is likely to be queried given a visual scene and what kind of linguistic cues users naturally provide to describe a given target object in a situated environment. We propose methods to incorporate the visual saliency of the visual scene as a prior. Crowdsourced statistics of how people describe an object are also used as a prior. We have collected situated utterances from drivers using our research system, which was embedded in a real vehicle. We demonstrate that the proposed algorithms improve target identification rate by 15.1\%.},
booktitle = {Proceedings of the 2015 ACM on International Conference on Multimodal Interaction},
pages = {75–82},
numpages = {8},
keywords = {crowdsourcing, in-car interaction, multimodal interaction, situated dialog, visual saliency},
location = {Seattle, Washington, USA},
series = {ICMI '15}
}

@inproceedings{10.1007/978-3-642-39402-7_20,
author = {Donath, Axel and Kondermann, Daniel},
title = {Is crowdsourcing for optical flow ground truth generation feasible?},
year = {2013},
isbn = {9783642394010},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-642-39402-7_20},
doi = {10.1007/978-3-642-39402-7_20},
abstract = {In 2012, three new optical flow reference datasets have been published, two of them containing ground truth [1,2,3]. None of them contains ground truth for real-world, large-scale outdoor scenes with dynamically and independently moving objects. The reason is that no measurement devices exists to record such data with sufficiently high accuracy. Yet, ground truth is needed to assess the safety of e.g. driver assistance systems. To close this gap, based on existing, accurate ground truth, we analyse the performance of uninformed human motion annotators. Feature annotation bias and non-rigid motions are a major concern, limiting our results to pixel-accuracy. Our approach is the only way to create ground truth for dynamic outdoor sequences and feasible whenever pixel-accuracy is sufficient for performance analysis and piecewise rigid motions dominate the scene. Finally, we show that our approach is highly effective with respect to annotation cost per frame compared to our baseline method [4].},
booktitle = {Proceedings of the 9th International Conference on Computer Vision Systems},
pages = {193–202},
numpages = {10},
location = {St. Petersburg, Russia},
series = {ICVS'13}
}

@inproceedings{10.1109/CBI.2013.66,
author = {Tsaplin, Evgeny and Bushelenkova, Svetlana and Puchkova, Alexandra},
title = {Crowdsourcing in Telework as a New Scalable Business Model},
year = {2013},
isbn = {9780769550725},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/CBI.2013.66},
doi = {10.1109/CBI.2013.66},
abstract = {Business processes in our modern world flow with greater speed. This actual trend influences on the business from both sides: positive and negative. Some approaches, such as web 3.0, crowdsourcing have intention to create a comfortable ecosystem for business. In this article, we consider how appropriate data management helps building successful business.},
booktitle = {Proceedings of the 2013 IEEE 15th Conference on Business Informatics},
pages = {412–415},
numpages = {4},
keywords = {crowdsourcing, human recourse management, microwork, scalable business, semantic web, telemarketing, telework, web 3.0, web mining},
series = {CBI '13}
}

@inproceedings{10.5555/2050728.2050764,
author = {Sautter, Guido and B\"{o}hm, Klemens},
title = {High-throughput crowdsourcing mechanisms for complex tasks},
year = {2011},
isbn = {9783642247033},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {Crowdsourcing is popular for large-scale data processing endeavors that require human input. However, working with a large community of users raises new challenges. In particular, both possible misjudgment and dishonesty threaten the quality of the results. Common countermeasures are based on redundancy, giving way to a tradeoff between result quality and throughput. Ideally, measures should (1) maintain high throughput and (2) ensure high result quality at the same time. Existing work on crowdsourcing mostly focuses on result quality, paying little attention to throughput or even to that tradeoff. One reason is that the number of tasks (individual atomic units of work) is usually small. A further problem is that the tasks users work on are small as well. In consequence, existing result-improvement mechanisms do not scale to the number or complexity of tasks that arise, for instance, in proofreading and processing of digitized legacy literature. This paper proposes novel resultimprovement mechanisms that (1) are independent of the size and complexity of tasks and (2) allow to trade result quality for throughput to a significant extent. Both mathematical analyses and extensive simulations show the effectiveness of the proposed mechanisms.},
booktitle = {Proceedings of the Third International Conference on Social Informatics},
pages = {240–254},
numpages = {15},
keywords = {crowdsourcing, data quality, throughput},
location = {Singapore},
series = {SocInfo'11}
}

@inproceedings{10.1145/1807342.1807376,
author = {Horton, John Joseph and Chilton, Lydia B.},
title = {The labor economics of paid crowdsourcing},
year = {2010},
isbn = {9781605588223},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1807342.1807376},
doi = {10.1145/1807342.1807376},
abstract = {We present a model of workers supplying labor to paid crowdsourcing projects. We also introduce a novel method for estimating a worker's reservation wage - the key parameter in our labor supply model. We tested our model by presenting experimental subjects with real-effort work scenarios that varied in the offered payment and difficulty. As predicted, subjects worked less when the pay was lower. However, they did not work less when the task was more time-consuming. Interestingly, at least some subjects appear to be "target earners," contrary to the assumptions of the rational model. The strongest evidence for target earning is an observed preference for earning total amounts evenly divisible by 5, presumably because these amounts make good targets. Despite its predictive failures, we calibrate our model with data pooled from both experiments. We find that the reservation wages of our sample are approximately log normally distributed, with a median wage of $1.38/hour. We discuss how to use our calibrated model in applications.},
booktitle = {Proceedings of the 11th ACM Conference on Electronic Commerce},
pages = {209–218},
numpages = {10},
keywords = {amazon's mechanical turk, crowdsourcing, human computation},
location = {Cambridge, Massachusetts, USA},
series = {EC '10}
}

@inproceedings{10.1145/3557991.3567779,
author = {Luedemann, Kai and Nascimento, Mario A.},
title = {BikeVibes: An app for crowdsourcing open road quality data from a cyclist perspective},
year = {2022},
isbn = {9781450395397},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3557991.3567779},
doi = {10.1145/3557991.3567779},
abstract = {This paper presents BikeVibes1, an app that cyclists can use to log data regarding the smoothness of their rides. The main goal of BikeVibes is to facilitate the collection of anonymized open data about road quality that others can download and peruse. A few sample scenarios where having this type of crowdsourced data would be useful are as follows. A city can use the gathered data in order to determine which roads need to be maintained/upgraded since the quality of the road can be perceived very differently when riding a bike compared to driving a car. Likewise, a city can determine paths that are more frequently used by cyclists in order to decide where to build or upgrade dedicated bike lanes and/or how to prioritize maintenance. Also, third-party app developers can use the road quality data to suggest paths to cyclists based on smoothness, as this may be an important attribute for some people, e.g., in the case of parents riding bicycles hauling trailers with children. None of these scenarios could be easily contemplated without the availability of data such as that gathered through BikeVibes.},
booktitle = {Proceedings of the 15th ACM SIGSPATIAL International Workshop on Computational Transportation Science},
articleno = {8},
numpages = {4},
keywords = {crowdsensing, mobile app, open cycling data, road quality monitoring},
location = {Seattle, Washington},
series = {IWCTS '22}
}

@inproceedings{10.5555/2045274.2045277,
author = {Alonso, Omar},
title = {Crowdsourcing for information retrieval experimentation and evaluation},
year = {2011},
isbn = {9783642237072},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {Very recently, crowdsourcing has emerged as a viable alternative for conducting different types of experiments in a wide range of areas. Generally speaking and in the context of IR, crowdsourcing involves outsourcing tasks to a large group of people instead of assigning such tasks to an employee or editor. The availability of commercial crowdsourcing platforms offers vast access to an on-demand workforce. This new approach makes possible to conduct experiments extremely fast, with good results at a low cost. However, like in any experiment, there are several implementation details that would make an experiment work or fail. For large scale evaluation, deployment in practice is not that simple. Tasks have to be designed carefully with special emphasis on the user interface, instructions, content, and quality control.In this invited talk, I will explore some directions that may influence the outcome of a task and I will present a framework for conducting crowdsourcing experiments making some emphasis on a number of aspects that should be of importance for all sorts of IR-like tasks. Finally, I will outline research trends around human computation that promise to make this emerging field even more interesting in the near future.},
booktitle = {Proceedings of the Second International Conference on Multilingual and Multimodal Information Access Evaluation},
pages = {2},
numpages = {1},
location = {Amsterdam, The Netherlands},
series = {CLEF'11}
}

@inproceedings{10.1007/978-3-030-29387-1_25,
author = {Skorupska, Kinga and N\'{u}\~{n}ez, Manuel and Kope\'{c}, Wies\l{}aw and Nielek, Rados\l{}aw},
title = {A Comparative Study of Younger and Older Adults’ Interaction with a Crowdsourcing Android TV App for Detecting Errors in TEDx Video Subtitles},
year = {2019},
isbn = {978-3-030-29386-4},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-29387-1_25},
doi = {10.1007/978-3-030-29387-1_25},
abstract = {In this paper we report the results of a pilot study comparing the older and younger adults’ interaction with an Android TV application which enables users to detect errors in video subtitles. Overall, the interaction with the TV-mediated crowdsourcing system relying on language proficiency was seen as intuitive, fun and accessible, but also cognitively demanding; more so for younger adults who focused on the task of detecting errors, than for older adults who concentrated more on the meaning and edutainment aspect of the videos. We also discuss participants’ motivations and preliminary recommendations for the design of TV-enabled crowdsourcing tasks and subtitle QA systems.},
booktitle = {Human-Computer Interaction – INTERACT 2019: 17th IFIP TC 13 International Conference, Paphos, Cyprus, September 2–6, 2019, Proceedings, Part III},
pages = {455–464},
numpages = {10},
keywords = {Crowdsourcing, Smart TV, Android TV, Design evaluation, Subtitles, Older adults, Younger adults},
location = {Paphos, Cyprus}
}

@inproceedings{10.1109/ASONAM.2012.193,
author = {Brambilla, Marco and Bozzon, Alessandro},
title = {Web Data Management through Crowdsourcing Upon Social Networks},
year = {2012},
isbn = {9780769547992},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/ASONAM.2012.193},
doi = {10.1109/ASONAM.2012.193},
abstract = {Retrieval and management of Web data is becoming a more and more complex problem, due to the amount of information to be dealt with, to the diversity of the information sources and of the data formats, and to the evolving expectations of users. In particular, some tasks such as quality assessment, opinion making, and sense extraction cannot be completely delegated to automatic procedures. More and more users are increasingly relying on social interaction to complete and validate the results of their online activities. For instance, scouting "interesting" results, or suggesting new, unexpected search directions in information seeking processes occurs in most times aside of the search systems and processes, possibly instrumented and mediated by a social network. In this paper we propose paradigm that embodies crowds and social network communities as first-class sources for the information management and extraction on the Web. Our approach aims at filling the gap between traditional Web systems (CMS, search engines and others), which operate upon world-wide information, with social systems, capable of interacting with real people, in real time, to capture their opinions, suggestions, and emotions by leveraging crowd sourcing practices and making them viable upon a social network. This enormously enriches the data manipulation experience for the user can be enormously enriched.},
booktitle = {Proceedings of the 2012 International Conference on Advances in Social Networks Analysis and Mining (ASONAM 2012)},
pages = {1123–1127},
numpages = {5},
keywords = {Communities, Data models, Engines, Facebook, Humans, Object oriented modeling, Social network, Web information system, crowdsourcing, semantic Web},
series = {ASONAM '12}
}

@inproceedings{10.5555/2040283.2040295,
author = {Satzger, Benjamin and Psaier, Harald and Schall, Daniel and Dustdar, Schahram},
title = {Stimulating skill evolution in market-based crowdsourcing},
year = {2011},
isbn = {9783642230585},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {Crowdsourcing has emerged as an important paradigm in human problem-solving techniques on the Web. One application of crowdsourcing is to outsource certain tasks to the crowd that are difficult to implement in software. Another potential benefit of crowdsourcing is the on-demand allocation of a flexible workforce. Businesses may outsource tasks to the crowd based on temporary workload variations. A major challenge in crowdsourcing is to guarantee high-quality processing of tasks. We present a novel crowdsourcing marketplace that matches tasks to suitable workers based on auctions. The key to ensuring high quality lies in skilled members whose capabilities can be estimated correctly. We present a novel auction mechanism for skill evolution that helps to correctly estimate workers and to evolve skills that are needed. Evaluations show that this leads to improved crowdsourcing.},
booktitle = {Proceedings of the 9th International Conference on Business Process Management},
pages = {66–82},
numpages = {17},
keywords = {auctions, crowdsourcing, human-centric BPM, online communities, skill evolution, task markets},
location = {Clermont-Ferrand, France},
series = {BPM'11}
}

@inproceedings{10.5555/2484920.2485052,
author = {Venanzi, Matteo and Rogers, Alex and Jennings, Nicholas R.},
title = {Trust-based fusion of untrustworthy information in crowdsourcing applications},
year = {2013},
isbn = {9781450319935},
publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
address = {Richland, SC},
abstract = {In this paper, we address the problem of fusing untrustworthy reports provided from a crowd of observers, while simultaneously learning the trustworthiness of individuals. To achieve this, we construct a likelihood model of the users's trustworthiness by scaling the uncertainty of its multiple estimates with trustworthiness parameters. We incorporate our trust model into a fusion method that merges estimates based on the trust parameters and we provide an inference algorithm that jointly computes the fused output and the individual trustworthiness of the users based on the maximum likelihood framework. We apply our algorithm to cell tower local- isation using real-world data from the OpenSignal project and we show that it outperforms the state-of-the-art methods in both accuracy, by up to 21\%, and consistency, by up to 50\% of its predictions.},
booktitle = {Proceedings of the 2013 International Conference on Autonomous Agents and Multi-Agent Systems},
pages = {829–836},
numpages = {8},
keywords = {crowdsourcing, data fusion, information trustworthiness},
location = {St. Paul, MN, USA},
series = {AAMAS '13}
}

@inproceedings{10.1145/1566374.1566392,
author = {DiPalantino, Dominic and Vojnovic, Milan},
title = {Crowdsourcing and all-pay auctions},
year = {2009},
isbn = {9781605584584},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1566374.1566392},
doi = {10.1145/1566374.1566392},
abstract = {In this paper we present and analyze a model in which users select among, and subsequently compete in, a collection of contests offering various rewards. The objective is to capture the essential features of a crowdsourcing system, an environment in which diverse tasks are presented to a large community. We aim to demonstrate the precise relationship between incentives and participation in such systems.We model contests as all-pay auctions with incomplete information; as a consequence of revenue equivalence, our model may also be interpreted more broadly as one in which users select among auctions of heterogeneous goods. We present two regimes in which we find an explicit correspondence in equilibrium between the offered rewards and the users' participation levels. The regimes respectively model situations in which different contests require similar or unrelated skills. Principally, we find that rewards yield logarithmically diminishing returns with respect to participation levels. We compare these results to empirical data from the crowdsourcing site Taskcn.com; we find that as we condition the data on more experienced users, the model more closely conforms to the empirical data.},
booktitle = {Proceedings of the 10th ACM Conference on Electronic Commerce},
pages = {119–128},
numpages = {10},
keywords = {all-pay auctions, contests, crowdsourcing, game theory},
location = {Stanford, California, USA},
series = {EC '09}
}

@inproceedings{10.1145/1851182.1851228,
author = {Choffnes, David R. and Bustamante, Fabi\'{a}n E. and Ge, Zihui},
title = {Crowdsourcing service-level network event monitoring},
year = {2010},
isbn = {9781450302012},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1851182.1851228},
doi = {10.1145/1851182.1851228},
abstract = {The user experience for networked applications is becoming a key benchmark for customers and network providers. Perceived user experience is largely determined by the frequency, duration and severity of network events that impact a service. While today's networks implement sophisticated infrastructure that issues alarms for most failures, there remains a class of silent outages (e.g., caused by configuration errors) that are not detected. Further, existing alarms provide little information to help operators understand the impact of network events on services. Attempts to address this through infrastructure that monitors end-to-end performance for customers have been hampered by the cost of deployment and by the volume of data generated by these solutions.We present an alternative approach that pushes monitoring to applications on end systems and uses their collective view to detect network events and their impact on services - an approach we call Crowdsourcing Event Monitoring (CEM). This paper presents a general framework for CEM systems and demonstrates its effectiveness for a P2P application using a large dataset gathered from BitTorrent users and confirmed network events from two ISPs. We discuss how we designed and deployed a prototype CEM implementation as an extension to BitTorrent. This system performs online service-level network event detection through passive monitoring and correlation of performance in end-users' applications.},
booktitle = {Proceedings of the ACM SIGCOMM 2010 Conference},
pages = {387–398},
numpages = {12},
keywords = {P2P, anomaly detection, crowdsourcing, service-level network events},
location = {New Delhi, India},
series = {SIGCOMM '10}
}

@article{10.1145/1851275.1851228,
author = {Choffnes, David R. and Bustamante, Fabi\'{a}n E. and Ge, Zihui},
title = {Crowdsourcing service-level network event monitoring},
year = {2010},
issue_date = {October 2010},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {40},
number = {4},
issn = {0146-4833},
url = {https://doi.org/10.1145/1851275.1851228},
doi = {10.1145/1851275.1851228},
abstract = {The user experience for networked applications is becoming a key benchmark for customers and network providers. Perceived user experience is largely determined by the frequency, duration and severity of network events that impact a service. While today's networks implement sophisticated infrastructure that issues alarms for most failures, there remains a class of silent outages (e.g., caused by configuration errors) that are not detected. Further, existing alarms provide little information to help operators understand the impact of network events on services. Attempts to address this through infrastructure that monitors end-to-end performance for customers have been hampered by the cost of deployment and by the volume of data generated by these solutions.We present an alternative approach that pushes monitoring to applications on end systems and uses their collective view to detect network events and their impact on services - an approach we call Crowdsourcing Event Monitoring (CEM). This paper presents a general framework for CEM systems and demonstrates its effectiveness for a P2P application using a large dataset gathered from BitTorrent users and confirmed network events from two ISPs. We discuss how we designed and deployed a prototype CEM implementation as an extension to BitTorrent. This system performs online service-level network event detection through passive monitoring and correlation of performance in end-users' applications.},
journal = {SIGCOMM Comput. Commun. Rev.},
month = aug,
pages = {387–398},
numpages = {12},
keywords = {P2P, anomaly detection, crowdsourcing, service-level network events}
}

@inproceedings{10.1145/2523429.2532331,
author = {Jussila, Jari and Laine, Tom and Rautiainen, Mika and K\"{a}rkk\"{a}inen, Hannu and Ruohisto, Janne and Erkinheimo, Pia and Myhrberg, Markus},
title = {Future of crowdsourcing and value creation in different media environments},
year = {2013},
isbn = {9781450319928},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2523429.2532331},
doi = {10.1145/2523429.2532331},
abstract = {The focal theme of the panel is crowdsourcing and its future. Crowdsourcing is a relatively new concept, meaning broadly the act of outsourcing a job that is traditionally performed by e.g. an employee of a firm to an undefined, generally large group of people. Famous cases of crowdsourcing include Iron Sky the movie, crowdsourcing part of their fund raising and even parts of the actual movie making to movie fans; the intermediary firm InnoCentive offering the opportunity for other firms to crowdsource e.g. parts of their product development to crowds of people, and Canadian GoldCorp mining corporation crowdsourcing gold resource finding to both professionals and amateurs. Other crowdsourcing objectives include various tasks normally held within companies, such as marketing campaign design, product design, software testing, etc. The panel aims to provide fresh views for the opportunities of crowdsourcing from different angles, including various media environments and industry sectors, companies offering novel crowdsourcing services and platforms, as well as the viewpoint of value creation and business.Panel involves several experts on crowdsourcing, having experience from different fields, as well as business-oriented and academic-oriented moderators.},
booktitle = {Proceedings of International Conference on Making Sense of Converging Media},
pages = {339–340},
numpages = {2},
keywords = {Crowdfunding, Crowdsourcing, Value Creation},
location = {Tampere, Finland},
series = {AcademicMindTrek '13}
}

@inproceedings{10.1145/1969289.1969318,
author = {Wald, M.},
title = {Crowdsourcing correction of speech recognition captioning errors},
year = {2011},
isbn = {9781450304764},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1969289.1969318},
doi = {10.1145/1969289.1969318},
abstract = {In this paper, we describe a tool that facilitates crowdsourcing correction of speech recognition captioning errors to provide a sustainable method of making videos accessible to people who find it difficult to understand speech through hearing alone.},
booktitle = {Proceedings of the International Cross-Disciplinary Conference on Web Accessibility},
articleno = {22},
numpages = {2},
keywords = {accessibility},
location = {Hyderabad, Andhra Pradesh, India},
series = {W4A '11}
}

@inproceedings{10.1109/ICECCS.2011.34,
author = {Skopik, Florian and Schall, Daniel and Dustdar, Schahram},
title = {Computational Social Network Management in Crowdsourcing Environments},
year = {2011},
isbn = {9780769543819},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/ICECCS.2011.34},
doi = {10.1109/ICECCS.2011.34},
abstract = {Flexible interactions in complex social and service-oriented collaboration systems increasingly demand for automated adaptation techniques to optimize partner discovery and selection. Today, applications of complex service-oriented systems can be found in crowd sourcing environments. In such environments, collaborations are typically short-lived and strongly influenced by incentives and actor behavior. As actors prove their reliable and dependable behavior in jointly performed activities, they become increasingly considered as invaluable partners. A social network builds a strong basis to enable successful collaborations between crowd members. In order to keep track of the dynamics in such systems, it is inevitable to apply an autonomous approach to manage social network structures automatically using captured interaction data. Thus, we introduce an adaptation concept that accounts for emerging social relations based on varying interaction behavior of collaboration partners. We describe the foundational concepts for dynamic social link management in Web-based collaborations. We highlight major concerns of computational models in highly dynamic networks and deal with temporal aspects such as supporting the emergence of relations, efficient update mechanisms, and aging of relations.},
booktitle = {Proceedings of the 2011 16th IEEE International Conference on Engineering of Complex Computer Systems},
pages = {273–282},
numpages = {10},
keywords = {computational social network management, emergence of social relations, service-oriented crowdsourcing},
series = {ICECCS '11}
}

@inproceedings{10.1109/SCC.2014.12,
author = {Sharifi, Mahdi and Manaf, Azizah Abdul and Memariani, Ali and Movahednejad, Homa and Dastjerdi, Amir Vahid},
title = {Consensus-Based Service Selection Using Crowdsourcing Under Fuzzy Preferences of Users},
year = {2014},
isbn = {9781479950669},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/SCC.2014.12},
doi = {10.1109/SCC.2014.12},
abstract = {Different evaluator entities, either human agents (e.g., experts) or software agents (e.g., monitoring services), are involved in the assessment of QoS parameters of candidate services, which leads to diversity in service assessments. This diversity makes the service selection a challenging task, especially when numerous qualities of service criteria and range of providers are considered. To address this problem, this study first presents a consensus-based service assessment methodology that utilizes consensus theory to evaluate the service behavior for single QoS criteria using the power of crowdsourcing. To this end, trust level metrics are introduced to measure the strength of a consensus based on the trustworthiness levels of crowd members. The peers converged to the most trustworthy evaluation. Next, the fuzzy inference engine was used to aggregate each obtained assessed QoS value based on user preferences because we address multiple QoS criteria in real life scenarios. The proposed approach was tested and illustrated via two case studies that prove its applicability.},
booktitle = {Proceedings of the 2014 IEEE International Conference on Services Computing},
pages = {17–26},
numpages = {10},
keywords = {Consensus, Fuzzy aggregation, Service selection, Trust, Web service},
series = {SCC '14}
}

@inproceedings{10.1109/DASC.2014.54,
author = {Yue, Dejun and Yu, Ge and Shen, Derong and Yu, Xiaocong},
title = {A Weighted Aggregation Rule in Crowdsourcing Systems for High Result Accuracy},
year = {2014},
isbn = {9781479950799},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/DASC.2014.54},
doi = {10.1109/DASC.2014.54},
abstract = {Many challenging problems could be better solved by exploiting crowdsourcing platforms than traditional machine-based methods. However, data quality in crowdsourcing applications has become a crucial aspect since crowdsourcing workers may have different capabilities. In this paper, we propose a novel weighted aggregation rule (WAR) to improve the result accuracy in crowdsourcing systems. According to the agreement of answers given by the workers, we classify all the tasks into the high-agreement tasks and low-agreement tasks. For the high-agreement tasks, we use simple majority voting to select the correct answer while ensuring the result accuracy. For the low-agreement tasks, we adopt weighted majority voting strategy, which assigns a weight for each worker according to his performance on the high-agreement tasks. We evaluate the effectiveness of our proposed method using three real-world datasets on AMT. The experimental results show that our method achieves excellent result accuracy.},
booktitle = {Proceedings of the 2014 IEEE 12th International Conference on Dependable, Autonomic and Secure Computing},
pages = {265–270},
numpages = {6},
keywords = {aggregation rule, agreement, crowdsourcing, majority voting},
series = {DASC '14}
}

@inproceedings{10.5555/1866696.1866715,
author = {Munro, Robert and Bethard, Steven and Kuperman, Victor and Lai, Vicky Tzuyin and Melnick, Robin and Potts, Christopher and Schnoebelen, Tyler and Tily, Harry},
title = {Crowdsourcing and language studies: the new generation of linguistic data},
year = {2010},
publisher = {Association for Computational Linguistics},
address = {USA},
abstract = {We present a compendium of recent and current projects that utilize crowdsourcing technologies for language studies, finding that the quality is comparable to controlled laboratory experiments, and in some cases superior. While crowdsourcing has primarily been used for annotation in recent language studies, the results here demonstrate that far richer data may be generated in a range of linguistic disciplines from semantics to psycholinguistics. For these, we report a number of successful methods for evaluating data quality in the absence of a 'correct' response for any given data point.},
booktitle = {Proceedings of the NAACL HLT 2010 Workshop on Creating Speech and Language Data with Amazon's Mechanical Turk},
pages = {122–130},
numpages = {9},
location = {Los Angeles, California},
series = {CSLDAMT '10}
}

@inproceedings{10.5555/1927229.1927272,
author = {Karnin, Ehud D. and Walach, Eugene and Drory, Tal},
title = {Crowdsourcing in the document processing practice},
year = {2010},
isbn = {3642169848},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {The processing of scanned documents calls for automatic recognition of the text by OCR (Optical Character Recognition) computer programs, followed by human validation and correction. Crowdsourcing of these essential manual tasks is a good option, provided one can take care of some key challenges, so that the quality level expected by the customer is met. We show how tools for efficient validation and correction are adapted and enhanced to address issues associated with crowdsourcing, such as data privacy, quality control, crowd monitoring, and job quality assurance. We started to implement these ideas and technologies in our COoperative eNgine for Correction of ExtRacted Text (CONCERT), which is used in book digitization projects.},
booktitle = {Proceedings of the 10th International Conference on Current Trends in Web Engineering},
pages = {408–411},
numpages = {4},
keywords = {documents processing, enterprise crowdsourcing, productivity tools, quality assurance, quality control},
location = {Vienna, Austria},
series = {ICWE'10}
}

@proceedings{10.1145/2593728,
title = {CSI-SE 2014: Proceedings of the 1st International Workshop on CrowdSourcing in Software Engineering},
year = {2014},
isbn = {9781450328579},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Hyderabad, India}
}

@inproceedings{10.1109/ICComm.2018.8430109,
author = {Greu, Victor and Ciot\^{\I}rnae, Petric\u{a} and Tuundefined\u{A}, Leontin and Popescu, Florin Gabriel},
title = {Human and Artificial Intelligence Driven Incentive-Operation Model and Algorithms for a Multi-Purpose Integrated Crowdsensing-Crowdsourcing Scalable System},
year = {2018},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICComm.2018.8430109},
doi = {10.1109/ICComm.2018.8430109},
abstract = {The future sensing systems seem to need more performant crowdsensing, using highest technologies as artificial intelligence, but being also more complex by volunteer participation and progressively changing from crowdsensing to crowdsourcing. Our work main idea is to use human/artificial intelligence in order to provide highest incentives arguments and commitments for participants and users, transforming data into information and eventually in knowledge. The human/artificial intelligence support is used first to find the most desired/used tasks/targets/questions/issues, then to control the crowdsensing/crowdsourcing operation with learned artificial intelligence rules, based on two algorithms, first for implementing an optimal efficiency tasks covering strategy as reference and second for attracting participants to enlarge/improve accuracy of service by extending crowdsensing-crowdsourcing with correlation incentive/operation added features.},
booktitle = {2018 International Conference on Communications (COMM)},
pages = {213–218},
numpages = {6},
location = {Bucharest}
}

@inproceedings{10.1145/2598153.2602225,
author = {Bozzon, Alessandro and Aroyo, Lora and Cremonesi, Paolo},
title = {First International Workshop on User Interfaces for Crowdsourcing and Human Computation},
year = {2014},
isbn = {9781450327756},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2598153.2602225},
doi = {10.1145/2598153.2602225},
abstract = {Recent years witnessed an explosion in the number and variety of data crowdsourcing initiatives. From OpenStreetMap to Amazon Mechanical Turk, developers and practitioners have been striving to create user interfaces able to effectively and efficiently support the creation, exploration, and analysis of crowdsourced information.The extensive usage of crowdsourcing techniques brings a major change of paradigm with respect to traditional user interface for data collection and exploration, as effectiveness, speed, and interaction quality concerns play a central role in supporting very demanding incentives, including monetary ones.The First International Workshop on User Interfaces for Crowdsourcing and Human Computation (CrowdUI 2014), co-located with the AVI 2014 conference, brought together researchers and practitioners from a wide range of areas interested in discussing the user interaction challenges posed by crowdsourcing systems.},
booktitle = {Proceedings of the 2014 International Working Conference on Advanced Visual Interfaces},
pages = {398–400},
numpages = {3},
keywords = {crowdsourcing, human computation, user incentives, user interfaces},
location = {Como, Italy},
series = {AVI '14}
}

@inproceedings{10.1145/2390803.2390812,
author = {Avlonitis, Markos and Chorianopoulos, Konstantinos and Shamma, David Ayman},
title = {Crowdsourcing user interactions within web video through pulse modeling},
year = {2012},
isbn = {9781450315890},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2390803.2390812},
doi = {10.1145/2390803.2390812},
abstract = {Semantic video research has employed crowdsourcing techniques on social web video data sets such as comments, tags, and annotations, but these data sets require an extra effort on behalf of the user. We propose a pulse modeling method, which analyzes implicit user interactions within web video, such as rewind. In particular, we have modeled the user information seeking behavior as a time series and the semantic regions as a discrete pulse of fixed width. We constructed these pulses from user interactions with a documentary video that has a very rich visual style with too many cuts and camera angles/frames for the same scene. Next, we calculated the correlation coefficient between dynamically detected user pulses at the local maximums and the reference pulse. We have found when people are actively seeking for information in a video, their activity (these pulses) significantly matches the semantics of the video. This proposed pulse analysis method complements previous work in content-based information retrieval and provides an additional user-based dimension for modeling the semantics of a web video.},
booktitle = {Proceedings of the ACM Multimedia 2012 Workshop on Crowdsourcing for Multimedia},
pages = {19–20},
numpages = {2},
keywords = {implicit, interaction, pragmatics, user activity, video},
location = {Nara, Japan},
series = {CrowdMM '12}
}

@inproceedings{10.1145/2103354.2103373,
author = {Oomen, Johan and Aroyo, Lora},
title = {Crowdsourcing in the cultural heritage domain: opportunities and challenges},
year = {2011},
isbn = {9781450308243},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2103354.2103373},
doi = {10.1145/2103354.2103373},
abstract = {Galleries, Libraries, Archives and Museums (short: GLAMs) around the globe are beginning to explore the potential of crowdsourcing, i. e. outsourcing specific activities to a community though an open call. In this paper, we propose a typology of these activities, based on an empirical study of a substantial amount of projects initiated by relevant cultural heritage institutions. We use the Digital Content Life Cycle model to study the relation between the different types of crowdsourcing and the core activities of heritage organizations. Finally, we focus on two critical challenges that will define the success of these collaborations between amateurs and professionals: (1) finding sufficient knowledgeable, and loyal users; (2) maintaining a reasonable level of quality. We thus show the path towards a more open, connected and smart cultural heritage: open (the data is open, shared and accessible), connected (the use of linked data allows for interoperable infrastructures, with users and providers getting more and more connected), and smart (the use of knowledge and web technologies allows us to provide interesting data to the right users, in the right context, anytime, anywhere -- both with involved users/consumers and providers). It leads to a future cultural heritage that is open, has intelligent infrastructures and has involved users, consumers and providers.},
booktitle = {Proceedings of the 5th International Conference on Communities and Technologies},
pages = {138–149},
numpages = {12},
keywords = {crowdsourcing, heritage, lifecycle model, metadata, tagging},
location = {Brisbane, Australia},
series = {C&amp;T '11}
}

@inproceedings{10.1145/2463728.2463814,
author = {Burov, Vasiliy and Patarakin, Evgeny and Yarmakhov, Boris},
title = {A crowdsourcing model for public consultations on draft laws},
year = {2012},
isbn = {9781450312004},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2463728.2463814},
doi = {10.1145/2463728.2463814},
abstract = {The paper discusses an innovative approach to lawmaking. In the proposed model a draft law is split into segments and improved by a network community which members can vote for the segments and suggest their own versions. Several cases of public consultations of Russian Laws based on Wikivote approach are presented and analyzed.},
booktitle = {Proceedings of the 6th International Conference on Theory and Practice of Electronic Governance},
pages = {450–451},
numpages = {2},
keywords = {collaboration, crowdsourcing, lawmaking, wiki},
location = {Albany, New York, USA},
series = {ICEGOV '12}
}

@inproceedings{10.1145/1979742.1979745,
author = {Xu, Anbang and Bailey, Brian P.},
title = {A crowdsourcing model for receiving design critique},
year = {2011},
isbn = {9781450302685},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1979742.1979745},
doi = {10.1145/1979742.1979745},
abstract = {Designers in many domains are increasingly turning to online communities to receive critiques of early design ideas. However, members of these communities may not contribute an effective critique due to limited skills, motivation, or time, and therefore many critiques may not go beyond "I (don't) like it". We propose a new approach for designers to receive online critique. Our approach is novel because it adopts a theoretical framework for effective critique and implements the framework on a popular crowdsourcing platform. Preliminary results show that our approach allows designers to acquire quality critiques in a timely manner that compare favorably with critiques produced from a well-known online community.},
booktitle = {CHI '11 Extended Abstracts on Human Factors in Computing Systems},
pages = {1183–1188},
numpages = {6},
keywords = {critique, crowdsourcing, design},
location = {Vancouver, BC, Canada},
series = {CHI EA '11}
}

@inproceedings{10.5555/2908738.2908747,
author = {Viappiani, Paolo and Zilles, Sandra and Hamilton, Howard J. and Boutilier, Craig},
title = {A Bayesian concept learning approach to crowdsourcing},
year = {2011},
publisher = {AAAI Press},
abstract = {We develop a Bayesian approach to concept learning for crowdsourcing applications. A probabilistic belief over possible concept definitions is maintained and updated according to (noisy) observations from experts, whose behaviors are modeled using discrete types. We propose recommendation techniques, inference methods, and query selection strategies to assist a user charged with choosing a configuration that satisfies some (partially known) concept. Our model is able to simultaneously learn the concept definition and the types of the experts. We evaluate our model with simulations, showing that our Bayesian strategies are effective even in large concept spaces with many uninformative experts.},
booktitle = {Proceedings of the 13th AAAI Conference on Interactive Decision Theory and Game Theory},
pages = {60–67},
numpages = {8},
series = {AAAIWS'11-13}
}

@inproceedings{10.1145/2677832.2677847,
author = {Lin, Zeqi and Zhao, Junfeng and Xie, Bing},
title = {A graph database based crowdsourcing infrastructure for modelling and searching code structure},
year = {2014},
isbn = {9781450333030},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2677832.2677847},
doi = {10.1145/2677832.2677847},
abstract = {Software reuse offers a solution to eliminate repeated work and improve efficiency and quality in the software development. In order to reuse existing software resources, software developers usually need to understand code structure of them. However, code structure is usually too complex to figure out. Therefore, it is helpful to demonstrate software developers the code structure they want to know. This paper presents a graph database based crowdsourcing infrastructure for modelling and searching code structure. In this paper, a graph based modelling paradigm of code structure is provided, which solves the problem that how code structure should be demonstrated. Software developers' search purposes are analyzed by natural language processing technique. A crowdsourcing mechanism is provided to integrate different code structure analysis algorithms for these different search purposes. Our work improves the efficiency of software reuse, and it is validated through an industrial case study.},
booktitle = {Proceedings of the 6th Asia-Pacific Symposium on Internetware},
pages = {15–24},
numpages = {10},
keywords = {Code Structure, Crowdsourcing, Graph Database, Software Reuse},
location = {Hong Kong, China},
series = {Internetware '14}
}

@inproceedings{10.1145/3246861,
author = {Castillo, Carlos},
title = {Session details: Session 6 -- Big Data Analytics and Crowdsourcing for Public Health 2},
year = {2015},
isbn = {9781450334921},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3246861},
doi = {10.1145/3246861},
booktitle = {Proceedings of the 5th International Conference on Digital Health 2015},
location = {Florence, Italy},
series = {DH '15}
}

@inproceedings{10.1109/SMC.2013.85,
author = {Karam, Roula and Melchiori, Michele},
title = {A Crowdsourcing-Based Framework for Improving Geo-spatial Open Data},
year = {2013},
isbn = {9781479906529},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/SMC.2013.85},
doi = {10.1109/SMC.2013.85},
abstract = {Nowadays, more and more people rely on freely available user-generated spatial content, known as Volunteered Geographic Information (VGI). Therefore, spatial data creation is no more exclusively in the hands of professionals and Linked Open Data (LOD) techniques have a role in promoting such online and freely accessible spatial information. However, the volume of VGI data is constantly growing so evaluating its quality is a basic need, especially in urban environments where Points of Interest change frequently and human feedbacks are crucial to get frequent updates of their descriptions. In this context, we propose a crowd sourcing-based framework, relying on linked data principles and devised to collect, organize and rank user-generated content in order to improve accuracy and completeness of geo-spatial LOD. To this purpose, metrics have been defined for evaluating both users and contents.},
booktitle = {Proceedings of the 2013 IEEE International Conference on Systems, Man, and Cybernetics},
pages = {468–473},
numpages = {6},
keywords = {Crowdsourcing, Linked Open Data, Location Based Services, Spatial Databases, Trust Metrics},
series = {SMC '13}
}

@inproceedings{10.1109/MDM.2014.59,
author = {Tiwari, Sunita and Kaushik, Saroj},
title = {Information Enrichment for Tourist Spot Recommender System Using Location Aware Crowdsourcing},
year = {2014},
isbn = {9781479957057},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/MDM.2014.59},
doi = {10.1109/MDM.2014.59},
abstract = {With the increase in number of available interesting locations, it becomes difficult for users to find interesting ones, thus imposes a need for recommender systems to suggest interesting locations. Further, to ease the user's decision making, the amount of supplementary information, such as right time to visit, weather conditions, traffic condition, right mode of transport, crowdedness, security alerts, etc., may be annotated with the list of recommended locations. This paper explores the possibility of enriching tourist locations using crowd sourcing approach, which can be used by Tourist Spot Recommender System (TSRS) for mobile users. Proposed crowd sourcing system focuses on getting work done from the crowd currently available at the location under consideration. In proposed system, the contributed information is not limited to ones available on blogs, web pages and sensor-readings from the device etc., but includes proactively-generated user's opinions and perspectives, that are processed to offer immediate knowledge. Our system works in collaboration with a TSRS, takes the list of locations to be recommended to the current user and performs just-in-time information enrichment for those selected set of locations. We have implemented a prototype of proposed systems using java android software development toolkit and evaluated this system by 76 real users.},
booktitle = {Proceedings of the 2014 IEEE 15th International Conference on Mobile Data Management - Volume 02},
pages = {11–14},
numpages = {4},
keywords = {Collective Intelligence, Crowdsourcing, Information Enrichment, Pervasive Computing, Tourist Spot Recommender System, Tourist Spots},
series = {MDM '14}
}

@inproceedings{10.5555/2908698.2908701,
author = {Chandler, Dana and Horton, John},
title = {Labor allocation in paid crowdsourcing: experimental evidence on positioning, nudges and prices},
year = {2011},
publisher = {AAAI Press},
abstract = {This paper reports the results of a natural field experiment where workers from a paid crowdsourcing environment selfselect into tasks and are presumed to have limited attention. In our experiment, workers labeled any of six pictures from a 2 \texttimes{} 3 grid of thumbnail images. In the absence of any incentives, workers exhibit a strong default bias and tend to select images from the top-left ("focal") position; the bottomright ("non-focal") position, was the least preferred. We attempted to overcome this bias and increase the rate at which workers selected the least preferred task, by using a combination of monetary and non-monetary incentives. We also varied the saliency of these incentives by placing them in either the focal or non-focal position. Although both incentive types caused workers to re-allocate their labor, monetary incentives were more effective. Most interestingly, both incentive types worked better when they were placed in the focal position and made more salient. In fact, salient non-monetary incentives worked about as well as non-salient monetary ones. Our evidence suggests that user interface and cognitive biases play an important role in online labor markets and that salience can be used by employers as a kind of "incentive multiplier".},
booktitle = {Proceedings of the 11th AAAI Conference on Human Computation},
pages = {14–19},
numpages = {6},
series = {AAAIWS'11-11}
}

@inproceedings{10.1007/978-3-642-12275-0_57,
author = {Alonso, Omar and Schenkel, Ralf and Theobald, Martin},
title = {Crowdsourcing assessments for XML ranked retrieval},
year = {2010},
isbn = {3642122744},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-642-12275-0_57},
doi = {10.1007/978-3-642-12275-0_57},
abstract = {Crowdsourcing has gained a lot of attention as a viable approach for conducting IR evaluations. This paper shows through a series of experiments on INEX data that crowdsourcing can be a good alternative for relevance assessment in the context of XML retrieval.},
booktitle = {Proceedings of the 32nd European Conference on Advances in Information Retrieval},
pages = {602–606},
numpages = {5},
location = {Milton Keynes, UK},
series = {ECIR'2010}
}

@inproceedings{10.1145/2801694.2801705,
author = {Zhao, Cong and Shi, Fengrui and Huang, Ran and Yang, Xinyu and McCann, Julie},
title = {Trustworthy Device Pairing for Opportunistic Device-to-Device Communications in Mobile Crowdsourcing Systems},
year = {2015},
isbn = {9781450337014},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2801694.2801705},
doi = {10.1145/2801694.2801705},
abstract = {Mobile Crowdsourcing System is an emerging service paradigm base on numerous personal smart devices, where the Device-to-Device communication among opportunistically encountered participating devices is an indispensable part of task allocation, file transmission and data collaboration. Considering that participating devices are privately held and opportunistically encountered, we design the Trustworthy Device Pairing (TDP) scheme that realizes user-transparent sharing secret key negotiation and reliable peer device determination for trustworthy spontaneous D2D transactions. TDP is demonstrated to be effective based on our proof-of-concept implementation, and a further evaluation on efficiency will be conducted.},
booktitle = {Proceedings of the 2015 Workshop on Wireless of the Students, by the Students, \&amp; for the Students},
pages = {4–6},
numpages = {3},
keywords = {device pairing, device-to-device communication, mobile crowdsourcing system, trustworthy},
location = {Paris, France},
series = {S3 '15}
}

@inproceedings{10.5555/1939281.1939323,
author = {Vukovic, Maja and Bartolini, Claudio},
title = {Towards a research agenda for enterprise crowdsourcing},
year = {2010},
isbn = {3642165575},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {Over the past few years the crowdsourcing paradigm has evolved from its humble beginnings as isolated purpose-built initiatives, such as Wikipedia and Elance and Mechanical Turk to a growth industry employing over 2 million knowledge workers, contributing over half a billion dollars to the digital economy. Web 2.0 provides the technological foundations upon which the crowdsourcing paradigm evolves and operates, enabling networked experts to work collaboratively to complete a specific task. Enterprise crowdsourcing poses interesting challenges for both academic and industrial research along the social, legal, and technological dimensions.In this paper we describe the challenges that researchers and practitioners face when thinking about various aspects of enterprise crowdsourcing. First, to establish technological foundations, what are the interaction models and protocols between the Enterprise and the crowd. Secondly, how is crowdsourcing going to face the challenges in quality assurance, enabling Enterprises to optimally leverage the scalable workforce. Thirdly, what are the novel (Web) applications enabled by Enterprise crowdsourcing.},
booktitle = {Proceedings of the 4th International Conference on Leveraging Applications of Formal Methods, Verification, and Validation - Volume Part I},
pages = {425–434},
numpages = {10},
keywords = {business process modeling, crowdsourcing},
location = {Heraklion, Crete, Greece},
series = {ISoLA'10}
}

@inproceedings{10.1007/978-3-319-22479-4_2,
author = {Panagiotopoulos, Panos and Bowen, Frances},
title = {Conceptualising the Digital Public in Government Crowdsourcing: Social Media and the Imagined Audience},
year = {2015},
isbn = {978-3-319-22478-7},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-319-22479-4_2},
doi = {10.1007/978-3-319-22479-4_2},
abstract = {Public sector organisations seem to be embracing social media for information dissemination and engagement, but less is know about their value as information sources. This paper draws from the notion of the imagined audience to examine how policy teams in the UK Department of Environment, Food and Rural Affairs (DEFRA) conceptualise the value of social media input. Findings from a series of interviews and workshops suggest that policy makers are broadly positive about sourcing useful input from social media in topics like farming and environmental policies, however audience awareness emerges as an important limitation. As different groups of the public use social media for professional activities, policy makers attempt to develop their own capacities to navigate through audiences and understand whom they are listening to. The paper makes suggestions about the technical, methodological and policy challenges of overcoming audience limitations on social media.},
booktitle = {Electronic Government},
pages = {19–30},
numpages = {12},
keywords = {Social media, Policy crowdsourcing, Digital engagement, UK government, Environment and farming, Case study},
location = {Thessaloniki
Greece}
}

@inproceedings{10.1145/3274895.3274902,
author = {Jonathan, Christopher and Mokbel, Mohamed F.},
title = {Stella: geotagging images via crowdsourcing},
year = {2018},
isbn = {9781450358897},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3274895.3274902},
doi = {10.1145/3274895.3274902},
abstract = {Geotagged data (e.g. images or news items) have empowered various important applications, e.g., search engines and news agencies. However, the lack of available geotagged data significantly reduces the impact of such applications. Meanwhile, existing geotagging approaches rely on the existence of prior knowledge, e.g., accurate training dataset for machine learning techniques. This paper presents Stella; a crowdsourcing framework for image geotagging. The high accuracy of Stella is resulted by being able to recruit workers near the image location even without knowing its location. In addition, Stella also return its confidence about the reported location to help users in understanding the result quality. Experimental evaluation shows that Stella consistently geotags an image with an average of 95\% accuracy and 90\% of confidence.},
booktitle = {Proceedings of the 26th ACM SIGSPATIAL International Conference on Advances in Geographic Information Systems},
pages = {169–178},
numpages = {10},
keywords = {crowdsourcing, geotagging framework, spatial crowdsourcing},
location = {Seattle, Washington},
series = {SIGSPATIAL '18}
}

@inproceedings{10.5555/2392701.2392708,
author = {Prabhakaran, Vinodkumar and Bloodgood, Michael and Diab, Mona and Dorr, Bonnie and Levin, Lori and Piatko, Christine D. and Rambow, Owen and Van Durme, Benjamin},
title = {Statistical modality tagging from rule-based annotations and crowdsourcing},
year = {2012},
publisher = {Association for Computational Linguistics},
address = {USA},
abstract = {We explore training an automatic modality tagger. Modality is the attitude that a speaker might have toward an event or state. One of the main hurdles for training a linguistic tagger is gathering training data. This is particularly problematic for training a tagger for modality because modality triggers are sparse for the overwhelming majority of sentences. We investigate an approach to automatically training a modality tagger where we first gathered sentences based on a high-recall simple rule-based modality tagger and then provided these sentences to Mechanical Turk annotators for further annotation. We used the resulting set of training data to train a precise modality tagger using a multi-class SVM that delivers good performance.},
booktitle = {Proceedings of the Workshop on Extra-Propositional Aspects of Meaning in Computational Linguistics},
pages = {57–64},
numpages = {8},
location = {Jeju, Republic of Korea},
series = {ExProM '12}
}

@inproceedings{10.1145/2030112.2030243,
author = {Vukovic, Maja and Kumara, Soundar},
title = {Second international workshop on ubiquitous crowdsourcing: towards a platform for crowd computing},
year = {2011},
isbn = {9781450306300},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2030112.2030243},
doi = {10.1145/2030112.2030243},
abstract = {With the adoption of mobile, digital and social media networked crowds are reporting and acting upon events in smart environments. Existing platforms for crowdsourcing, support specific activity types, such as micro-tasks on the Amazon's Mechanical Turk; and fall short of facilitating general mechanisms for setting up and maintaining crowd networks easily, flexibly and in a variety of domains. Building upon First International Workshop on Ubiquitous Crowdsourcing, in this edition we challenge researchers and practitioners to identify requirements for a platform for crowd computing, arising from experiences in deployment crowdsourcing applications, which engage crowd members as sensors, controllers and actuators in smart cities and environments. This workshop brings together researchers to produce a vision for the universal crowdsourcing platform, documenting it in a theme publication.},
booktitle = {Proceedings of the 13th International Conference on Ubiquitous Computing},
pages = {617–618},
numpages = {2},
keywords = {crowdsourcing},
location = {Beijing, China},
series = {UbiComp '11}
}

@inproceedings{10.1109/SocialCom.2013.38,
author = {Besaleva, Liliya I. and Weaver, Alfred C.},
title = {Applications of Social Networks and Crowdsourcing for Disaster Management Improvement},
year = {2013},
isbn = {9780769551371},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/SocialCom.2013.38},
doi = {10.1109/SocialCom.2013.38},
abstract = {Emergency resources are often insufficient to satisfy fully the demands for professional help and supplies after a public disaster. Furthermore, in a mass casualty situation, the emphasis shifts from ensuring the best possible outcome for each individual patient to ensuring the best possible outcome for the greatest number of patients. Historically, various manual and electronic medical triage systems have been used both under civil and military conditions to determine the order and priority of emergency treatment, transport, and best possible destination for the patients. Unfortunately, none of those solutions has proven flexible, accurate, scalable or unobtrusive enough to meet the public's expectations. In this paper, we provide insights into the trends, innovations, and challenges of contemporary crowd sourced e-Health and medical informatics applications in the context of emergency preparedness and response. Additionally, we demonstrate a system, called Crowd Help, for real-time patient assessment which uses mobile electronic triaging accomplished via crowd sourced information. With the use of our system, emergency management professionals receive most of the information they need for preparing themselves to provide timely and accurate treatments of their patients even before dispatching a response team to the event.},
booktitle = {Proceedings of the 2013 International Conference on Social Computing},
pages = {213–219},
numpages = {7},
keywords = {crowdsourcing, e-health, emergency preparedness, mobile technologies, triage},
series = {SOCIALCOM '13}
}

@inproceedings{10.1145/2660114.2660122,
author = {Naderi, Babak and Wechsung, Ina and Polzehl, Tim and M\"{o}ller, Sebastian},
title = {Development and Validation of Extrinsic Motivation Scale for Crowdsourcing Micro-task Platforms},
year = {2014},
isbn = {9781450331289},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2660114.2660122},
doi = {10.1145/2660114.2660122},
abstract = {In this paper, we introduce a scale for measuring the extrinsic motivation of crowd workers. The new questionnaire is strongly based on the Work Extrinsic Intrinsic Motivation Scale (WEIMS) [17] and theoretically follows the Self-Determination Theory (SDT) of motivation. The questionnaire has been applied and validated in a crowdsourcing micro-task platform. This instrument can be used for studying the dynamics of extrinsic motivation by taking into account individual differences and provide meaningful insights which will help to design a proper incentives framework for each crowd worker that eventually leads to a better performance, an increased well-being, and higher overall quality.},
booktitle = {Proceedings of the 2014 International ACM Workshop on Crowdsourcing for Multimedia},
pages = {31–36},
numpages = {6},
keywords = {cfa, crowdsourcing, motivation, sdt},
location = {Orlando, Florida, USA},
series = {CrowdMM '14}
}

@inproceedings{10.1145/3511808.3557297,
author = {Xu, Xiaojia and Liu, An and Liu, Guanfeng and Li, Zhixu and Zhao, Lei},
title = {Drive Less but Finish More: Food Delivery based on Multi-Level Workers in Spatial Crowdsourcing},
year = {2022},
isbn = {9781450392365},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3511808.3557297},
doi = {10.1145/3511808.3557297},
abstract = {In this paper, we study the problem of on-demand food delivery in a new setting where two groups of workers -- riders and taxi drivers (drivers for short) -- cooperate with each other for better service. The riders are responsible for the first and the last mile, and the drivers are in charge of the cross-community transportation. We show this problem is generally NP-hard by a reduction from the well-known 3-dimensional matching (3DM). To tackle with this problem, we first reduce it to the maximum independent set problem and use a simple greedy strategy to design an approximate algorithm which has a polynomial time. Considering the exponents in the polynomial are not very small, we then transform the 3DM into two rounds of 2-dimensional matching and propose a fast algorithm to solve it. Though 3DM problem is NP-hard, we find the cooperation between riders and drivers form a special tripartite graph, based on which we construct a flow network and employ the min-cost max-flow algorithm to efficiently compute the exact solution. We conduct extensive experiments to show the efficiency and the effectiveness of our proposed algorithms.},
booktitle = {Proceedings of the 31st ACM International Conference on Information \&amp; Knowledge Management},
pages = {2331–2340},
numpages = {10},
keywords = {online food delivery, spatial crowdsourcing, task assignment},
location = {Atlanta, GA, USA},
series = {CIKM '22}
}

@inproceedings{10.1145/2593728.2593729,
author = {Prikladnicki, Rafael and Machado, Leticia and Carmel, Erran and de Souza, Cleidson R. B.},
title = {Brazil software crowdsourcing: a first step in a multi-year study},
year = {2014},
isbn = {9781450328579},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2593728.2593729},
doi = {10.1145/2593728.2593729},
abstract = {Crowdsourcing means outsourcing to a large network of people—a crowd. This form of managing work allocation has become much more sophisticated in recent years due to improvements in technology and changes in the work ecosystem. Crowdsourcing portends-- not only the disruption of outsourcing-- but the disruption of the entire global labor market. Small, atomized, tasks that can be completed and paid for in small increments are unprecedented in the history of work. Software has been the pioneer in all the large mega-trends of the last generation: in computer technology, technological entrepreneurship, offshore outsourcing, and now-- in crowdsourcing. This paper describes the starting point of a research project that aims to investigate the Brazilian software labor and industry markets. These markets are being transformed and disrupted as a result of the new phenomena of crowdsourcing. To be more specific, we aim to understand how the three elements of crowdsourcing are emerging in Brazil – the buyers, the platforms, and the crowd. The goal of our project is to identify the challenges faced by Brazilian software developers engaged in crowdsourcing platforms as well as their best practices in order to provide recommendations to the government and support for new developers interested in joining this market.},
booktitle = {Proceedings of the 1st International Workshop on CrowdSourcing in Software Engineering},
pages = {1–4},
numpages = {4},
keywords = {Crowdsourcing, human labor, software development},
location = {Hyderabad, India},
series = {CSI-SE 2014}
}

@inproceedings{10.1145/3320435.3320439,
author = {Mavridis, Panagiotis and Huang, Owen and Qiu, Sihang and Gadiraju, Ujwal and Bozzon, Alessandro},
title = {Chatterbox: Conversational Interfaces for Microtask Crowdsourcing},
year = {2019},
isbn = {9781450360210},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3320435.3320439},
doi = {10.1145/3320435.3320439},
abstract = {Conversational interfaces can facilitate human-computer interactions. Whether or not conversational interfaces can improve worker experience and work quality in crowdsourcing marketplaces has remained unanswered. We investigate the suitability of text-based conversational interfaces for microtask crowdsourcing. We designed a rigorous experimental campaign aimed at gauging the interest and acceptance by crowdworkers for this type of work interface. We compared Web and conversational interfaces for five common microtask types and measured the execution time, quality of work, and the perceived satisfaction of 316 workers recruited from the FigureEight platform. We show that conversational interfaces can be used effectively for crowdsourcing microtasks, resulting in a high satisfaction from workers, and without having a negative impact on task execution time or work quality.},
booktitle = {Proceedings of the 27th ACM Conference on User Modeling, Adaptation and Personalization},
pages = {243–251},
numpages = {9},
keywords = {chatbots, conversational agents, microtask crowdsourcing},
location = {Larnaca, Cyprus},
series = {UMAP '19}
}

@inproceedings{10.5555/2484920.2485027,
author = {Lev, Omer and Polukarov, Maria and Bachrach, Yoram and Rosenschein, Jeffrey S.},
title = {Mergers and collusion in all-pay auctions and crowdsourcing contests},
year = {2013},
isbn = {9781450319935},
publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
address = {Richland, SC},
abstract = {We study the effects of bidder collaboration in all-pay auctions. We analyse both mergers, where the remaining players are aware of the agreement between the cooperating participants, and collusion, where the remaining players are unaware of this agreement. We examine two scenarios: the sum-profit model where the auctioneer obtains the sum of all submitted bids, and the max-profit model of crowdsourcing contests where the auctioneer can only use the best submissions and thus obtains only the winning bid. We show that while mergers do not change the expected utility of the participants, or the principal's utility in the sum-profit model, collusion transfers the utility from the non-colluders to the colluders. Surprisingly, we find that in some cases such collaboration can increase the social welfare. Moreover, mergers and, curiously, also collusion can even be beneficial to the auctioneer under certain conditions.},
booktitle = {Proceedings of the 2013 International Conference on Autonomous Agents and Multi-Agent Systems},
pages = {675–682},
numpages = {8},
keywords = {all-pay auction, collusion, crowdsourcing, mergers},
location = {St. Paul, MN, USA},
series = {AAMAS '13}
}

@inproceedings{10.5555/3042817.3042944,
author = {Chen, Xi and Lin, Qihang and Zhou, Dengyong},
title = {Optimistic knowledge gradient policy for optimal budget allocation in crowdsourcing},
year = {2013},
publisher = {JMLR.org},
abstract = {In real crowdsourcing applications, each label from a crowd usually comes with a certain cost. Given a pre-fixed amount of budget, since different tasks have different ambiguities and different workers have different expertises, we want to find an optimal way to allocate the budget among instance-worker pairs such that the overall label quality can be maximized. To address this issue, we start from the simplest setting in which all workers are assumed to be perfect. We formulate the problem as a Bayesian Markov Decision Process (MDP). Using the dynamic programming (DP) algorithm, one can obtain the optimal allocation policy for a given budget. However, DP is computationally intractable. To solve the computational challenge, we propose a novel approximate policy which is called optimistic knowledge gradient. It is practically efficient while theoretically its consistency can be guaranteed. We then extend the MDP framework to deal with inhomogeneous workers and tasks with contextual information available. The experiments on both simulated and real data demonstrate the superiority of our method.},
booktitle = {Proceedings of the 30th International Conference on International Conference on Machine Learning - Volume 28},
pages = {III–64–III–72},
location = {Atlanta, GA, USA},
series = {ICML'13}
}

@inproceedings{10.1109/NGMAST.2014.59,
author = {Mirri, Silvia and Prandi, Catia and Salomoni, Paola and Callegati, Franco and Campi, Aldo},
title = {On Combining Crowdsourcing, Sensing and Open Data for an Accessible Smart City},
year = {2014},
isbn = {9781479950737},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/NGMAST.2014.59},
doi = {10.1109/NGMAST.2014.59},
abstract = {This work presents a novel geospatial mapping service, based on OpenStreetMap, which has been designed and developed in order to provide personalized path to users with special needs. This system gathers data related to barriers and facilities of the urban environment via crowd sourcing and sensing done by users. It also considers open data provided by bus operating companies to identify the actual accessibility feature and the real time of arrival at the stops of the buses. The resulting service supports citizens with reduced mobility (users with disabilities and/or elderly people) suggesting urban paths accessible to them and providing information related to travelling time, which are tailored to their abilities to move and to the bus arrival time. The manuscript demonstrates the effectiveness of the approach by means of a case study focusing on the differences between the solutions provided by our system and the ones computed by main stream geospatial mapping services.},
booktitle = {Proceedings of the 2014 Eighth International Conference on Next Generation Mobile Apps, Services and Technologies},
pages = {294–299},
numpages = {6},
keywords = {crowdsourcing, geospatial mapping systems, open data, sensing, smart city, urban accessibility},
series = {NGMAST '14}
}

@inproceedings{10.5555/3007337.3007471,
author = {Tran-Thanh, Long and Stein, Sebastian and Rogers, Alex and Jennings, Nicholas R.},
title = {Efficient crowdsourcing of unknown experts using multi-armed bandits},
year = {2012},
isbn = {9781614990970},
publisher = {IOS Press},
address = {NLD},
abstract = {We address the expert crowdsourcing problem, in which an employer wishes to assign tasks to a set of available workers with heterogeneous working costs. Critically, as workers produce results of varying quality, the utility of each assigned task is unknown and can vary both between workers and individual tasks. Furthermore, in realistic settings, workers are likely to have limits on the number of tasks they can perform and the employer will have a fixed budget to spend on hiring workers. Given these constraints, the objective of the employer is to assign tasks to workers in order to maximise the overall utility achieved. To achieve this, we introduce a novel multi-armed bandit (MAB) model, the bounded MAB, that naturally captures the problem of expert crowdsourcing. We also propose an algorithm to solve it efficiently, called bounded ε-first, which uses the first εB of its total budget B to derive estimates of the workers' quality characteristics (exploration), while the remaining (1 - ε)B is used to maximise the total utility based on those estimates (exploitation). We show that using this technique allows us to derive an O(B⅔) upper bound on our algorithm's performance regret (i.e. the expected difference in utility between the optimal and our algorithm). In addition, we demonstrate that our algorithm outperforms existing crowdsourcing methods by up to 155\% in experiments based on real-world data from a prominent crowdsourcing site, while achieving up to 75\% of a hypothetical optimal with full information.},
booktitle = {Proceedings of the 20th European Conference on Artificial Intelligence},
pages = {768–773},
numpages = {6},
location = {Montpellier, France},
series = {ECAI'12}
}

@inproceedings{10.1145/2009916.2009947,
author = {Kazai, Gabriella and Kamps, Jaap and Koolen, Marijn and Milic-Frayling, Natasa},
title = {Crowdsourcing for book search evaluation: impact of hit design on comparative system ranking},
year = {2011},
isbn = {9781450307574},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2009916.2009947},
doi = {10.1145/2009916.2009947},
abstract = {The evaluation of information retrieval (IR) systems over special collections, such as large book repositories, is out of reach of traditional methods that rely upon editorial relevance judgments. Increasingly, the use of crowdsourcing to collect relevance labels has been regarded as a viable alternative that scales with modest costs. However, crowdsourcing suffers from undesirable worker practices and low quality contributions. In this paper we investigate the design and implementation of effective crowdsourcing tasks in the context of book search evaluation. We observe the impact of aspects of the Human Intelligence Task (HIT) design on the quality of relevance labels provided by the crowd. We assess the output in terms of label agreement with a gold standard data set and observe the effect of the crowdsourced relevance judgments on the resulting system rankings. This enables us to observe the effect of crowdsourcing on the entire IR evaluation process. Using the test set and experimental runs from the INEX 2010 Book Track, we find that varying the HIT design, and the pooling and document ordering strategies leads to considerable differences in agreement with the gold set labels. We then observe the impact of the crowdsourced relevance label sets on the relative system rankings using four IR performance metrics. System rankings based on MAP and Bpref remain less affected by different label sets while the Precision@10 and nDCG@10 lead to dramatically different system rankings, especially for labels acquired from HITs with weaker quality controls. Overall, we find that crowdsourcing can be an effective tool for the evaluation of IR systems, provided that care is taken when designing the HITs.},
booktitle = {Proceedings of the 34th International ACM SIGIR Conference on Research and Development in Information Retrieval},
pages = {205–214},
numpages = {10},
keywords = {book search, crowdsourcing quality, prove it},
location = {Beijing, China},
series = {SIGIR '11}
}

@inproceedings{10.1109/UIC-ATC-ScalCom.2014.110,
author = {Jin, Li and Han, Ming and Liu, Gangli and Feng, Ling},
title = {Detecting Cruising Flagged Taxis' Passenger-Refusal Behaviors Using Traffic Data and Crowdsourcing},
year = {2014},
isbn = {9781479976461},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/UIC-ATC-ScalCom.2014.110},
doi = {10.1109/UIC-ATC-ScalCom.2014.110},
abstract = {With the development of urban traffic, taxi accounts for a large proportion to serve people's daily travel. Besides prebooked taxis, flagged taxis also run in many places of the world, allowing passengers to wave at the driver on the side of the road to flag down the taxi as it is approaching. For flagged taxis service, taxi drivers' passenger-refusal (TPR) behaviors have become a serious problem due to rush hours, bad weather and destination choosy, which hurt the living quality and the reputation of the city. With more and more GPS-equipped taxis and smart phones available, detecting such abnormal TPR behaviors from GPS trajectories has become feasible. In this paper, we model the TPR behaviors and develop a system named Crowd TPR to detect TPR behaviors in real time by combining traffic data and crowd sourcing. It efficiently serves real-time monitoring requests from the traffic administrators and generates proper pick-up spots for passengers to select. In our system, we firstly propose a dynamic grid granularity selection method to achieve efficient map-matching and build spatio-temporal index on the road network. Through modeling taxis' routing behaviors, we predict arriving locations of taxis and recommend pick-up spots for passengers to push cluster-based human intelligent tasks (HITs). After passengers submit HIT results to Crowd TPR, we verify TPR behaviors by modeling the anomalous features. We build our system using real trajectory datasets generated by 33,000+ taxis and validate the system with extensive evaluations including over 1-month user study.},
booktitle = {Proceedings of the 2014 IEEE 11th Intl Conf on Ubiquitous Intelligence and Computing and 2014 IEEE 11th Intl Conf on Autonomic and Trusted Computing and 2014 IEEE 14th Intl Conf on Scalable Computing and Communications and Its Associated Workshops (UIC-ATC-ScalCom)},
pages = {18–25},
numpages = {8},
keywords = {crowdsourcing, flagged taxi, gps trajectory, human intelligent task, passenger-refusal},
series = {UIC-ATC-SCALCOM '14}
}

@inproceedings{10.1145/2556288.2557155,
author = {Alagarai Sampath, Harini and Rajeshuni, Rajeev and Indurkhya, Bipin},
title = {Cognitively inspired task design to improve user performance on crowdsourcing platforms},
year = {2014},
isbn = {9781450324731},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2556288.2557155},
doi = {10.1145/2556288.2557155},
abstract = {Recent research in human computation has focused on improving the quality of work done by crowd workers on crowdsourcing platforms. Multiple approaches have been adopted like filtering crowd workers through qualification tasks, and aggregating responses from multiple crowd workers to obtain consensus. We investigate here how improving the presentation of the task itself by using cognitively inspired features affects the performance of crowd workers. We illustrate this with a case-study for the task of extracting text from scanned images. We generated six task-presentation designs by modifying two parameters - visual saliency of the target fields and working memory requirements - and conducted experiments on Amazon Mechanical Turk (AMT) and with an eye-tracker in the lab setting. Our results identify which task-design parameters (e.g. highlighting target fields) result in improved performance, and which ones do not (e.g. reducing the number of distractors). In conclusion, we claim that the use of cognitively inspired features for task design is a powerful technique for maximizing the performance of crowd workers.},
booktitle = {Proceedings of the SIGCHI Conference on Human Factors in Computing Systems},
pages = {3665–3674},
numpages = {10},
keywords = {cognitive psychology, crowdsourcing, eye tracking, mechanical turk, task design, visual saliency, working memory},
location = {Toronto, Ontario, Canada},
series = {CHI '14}
}

@inproceedings{10.1145/2030100.2030108,
author = {Jayakanthan, Ranganathan and Sundararajan, Deepak},
title = {Enterprise crowdsourcing solutions for software development and ideation},
year = {2011},
isbn = {9781450309271},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2030100.2030108},
doi = {10.1145/2030100.2030108},
abstract = {This paper looks into two crowdsourcing applications which attempt to tackle software development and ideation problems in an enterprise environment. The software development crowdsourcing application (Crowdsourcer) is used for internal crowdsourcing of client and internal software development projects across self organizing groups and individuals. Crowdsourcer relies on workflows to identify capable free agents from inside the organization to design, build, test and appraise software projects through a functionality set which rely on crowdsourcing patterns such as virtual marketplaces, intelligent auctions and bidding processes, collaborative project management and reputation systems with virtual currencies and reward systems. The ideation crowdsourcing application features crowdsourced idea creation along with workflows to form actualization groups, which help in bringing an idea to fruition from concept stage. With advanced filter mechanisms, a reputation system and a system of peer based participatory marketplace for evaluation of ideas, this application features sophisticated crowdsourcing patterns to identify and execute ideas through harnessing the wisdom of the crowds.},
booktitle = {Proceedings of the 2nd International Workshop on Ubiquitous Crowdsouring},
pages = {25–28},
numpages = {4},
keywords = {crowd, crowdsourcing, enterprise crowdsourcing, social networks, social software, ubiquitous, web 2.0},
location = {Beijing, China},
series = {UbiCrowd '11}
}

@inproceedings{10.1007/978-3-642-34222-6_13,
author = {Ebden, Mark and Huynh, Trung Dong and Moreau, Luc and Ramchurn, Sarvapali and Roberts, Stephen},
title = {Network analysis on provenance graphs from a crowdsourcing application},
year = {2012},
isbn = {9783642342219},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-642-34222-6_13},
doi = {10.1007/978-3-642-34222-6_13},
abstract = {Crowdsourcing has become a popular means for quickly achieving various tasks in large quantities. CollabMap is an online mapping application in which we crowdsource the identification of evacuation routes in residential areas to be used for planning large-scale evacuations. So far, approximately 38,000 micro-tasks have been completed by over 100 contributors. In order to assist with data verification, we introduced provenance tracking into the application, and approximately 5,000 provenance graphs have been generated. They have provided us various insights into the typical characteristics of provenance graphs in the crowdsourcing context. In particular, we have estimated probability distribution functions over three selected characteristics of these provenance graphs: the node degree, the graph diameter, and the densification exponent. We describe methods to define these three characteristics across specific combinations of node types and edge types, and present our findings in this paper. Applications of our methods include rapid comparison of one provenance graph versus another, or of one style of provenance database versus another. Our results also indicate that provenance graphs represent a suitable area of exploitation for existing network analysis tools concerned with modelling, prediction, and the inference of missing nodes and edges.},
booktitle = {Proceedings of the 4th International Conference on Provenance and Annotation of Data and Processes},
pages = {168–182},
numpages = {15},
location = {Santa Barbara, CA},
series = {IPAW'12}
}

@inproceedings{10.1145/2009916.2010039,
author = {Blanco, Roi and Halpin, Harry and Herzig, Daniel M. and Mika, Peter and Pound, Jeffrey and Thompson, Henry S. and Tran Duc, Thanh},
title = {Repeatable and reliable search system evaluation using crowdsourcing},
year = {2011},
isbn = {9781450307574},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2009916.2010039},
doi = {10.1145/2009916.2010039},
abstract = {The primary problem confronting any new kind of search task is how to boot-strap a reliable and repeatable evaluation campaign, and a crowd-sourcing approach provides many advantages. However, can these crowd-sourced evaluations be repeated over long periods of time in a reliable manner? To demonstrate, we investigate creating an evaluation campaign for the semantic search task of keyword-based ad-hoc object retrieval. In contrast to traditional search over web-pages, object search aims at the retrieval of information from factual assertions about real-world objects rather than searching over web-pages with textual descriptions. Using the first large-scale evaluation campaign that specifically targets the task of ad-hoc Web object retrieval over a number of deployed systems, we demonstrate that crowd-sourced evaluation campaigns can be repeated over time and still maintain reliable results. Furthermore, we show how these results are comparable to expert judges when ranking systems and that the results hold over different evaluation and relevance metrics. This work provides empirical support for scalable, reliable, and repeatable search system evaluation using crowdsourcing.},
booktitle = {Proceedings of the 34th International ACM SIGIR Conference on Research and Development in Information Retrieval},
pages = {923–932},
numpages = {10},
keywords = {crowdsourcing, evaluation, retrieval, search engines},
location = {Beijing, China},
series = {SIGIR '11}
}

@inproceedings{10.1007/978-3-030-34407-8_6,
author = {Schnitzer, Steffen and Neitzel, Svenja and Schmidt, Sebastian and Rensing, Christoph},
title = {Results of a Survey About the Perceived Task Similarities in Micro Task Crowdsourcing Systems},
year = {2019},
isbn = {978-3-030-33906-7},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-34407-8_6},
doi = {10.1007/978-3-030-34407-8_6},
abstract = {Recommender mechanisms can support the assignment of jobs in crowdsourcing platforms. The use of recommendations can improve the quality and outcome for both worker and requester. Workers expect to get tasks similar to previously finished ones as recommendations, as a preceding study shows. Such similarities between tasks have to be identified and analyzed in order to create task recommendation systems that fulfil the workers’ requirements. How workers characterize task similarity has been left open in the previous study. Therefore, this work provides an empirical study on how workers perceive the similarities between tasks. Different similarity aspects (e.g., the complexity, required action or the requester of the task) are evaluated towards their usefulness and the results are discussed. Worker characteristics, such as age, experience and country of origin are taken into account to determine how different worker groups judge similarity aspects of tasks.},
booktitle = {Behavioral Analytics in Social and Ubiquitous Environments: 6th International Workshop on Mining Ubiquitous and Social Environments, MUSE 2015, Porto, Portugal, September 7, 2015; 6th International Workshop on Modeling Social Media, MSM 2015, Florence, Italy, May 19, 2015; 7th International Workshop on Modeling Social Media, MSM 2016, Montreal, QC, Canada, April 12, 2016; Revised Selected Papers},
pages = {107–125},
numpages = {19},
keywords = {Crowdsourcing, Recommender systems, User survey},
location = {Porto, Portugal}
}

@inproceedings{10.1145/2463676.2465307,
author = {Gao, Jinyang and Liu, Xuan and Ooi, Beng Chin and Wang, Haixun and Chen, Gang},
title = {An online cost sensitive decision-making method in crowdsourcing systems},
year = {2013},
isbn = {9781450320375},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2463676.2465307},
doi = {10.1145/2463676.2465307},
abstract = {Crowdsourcing has created a variety of opportunities for many challenging problems by leveraging human intelligence. For example, applications such as image tagging, natural language processing, and semantic-based information retrieval can exploit crowd-based human computation to supplement existing computational algorithms. Naturally, human workers in crowdsourcing solve problems based on their knowledge, experience, and perception. It is therefore not clear which problems can be better solved by crowdsourcing than solving solely using traditional machine-based methods. Therefore, a cost sensitive quantitative analysis method is needed.In this paper, we design and implement a cost sensitive method for crowdsourcing. We online estimate the profit of the crowdsourcing job so that those questions with no future profit from crowdsourcing can be terminated. Two models are proposed to estimate the profit of crowdsourcing job, namely the linear value model and the generalized non-linear model. Using these models, the expected profit of obtaining new answers for a specific question is computed based on the answers already received. A question is terminated in real time if the marginal expected profit of obtaining more answers is not positive. We extends the method to publish a batch of questions in a HIT. We evaluate the effectiveness of our proposed method using two real world jobs on AMT. The experimental results show that our proposed method outperforms all the state-of-art methods.},
booktitle = {Proceedings of the 2013 ACM SIGMOD International Conference on Management of Data},
pages = {217–228},
numpages = {12},
keywords = {crowdsourcing, decision-making},
location = {New York, New York, USA},
series = {SIGMOD '13}
}

@inproceedings{10.5555/2893873.2893985,
author = {Jain, Shweta and Narayanaswamy, Balakrishnan and Narahari, Y.},
title = {A multiarmed bandit incentive mechanism for crowdsourcing demand response in smart grids},
year = {2014},
publisher = {AAAI Press},
abstract = {Demand response is a critical part of renewable integration and energy cost reduction goals across the world. Motivated by the need to reduce costs arising from electricity shortage and renewable energy fluctuations, we propose a novel multiarmed bandit mechanism for demand response (MAB-MDR) which makes monetary offers to strategic consumers who have unknown response characteristics, to incetivize reduction in demand. Our work is inspired by a novel connection we make to crowdsourcing mechanisms. The proposed mechanism incorporates realistic features of the demand response problem including time varying and quadratic cost function. The mechanism marries auctions, that allow users to report their preferences, with online algorithms, that allow distribution companies to learn user-specific parameters. We show that MAB-MDR is dominant strategy incentive compatible, individually rational, and achieves sublinear regret. Such mechanisms can be effectively deployed in smart grids using new information and control architecture innovations and lead to welcome savings in energy costs.},
booktitle = {Proceedings of the Twenty-Eighth AAAI Conference on Artificial Intelligence},
pages = {721–727},
numpages = {7},
location = {Qu\'{e}bec City, Qu\'{e}bec, Canada},
series = {AAAI'14}
}

@inproceedings{10.1145/3529372.3533285,
author = {Oelen, Allard and Stocker, Markus and Auer, S\"{o}ren},
title = {TinyGenius: intertwining natural language processing with microtask crowdsourcing for scholarly knowledge graph creation},
year = {2022},
isbn = {9781450393454},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3529372.3533285},
doi = {10.1145/3529372.3533285},
abstract = {As the number of published scholarly articles grows steadily each year, new methods are needed to organize scholarly knowledge so that it can be more efficiently discovered and used. Natural Language Processing (NLP) techniques are able to autonomously process scholarly articles at scale and to create machine readable representations of the article content. However, autonomous NLP methods are by far not sufficiently accurate to create a high-quality knowledge graph. Yet quality is crucial for the graph to be useful in practice. We present TinyGenius, a methodology to validate NLP-extracted scholarly knowledge statements using microtasks performed with crowdsourcing. The scholarly context in which the crowd workers operate has multiple challenges. The explainability of the employed NLP methods is crucial to provide context in order to support the decision process of crowd workers. We employed TinyGenius to populate a paper-centric knowledge graph, using five distinct NLP methods. In the end, the resulting knowledge graph serves as a digital library for scholarly articles.},
booktitle = {Proceedings of the 22nd ACM/IEEE Joint Conference on Digital Libraries},
articleno = {5},
numpages = {5},
keywords = {crowdsourcing microtasks, intelligent user interfaces, knowledge graph validation, scholarly knowledge graphs},
location = {Cologne, Germany},
series = {JCDL '22}
}

@inproceedings{10.1145/3372787.3390435,
author = {Abhinav, Kumar and Bhatia, Gurpriya Kaur and Dubey, Alpana and Jain, Sakshi and Bhardwaj, Nitish},
title = {TasRec: a framework for task recommendation in crowdsourcing},
year = {2020},
isbn = {9781450370936},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3372787.3390435},
doi = {10.1145/3372787.3390435},
abstract = {Lately, crowdsourcing has emerged as a viable option of getting work done by leveraging the collective intelligence of the crowd. With many tasks posted every day, the size of crowdsourcing platforms is growing exponentially. Hence, workers face an important challenge of selecting the right task. Despite the task filtering criteria available on the platform to select the right task, crowd workers find it difficult to choose the most relevant task and must glean through the filtered tasks to find the relevant tasks. In this paper, we propose a framework for recommending tasks to workers. The proposed framework evaluates the worker's fitment over the tasks based on worker's preference, past tasks (s)he has performed, and tasks done by similar workers. We evaluated our approach on the datasets collected from popular crowdsourcing platform. Our experimental results based on 5,000 tasks and 3,000 workers show that the recommendation made by our framework is significantly better as compared to the baseline approach.},
booktitle = {Proceedings of the 15th International Conference on Global Software Engineering},
pages = {86–95},
numpages = {10},
keywords = {crowdsourcing, personalization, recommendation, task selection},
location = {Seoul, Republic of Korea},
series = {ICGSE '20}
}

@inproceedings{10.1007/978-3-642-24873-3_21,
author = {Viappiani, Paolo and Zilles, Sandra and Hamilton, Howard J. and Boutilier, Craig},
title = {Learning Complex Concepts Using Crowdsourcing: A Bayesian Approach},
year = {2011},
isbn = {978-3-642-24872-6},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-642-24873-3_21},
doi = {10.1007/978-3-642-24873-3_21},
abstract = {We develop a Bayesian approach to concept learning for crowdsourcing applications. A probabilistic belief over possible concept definitions is maintained and updated according to (noisy) observations from experts, whose behaviors are modeled using discrete types. We propose recommendation techniques, inference methods, and query selection strategies to assist a user charged with choosing a configuration that satisfies some (partially known) concept. Our model is able to simultaneously learn the concept definition and the types of the experts. We evaluate our model with simulations, showing that our Bayesian strategies are effective even in large concept spaces with many uninformative experts.},
booktitle = {Algorithmic Decision Theory: Second International Conference, ADT 2011, Piscataway, NJ, USA, October 26-28, 2011. Proceedings},
pages = {277–291},
numpages = {15},
keywords = {Bayesian Approach, Recommender System, Latent Concept, Current Belief, True Concept},
location = {Piscataway, NJ, USA}
}

@inproceedings{10.1145/3406865.3418318,
author = {Ram\'{\i}rez, Jorge and Baez, Marcos and Casati, Fabio and Cernuzzi, Luca and Benatallah, Boualem},
title = {DREC: towards a Datasheet for Reporting Experiments in Crowdsourcing},
year = {2020},
isbn = {9781450380591},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3406865.3418318},
doi = {10.1145/3406865.3418318},
abstract = {Factors such as instructions, payment schemes, platform demographics, along with strategies for mapping studies into crowdsourcing environments, play an important role in the reproducibility of results. However, inferring these details from scientific articles is often a challenging endeavor, calling for the development of proper reporting guidelines. This paper makes the first steps towards this goal, by describing an initial taxonomy of relevant attributes for crowdsourcing experiments, and providing a glimpse into the state of reporting by analyzing a sample of CSCW papers.},
booktitle = {Companion Publication of the 2020 Conference on Computer Supported Cooperative Work and Social Computing},
pages = {377–382},
numpages = {6},
keywords = {crowdsourcing, crowdsourcing experiments, guidelines for reporting experiments, taxonomy},
location = {Virtual Event, USA},
series = {CSCW '20 Companion}
}

@inproceedings{10.5555/2606265.2606914,
author = {Chaisiri, Sivadon},
title = {Utilizing Human Intelligence in a Crowdsourcing Marketplace for Big Data Processing},
year = {2013},
isbn = {9781479920815},
publisher = {IEEE Computer Society},
address = {USA},
abstract = {Crowd sourcing emerging as a distributed problem-solving model can dispatch tasks to a large number of human workers. The workers' human intelligence can be utilized to execute tasks which cannot be efficiently performed by computer-based systems. A crowd sourcing environment supports various data processing tasks, for example, data collection, data annotation, data validation, data classification, and natural language processing. In particular, the crowd sourcing environment can be applied for big data processing as well. In this paper, we propose Human-Intelligence-as-a-Service (HIaaS) which is a cloud computing service model utilizing human intelligence in a crowd sourcing marketplace to process big data. We also propose an optimization model based on stochastic programming for provisioning human workers in a HIaaS-based marketplace. Numerical studies are performed to evaluate the optimization model. A video rating system handling a big data set is an illustrative example analyzed in the studies. The results show that the model can efficiently reduce the cost to provision workers for processing a big data job.},
booktitle = {Proceedings of the 2013 International Conference on Parallel and Distributed Systems},
pages = {633–638},
numpages = {6},
keywords = {big data, cloud computing, crowdsourcing, stochastic programming},
series = {ICPADS '13}
}

@inproceedings{10.1145/2568225.2568249,
author = {Stol, Klaas-Jan and Fitzgerald, Brian},
title = {Two's company, three's a crowd: a case study of crowdsourcing software development},
year = {2014},
isbn = {9781450327565},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2568225.2568249},
doi = {10.1145/2568225.2568249},
abstract = {Crowdsourcing is an emerging and promising approach which involves delegating a variety of tasks to an unknown workforce - the crowd. Crowdsourcing has been applied quite successfully in various contexts from basic tasks on Amazon Mechanical Turk to solving complex industry problems, e.g. InnoCentive. Companies are increasingly using crowdsourcing to accomplish specific software development tasks. However, very little research exists on this specific topic. This paper presents an in-depth industry case study of crowdsourcing software development at a multinational corporation. Our case study highlights a number of challenges that arise when crowdsourcing software development. For example, the crowdsourcing development process is essentially a waterfall model and this must eventually be integrated with the agile approach used by the company. Crowdsourcing works better for specific software development tasks that are less complex and stand-alone without interdependencies. The development cost was much greater than originally expected, overhead in terms of company effort to prepare specifications and answer crowdsourcing community queries was much greater, and the time-scale to complete contests, review submissions and resolve quality issues was significant. Finally, quality issues were pushed later in the lifecycle given the lengthy process necessary to identify and resolve quality issues. Given the emphasis in software engineering on identifying bugs as early as possible, this is quite problematic.},
booktitle = {Proceedings of the 36th International Conference on Software Engineering},
pages = {187–198},
numpages = {12},
keywords = {Crowdsourcing, case study, challenges, software development},
location = {Hyderabad, India},
series = {ICSE 2014}
}

@inproceedings{10.1145/2525314.2525370,
author = {Deng, Dingxiong and Shahabi, Cyrus and Demiryurek, Ugur},
title = {Maximizing the number of worker's self-selected tasks in spatial crowdsourcing},
year = {2013},
isbn = {9781450325219},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2525314.2525370},
doi = {10.1145/2525314.2525370},
abstract = {With the progress of mobile devices and wireless broadband, a new eMarket platform, termed spatial crowdsourcing is emerging, which enables workers (aka crowd) to perform a set of spatial tasks (i.e., tasks related to a geographical location and time) posted by a requester. In this paper, we study a version of the spatial crowd-sourcing problem in which the workers autonomously select their tasks, called the worker selected tasks (WST) mode. Towards this end, given a worker, and a set of tasks each of which is associated with a location and an expiration time, we aim to find a schedule for the worker that maximizes the number of performed tasks. We first prove that this problem is NP-hard. Subsequently, for small number of tasks, we propose two exact algorithms based on dynamic programming and branch-and-bound strategies. Since the exact algorithms cannot scale for large number of tasks and/or limited amount of resources on mobile platforms, we also propose approximation and progressive algorithms. We conducted a thorough experimental evaluation on both real-world and synthetic data to compare the performance and accuracy of our proposed approaches.},
booktitle = {Proceedings of the 21st ACM SIGSPATIAL International Conference on Advances in Geographic Information Systems},
pages = {324–333},
numpages = {10},
keywords = {crowdsourcing, spatial crowdsourcing, spatial task assignment},
location = {Orlando, Florida},
series = {SIGSPATIAL'13}
}

@inproceedings{10.1145/3246859,
author = {Abdelmalik, Philip},
title = {Session details: Session 4 -- Big Data Analytics and Crowdsourcing for Public Health 1},
year = {2015},
isbn = {9781450334921},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3246859},
doi = {10.1145/3246859},
booktitle = {Proceedings of the 5th International Conference on Digital Health 2015},
location = {Florence, Italy},
series = {DH '15}
}

@inproceedings{10.5555/1564131.1564137,
author = {Hsueh, Pei-Yun and Melville, Prem and Sindhwani, Vikas},
title = {Data quality from crowdsourcing: a study of annotation selection criteria},
year = {2009},
publisher = {Association for Computational Linguistics},
address = {USA},
abstract = {Annotation acquisition is an essential step in training supervised classifiers. However, manual annotation is often time-consuming and expensive. The possibility of recruiting annotators through Internet services (e.g., Amazon Mechanic Turk) is an appealing option that allows multiple labeling tasks to be outsourced in bulk, typically with low overall costs and fast completion rates. In this paper, we consider the difficult problem of classifying sentiment in political blog snippets. Annotation data from both expert annotators in a research lab and non-expert annotators recruited from the Internet are examined. Three selection criteria are identified to select high-quality annotations: noise level, sentiment ambiguity, and lexical uncertainty. Analysis confirm the utility of these criteria on improving data quality. We conduct an empirical study to examine the effect of noisy annotations on the performance of sentiment classification models, and evaluate the utility of annotation selection on classification accuracy and efficiency.},
booktitle = {Proceedings of the NAACL HLT 2009 Workshop on Active Learning for Natural Language Processing},
pages = {27–35},
numpages = {9},
location = {Boulder, Colorado},
series = {HLT '09}
}

@inproceedings{10.5555/2772879.2773291,
author = {Biswas, Arpita and Jain, Shweta and Mandal, Debmalya and Narahari, Y.},
title = {A Truthful Budget Feasible Multi-Armed Bandit Mechanism for Crowdsourcing Time Critical Tasks},
year = {2015},
isbn = {9781450334136},
publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
address = {Richland, SC},
abstract = {Motivated by allocation and pricing problems faced by service requesters on modern crowdsourcing platforms, we study a multi-armed bandit (MAB) problem with several real-world features: (a) the requester wishes to crowdsource a number of tasks but has a fixed budget which leads to a trade-off between cost and quality while allocating tasks to workers; (b) each task has a fixed deadline and a worker who is allocated a task is not available until this deadline; (c) the qualities (probability of completing a task successfully within deadline) of crowd workers are not known; and (d) the crowd workers are strategic about their costs. We propose a mechanism that maximizes the expected number of successfully completed tasks, assuring budget feasibility, incentive compatibility, and individual rationality. We establish an upper bound of O(B2/3(K ln(KB))1/3) on the expected regret of the proposed mechanism with respect to an appropriate benchmark algorithm, where B is the total budget and K is the number of workers. Next, we provide a characterization of any deterministic truthful mechanism that solves the above class of problems and use this characterization to establish a lower bound of Ω(B2=3K1=3) on the expected regret for any budgeted MAB mechanism satisfying the above properties.},
booktitle = {Proceedings of the 2015 International Conference on Autonomous Agents and Multiagent Systems},
pages = {1101–1109},
numpages = {9},
keywords = {crowdsourcing, mechanism design, multi-armed bandits, online learning, rational agents, regret bound},
location = {Istanbul, Turkey},
series = {AAMAS '15}
}

@inproceedings{10.1109/HICSS.2013.64,
author = {Walter, Thomas P. and Back, Andrea},
title = {A Text Mining Approach to Evaluate Submissions to Crowdsourcing Contests},
year = {2013},
isbn = {9780769548920},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/HICSS.2013.64},
doi = {10.1109/HICSS.2013.64},
abstract = {This survey deals with the problem of evaluating the submissions to crowd sourcing websites on which data is increasing rapidly in both volume and complexity. Usually expert committees are installed to rate submissions, select winners and adjust monetary rewards. Thus, with an increasing number of submissions, this process is getting more complex, time-consuming and hence expensive. In this paper we suggest following text mining methodology, foremost similarity measurements and clustering algorithms, to evaluate the quality of submissions to crowd sourcing contests semi-automatically. We evaluate our approach by comparing text mining based measurement of more than 40'000 submissions with the real-world decisions made by expert committees using Precision and Recall together with F1-score.},
booktitle = {Proceedings of the 2013 46th Hawaii International Conference on System Sciences},
pages = {3109–3118},
numpages = {10},
keywords = {crowdsourcing, ideation contest, information retrieval, text mining},
series = {HICSS '13}
}

@inproceedings{10.1145/1600150.1600168,
author = {Stewart, Osamuyimen and Huerta, Juan M. and Sader, Melissa},
title = {Designing crowdsourcing community for the enterprise},
year = {2009},
isbn = {9781605586724},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1600150.1600168},
doi = {10.1145/1600150.1600168},
abstract = {In this paper, we describe the design principles used for implementing crowdsourcing within the enterprise. This is based on our distinction between two kinds of crowdsourcing: enterprise (inside a firewall) versus the public domain. Whereas public domain crowdsourcing offers monetary rewards in exchange for participation, we show that identifying the right social objects and using these in designing the incentive model is sufficient to incent, motivate, and sustain participation levels in enterprise crowdsourcing. Finally, we show that the systematic integration of the characteristics of levels of participation into the design, e.g., the distinction between direct and indirect crowdsourcing, is sufficient for optimizing users' participation and contributions.},
booktitle = {Proceedings of the ACM SIGKDD Workshop on Human Computation},
pages = {50–53},
numpages = {4},
keywords = {crowdsourcing, design principles, incentives, social objects},
location = {Paris, France},
series = {HCOMP '09}
}

@inproceedings{10.5555/2484920.2485063,
author = {Tran-Thanh, Long and Venanzi, Matteo and Rogers, Alex and Jennings, Nicholas R.},
title = {Efficient budget allocation with accuracy guarantees for crowdsourcing classification tasks},
year = {2013},
isbn = {9781450319935},
publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
address = {Richland, SC},
abstract = {In this paper we address the problem of budget allocation for redundantly crowdsourcing a set of classification tasks where a key challenge is to find a trade-off between the total cost and the accuracy of estimation. We propose CrowdBudget, an agent-based budget allocation algorithm, that efficiently divides a given budget among different tasks in order to achieve low estimation error. In particular, we prove that CrowdBudget can achieve at most max{0, K/2- O,(√B)} estimation error with high probability, where K is the number of tasks and B is the budget size. This result significantly outperforms the current best theoretical guarantee from Karger et al,. In addition, we demonstrate that our algorithm outperforms existing methods by up to 40\% in experiments based on real-world data from a prominent database of crowdsourced classification responses.},
booktitle = {Proceedings of the 2013 International Conference on Autonomous Agents and Multi-Agent Systems},
pages = {901–908},
numpages = {8},
keywords = {budget limit, crowdsourcing, regret bounds, task allocation},
location = {St. Paul, MN, USA},
series = {AAMAS '13}
}

@inproceedings{10.1109/ICEE.2010.1230,
author = {Peng, Guo},
title = {The Preliminary Settlement of Crowdsourcing Legal Issues},
year = {2010},
isbn = {9780769539973},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/ICEE.2010.1230},
doi = {10.1109/ICEE.2010.1230},
abstract = {Crowdsourcing was defined as the act of taking tasks traditionally performed by an employee or contractor, and outsourcing them to a group (crowd) of people or community in the form of an open call. The legal issues of crowdsourcing is mainly manifested in three aspects: the legal relationship between contract-issuing party and WitKey, the risk of misuse of personal information and the protection of intellectual property rights. Initial solutions to the problems include: to sign an underlying contract, disclosure of basic information on contract-issuing party and the provisions of fixed and processing personal information about Witkey.},
booktitle = {Proceedings of the 2010 International Conference on E-Business and E-Government},
pages = {4898–4900},
numpages = {3},
keywords = {crowdsourcing, legal issues, settlement},
series = {ICEE '10}
}

@proceedings{10.1145/2506364,
title = {CrowdMM '13: Proceedings of the 2nd ACM international workshop on Crowdsourcing for multimedia},
year = {2013},
isbn = {9781450323963},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {Crowdsourcing for multimedia exploits a combination of human intelligence and a large number of individual human contributions. In the field of multimedia, crowdsourcing is just beginning to realize its full potential to contribute to techniques, systems and data sets that advance the state of the art. The ACM Multimedia 2013 Workshop on Crowdsourcing for Multimedia (CrowdMM 2013) provides a forum for the presentation of crowdsourcing techniques for multimedia and the discussion of innovative ideas that will allow the field of multimedia research to make use of crowdsourcing.The workshop uses presentations, posters and a panel to promote discussion and exchange between researchers concerning the scope and future of crowdsourcing. The workshop defines crowdsourcing in the broad sense: it encompasses both unsolicited human contributions, e.g., tags assigned by users to images, and also solicited contributions, e.g., annotations gathered by making use of crowdsourcing platforms that micro-outsource tasks to a large pool of human workers. As in 2012, the workshop pursues two high-level goals. First, it provides a venue to encourage multimedia research making use of human intelligence and taking advantage of human plurality. Second, the workshop gives an impetus to the multimedia community to define best practices for the use of crowdsourcing in multimedia and to shape emerging paradigms that will allow crowdsourcing to push forward the state of the art in multimedia research.},
location = {Barcelona, Spain}
}

@inproceedings{10.5555/1996889.1996910,
author = {Alonso, Omar and Baeza-Yates, Ricardo},
title = {Design and implementation of relevance assessments using crowdsourcing},
year = {2011},
isbn = {9783642201608},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {In the last years crowdsourcing has emerged as a viable platform for conducting relevance assessments. The main reason behind this trend is that makes possible to conduct experiments extremely fast, with good results and at low cost. However, like in any experiment, there are several details that would make an experiment work or fail. To gather useful results, user interface guidelines, inter-agreement metrics, and justification analysis are important aspects of a successful crowdsourcing experiment. In this work we explore the design and execution of relevance judgments using Amazon Mechanical Turk as crowdsourcing platform, introducing a methodology for crowdsourcing relevance assessments and the results of a series of experiments using TREC 8 with a fixed budget. Our findings indicate that workers are as good as TREC experts, even providing detailed feedback for certain query-document pairs. We also explore the importance of document design and presentation when performing relevance assessment tasks. Finally, we show our methodology at work with several examples that are interesting in their own.},
booktitle = {Proceedings of the 33rd European Conference on Advances in Information Retrieval},
pages = {153–164},
numpages = {12},
location = {Dublin, Ireland},
series = {ECIR'11}
}

@inproceedings{10.1007/978-3-642-20161-5_16,
author = {Alonso, Omar and Baeza-Yates, Ricardo},
title = {Design and Implementation of Relevance Assessments Using Crowdsourcing},
year = {2011},
isbn = {9783642201608},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-642-20161-5_16},
doi = {10.1007/978-3-642-20161-5_16},
abstract = {In the last years crowdsourcing has emerged as a viable platform for conducting relevance assessments. The main reason behind this trend is that makes possible to conduct experiments extremely fast, with good results and at low cost. However, like in any experiment, there are several details that would make an experiment work or fail. To gather useful results, user interface guidelines, inter-agreement metrics, and justification analysis are important aspects of a successful crowdsourcing experiment. In this work we explore the design and execution of relevance judgments using Amazon Mechanical Turk as crowdsourcing platform, introducing a methodology for crowdsourcing relevance assessments and the results of a series of experiments using TREC 8 with a fixed budget. Our findings indicate that workers are as good as TREC experts, even providing detailed feedback for certain query-document pairs. We also explore the importance of document design and presentation when performing relevance assessment tasks. Finally, we show our methodology at work with several examples that are interesting in their\"{\i} \'{z}own.},
booktitle = {Proceedings of the 33rd European Conference on Advances in Information Retrieval - Volume 6611},
pages = {153–164},
numpages = {12},
location = {Dublin, Ireland},
series = {ECIR 2011}
}

@inproceedings{10.1007/978-3-319-03889-6_13,
author = {Zhang, Gang and Chen, Haopeng},
title = {Quality Control of Massive Data for Crowdsourcing in Location-Based Services},
year = {2013},
isbn = {978-3-319-03888-9},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-319-03889-6_13},
doi = {10.1007/978-3-319-03889-6_13},
abstract = {Crowdsourcing has become a prospective paradigm for commercial purposes in the past decade, since it is based on a simple but powerful concept that virtually anyone has the potential to plug in valuable information, which brings a lot of benefits such as low cost and high immediacy, particularly in some location-based services (LBS). On the other side, there also exist many problems need to be solved in crowdsourcing. For example, the quality control for crowdsourcing systems has been identified as a significant challenge, which includes how to handle massive data more efficiently, how to discriminate poor quality content in workers’ submission and so on. In this paper, we put forward an approach to control the crowdsourcing quality by evaluating workers’ performance according to their submitted contents. Our experiments have demonstrated the effectiveness and efficiency of the approach.},
booktitle = {Algorithms and Architectures for Parallel Processing: 13th International Conference, ICA3PP 2013, Vietri Sul Mare, Italy, December 18-20, 2013, Proceedings, Part II},
pages = {112–121},
numpages = {10},
keywords = {crowdsourcing, massive data, quality control, location-based services (LBS)},
location = {Vietri sul Mare, Italy}
}

@inproceedings{10.5555/1866696.1866723,
author = {Grady, Catherine and Lease, Matthew},
title = {Crowdsourcing document relevance assessment with Mechanical Turk},
year = {2010},
publisher = {Association for Computational Linguistics},
address = {USA},
abstract = {We investigate human factors involved in designing effective Human Intelligence Tasks (HITs) for Amazon's Mechanical Turk. In particular, we assess document relevance to search queries via MTurk in order to evaluate search engine accuracy. Our study varies four human factors and measures resulting experimental outcomes of cost, time, and accuracy of the assessments. While results are largely inconclusive, we identify important obstacles encountered, lessons learned, related work, and interesting ideas for future investigation. Experimental data is also made publicly available for further study by the community.},
booktitle = {Proceedings of the NAACL HLT 2010 Workshop on Creating Speech and Language Data with Amazon's Mechanical Turk},
pages = {172–179},
numpages = {8},
location = {Los Angeles, California},
series = {CSLDAMT '10}
}

@inproceedings{10.1145/2232817.2232840,
author = {Chen, Yinlin and Bogen, Paul Logasa and Hsieh, Haowei and Fox, Edward A. and Cassel, Lillian N.},
title = {Categorization of computing education resources with utilization of crowdsourcing},
year = {2012},
isbn = {9781450311540},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2232817.2232840},
doi = {10.1145/2232817.2232840},
abstract = {The Ensemble Portal harvests resources from multiple heterogeneous federated collections. Managing these dynamically increasing collections requires an automatic mechanism to categorize records in to corresponding topics. We propose an approach to use existing ACM DL metadata to build classifiers for harvested resources in the Ensemble project. We also present our experience with utilizing the Amazon Mechanical Turk platform to build ground truth training data sets from Ensemble collections.},
booktitle = {Proceedings of the 12th ACM/IEEE-CS Joint Conference on Digital Libraries},
pages = {121–124},
numpages = {4},
keywords = {amazon mechanical turk, classification, digital libraries, machine learning},
location = {Washington, DC, USA},
series = {JCDL '12}
}

@inproceedings{10.5555/2393015.2393073,
author = {Post, Matt and Callison-Burch, Chris and Osborne, Miles},
title = {Constructing parallel corpora for six Indian languages via crowdsourcing},
year = {2012},
publisher = {Association for Computational Linguistics},
address = {USA},
abstract = {Recent work has established the efficacy of Amazon's Mechanical Turk for constructing parallel corpora for machine translation research. We apply this to building a collection of parallel corpora between English and six languages from the Indian subcontinent: Bengali, Hindi, Malayalam, Tamil, Telugu, and Urdu. These languages are low-resource, under-studied, and exhibit linguistic phenomena that are difficult for machine translation. We conduct a variety of baseline experiments and analysis, and release the data to the community.},
booktitle = {Proceedings of the Seventh Workshop on Statistical Machine Translation},
pages = {401–409},
numpages = {9},
location = {Montreal, Canada},
series = {WMT '12}
}

@inproceedings{10.5555/2029127.2029152,
author = {Koch, Giordano and F\"{u}ller, Johann and Brunswicker, Sabine},
title = {Online crowdsourcing in the public sector: how to design open government platforms},
year = {2011},
isbn = {9783642217951},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {The trend towards "open innovation" has revitalized firm's interest in tapping into external innovation sources. Firms purposively open their business models to connect internal and external ideas, and to co-create value with partners and users. Internet-based crowdsourcing and co-creation platforms have changed the way how firms implement open innovation. They allow new participatory problem solving and value-creation processes. However, the current discussion on open innovation has hardly touched upon the public sector. This paper investigates if crowdsourcing platforms can be applied in the governmental context, and under which conditions. Results show that crowdsourcing may generate strong interest among citizens and may serve as source of new high quality input. However, our findings also indicate that design principles derived from open innovation projects in the corporate world may not be directly applied in the governmental context; they need to be adjusted and complemented.},
booktitle = {Proceedings of the 4th International Conference on Online Communities and Social Computing},
pages = {203–212},
numpages = {10},
keywords = {crowdsourcing, design principles, open government, open innovation, public management, virtual co-creation platform},
location = {Orlando, FL},
series = {OCSC'11}
}

@inproceedings{10.1145/3410670.3410862,
author = {Ceccarini, Chiara and Delnevo, Giovanni and Prandi, Catia},
title = {FruGar: Exploiting Deep Learning and Crowdsourcing for Frugal Gardening},
year = {2020},
isbn = {9781450380782},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3410670.3410862},
doi = {10.1145/3410670.3410862},
abstract = {To make smart cities more sustainable, and technologies and smart services accessible to a broader range of people, the frugal innovation paradigm comes in handy. In such a scenario, an interesting issue to investigate is gardening (both home and community gardening), considered a possibility toward sustainable development and environmental resilience. Following this line of thought, in this paper, we present our system, called FruGar (frugal gardening), designed and developed to facilitate casual citizens while gardening. In particular, our approach takes advantage of machine learning and crowdsourcing to provide casual citizens with a frugal tool for plant disease detection.},
booktitle = {Proceedings of the 1st Workshop on Experiences with the Design and Implementation of Frugal Smart Objects},
pages = {7–11},
numpages = {5},
keywords = {convolutional neural network, plant disease identification, web application},
location = {London, United Kingdom},
series = {FRUGALTHINGS'20}
}

@inproceedings{10.5555/1964698.1964721,
author = {Fu, Wai-Tat and Liao, Vera},
title = {Crowdsourcing quality control of online information: a quality-based cascade model},
year = {2011},
isbn = {9783642196553},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {We extend previous cascade models of social influence to investigate how the exchange of quality information among users may moderate cascade behavior, and the extent to which it may influence the effectiveness of collective user recommendations on quality control of information. We found that while cascades do sometimes occur, their effects depend critically on the accuracies of individual quality assessments of information contents. Contrary to predictions of cascade models of information flow, quality-based cascades tend to reinforce the propagation of individual quality assessments rather than being the primary sources that drive the assessments. We found even small increase in individual accuracies will significantly improve the overall effectiveness of crowdsourcing quality control. Implication to domains such as online health information Web sites or product reviews are discussed.},
booktitle = {Proceedings of the 4th International Conference on Social Computing, Behavioral-Cultural Modeling and Prediction},
pages = {147–154},
numpages = {8},
keywords = {information cascades, social dynamics, user reviews, web quality},
location = {College Park, MD},
series = {SBP'11}
}

@inproceedings{10.1109/HICSS.2015.549,
author = {Buettner, Ricardo},
title = {A Systematic Literature Review of Crowdsourcing Research from a Human Resource Management Perspective},
year = {2015},
isbn = {9781479973675},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/HICSS.2015.549},
doi = {10.1109/HICSS.2015.549},
abstract = {From a human resource management perspective I review the crowd sourcing literature included in top peer-reviewed journals and conferences, and build up a comprehensive picture. Based on this I identify empirical and design-oriented research needs.},
booktitle = {Proceedings of the 2015 48th Hawaii International Conference on System Sciences},
pages = {4609–4618},
numpages = {10},
keywords = {coordination problem, crowdsourcing, human resource management perspective, literature review, research framework},
series = {HICSS '15}
}

@inproceedings{10.5555/2343576.2343643,
author = {Kamar, Ece and Hacker, Severin and Horvitz, Eric},
title = {Combining human and machine intelligence in large-scale crowdsourcing},
year = {2012},
isbn = {0981738117},
publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
address = {Richland, SC},
abstract = {We show how machine learning and inference can be harnessed to leverage the complementary strengths of humans and computational agents to solve crowdsourcing tasks. We construct a set of Bayesian predictive models from data and describe how the models operate within an overall crowd-sourcing architecture that combines the efforts of people and machine vision on the task of classifying celestial bodies defined within a citizens' science project named Galaxy Zoo. We show how learned probabilistic models can be used to fuse human and machine contributions and to predict the behaviors of workers. We employ multiple inferences in concert to guide decisions on hiring and routing workers to tasks so as to maximize the efficiency of large-scale crowdsourcing processes based on expected utility.},
booktitle = {Proceedings of the 11th International Conference on Autonomous Agents and Multiagent Systems - Volume 1},
pages = {467–474},
numpages = {8},
keywords = {complementary computing, consensus tasks, crowdsourcing, decision-theoretic reasoning},
location = {Valencia, Spain},
series = {AAMAS '12}
}

@inproceedings{10.5555/3045118.3045231,
author = {Abbasi-Yadkori, Yasin and Bartlett, Peter L. and Chen, Xi and Malek, Alan},
title = {Large-scale Markov decision problems with KL control cost and its application to crowdsourcing},
year = {2015},
publisher = {JMLR.org},
abstract = {We study average and total cost Markov decision problems with large state spaces. Since the computational and statistical cost of finding the optimal policy scales with the size of the state space, we focus on searching for near-optimality in a low-dimensional family of policies. In particular, we show that for problems with a Kullback-Leibler divergence cost function, we can recast policy optimization as a convex optimization and solve it approximately using a stochastic subgradient algorithm. This method scales in complexity with the family of policies but not the state space. We show that the performance of the resulting policy is close to the best in the low-dimensional family. We demonstrate the efficacy of our approach by optimizing a policy for budget allocation in crowd labeling, an important crowdsourcing application.},
booktitle = {Proceedings of the 32nd International Conference on International Conference on Machine Learning - Volume 37},
pages = {1053–1062},
numpages = {10},
location = {Lille, France},
series = {ICML'15}
}

@inproceedings{10.1145/2396761.2398697,
author = {Kazai, Gabriella and Kamps, Jaap and Milic-Frayling, Natasa},
title = {The face of quality in crowdsourcing relevance labels: demographics, personality and labeling accuracy},
year = {2012},
isbn = {9781450311564},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2396761.2398697},
doi = {10.1145/2396761.2398697},
abstract = {Information retrieval systems require human contributed relevance labels for their training and evaluation. Increasingly such labels are collected under the anonymous, uncontrolled conditions of crowdsourcing, leading to varied output quality. While a range of quality assurance and control techniques have now been developed to reduce noise during or after task completion, little is known about the workers themselves and possible relationships between workers' characteristics and the quality of their work. In this paper, we ask how do the relatively well or poorly-performing crowds, working under specific task conditions, actually look like in terms of worker characteristics, such as demographics or personality traits. Our findings show that the face of a crowd is in fact indicative of the quality of their work.},
booktitle = {Proceedings of the 21st ACM International Conference on Information and Knowledge Management},
pages = {2583–2586},
numpages = {4},
keywords = {crowdsourcing, demographics, personality traits, worker accuracy},
location = {Maui, Hawaii, USA},
series = {CIKM '12}
}

@inproceedings{10.1109/EIDWT.2012.17,
author = {Carter, Sarah and Smith, Martin and Bali, Suveena and Sotiriadis, Stelios and Bessis, Nik and Hill, Richard},
title = {The Use of Crowdsourcing to Aid Quest Design in Games},
year = {2012},
isbn = {9780769547343},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/EIDWT.2012.17},
doi = {10.1109/EIDWT.2012.17},
abstract = {In recent years, the usage of emerging technologies such as Grids, Clouds, data-smashups and crowd sourcing has become more widespread. This owes a lot to the continued growth of the Internet technology which has contributed to people becoming more comfortable about sharing information online, and using the Internet to communicate with others for both personal and professional purposes. The recent surge in smart phone usage has also helped to increase people's use of technology wherever they are - leading to innovative use of mobile phones to source and generate data for a variety of purposes. In this paper, we discuss the recent emerging technologies with a special focus on the possible usage of the crowd sourcing concept. Specifically, we discuss methods of quality verification to assist computer game design scenario case. Within this context, we present a technical architecture to enable quest design in games and conclude with prompting future steps of our work.},
booktitle = {Proceedings of the 2012 Third International Conference on Emerging Intelligent Data and Web Technologies},
pages = {302–305},
numpages = {4},
keywords = {Accessibility, Contributor verification, Crowdsourcing, Emerging technologies, Game Design, Quests},
series = {EIDWT '12}
}

@inproceedings{10.1145/2063576.2063860,
author = {Kazai, Gabriella and Kamps, Jaap and Milic-Frayling, Natasa},
title = {Worker types and personality traits in crowdsourcing relevance labels},
year = {2011},
isbn = {9781450307178},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2063576.2063860},
doi = {10.1145/2063576.2063860},
abstract = {Crowdsourcing platforms offer unprecedented opportunities for creating evaluation benchmarks, but suffer from varied output quality from crowd workers who possess different levels of competence and aspiration. This raises new challenges for quality control and requires an in-depth understanding of how workers' characteristics relate to the quality of their work.In this paper, we use behavioral observations (HIT completion time, fraction of useful labels, label accuracy) to define five worker types: Spammer, Sloppy, Incompetent, Competent, Diligent. Using data collected from workers engaged in the crowdsourced evaluation of the INEX 2010 Book Track Prove It task, we relate the worker types to label accuracy and personality trait information along the `Big Five' personality dimensions.We expect that these new insights about the types of crowd workers and the quality of their work will inform how to design HITs to attract the best workers to a task and explain why certain HIT designs are more effective than others.},
booktitle = {Proceedings of the 20th ACM International Conference on Information and Knowledge Management},
pages = {1941–1944},
numpages = {4},
keywords = {bfi test, crowdsourcing relevance labels, worker typology},
location = {Glasgow, Scotland, UK},
series = {CIKM '11}
}

@proceedings{10.5555/2859851,
title = {CSI-SE '15: Proceedings of the 2015 IEEE/ACM 2nd International Workshop on CrowdSourcing in Software Engineering},
year = {2015},
isbn = {9781467370400},
publisher = {IEEE Computer Society},
address = {USA}
}

@inproceedings{10.1145/2390803.2390817,
author = {Korshunov, Pavel and Cai, Shuting and Ebrahimi, Touradj},
title = {Crowdsourcing approach for evaluation of privacy filters in video surveillance},
year = {2012},
isbn = {9781450315890},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2390803.2390817},
doi = {10.1145/2390803.2390817},
abstract = {Extensive adoption of video surveillance, affecting many aspects of the daily life, alarms the concerned public about the increasing invasion into personal privacy. To address these concerns, many tools have been proposed for protection of personal privacy in image and video. However, little is understood regarding the effectiveness of such tools and especially their impact on the underlying surveillance tasks. In this paper, we propose conducting a subjective evaluation using crowdsourcing to analyze the tradeoff between the preservation of privacy offered by these tools and the intelligibility of activities under video surveillance. As an example, the proposed method is used to compare several commonly employed privacy protection techniques, such as blurring, pixelization, and masking applied to indoor surveillance video. Facebook based crowdsourcing application was specifically developed to gather the subjective evaluation data. Based on more than one hundred participants, the evaluation results demonstrate that the pixelization filter provides the best performance in terms of balance between privacy protection and intelligibility. The results obtained with crowdsourcing application were compared with results of previous work using more conventional subjective tests showing that they are highly correlated.},
booktitle = {Proceedings of the ACM Multimedia 2012 Workshop on Crowdsourcing for Multimedia},
pages = {35–40},
numpages = {6},
keywords = {crowdsourcing, evaluation methodology, intelligibility, privacy protection tools, video surveillance},
location = {Nara, Japan},
series = {CrowdMM '12}
}

@inproceedings{10.1145/2660114.2660121,
author = {Melenhorst, Mark and Men\'{e}ndez Blanco, Mar\'{\i}a and Larson, Martha},
title = {A Crowdsourcing Procedure for the Discovery of Non-Obvious Attributes of Social Images},
year = {2014},
isbn = {9781450331289},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2660114.2660121},
doi = {10.1145/2660114.2660121},
abstract = {Research on mid-level image representations has conventionally concentrated relatively obvious attributes and overlooked non-obvious attributes, i.e., characteristics that are not readily observable when images are viewed independently of their context or function. Non-obvious attributes are not necessarily easily nameable, but nonetheless they play a systematic role in people's interpretation of images. Clusters of related non-obvious attributes, called interpretation dimensions, emerge when people are asked to compare images, and provide important insight on aspects of social images that are considered relevant. In contrast to aesthetic or affective approaches to image analysis, non-obvious attributes are not related to the personal perspective of the viewer. Instead, they encode a conventional understanding of the world, which is tacit, rather than explicitly expressed. This paper provides an introduction to the notion of non-obvious image attributes of social images and introduces a procedure for discovering non-obvious attributes using crowdsourcing.},
booktitle = {Proceedings of the 2014 International ACM Workshop on Crowdsourcing for Multimedia},
pages = {45–48},
numpages = {4},
keywords = {content-based image indexing, crowdsourcing, human image understanding, social images, triadic elicitation},
location = {Orlando, Florida, USA},
series = {CrowdMM '14}
}

@inproceedings{10.1109/HICSS.2014.67,
author = {Boughzala, Imed and Vreede, Triparna de and Nguyen, Cuong and Vreede, Gert-Jan de},
title = {Towards a Maturity Model for the Assessment of Ideation in Crowdsourcing Projects},
year = {2014},
isbn = {9781479925049},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/HICSS.2014.67},
doi = {10.1109/HICSS.2014.67},
abstract = {Social media technology has enabled virtual collaborative environments such that people can actively interact, share knowledge, coordinate activities, solve problems and co-create value. Organizations have begun to leverage approaches and technologies to involve numerous people from outside their boundaries to perform organizational tasks. Crowd sourcing is a collaboration model enabled by people-centric web technologies to solve individual, organizational, and societal problems using a dynamically formed crowd of people who respond to an open call for participation. Despite the success and popularity of this phenomenon, there appears to be a lack of guidance on how to organize the ideation processes in crowd sourcing. To address this need, we propose a Crowd sourcing Ideation Maturity Assessment Model (CIMAM). The CIMAM is intended to be sufficiently generic to be applied to different types of crowd sourcing initiatives/projects. It can be used by external assessors or by crowd sourcing organizers themselves for self-assessments. CIMAM was developed through a literature review and built on the constructs of Pedersen et al. model. This paper contributes to research by examining the various factors influencing crowd engagement and productivity.},
booktitle = {Proceedings of the 2014 47th Hawaii International Conference on System Sciences},
pages = {483–490},
numpages = {8},
keywords = {Crowdsourcing, Ideation, Maturity models, design science, mass collaboration},
series = {HICSS '14}
}

@inproceedings{10.1145/3397536.3422335,
author = {Li, Wei and Chen, Haiquan and Ku, Wei-Shinn and Qin, Xiao},
title = {Turbo-GTS: Scaling Mobile Crowdsourcing using Workload-Balancing Bisection Tree},
year = {2020},
isbn = {9781450380195},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3397536.3422335},
doi = {10.1145/3397536.3422335},
abstract = {In mobile crowdsourcing, workers are financially motivated to perform self-selected tasks to maximize their revenue. Unfortunately, the existing task scheduling approaches in mobile crowdsourcing fail to scale for massive tasks and large geographic areas. We present Turbo-GTS, a system that assigns tasks to each worker to maximize the total number of the tasks that can be completed for an entire worker group while taking into account various spatial and temporal constraints, such as task execution duration, task expiration time, and worker/task geographic locations. The core of Turbo-GTS is WBT-NNH and WBT-NUD, our two newly developed scheduling algorithms, which build on the algorithms, QT-NNH and QT-NUD, proposed in our prior work [5]. The key idea is that Turbo-GTS performs dynamic workload balancing among all workers using the proposed Workload-balancing Bisection Tree (WBT) in support of large-scale Geo-Task Scheduling (GTS). Turbo-GTS includes an interactive interface for users to load the current task/worker distributions and compare the task assignment of each worker returned by different algorithms in a real-time fashion. Using the Foursquare mobile user check-in data in New York City and Tokyo, we show the superiority of Turbo-GTS over the state of the art in terms of the total number of the tasks that can be accomplished by the entire worker group and the corresponding running time. We also demonstrate the front-end interface of Turbo-GTS with two exploratory use cases in New York City.},
booktitle = {Proceedings of the 28th International Conference on Advances in Geographic Information Systems},
pages = {155–158},
numpages = {4},
keywords = {Mobile Crowdsourcing, Quadtree, Workload balancing},
location = {Seattle, WA, USA},
series = {SIGSPATIAL '20}
}

@inproceedings{10.1109/DSDIS.2015.50,
author = {Dubey, Rameshwar and Luo, Zongwei and Xu, Meiling and Wamba, Samuel Fosso},
title = {Developing an Integration Framework for Crowdsourcing and Internet of Things with Applications for Disaster Response},
year = {2015},
isbn = {9781509002146},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/DSDIS.2015.50},
doi = {10.1109/DSDIS.2015.50},
abstract = {The crowdsourcing and internet of things (IoT) have played a significant role in revolutionizing the information age. There is significant literature which has attempted to investigate the role of crowdsourcing and IoT in improving disaster response. However there is hardly any literature which attempted to reflect upon integration of crowdsourcing and IoT to explain better coordination among disaster relief workers to improve disaster response. In response to pressing need, we have attempted to develop a theoretical framework which can help disaster relief workers to improve their coordination using valuable information derived using comprehensive crowdsourcing framework. In this study we have used two-prong research strategies. First we have conducted extensive review of articles published in reputable journals, magazines and blogs by eminent practitioners and policy makers followed by case studies: stampede in Godavari River at Rajahmundry (2015), earthquake in Nepal (2015), flood in Uttarakhand (2013). Finally we have concluded our research findings with further research directions.},
booktitle = {Proceedings of the 2015 IEEE International Conference on Data Science and Data Intensive Systems (DSDIS)},
pages = {520–524},
numpages = {5},
series = {DSDIS '15}
}

@inproceedings{10.1145/2494091.2499574,
author = {Hara, Tenshi and Springer, Thomas and Bombach, Gerd and Schill, Alexander},
title = {Decentralised approach for a reusable crowdsourcing platform utilising standard web servers},
year = {2013},
isbn = {9781450322157},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2494091.2499574},
doi = {10.1145/2494091.2499574},
abstract = {Crowdsourcing has gained increasing interest during the last years as means for solving complex tasks with the help of a flexible group of contributors. The crowd can contribute with collecting data in the field, completing map information or votes for ideas or products. Even though the participation of large numbers of users with heterogeneous devices in crowdsourcing is a highly recurrent task, generic infrastructures for crowdsourcing can be hardly found. Especially the management of users, mobile devices and contributed data has to be repetitively implemented in new projects. To ease the development of crowdsourcing applications, in this paper we propose a generic platform for crowdsourcing supporting diverse crowdsourcing scenarios, the ability to handle large numbers of users and the involvement of heterogeneous mobile devices. The evaluation is based on scalability and performance experiments in order to demonstrate the feasibility of our approach.},
booktitle = {Proceedings of the 2013 ACM Conference on Pervasive and Ubiquitous Computing Adjunct Publication},
pages = {1063–1074},
numpages = {12},
keywords = {crowdsourcing, location-based services},
location = {Zurich, Switzerland},
series = {UbiComp '13 Adjunct}
}

@inproceedings{10.5555/2615731.2616086,
author = {Jain, Shweta and Gujar, Sujit and Zoeter, Onno and Narahari, Y.},
title = {A quality assuring multi-armed bandit crowdsourcing mechanism with incentive compatible learning},
year = {2014},
isbn = {9781450327381},
publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
address = {Richland, SC},
abstract = {We develop a novel multi-armed bandit (MAB) mechanism for the problem of selecting a subset of crowd workers to achieve an assured accuracy for each binary labelling task in a cost optimal way. This problem is challenging because workers have unknown qualities and strategic costs.},
booktitle = {Proceedings of the 2014 International Conference on Autonomous Agents and Multi-Agent Systems},
pages = {1609–1610},
numpages = {2},
keywords = {crowdsourcing, mechanism design, multi-armed bandit},
location = {Paris, France},
series = {AAMAS '14}
}

@inproceedings{10.1145/2494603.2480319,
author = {Akiki, Pierre and Bandara, Arosha and Yu, Yijun},
title = {Crowdsourcing user interface adaptations for minimizing the bloat in enterprise applications},
year = {2013},
isbn = {9781450321389},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2494603.2480319},
doi = {10.1145/2494603.2480319},
abstract = {Bloated software systems encompass a large number of features resulting in an increase in visual complexity. Enterprise applications are a common example of such types of systems. Since many users only use a distinct subset of the available features, providing a mechanism to tailor user interfaces according to each user's needs helps in decreasing the bloat thereby reducing the visual complexity. Crowdsourcing can be a means for speeding up the adaptation process by engaging and leveraging the enterprise application communities. This paper presents a tool supported model-driven mechanism for crowdsourcing user interface adaptations. We evaluate our proposed mechanism and tool through a basic preliminary user study.},
booktitle = {Proceedings of the 5th ACM SIGCHI Symposium on Engineering Interactive Computing Systems},
pages = {121–126},
numpages = {6},
keywords = {adaptable user interfaces, bloated ui, crowdsourcing, enterprise applications, model-driven engineering},
location = {London, United Kingdom},
series = {EICS '13}
}

@inproceedings{10.1145/2491055.2491075,
author = {Estermann, Beat},
title = {Are memory institutions ready for open data and crowdsourcing? results of a pilot survey from Switzerland},
year = {2013},
isbn = {9781450318525},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2491055.2491075},
doi = {10.1145/2491055.2491075},
abstract = {Since the advent of the World Wide Web, the cultural heritage sector has undergone a series of changes. In a pilot survey among memory institutions (galleries, libraries, archives, museums) in Switzerland we have focused on two recent trends -- open data and crowdsourcing -- asking to what extent heritage institutions are ready to adopt open data policies and to embrace crowdsourcing strategies. The results suggest that so far, only very few institutions have adopted an open data policy. There are however signs that this may soon change: A majority of the surveyed institutions considers open data as important and believes that the opportunities prevail over the risks. Some obstacles however still need to be overcome, in particular the institutions' reservations with regard to "free" licensing and their fear of losing control. With regard to crowdsourcing the data suggest that the adoption process will be slower than for open data. Although approximately 10\% of the responding institutions seem already to experiment with crowdsourcing, there is no general breakthrough in sight, as a majority of respondents remain skeptical with regard to the benefits.},
booktitle = {Proceedings of the 9th International Symposium on Open Collaboration},
articleno = {29},
numpages = {3},
keywords = {crowdsourcing, cultural heritage, digitization, memory institutions, open data, participation, semantic web, user engagement},
location = {Hong Kong, China},
series = {WikiSym '13}
}

@inproceedings{10.1109/PIMRC.2015.7343628,
author = {Zhang, Liye and Valaee, Shahrokh and Zhang, Le and Xu, Yubin and Ma, Lin},
title = {Signal propagation-based outlier reduction technique (SPORT) for crowdsourcing in indoor localization using fingerprints},
year = {2015},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/PIMRC.2015.7343628},
doi = {10.1109/PIMRC.2015.7343628},
abstract = {Crowdsourcing allows for rapid deployment of indoor localization systems. However, compared to the conventional methods, crowdsourcing might collect fewer received signal strength (RSS) values, hence result in greater influence to outliers in RSS values. In this paper, we propose an algorithm to detect such outliers and to substitute them with more suitable RSS values. In particular, we investigate the relationship of RSS values between adjacent locations using a signal propagation model and show that the outliers can be corrected using a signal propagation model. We propose the Signal Propagation-based Outlier Reduction Technic (SPORT) for identifying and adjusting outlier values in both the offline training phase and the online localization phase. Experimental results show that SPORT greatly smoothens the radio map and improves the location accuracy.},
booktitle = {2015 IEEE 26th Annual International Symposium on Personal, Indoor, and Mobile Radio Communications (PIMRC)},
pages = {2008–2013},
numpages = {6},
location = {Hong Kong, China}
}

@inproceedings{10.1145/1963192.1963206,
author = {Chen, Yan-Ying and Hsu, Winston H. and Liao, Hong-Yuan Mark},
title = {Learning facial attributes by crowdsourcing in social media},
year = {2011},
isbn = {9781450306379},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1963192.1963206},
doi = {10.1145/1963192.1963206},
abstract = {Facial attributes such as gender, race, age, hair style, etc., carry rich information for locating designated persons and profiling the communities from image/video collections (e.g., surveillance videos or photo albums). For plentiful facial attributes in photos and videos, collecting costly manual annotations for training detectors is time-consuming. We propose an automatic facial attribute detection method by exploiting the great amount of weakly labelled photos in social media. Our work can (1) automatically extract training images from the semantic-consistent user groups and (2) filter out noisy training photos by multiple mid-level features (by voting). Moreover, we introduce a method to harvest less-biased negative data for preventing uneven distribution of certain attributes. The experiments show that our approach can automatically acquire training photos for facial attributes and is on par with that by manual annotations.},
booktitle = {Proceedings of the 20th International Conference Companion on World Wide Web},
pages = {25–26},
numpages = {2},
keywords = {crowdsourcing, facial attribute, social media},
location = {Hyderabad, India},
series = {WWW '11}
}

@inproceedings{10.1145/2556288.2556986,
author = {Kim, Juho and Nguyen, Phu Tran and Weir, Sarah and Guo, Philip J. and Miller, Robert C. and Gajos, Krzysztof Z.},
title = {Crowdsourcing step-by-step information extraction to enhance existing how-to videos},
year = {2014},
isbn = {9781450324731},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2556288.2556986},
doi = {10.1145/2556288.2556986},
abstract = {Millions of learners today use how-to videos to master new skills in a variety of domains. But browsing such videos is often tedious and inefficient because video player interfaces are not optimized for the unique step-by-step structure of such videos. This research aims to improve the learning experience of existing how-to videos with step-by-step annotations.We first performed a formative study to verify that annotations are actually useful to learners. We created ToolScape, an interactive video player that displays step descriptions and intermediate result thumbnails in the video timeline. Learners in our study performed better and gained more self-efficacy using ToolScape versus a traditional video player.To add the needed step annotations to existing how-to videos at scale, we introduce a novel crowdsourcing workflow. It extracts step-by-step structure from an existing video, including step times, descriptions, and before and after images. We introduce the Find-Verify-Expand design pattern for temporal and visual annotation, which applies clustering, text processing, and visual analysis algorithms to merge crowd output. The workflow does not rely on domain-specific customization, works on top of existing videos, and recruits untrained crowd workers. We evaluated the workflow with Mechanical Turk, using 75 cooking, makeup, and Photoshop videos on YouTube. Results show that our workflow can extract steps with a quality comparable to that of trained annotators across all three domains with 77\% precision and 81\% recall.},
booktitle = {Proceedings of the SIGCHI Conference on Human Factors in Computing Systems},
pages = {4017–4026},
numpages = {10},
keywords = {crowdsourcing, how-to videos, video annotation.},
location = {Toronto, Ontario, Canada},
series = {CHI '14}
}

@inproceedings{10.1145/3299869.3320238,
author = {Han, Siyuan and Xu, Zihuan and Zeng, Yuxiang and Chen, Lei},
title = {Fluid: A Blockchain based Framework for Crowdsourcing},
year = {2019},
isbn = {9781450356435},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3299869.3320238},
doi = {10.1145/3299869.3320238},
abstract = {Recently, crowdsourcing has emerged as a new computing paradigm to solve problems that need human intrinsic, such as image annotation. However, there are two limitations in existing crowdsourcing platforms, i.e. non-transparent incentive mechanism and isolated profiles of workers, which harms the interests of both requesters and workers. Meanwhile, Blockchain technology introduces a solution to build a transparent, immutable data model in the Byzantine environment. Moreover, Blockchain systems (e.g. Ethereum) can also support the Tuning-complete script called smart contracts. Thus, we are motivated to use the feature of the transparent data model and smart contract in Blockchain to address the two limitations. Based on the proposed solutions, we have designed a Blockchain based framework which supports foundations of general crowdsourcing platforms. In addition, our framework also has following novel features: (1) it provides the transparent incentive mechanisms; (2) it supports a trusted worker's profile sharing in a cross-platform mode.},
booktitle = {Proceedings of the 2019 International Conference on Management of Data},
pages = {1921–1924},
numpages = {4},
keywords = {blockchain, crowdsourcing, incentive mechanism},
location = {Amsterdam, Netherlands},
series = {SIGMOD '19}
}

@inproceedings{10.1145/2396761.2398689,
author = {McCreadie, Richard and Macdonald, Craig and Ounis, Iadh and Giles, Jim and Jabr, Ferris},
title = {An examination of content farms in web search using crowdsourcing},
year = {2012},
isbn = {9781450311564},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2396761.2398689},
doi = {10.1145/2396761.2398689},
abstract = {On the Web, content farms produce articles engineered such that search engines rank them highly, in order to turn a profit from online advertising. Recently, content farms have increasingly been the target of demotion strategies by Web search engines, since content farm articles are often considered to be of suspect quality. In this paper, we study the prevalence of content farms in the results returned by three major Web search engines over time. In particular, we develop a crowdsourced approach to identify content farm articles from the results returned by these search engines. Our results show that between the period of March and August 2011, the number of content farm articles observed on a number of indicative queries was reduced by up to 55\% in the top ranks.},
booktitle = {Proceedings of the 21st ACM International Conference on Information and Knowledge Management},
pages = {2551–2554},
numpages = {4},
keywords = {content farms, crowdsourcing, web search},
location = {Maui, Hawaii, USA},
series = {CIKM '12}
}

@inproceedings{10.1145/2637064.2637102,
author = {Naiem, Ghada Farouk and Inoue, Sozo},
title = {A Method for Assessing User-generated Tests for Online Courses Exploiting Crowdsourcing Concept},
year = {2014},
isbn = {9781450327473},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2637064.2637102},
doi = {10.1145/2637064.2637102},
abstract = {In this research, we focus on the challenge of user-generated tests, where crowdsourcing of users have the chance of making new and innovative questions that increases the allowance for a teacher to create a new test. This approach replaces the dependence of stored questions/ answers pair that encourage memory skills not learning skills. The limitation of how to accept these questions contents is raised up. In this paper we propose a question acceptance method based on two measures that weight the question's score, which are difficulty and goodness factors. Since the formalization of assessment and goodness which we made in this paper are recursive, we also propose a heuristic algorithm for iteratively calculating both values. Moreover, we show simulation result to confirm that the algorithm converges, and that it reflects the difficulty and the goodness factors.},
booktitle = {Proceedings of the 2014 International Workshop on Web Intelligence and Smart Sensing},
pages = {1–6},
numpages = {6},
location = {Saint Etienne, France},
series = {IWWISS '14}
}

@inproceedings{10.1145/2009916.2010089,
author = {Huang, Yen-Ta and Cheng, An-Jung and Hsieh, Liang-Chi and Hsu, Winston and Chang, Kuo-Wei},
title = {Region-based landmark discovery by crowdsourcing geo-referenced photos},
year = {2011},
isbn = {9781450307574},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2009916.2010089},
doi = {10.1145/2009916.2010089},
abstract = {We propose a novel model for landmark discovery that locates region-based landmarks on map in contrast to the traditional point-based landmarks. The proposed method preserves more information and automatically identifies candidate regions on map by crowdsourcing geo-referenced photos. Gaussian kernel convolution is applied to remove noises and generate detected region. We adopt F1 measure to evaluate discovered landmarks and manually check the association between tags and regions. The experiment results show that more than 90\% of attractions in the selected city can be correctly located by this method.},
booktitle = {Proceedings of the 34th International ACM SIGIR Conference on Research and Development in Information Retrieval},
pages = {1141–1142},
numpages = {2},
keywords = {crowdsourcing, geo-referenced photo., region-based},
location = {Beijing, China},
series = {SIGIR '11}
}

@inproceedings{10.5555/2908698.2908717,
author = {Lease, Matthew},
title = {On quality control and machine learning in crowdsourcing},
year = {2011},
publisher = {AAAI Press},
abstract = {The advent of crowdsourcing has created a variety of new opportunities for improving upon traditional methods of data collection and annotation. This in turn has created intriguing new opportunities for data-driven machine learning (ML). Convenient access to crowd workers for simple data collection has further generalized to leveraging more arbitrary crowd-based human computation (von Ahn 2005) to supplement automated ML. While new potential applications of crowdsourcing continue to emerge, a variety of practical and sometimes unexpected obstacles have already limited the degree to which its promised potential can be actually realized in practice. This paper considers two particular aspects of crowdsourcing and their interplay, data quality control (QC) and ML, reflecting on where we have been, where we are, and where we might go from here.},
booktitle = {Proceedings of the 11th AAAI Conference on Human Computation},
pages = {97–102},
numpages = {6},
series = {AAAIWS'11-11}
}

@inproceedings{10.1109/COMPSAC.2015.117,
author = {Kubota, Takuya and Aritsugi, Masayoshi},
title = {How Many Ground Truths Should We Insert? Having Good Quality of Labeling Tasks in Crowdsourcing},
year = {2015},
isbn = {9781467365642},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/COMPSAC.2015.117},
doi = {10.1109/COMPSAC.2015.117},
abstract = {Having a lot of labels of good quality by crowd sourcing has attracted considerable interest recently. Ground truths can be helpful to this end, but prior work does not adequately address how many ground truths should be used. This paper presents a method for determining the number of ground truths. The number is determined by iteratively calculating the expected quality of labels if a ground truth is inserted into labeling tasks and comparing it with the limit of estimation quality of labels expectedly obtained by crowd sourcing. Our method can be applied to general EM algorithm-based approaches to estimating consensus labels of good quality. We compare our method with an EM algorithm-based approach, which is adopted to our method in the discussions of this paper, in terms of both efficiency of collecting labels from crowd and quality of labels obtained from the collected ones.},
booktitle = {Proceedings of the 2015 IEEE 39th Annual Computer Software and Applications Conference - Volume 02},
pages = {796–805},
numpages = {10},
keywords = {Condorcet Jury Theorem, EM algorithm, human computation},
series = {COMPSAC '15}
}

@inproceedings{10.5555/2209408.2209425,
author = {Theodosiou, Zenonas and Tsapatsoulis, Nicolas},
title = {On the creation of visual models for keywords through crowdsourcing},
year = {2012},
isbn = {9781618040749},
publisher = {World Scientific and Engineering Academy and Society (WSEAS)},
address = {Stevens Point, Wisconsin, USA},
abstract = {Crowdsourcing annotation is a recent development since a complete and elaborate annotation of the content of an image is an extremely labour-intensive and time consuming task. In this paper we examine the possibility to build accurate visual models for keywords created through crowdsourcing. Specifically, 8 different keywords related to athletics domain have been modelled using MPEG-7 and Histogram of Oriented Gradients (HOG) low level features and the Sequential Minimal Optimization (SMO) classifier. The experimental results have been examined using accuracy metrics and are very promising showing the ability of the visual models to classify the images into the 8 classes with the highest average accuracy rate of 73.13\% in the purpose of the HOG features.},
booktitle = {Proceedings of the 11th International Conference on Applications of Electrical and Computer Engineering},
pages = {96–100},
numpages = {5},
keywords = {crowdsourced annotation, low level features, visual models},
location = {Athens, Greece},
series = {ACA'12}
}

@inproceedings{10.1145/2638728.2638762,
author = {Huang, Yun and Wang, Yang and White, Corey},
title = {Designing a mobile system for public safety using open crime data and crowdsourcing},
year = {2014},
isbn = {9781450330473},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2638728.2638762},
doi = {10.1145/2638728.2638762},
abstract = {With more cities opening up crime data and the proliferation of participatory sensing, we explore ways to improve public safety of a local community by using open crime data and crowdsourcing. We first conducted an online survey to better understand the public safety needs of the Syracuse University (SU) community. Inspired by the survey results, we developed and deployed an Android mobile app in collaboration with the Department of Public Safety (DPS) at SU; the app integrates published safety incidents on a Google Map and SU campus alerts. We present our experience of co-designing this system with the DPS, challenges and experience of our initial app release. To design effective crowdsourcing of public safety information, we conducted a lab experiment to investigate what factors affect people's sharing decisions. The results suggest that both time of day and type of location significantly affect people's sharing decisions. These insights inform a re-design of our system to "nudge" people to report safety related information timely.},
booktitle = {Proceedings of the 2014 ACM International Joint Conference on Pervasive and Ubiquitous Computing: Adjunct Publication},
pages = {67–70},
numpages = {4},
keywords = {crowdsourcing, mobile app, public safety},
location = {Seattle, Washington},
series = {UbiComp '14 Adjunct}
}

@proceedings{10.1145/2787394,
title = {C2B(1)D '15: Proceedings of the 2015 ACM SIGCOMM Workshop on Crowdsourcing and Crowdsharing of Big (Internet) Data},
year = {2015},
isbn = {9781450335393},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {On behalf of the organizers of C2B(I)D we welcome you to London. The workshop came about due to our realization of the steady growth in Internet-wide measurement infrastructures that have been applied towards a wide range of experiments. These experiments have created "big (Internet) data" with rich semantic content of the structure, dynamics, and usage of today's Internet at all levels ranging from physical, to application and service layer. Internet research remains hampered by wellknown issues, such as limited geographic and network diversity captured and the ever-present tension between privacy, measurement visibility and experimental control. Researchers have often re-purposed less-than-ideal, but readily available data collected by third parties, despite the many associated risks.To link the various strands of work of researchers who have resorted to crowdsourcing by enlisting a large and diverse set of end hosts and turning them into vantage points, we sought to address the key underlying issues: How to enlist these vantage points in the right locations? What sorts of experiments are technically and ethically viable? What is the right experimental model? How could we build a federation of platforms and curate and store crowdsourced big Internet data? We sought papers from the community and received a diverse set of papers from different slices of the research community.Our work was greatly reduced by the program committee drawn from a broad swath of industry, academia, and governmental organizations. They reviewed the papers in a short time and helped generate the technical program that includes a keynote, 3 technical sessions, and a panel. The sessions illustrate the breadth and depth of this emerging field and cover aspects that are concerned with the design and use of crowdsourcing systems and their many applications to a diverse set of networking problems (e.g., coverage prediction in mobile networks, characterization of broadband access, protocol design).},
location = {London, United Kingdom}
}

@inproceedings{10.1145/2691195.2691208,
author = {Halder, Buddhadeb},
title = {Crowdsourcing collection of data for crisis governance in the post-2015 world: potential offers and crucial challenges},
year = {2014},
isbn = {9781605586113},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2691195.2691208},
doi = {10.1145/2691195.2691208},
abstract = {The practice of 'crowdsourcing' under the new technological regime has opened doors of huge data repositories. In recent years, crowdsourcing have expanded rapidly allowing citizens to connect with each other, governments to connect with common mass, to coordinate disaster response work, to map political conflicts, acquiring information quickly and participating in issues that affect day-to-day life of citizens. Crowdsourcing has the potentiality to offer smart governance by gathering and analyzing massive data from citizens. As data is a key enabler to proper public governance, this paper aims to provide a picture of potential offers that 'crowdsourcing' could make in support of crisis governance in the Post-2015 World, while it illustrates some critical challenges of data protection and privacy in different service sectors. Lastly, with a brief analysis on privacy, online data protection; and safety level of some crowdsourcing tools, this paper proposes brief guidelines for different stakeholders and some future works to avoid some mismanagement of crowdsourced data to protect data, privacy and security of end users.},
booktitle = {Proceedings of the 8th International Conference on Theory and Practice of Electronic Governance},
pages = {1–10},
numpages = {10},
keywords = {PET, big data, crisis governance, data, online data protection, policy, privacy, public governance},
location = {Guimaraes, Portugal},
series = {ICEGOV '14}
}

@inproceedings{10.1145/2461381.2461388,
author = {Chon, Yohan and Kim, Yunjong and Cha, Hojung},
title = {Autonomous place naming system using opportunistic crowdsensing and knowledge from crowdsourcing},
year = {2013},
isbn = {9781450319591},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2461381.2461388},
doi = {10.1145/2461381.2461388},
abstract = {A user's location information is commonly used in diverse mobile services, yet providing the actual name or semantic meaning of a place is challenging. Previous works required manual user interventions for place naming, such as searching by additional keywords and/or selecting place in a list. We believe that applying mobile sensing techniques to this problem can greatly reduce user intervention. In this paper, we present an autonomous place naming system using opportunistic crowdsensing and knowledge from crowdsourcing. Our goal is to provide a place name from a person's perspective: that is, functional name (e.g., food place, shopping place), business name (e.g., Starbucks, Apple Store), or personal name (e.g., my home, my workplace). The main idea is to bridge the gap between crowdsensing data from smartphone users and location information in social network services. The proposed system automatically extracts a wide range of semantic features about the places from both crowdsensing data and social networks to model a place name. We then infer the place name by linking the crowdsensing data with knowledge in social networks. Extensive evaluations with real deployments show that the proposed system outperforms the related approaches and greatly reduces user intervention for place naming.},
booktitle = {Proceedings of the 12th International Conference on Information Processing in Sensor Networks},
pages = {19–30},
numpages = {12},
keywords = {location naming, location-based services, smartphone sensing},
location = {Philadelphia, Pennsylvania, USA},
series = {IPSN '13}
}

@inproceedings{10.1007/978-3-642-31040-9_8,
author = {Burger, John D. and Doughty, Emily and Bayer, Sam and Tresner-Kirsch, David and Wellner, Ben and Aberdeen, John and Lee, Kyungjoon and Kann, Maricel G. and Hirschman, Lynette},
title = {Validating candidate gene-mutation relations in MEDLINE abstracts via crowdsourcing},
year = {2012},
isbn = {9783642310393},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-642-31040-9_8},
doi = {10.1007/978-3-642-31040-9_8},
abstract = {We describe an experiment to elicit judgments on the validity of gene-mutation relations in MEDLINE abstracts via crowdsourcing. The biomedical literature contains rich information on such relations, but the correct pairings are difficult to extract automatically because a single abstract may mention multiple genes and mutations. We ran an experiment presenting candidate gene-mutation relations as Amazon Mechanical Turk HITs (human intelligence tasks). We extracted candidate mutations from a corpus of 250 MEDLINE abstracts using EMU combined with curated gene lists from NCBI. The resulting document-level annotations were projected into the abstract text to highlight mentions of genes and mutations for review. Reviewers returned results within 36 hours. Initial weighted results evaluated against a gold standard of expert curated gene-mutation relations achieved 85\% accuracy, with the best reviewer achieving 91\% accuracy. We expect performance to increase with further experimentation, providing a scalable approach for rapid manual curation of important biological relations.},
booktitle = {Proceedings of the 8th International Conference on Data Integration in the Life Sciences},
pages = {83–91},
numpages = {9},
keywords = {annotation, crowdsourcing, mutations, text mining},
location = {College Park, MD},
series = {DILS'12}
}

@proceedings{10.1145/2390803,
title = {CrowdMM '12: Proceedings of the ACM multimedia 2012 workshop on Crowdsourcing for multimedia},
year = {2012},
isbn = {9781450315890},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {Crowdsourcing for multimedia involves exploiting both human intelligence and the combination of a large number of individual human contributions (i.e., the 'wisdom of the crowd') to develop techniques, systems and data sets that advance the state of the art. The ACM Multimedia 2012 Workshop on Crowdsourcing for Multimedia (CrowdMM 2012) provides a forum presenting crowdsourcing techniques for multimedia, as well as innovative ideas exemplifying how multimedia research can benefit from crowdsourcing.Through presented papers, invited talks and a panel, the workshop aims to promote interactive discussion on the scope and research potentials of crowdsourcing. The workshop views crowdsourcing in the broad sense: it encompasses both unsolicited human contributions, e.g., tags assigned by users to images, and also solicited contributions, e.g., annotations gathered by making use of crowdsourcing platforms that micro-outsource tasks to a large pool of human workers. The high-level goals of the workshop are twofold. First, it provides a venue to encourage multimedia research making use of human intelligence and taking advantage of human plurality. Second, the workshop gives an impetus to the multimedia community to define best practices for the use of crowdsourcing in multimedia and to shape emerging paradigms that will allow crowdsourcing to push forward the state of the art in multimedia research.We are excited about the variety of papers that are included in the workshop program, which reflect the global momentum in research in this field. The selected works cover several key topics in the crowdsourcing research spectrum, such as how to control the quality of crowd input, how to present complex tasks for a crowd to solve, and how to apply crowdsourcing in a variety of novel applications. We also like to extend a special word of thanks to Prof. Masataka Goto of the National Institute of Advanced Industrial Science and Technology (AIST), Japan, for his keynote speech entitled "PodCastle and Songle: Crowdsourcing-Based Web Services for Spoken Content Retrieval and Active Music Listening." His keynote discusses how crowdsourcing can be utilized to advance speech-recognition and music-understanding technologies and systems.},
location = {Nara, Japan}
}

@inproceedings{10.5555/1866696.1866709,
author = {Finin, Tim and Murnane, Will and Karandikar, Anand and Keller, Nicholas and Martineau, Justin and Dredze, Mark},
title = {Annotating named entities in Twitter data with crowdsourcing},
year = {2010},
publisher = {Association for Computational Linguistics},
address = {USA},
abstract = {We describe our experience using both Amazon Mechanical Turk (MTurk) and Crowd-Flower to collect simple named entity annotations for Twitter status updates. Unlike most genres that have traditionally been the focus of named entity experiments, Twitter is far more informal and abbreviated. The collected annotations and annotation techniques will provide a first step towards the full study of named entity recognition in domains like Facebook and Twitter. We also briefly describe how to use MTurk to collect judgements on the quality of "word clouds."},
booktitle = {Proceedings of the NAACL HLT 2010 Workshop on Creating Speech and Language Data with Amazon's Mechanical Turk},
pages = {80–88},
numpages = {9},
location = {Los Angeles, California},
series = {CSLDAMT '10}
}

@inproceedings{10.5555/1920331.1920448,
author = {Geisler, Gary and Willard, Geoff and Whitworth, Eryn},
title = {Crowdsourcing the indexing of film and television media},
year = {2010},
publisher = {American Society for Information Science},
address = {USA},
abstract = {In this paper we describe a project that explores how advances in information technology could be used to make film and television media more accessible to both scholarly and non-scholarly audiences. By indexing, at a detailed level, a range of time-synchronized and non-time-synchronized elements in a test collection of 12 films and 8 television programs, we demonstrate how structured data representing many aspects of media content can be produced in a streamlined manner, and discuss how this work could potentially be augmented with automated indexing to be more efficient. We present examples of how this data can be utilized to produce a variety of tools and artifacts that make film and television media more accessible, and suggest that crowdsourcing could be an effective strategy for accomplishing this work on a larger scale. This research contributes to the growing body of literature exploring how multimedia collections can be made more accessible and useful for a variety of purposes.},
booktitle = {Proceedings of the 73rd ASIS&amp;T Annual Meeting on Navigating Streams in an Information Ecosystem - Volume 47},
articleno = {82},
numpages = {10},
keywords = {crowdsourcing, film and television, indexing, media, moving images, video, visualization},
location = {Pittsburgh, Pennsylvania},
series = {ASIS&amp;T '10}
}

@inproceedings{10.5555/1884110.1884148,
author = {Vukovic, Maja and Laredo, Jim and Rajagopal, Sriram},
title = {Challenges and experiences in deploying enterprise crowdsourcing service},
year = {2010},
isbn = {3642139108},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {The value of crowdsourcing, arising from an instant access to a scalable expert network on-line, has been demonstrated by many success stories, such as GoldCorp, Netflix, and TopCoder. For enterprises, crowdsourcing promises significant cost-savings, quicker task completion times, and formation of expert communities (both within and outside the enterprise). Many aspects of the vision of enterprise crowdsourcing are under vigorous refinement. The reasons for this lack of progress, beyond the isolated and purpose-specific crowdsourcing efforts, are manifold. In this paper, we present our experience in deploying an enterprise crowdsourcing service in the IT Inventory Management domain. We focus on the technical and sociological challenges of creating enterprise crowdsourcing service that are generalpurpose, and that extend beyond mere specific-purpose, run-once prototypes. Such systems are deployed to the extent that they become an integrated part of business processes. Only when such degree of integration is achieved, the enteprises can fully adopt crowdsourcing and reap its benefits. We discuss the challenges in creating and deploying the enterprise crowdsourcing platform, and articulate current technical, governance and sociological issues towards defining a research agenda.},
booktitle = {Proceedings of the 10th International Conference on Web Engineering},
pages = {460–467},
numpages = {8},
keywords = {crowdsourcing process, enterprise crowdsourcing, governance},
location = {Vienna, Austria},
series = {ICWE'10}
}

@inproceedings{10.1007/978-3-319-05359-2_11,
author = {Burger, Valentin and Hirth, Matthias and Schwartz, Christian and Hoβfeld, Tobias and Tran-Gia, Phuoc},
title = {Increasing the Coverage of Vantage Points in Distributed Active Network Measurements by Crowdsourcing},
year = {2014},
isbn = {9783319053585},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-319-05359-2_11},
doi = {10.1007/978-3-319-05359-2_11},
abstract = {Internet video constitutes more than half of all consumer traffic. Most of the video traffic is delivered by content delivery networks (CDNs). The huge amount of traffic from video CDNs poses problems to access providers. To understand and monitor the impact of video traffic on access networks and the topology of CDNs, distributed active measurements are needed. Recently used measurement platforms are mainly hosted in National Research and Education Networks (NRENs). However, the view of these platforms on the CDN is very limited, since the coverage of NRENs is low in developing countries. Furthermore, campus networks do not reflect the characteristics of end user access networks. We propose to use crowdsourcing to increase the coverage of vantage points in distributed active network measurements. In this study, we compare measurements of a global CDN conducted in PlanetLab with measurements assigned to workers of a crowdsourcing platform. Thus, the coverage of vantage points and the sampled part of the global video CDN are analyzed. Our results show that the capability of PlanetLab to measure global CDNs is rather low, since the vast majority of requests is directed to the US. By using a crowdsourcing platform we obtain a diverse set of vantage points that reveals more than twice as many autonomous systems deploying video servers.},
booktitle = {Proceedings of the 17th International GI/ITG Conference on Measurement, Modelling, and Evaluation of Computing Systems and Dependability and Fault Tolerance - Volume 8376},
pages = {151–161},
numpages = {11},
location = {Bamberg, Germany},
series = {MMB \&amp; DFT 2014}
}

@inproceedings{10.1145/2457413.2457426,
author = {Liao, Chen-Chih and Hsu, Cheng-Hsin},
title = {A detour planning algorithm in crowdsourcing systems for multimedia content gathering},
year = {2013},
isbn = {9781450318938},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2457413.2457426},
doi = {10.1145/2457413.2457426},
abstract = {Crowdsourcing is a popular paradigm that outsources tasks to general publics. In crowdsourcing systems, requesters submit requests and workers perform selective requests for rewards. We propose a crowdsourcing system for multimedia content gathering, which is a new class of crowdsourcing systems with requests that must be performed at specific locations and time. The workers in our systems gather multimedia content using their smartphones and share the collected content over the Internet. Our system computes a detour path for each worker, who supplies his/her destination with a deadline to the system, and do not mind to take detour paths and perform some requests for profits. More precisely, we develop a detour planning algorithm to produce the optimal detour paths for individual workers. Using extensive trace-driven simulations, we demonstrate the effectiveness and efficiency of our algorithm: for example, it helps workers to achieve optimal profits (up to ~ 100\% improvement compared to baseline solutions) and runs in real-time (&lt; 82 ms). Developing a working prototype on Android OS and addressing other challenging aspects of the considered systems are among our future tasks.},
booktitle = {Proceedings of the 5th Workshop on Mobile Video},
pages = {55–60},
numpages = {6},
location = {Oslo, Norway},
series = {MoVid '13}
}

@inproceedings{10.1145/2212776.2223826,
author = {Zhu, Shaojian and Kane, Shaun and Feng, Jinjuan and Sears, Andrew},
title = {A crowdsourcing quality control model for tasks distributed in parallel},
year = {2012},
isbn = {9781450310161},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2212776.2223826},
doi = {10.1145/2212776.2223826},
abstract = {Quality control for crowdsourcing systems has been identified as a significant challenge [2]. We propose a data-driven model for quality control in the context of crowdsourcing systems with the goal of assessing the quality of each individual contribution for parallel distributed tasks (allowing multiple people working on a same task). The model is initiated with a data training process providing a rough estimate for several quality-related performance measures (e.g. time spent on a task). The initial estimates are combined with observations of results produced by workers to estimate the quality for each individual contribution. We conduct a study to evaluate the model in the context of improving speech recognition-based text correction using MTurk services. Results indicate that the model accurately predicts quality for more than 92\% of the non-negative (useful) contributions and 96\% of the negative (useless) ones.},
booktitle = {CHI '12 Extended Abstracts on Human Factors in Computing Systems},
pages = {2501–2506},
numpages = {6},
keywords = {crowdsourcing, parallel distribution, quality control},
location = {Austin, Texas, USA},
series = {CHI EA '12}
}

@inproceedings{10.1145/2591888.2591899,
author = {Seidel, Claudius E. and Thapa, Basanta E. P. and Plattfaut, Ralf and Niehaves, Bj\"{o}rn},
title = {Selective crowdsourcing for open process innovation in the public sector: are expert citizens really willing to participate?},
year = {2013},
isbn = {9781450324564},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2591888.2591899},
doi = {10.1145/2591888.2591899},
abstract = {In light of current challenges that modern societies are facing (e.g. demographic change, financial budgetary constraints, and demand for individualized public services), public administrations need to find innovative ways to deliver public services more efficiently. One possible solution to the dilemma of shrinking resources and increasing demands is open innovation. Various papers have already established the idea of crowdsourcing as a means of open innovation in the public sector. In order to enrich theory and practice in the field of collaborative innovation processes, this research focuses on the willingness of citizens to participate in crowdsourcing for innovation. More specifically, we highlight the role of expert citizens in selective crowdsourcing for complex tasks in the public sector with the concrete example of process innovations. We examine different levels of willingness to participate in crowdsourcing by means of a quantitative analysis of a questionnaire survey with n=128 German citizens. Our analysis shows that citizens are indeed motivated to participate in selective crowdsourcing to generate solutions to complex problems in the public sector. Although mobilizing adequate experts for complex tasks may seem challenging, we find that expert citizens actually have a higher willingness to collaborate on complex as well as simple tasks than non-experts. Additionally, financial incentives remain a relevant instrument in the design of citizensourcing projects. Ultimately, the role of age as an influence to participate in crowdsourcing will be discussed.},
booktitle = {Proceedings of the 7th International Conference on Theory and Practice of Electronic Governance},
pages = {64–72},
numpages = {9},
keywords = {citizensourcing, incentivization, motivation, process innovation, public administration, selective crowdsourcing},
location = {Seoul, Republic of Korea},
series = {ICEGOV '13}
}

@inproceedings{10.1145/2675316.2675319,
author = {Fonteles, Andr\'{e} Sales and Bouveret, Sylvain and Gensel, J\'{e}r\^{o}me},
title = {Towards matching improvement between spatio-temporal tasks and workers in mobile crowdsourcing market systems},
year = {2014},
isbn = {9781450331425},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2675316.2675319},
doi = {10.1145/2675316.2675319},
abstract = {Crowdsourcing market systems (CMS) are platforms that enable one to publish tasks that others are intended to accomplished. Usually, these are systems where users, called workers, perform tasks using desktop computers. Recently, mobile CMSs have appeared with tasks that exploit the mobility and the location of workers. For example, if a third party system requires a picture of a given place, it may publish a task asking for some worker to go there, take this picture and upload it. One problem of CMSs is that the more tasks they have, the harder it is for workers to find and choose one they are interested in. Besides, workers who accomplish tasks may have no particular experience and consequently provide bad results for tasks. In order to improve the matching between workers and spatio-temporal tasks in mobile CMSs, we propose a conceptual framework that consists of two mechanisms. One considers the requirements of a task for selecting suitable workers, while the other recommends tasks for a worker according to his preferences and skills. As a result, workers spend less time searching tasks, more working on it, providing results with higher quality.},
booktitle = {Proceedings of the Third ACM SIGSPATIAL International Workshop on Mobile Geographic Information Systems},
pages = {43–50},
numpages = {8},
keywords = {crowdsourcing market systems, mobile crowdsourcing, recommender systems, spatial crowdsourcing, task matching},
location = {Dallas, Texas},
series = {MobiGIS '14}
}

@inproceedings{10.1007/978-3-642-36415-0_9,
author = {Jones, Gareth J. F.},
title = {An introduction to crowdsourcing for language and multimedia technology research},
year = {2012},
isbn = {9783642364143},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-642-36415-0_9},
doi = {10.1007/978-3-642-36415-0_9},
abstract = {Language and multimedia technology research often relies on large manually constructed datasets for training or evaluation of algorithms and systems. Constructing these datasets is often expensive with significant challenges in terms of recruitment of personnel to carry out the work. Crowdsourcing methods using scalable pools of workers available on-demand offers a flexible means of rapid low-cost construction of many of these datasets to support existing research requirements and potentially promote new research initiatives that would otherwise not be possible.},
booktitle = {Proceedings of the 2012 International Conference on Information Retrieval Meets Information Visualization},
pages = {132–154},
numpages = {23},
keywords = {crowdsourcing, human computation, human language technologies, multimedia technologies},
location = {Zinal, Switzerland},
series = {PROMISE'12}
}

@inproceedings{10.1007/978-3-319-26190-4_20,
author = {El Maarry, Kinda and G\"{u}ntzer, Ulrich and Balke, Wolf-Tilo},
title = {A Majority of Wrongs Doesn't Make It Right - On Crowdsourcing Quality for Skewed Domain Tasks},
year = {2015},
isbn = {9783319261898},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-319-26190-4_20},
doi = {10.1007/978-3-319-26190-4_20},
abstract = {Today, crowdsourcing has emerged as a promising paradigm for annotating, structuring, and managing Web data. Still, as long as the problem of the crowd workers' trustworthiness in terms of result quality is not essentially solved, all these efforts remain doubtful. Therefore, in this paper we look at today's dominant quality assurance techniques and investigate how they cope with Web data, i.e. typical long-tail distributions, making it easy for strategic spammers to guess the prevalent answers and thus to go undetected. We provide a thorough theoretical analysis, quantifying the success of different methods on such skewed domains by means of test theory and show their individual weaknesses. Exploiting our case study analysis, we propose a simple privacy-preserving, task-agnostic model to improve test reliability, while actually decreasing overhead costs for quality assurance. Finally, we show the stability of our method for even higher numbers of spammers in controlled crowdsourcing experiments.},
booktitle = {Proceedings, Part I, of the 16th International Conference on Web Information Systems Engineering --- WISE 2015 - Volume 9418},
pages = {293–308},
numpages = {16},
keywords = {Crowdsourcing, Fraud detection, Quality control, Result quality}
}

@inproceedings{10.1145/3342280.3342307,
author = {Mok, Ricky K. P. and Kawaguti, Ginga and claffy, kc},
title = {QUINCE: A unified crowdsourcing-based QoE measurement platform},
year = {2019},
isbn = {9781450368865},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3342280.3342307},
doi = {10.1145/3342280.3342307},
abstract = {Assessing QoE in situ is a challenging task. Researchers have employed crowdsourcing-based approaches to achieve scale and diversity of subjects, but not without confounding factors. Apart from subjective bias of users, environmental factors introduce variance to QoE measurements. We propose QUINCE, a QoE measurement platform, which uses a gamified approach to enable longitudinal study with repeated and varying measurements in a single platform. We leveraged existing Internet measurement data and infrastructures to integrate three different types of network and QoE measurements to yield a more comprehensive view of subjects. Our preliminarily results show that QUINCE achieves a high level of engagement from subjects and collects data that is useful for correlating network performance and YouTube video streaming QoE.},
booktitle = {Proceedings of the ACM SIGCOMM 2019 Conference Posters and Demos},
pages = {60–62},
numpages = {3},
keywords = {QoE, crowdsourcing, network measurement},
location = {Beijing, China},
series = {SIGCOMM Posters and Demos '19}
}

@inproceedings{10.5555/2900929.2901104,
author = {Nath, Swaprava and Dayama, Pankaj and Garg, Dinesh and Narahari, Y. and Zou, James},
title = {Threats and trade-offs in resource critical crowdsourcing tasks over networks},
year = {2012},
publisher = {AAAI Press},
abstract = {In recent times, crowdsourcing over social networks has emerged as an active tool for complex task execution. In this paper, we address the problem faced by a planner to incentivize agents in the network to execute a task and also help in recruiting other agents for this purpose.We study this mechanism design problem under two natural resource optimization settings: (1) cost critical tasks, where the planner's goal is to minimize the total cost, and (2) time critical tasks, where the goal is to minimize the total time elapsed before the task is executed. We define a set of fairness properties that should be ideally satisfied by a crowdsourcing mechanism. We prove that no mechanism can satisfy all these properties simultaneously. We relax some of these properties and define their approximate counterparts. Under appropriate approximate fairness criteria, we obtain a non-trivial family of payment mechanisms. Moreover, we provide precise characterizations of cost critical and time critical mechanisms.},
booktitle = {Proceedings of the Twenty-Sixth AAAI Conference on Artificial Intelligence},
pages = {2447–2448},
numpages = {2},
location = {Toronto, Ontario, Canada},
series = {AAAI'12}
}

@inproceedings{10.1145/1357054.1357127,
author = {Kittur, Aniket and Chi, Ed H. and Suh, Bongwon},
title = {Crowdsourcing user studies with Mechanical Turk},
year = {2008},
isbn = {9781605580111},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1357054.1357127},
doi = {10.1145/1357054.1357127},
abstract = {User studies are important for many aspects of the design process and involve techniques ranging from informal surveys to rigorous laboratory studies. However, the costs involved in engaging users often requires practitioners to trade off between sample size, time requirements, and monetary costs. Micro-task markets, such as Amazon's Mechanical Turk, offer a potential paradigm for engaging a large number of users for low time and monetary costs. Here we investigate the utility of a micro-task market for collecting user measurements, and discuss design considerations for developing remote micro user evaluation tasks. Although micro-task markets have great potential for rapidly collecting user measurements at low costs, we found that special care is needed in formulating tasks in order to harness the capabilities of the approach.},
booktitle = {Proceedings of the SIGCHI Conference on Human Factors in Computing Systems},
pages = {453–456},
numpages = {4},
keywords = {Mechanical Turk, Wikipedia, micro task, remote user study},
location = {Florence, Italy},
series = {CHI '08}
}

@inproceedings{10.1145/3415958.3433075,
author = {Kouvela, Maria and Dimitriadis, Ilias and Vakali, Athena},
title = {Bot-Detective: An explainable Twitter bot detection service with crowdsourcing functionalities},
year = {2020},
isbn = {9781450381154},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3415958.3433075},
doi = {10.1145/3415958.3433075},
abstract = {Popular microblogging platforms (such as Twitter) offer a fertile ground for open communication among humans, however, they also attract many bots and automated accounts "disguised" as human users. Typically, such accounts favor malicious activities such as phishing, public opinion manipulation and hate speech spreading, to name a few. Although several AI driven bot detection methods have been implemented, the justification of bot classification and characterization remains quite opaque and AI decisions lack in ethical responsibility. Most of these approaches operate with AI black-boxed algorithms and their efficiency is often questionable. In this work we propose Bot-Detective, a web service that takes into account both the efficient detection of bot users and the interpretability of the results as well. Our main contributions are summarized as follows: i) we propose a novel explainable bot-detection approach, which, to the best of authors' knowledge, is the first one to offer interpretable, responsible, and AI driven bot identification in Twitter, ii) we deploy a publicly available bot detection Web service which integrates an explainable ML framework along with users feedback functionality under an effective crowdsourcing mechanism; iii) we build the proposed service under a newly created annotated dataset by exploiting Twitter's rules and existing tools. This dataset is publicly shared for further use. In situ experimentation has showcased that Bot-Detective produces comprehensive and accurate results, with a promising service take up at scale.},
booktitle = {Proceedings of the 12th International Conference on Management of Digital EcoSystems},
pages = {55–63},
numpages = {9},
keywords = {bot detection, explainable AI, social bots, social influence, social networks},
location = {Virtual Event, United Arab Emirates},
series = {MEDES '20}
}

@inproceedings{10.1145/2442576.2442591,
author = {Kim, Sung-Hee and Yun, Hyokun and Yi, Ji Soo},
title = {How to filter out random clickers in a crowdsourcing-based study?},
year = {2012},
isbn = {9781450317917},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2442576.2442591},
doi = {10.1145/2442576.2442591},
abstract = {Crowdsourcing-based user studies have become increasingly popular in information visualization (InfoVis) and visual analytics (VA). However, it is still unclear how to deal with some undesired crowdsourcing workers, especially those who submit random responses simply to gain wages (random clickers, henceforth). In order to mitigate the impacts of random clickers, several studies simply exclude outliers, but this approach has a potential risk of losing data from participants whose performances are extreme even though they participated faithfully. In this paper, we evaluated the degree of randomness in responses from a crowdsourcing worker to infer whether the worker is a random clicker. Thus, we could reliably filter out random clickers and found that resulting data from crowdsourcing-based user studies were comparable with those of a controlled lab study. We also tested three representative reward schemes (piece-rate, quota, and punishment schemes) with four different levels of compensations ($0.00, $0.20, $1.00, and $4.00) on a crowdsourcing platform with a total of 1,500 crowdsourcing workers to investigate the influences that different payment conditions have on the number of random clickers. The results show that higher compensations decrease the proportion of random clickers, but such increase in participation quality cannot justify the associated additional costs. A detailed discussion on how to optimize the payment scheme and amount to obtain high-quality data economically is provided.},
booktitle = {Proceedings of the 2012 BELIV Workshop: Beyond Time and Errors - Novel Evaluation Methods for Visualization},
articleno = {15},
numpages = {7},
keywords = {SimulSort, crowdsouring-based study, information visualization, payment amount, payment schemes, random clickers},
location = {Seattle, Washington, USA},
series = {BELIV '12}
}

@inproceedings{10.1109/eScience.2012.6404453,
author = {Hu, Zhenghui and Wu, Wenjun},
title = {A satellite data portal developed for crowdsourcing data analysis and interpretation},
year = {2012},
isbn = {9781467344678},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/eScience.2012.6404453},
doi = {10.1109/eScience.2012.6404453},
abstract = {Satellite data products derived from the remote sensing observations describe features of the land, ocean and atmosphere. And by data processing, they can be used to study processes and trends on local/global scale for real-time environmental research and applications. However, the advances of cutting-edge remote sensing technology bring the challenge of data deluge for satellite data analysis and interpretation. With combinations of human intelligence and machine intelligence, we develop a satellite data portal for crowdsourcing data analysis and interpretation through teaching and learning to cope with the overwhelming data deluge. Compared with all the existing data portals and crowdsourcing systems, it is the first attempt to embed crowdsourcing into a data portal to provide integrated services of satellite data access and analysis.},
booktitle = {Proceedings of the 2012 IEEE 8th International Conference on E-Science (e-Science)},
pages = {1–8},
numpages = {8},
keywords = {Communities, Data analysis, Education, Portals, Publishing, Remote sensing, Satellites, crowdsourcing, data anaysis and interpretation, data deluge, data portal, remote sensing},
series = {E-SCIENCE '12}
}

@inproceedings{10.1145/3488560.3502182,
author = {Soprano, Michael and Roitero, Kevin and Bombassei De Bona, Francesco and Mizzaro, Stefano},
title = {Crowd_Frame: A Simple and Complete Framework to Deploy Complex Crowdsourcing Tasks Off-the-shelf},
year = {2022},
isbn = {9781450391320},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3488560.3502182},
doi = {10.1145/3488560.3502182},
abstract = {Due to their relatively low cost and ability to scale, crowdsourcing based approaches are widely used to collect a large amount of human annotated data. To this aim, multiple crowdsourcing platforms exist, where requesters can upload tasks and workers can carry them out and obtain payment in return. Such platforms share a task design and deploy workflow that is often counter-intuitive and cumbersome. To address this issue, we propose Crowd_Frame, a simple and complete framework which allows to develop and deploy diverse types of complex crowdsourcing tasks in an easy and customizable way. We show the abilities of the proposed framework and we make it available to researchers and practitioners.},
booktitle = {Proceedings of the Fifteenth ACM International Conference on Web Search and Data Mining},
pages = {1605–1608},
numpages = {4},
keywords = {crowdsourcing, framework, user behavior},
location = {Virtual Event, AZ, USA},
series = {WSDM '22}
}

@inproceedings{10.1109/COMPSAC.2013.133,
author = {Li, Ke and Xiao, Junchao and Wang, Yongji and Wang, Qing},
title = {Analysis of the Key Factors for Software Quality in Crowdsourcing Development: An Empirical Study on TopCoder.com},
year = {2013},
isbn = {9780769549866},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/COMPSAC.2013.133},
doi = {10.1109/COMPSAC.2013.133},
abstract = {Crowdsourcing is a distributed problem-solving and production model. It takes advantage of the internet technology, helps enterprises save cost and improve efficiency. However, uncertain quality is a significant challenge for crowdsourcing. On the basis of the existing literatures, this paper proposes 23 software quality factors from two aspects: platform and project. By using multiple regression analysis on the data of one of the most successful software crowdsourcing platforms TopCoder.com, this paper analyzes the impact of the factors on software quality and identifies six key factors, including the average quality score of the platform, the number of contemporary projects, the length of component document, the number of registered developers, the maximum rating of submitted developers, and the design score. According to the result, this paper suggests four aspects for enterprises to improve software quality: choosing the prosperous period of platform to post a project, reducing the scale of projects, attracting more and higher skillful developers to participate, and improving software design score.},
booktitle = {Proceedings of the 2013 IEEE 37th Annual Computer Software and Applications Conference},
pages = {812–817},
numpages = {6},
keywords = {crowdsourcing, factors, object-oriented design metrics, software development, software quality},
series = {COMPSAC '13}
}

@inproceedings{10.1145/2785830.2785889,
author = {Huang, Yun and White, Corey and Xia, Huichuan and Wang, Yang},
title = {Modeling Sharing Decision of Campus Safety Reports and Its Design Implications to Mobile Crowdsourcing for Safety},
year = {2015},
isbn = {9781450336529},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2785830.2785889},
doi = {10.1145/2785830.2785889},
abstract = {Current campus communication regarding safety-related issues can be improved for both efficiency and accessibility. We observed a unique opportunity to develop a mobile crowdsourcing system, which allows university community members to report safety related incidents to the campus police department and to share their reports with other users of the system. To better inform the design of such a system, we applied drift-diffusion models in cognitive psychology to model the effect of various factors on users' sharing tendency. We conducted a laboratory experiment with 30 participants. We also ran an MTurk study with 230 participants to explore the feature of anonymous sharing in the application design. In this paper we report various results, including the findings that the time of day, location, and type of crime each affects the likelihood and timeliness of sharing safety reports in several different ways. We also discuss the implications for design of mobile crowdsourcing systems for public safety in general.},
booktitle = {Proceedings of the 17th International Conference on Human-Computer Interaction with Mobile Devices and Services},
pages = {400–409},
numpages = {10},
keywords = {Decision Model, Mobile Crowdsourcing, Public Safety},
location = {Copenhagen, Denmark},
series = {MobileHCI '15}
}

@inproceedings{10.1007/978-3-642-35311-6_16,
author = {Nath, Swaprava and Dayama, Pankaj and Garg, Dinesh and Narahari, Yadati and Zou, James},
title = {Mechanism design for time critical and cost critical task execution via crowdsourcing},
year = {2012},
isbn = {9783642353109},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-642-35311-6_16},
doi = {10.1007/978-3-642-35311-6_16},
abstract = {An exciting application of crowdsourcing is to use social networks in complex task execution. In this paper, we address the problem of a planner who needs to incentivize agents within a network in order to seek their help in executing an atomic task as well as in recruiting other agents to execute the task. We study this mechanism design problem under two natural resource optimization settings: (1) cost critical tasks, where the planner's goal is to minimize the total cost, and (2) time critical tasks, where the goal is to minimize the total time elapsed before the task is executed. We identify a set of desirable properties that should ideally be satisfied by a crowdsourcing mechanism. In particular, sybil-proofness and collapse-proofness are two complementary properties in our desiderata. We prove that no mechanism can satisfy all the desirable properties simultaneously. This leads us naturally to explore approximate versions of the critical properties. We focus our attention on approximate sybil-proofness and our exploration leads to a parametrized family of payment mechanisms which satisfy collapse-proofness. We characterize the approximate versions of the desirable properties in cost critical and time critical domain.},
booktitle = {Proceedings of the 8th International Conference on Internet and Network Economics},
pages = {212–226},
numpages = {15},
location = {Liverpool, UK},
series = {WINE'12}
}

@inproceedings{10.1145/2470654.2470744,
author = {Hara, Kotaro and Le, Vicki and Froehlich, Jon},
title = {Combining crowdsourcing and google street view to identify street-level accessibility problems},
year = {2013},
isbn = {9781450318990},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2470654.2470744},
doi = {10.1145/2470654.2470744},
abstract = {Poorly maintained sidewalks, missing curb ramps, and other obstacles pose considerable accessibility challenges; however, there are currently few, if any, mechanisms to determine accessible areas of a city a priori. In this paper, we investigate the feasibility of using untrained crowd workers from Amazon Mechanical Turk (turkers) to find, label, and assess sidewalk accessibility problems in Google Street View imagery. We report on two studies: Study 1 examines the feasibility of this labeling task with six dedicated labelers including three wheelchair users; Study 2 investigates the comparative performance of turkers. In all, we collected 13,379 labels and 19,189 verification labels from a total of 402 turkers. We show that turkers are capable of determining the presence of an accessibility problem with 81\% accuracy. With simple quality control methods, this number increases to 93\%. Our work demonstrates a promising new, highly scalable method for acquiring knowledge about sidewalk accessibility.},
booktitle = {Proceedings of the SIGCHI Conference on Human Factors in Computing Systems},
pages = {631–640},
numpages = {10},
keywords = {accessible urban navigation, crowdsourcing accessibility, google street view, image labeling, mechanical turk},
location = {Paris, France},
series = {CHI '13}
}

@inproceedings{10.1145/1641309.1641348,
author = {Ganjisaffar, Yasser and Javanmardi, Sara and Lopes, Cristina},
title = {Leveraging crowdsourcing heuristics to improve search in Wikipedia},
year = {2009},
isbn = {9781605587301},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1641309.1641348},
doi = {10.1145/1641309.1641348},
abstract = {Wikipedia articles are usually accompanied with history pages, categories and talk pages. The meta--data available in these pages can be analyzed to gain a better understanding of the content and quality of the articles. We analyze the quality of search results of the current major Web search engines (Google, Yahoo! and Live) in Wikipedia. We discuss how the rich meta--data available in wiki pages can be used to provide better search results in Wikipedia. We investigate the effect of incorporating the extent of review of an article into ranking of search results. The extent of review is measured by the number of distinct editors who have contributed to the articles and is extracted by processing Wikipedia's history pages. Our experimental results show that re--ranking search results of the three major Web search engines, using the review feature, improves quality of their rankings for Wikipedia--specific searches.},
booktitle = {Proceedings of the 5th International Symposium on Wikis and Open Collaboration},
articleno = {27},
numpages = {2},
location = {Orlando, Florida},
series = {WikiSym '09}
}

@inproceedings{10.1109/IMIS.2011.89,
author = {Hirth, Matthias and Hoβfeld, Tobias and Tran-Gia, Phuoc},
title = {Anatomy of a Crowdsourcing Platform - Using the Example of Microworkers.com},
year = {2011},
isbn = {9780769543727},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/IMIS.2011.89},
doi = {10.1109/IMIS.2011.89},
abstract = {Since Jeff Howe introduced the term "crowdsourcing" in 2006 for the first time, crowd sourcing has be come a growing market in the current Internet. Thousands of workers categorize images, write articles or perform other small tasks on platforms like Amazon Mechanical Turk (MTurk), Micro workers or Short Task. In this work, we want to give an inside view of the usage data from Micro workers and show that there are significant differences to the well studied MTurk. Further, we have a look at Micro workers from the perspective for a worker, an employer and the platform owner, in order to answer their most important questions: What jobs are most paid? How do I get my work done most quickly? When are the users of my platform active?},
booktitle = {Proceedings of the 2011 Fifth International Conference on Innovative Mobile and Internet Services in Ubiquitous Computing},
pages = {322–329},
numpages = {8},
keywords = {Mechanical Turk, Microworkers, crowdsourcing, user statistics},
series = {IMIS '11}
}

@inproceedings{10.1145/2506182.2506194,
author = {Uzun, Abdulbaki and Lehmann, Lorenz and Geismar, Thilo and K\"{u}pper, Axel},
title = {Turning the OpenMobileNetwork into a live crowdsourcing platform for semantic context-aware services},
year = {2013},
isbn = {9781450319720},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2506182.2506194},
doi = {10.1145/2506182.2506194},
abstract = {The OpenMobileNetwork is a semantic model for mobile network topologies created by following the principles of Linked Data. By correlating data in the Linking Open Data Cloud with the OpenMobileNetwork, innovative Semantic Context-aware Services (CAS) can be realized that are not solely driven by classic context data (e.g., geographic location), but also include further information from the semantics of the context in use. So far, this open-source initiative provided rather static network topology data triplified from open-source cell databases, such as OpenCellID or OpenBMap. Integrating dynamic and live network context data (e.g., current traffic and number of users in a mobile network cell) by exploiting crowdsourcing methods will further improve the Semantic CAS experience since the historic and live state of a mobile network cell is a valuable data source to be taken into consideration when providing personalized services. The challenge in realizing such a crowdsourcing approach lies in motivating a significant number of users to contribute with their data. For this purpose, we have turned the OpenMobileNetwork from a static dataset into a Live Crowdsourcing Platform for Semantic CAS including a semantic cellular database based on extended network context ontologies, two smartphone clients, and a Measurement Framework for gamifying the crowdsourcing process of collecting network measurements. The measurement statistics highlight the effectiveness of this approach.},
booktitle = {Proceedings of the 9th International Conference on Semantic Systems},
pages = {89–96},
numpages = {8},
keywords = {context-aware services, linked data, mobile networks},
location = {Graz, Austria},
series = {I-SEMANTICS '13}
}

@proceedings{10.1145/2442657,
title = {CrowdKDD '12: Proceedings of the First International Workshop on Crowdsourcing and Data Mining},
year = {2012},
isbn = {9781450315579},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Beijing, China}
}

@inproceedings{10.1109/TAAI.2013.53,
author = {Lee, Hu-Cheng and Wu, Chao-Lin and Chen, Ling-Jyh},
title = {A Crowdsourcing-Based Approach to Assess Concentration Levels of Students in Class Videos},
year = {2013},
isbn = {9781479925292},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/TAAI.2013.53},
doi = {10.1109/TAAI.2013.53},
abstract = {Concentration is important for students to conduct efficient learning in a class, and an effective assessment of students' concentration level in a class is useful for students to review class materials after lessons, as well as for lecturers to adjust their teaching strategies for self-improvement. Although a number of concentration assessment approaches have been proposed, conventional approaches are generally time/money expensive (e.g., expert opinions), inaccurate (e.g., computer vision-based approaches), and intrusive (e.g., wearable sensor-based approaches). In this study, we propose a novel approach, called Concentration Level Assessment System (CLAS), which combines a markovian Doze-and-Wake Model (DAWM) and the emerging crowdsourcing technique to enable effective concentration assessment of class videos. Using realistic datasets of class videos, we conduct a comprehensive set of synthetic analysis and Internet experiments, the results demonstrate that CLAS is capable of yielding an accuracy up to 98\% with 86\% cost savings. Moreover, CLAS is simple, effective, and scalable, and it shows promises in facilitating advanced applications for efficiency, productivity, and safety in the future.},
booktitle = {Proceedings of the 2013 Conference on Technologies and Applications of Artificial Intelligence},
pages = {228–233},
numpages = {6},
keywords = {concentration assessment, crowdsourcing, experiment},
series = {TAAI '13}
}

@inproceedings{10.1145/2858036.2858588,
author = {Kaufman, Geoff and Flanagan, Mary and Punjasthitkul, Sukdith},
title = {Investigating the Impact of 'Emphasis Frames' and Social Loafing on Player Motivation and Performance in a Crowdsourcing Game},
year = {2016},
isbn = {9781450333627},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2858036.2858588},
doi = {10.1145/2858036.2858588},
abstract = {With an increasing reliance on crowdsourcing games as data-gathering tools, it is imperative to understand how to motivate and sustain high levels of voluntary contribution. To this end, the present work directly compared the impact of various "emphasis frames," highlighting distinct intrinsic motivational factors, used to describe an online game in which players provide descriptive metadata "tags" for digitized images. An initial study showed that, compared to frames emphasizing personal enjoyment or altruistic motivations, a frame emphasizing a "growing community of players" solicited significantly fewer contributions. A second study tested the hypothesis that this lower level of contribution resulted from social loafing (the tendency to exert less effort in collective tasks in which contributions are anonymous and pooled). Results revealed that, compared to a no-frame control condition, a frame emphasizing the preponderance of other players reduced contribution levels and game replay likelihood, whereas a frame emphasizing the scarcity of fellow players increased contribution and replay levels. Various strategies for counteracting social loafing in crowdsourcing contexts are discussed.},
booktitle = {Proceedings of the 2016 CHI Conference on Human Factors in Computing Systems},
pages = {4122–4128},
numpages = {7},
keywords = {crowdsourcing games, engagement, human computation, metadata, motivation, social loafing},
location = {San Jose, California, USA},
series = {CHI '16}
}

@inproceedings{10.5555/2392800.2392841,
author = {Bessho, Fumihiro and Harada, Tatsuya and Kuniyoshi, Yasuo},
title = {Dialog system using real-time crowdsourcing and Twitter large-scale corpus},
year = {2012},
publisher = {Association for Computational Linguistics},
address = {USA},
abstract = {We propose a dialog system that creates responses based on a large-scale dialog corpus retrieved from Twitter and real-time crowd-sourcing. Instead of using complex dialog management, our system replies with the utterance from the database that is most similar to the user input. We also propose a real-time crowdsourcing framework for handling the case in which there is no adequate response in the database.},
booktitle = {Proceedings of the 13th Annual Meeting of the Special Interest Group on Discourse and Dialogue},
pages = {227–231},
numpages = {5},
location = {Seoul, South Korea},
series = {SIGDIAL '12}
}

@inproceedings{10.5555/2586115.2586745,
author = {Lee, Hu-Cheng and Wu, Chao-Lin and Chen, Ling-Jyh},
title = {A Crowdsourcing-Based Approach to Assess Concentration Levels of Students in Class Videos},
year = {2013},
isbn = {9781479925285},
publisher = {IEEE Computer Society},
address = {USA},
abstract = {Concentration is important for students to conduct efficient learning in a class, and an effective assessment of students' concentration level in a class is useful for students to review class materials after lessons, as well as for lecturers to adjust their teaching strategies for self-improvement. Although a number of concentration assessment approaches have been proposed, conventional approaches are generally time/money expensive (e.g., expert opinions), inaccurate (e.g., computer vision-based approaches), and intrusive (e.g., wearable sensor-based approaches). In this study, we propose a novel approach, called Concentration Level Assessment System (CLAS), which combines a markovian Doze-and-Wake Model (DAWM) and the emerging crowdsourcing technique to enable effective concentration assessment of class videos. Using realistic datasets of class videos, we conduct a comprehensive set of synthetic analysis and Internet experiments, the results demonstrate that CLAS is capable of yielding an accuracy up to 98\% with 86\% cost savings. Moreover, CLAS is simple, effective, and scalable, and it shows promises in facilitating advanced applications for efficiency, productivity, and safety in the future.},
booktitle = {Proceedings of the 2013 Conference on Technologies and Applications of Artificial Intelligence},
pages = {228–233},
numpages = {6},
keywords = {concentration assessment, crowdsourcing, experiment},
series = {TAAI '13}
}

@inproceedings{10.5555/1996889.1996911,
author = {Kazai, Gabriella},
title = {In search of quality in crowdsourcing for search engine evaluation},
year = {2011},
isbn = {9783642201608},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {Crowdsourcing is increasingly looked upon as a feasible alternative to traditional methods of gathering relevance labels for the evaluation of search engines, offering a solution to the scalability problem that hinders traditional approaches. However, crowdsourcing raises a range of questions regarding the quality of the resulting data. What indeed can be said about the quality of the data that is contributed by anonymous workers who are only paid cents for their efforts? Can higher pay guarantee better quality? Do better qualified workers produce higher quality labels? In this paper, we investigate these and similar questions via a series of controlled crowdsourcing experiments where we vary pay, required effort and worker qualifications and observe their effects on the resulting label quality, measured based on agreement with a gold set.},
booktitle = {Proceedings of the 33rd European Conference on Advances in Information Retrieval},
pages = {165–176},
numpages = {12},
keywords = {IR evaluation, crowdsourcing, relevance data gathering},
location = {Dublin, Ireland},
series = {ECIR'11}
}

@proceedings{10.5555/2874376,
title = {CrowdSem'13: Proceedings of the 1st International Conference on Crowdsourcing the Semantic Web - Volume 1030},
year = {2013},
publisher = {CEUR-WS.org},
address = {Aachen, DEU},
location = {Sydney, Australia}
}

@inproceedings{10.1007/978-3-642-20161-5_17,
author = {Kazai, Gabriella},
title = {In Search of Quality in Crowdsourcing for Search Engine Evaluation},
year = {2011},
isbn = {9783642201608},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-642-20161-5_17},
doi = {10.1007/978-3-642-20161-5_17},
abstract = {Crowdsourcing is increasingly looked upon as a feasible alternative to traditional methods of gathering relevance labels for the evaluation of search engines, offering a solution to the scalability problem that hinders traditional approaches. However, crowdsourcing raises a range of questions regarding the quality of the resulting data. What indeed can be said about the quality of the data that is contributed by anonymous workers who are only paid cents for their efforts? Can higher pay guarantee better quality? Do better qualified workers produce higher quality labels? In this paper, we investigate these and similar questions via a series of controlled crowdsourcing experiments where we vary pay, required effort and worker qualifications and observe their effects on the resulting label quality, measured based on agreement with a gold set.},
booktitle = {Proceedings of the 33rd European Conference on Advances in Information Retrieval - Volume 6611},
pages = {165–176},
numpages = {12},
keywords = {IR evaluation, crowdsourcing, relevance data gathering},
location = {Dublin, Ireland},
series = {ECIR 2011}
}

@inproceedings{10.1109/IMIS.2011.91,
author = {Hirth, Matthias and Hoβfeld, Tobias and Tran-Gia, Phuoc},
title = {Cost-Optimal Validation Mechanisms and Cheat-Detection for Crowdsourcing Platforms},
year = {2011},
isbn = {9780769543727},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/IMIS.2011.91},
doi = {10.1109/IMIS.2011.91},
abstract = {Crowd sourcing is becoming more and more important for commercial purposes. With the growth of crowd sourcing platforms like MTurk or Micro workers, a huge work force and a large knowledge base can be easily accessed and utilized. But due to the anonymity of the workers, they are encouraged to cheat the employers in order to maximize their income. Thus, this paper presents two crowd-based approaches to validate the submitted work. Both approaches are evaluated with regard to their detection quality, their costs and their applicability to different types of typical crowd sourcing tasks.},
booktitle = {Proceedings of the 2011 Fifth International Conference on Innovative Mobile and Internet Services in Ubiquitous Computing},
pages = {316–321},
numpages = {6},
keywords = {cheat-detection mechanism, crowdsourcing},
series = {IMIS '11}
}

@inproceedings{10.1145/3242587.3242598,
author = {Bragg, Jonathan and Mausam and Weld, Daniel S.},
title = {Sprout: Crowd-Powered Task Design for Crowdsourcing},
year = {2018},
isbn = {9781450359481},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3242587.3242598},
doi = {10.1145/3242587.3242598},
abstract = {While crowdsourcing enables data collection at scale, ensuring high-quality data remains a challenge. In particular, effective task design underlies nearly every reported crowdsourcing success, yet remains difficult to accomplish. Task design is hard because it involves a costly iterative process: identifying the kind of work output one wants, conveying this information to workers, observing worker performance, understanding what remains ambiguous, revising the instructions, and repeating the process until the resulting output is satisfactory. To facilitate this process, we propose a novel meta-workflow that helps requesters optimize crowdsourcing task designs and Sprout, our open-source tool, which implements this workflow. Sprout improves task designs by (1) eliciting points of confusion from crowd workers, (2) enabling requesters to quickly understand these misconceptions and the overall space of questions, and (3) guiding requesters to improve the task design in response. We report the results of a user study with two labeling tasks demonstrating that requesters strongly prefer Sprout and produce higher-rated instructions compared to current best practices for creating gated instructions (instructions plus a workflow for training and testing workers). We also offer a set of design recommendations for future tools that support crowdsourcing task design.},
booktitle = {Proceedings of the 31st Annual ACM Symposium on User Interface Software and Technology},
pages = {165–176},
numpages = {12},
location = {Berlin, Germany},
series = {UIST '18}
}

@inproceedings{10.1145/2666539.2666567,
author = {Machado, Leticia and Pereira, Graziela and Prikladnicki, Rafael and Carmel, Erran and de Souza, Cleidson R. B.},
title = {Crowdsourcing in the Brazilian IT industry: what we know and what we don't know},
year = {2014},
isbn = {9781450332248},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2666539.2666567},
doi = {10.1145/2666539.2666567},
abstract = {Crowdsourcing means outsourcing to a large network of people – a crowd. It has emerged as a new option for a global labor market; it is another valuable option in the ´make or buy´ software decision and has been gaining attention in countries where global software engineering plays a significant role, such as Brazil. The adoption of this practice in the Brazilian IT industry is not well known yet. For this reason, this paper presents findings from an empirical study about the topic, in the context of a multi-year study that has the goal of investigating how the Brazilian software labor and industry market is being transformed and disrupted by crowdsourcing. We interviewed professionals from several companies and identified how crowdsourcing is being adopted in Brazil, including possible benefits, main concerns and factors that may avoid some companies to adopt it from three different perspectives: the buyers, the platforms and the crowd. We also share our thoughts about the future of crowdsourcing in the country in the coming years.},
booktitle = {Proceedings of the 1st International Workshop on Crowd-Based Software Development Methods and Technologies},
pages = {7–12},
numpages = {6},
keywords = {Crowdsourcing, human labor, software development},
location = {Hong Kong, China},
series = {CrowdSoft 2014}
}

@inproceedings{10.1007/978-3-642-27997-3_19,
author = {Jayakanthan, Ranganathan and Sundararajan, Deepak},
title = {Enterprise crowdsourcing solution for software development in an outsourcing organization},
year = {2011},
isbn = {9783642279966},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-642-27997-3_19},
doi = {10.1007/978-3-642-27997-3_19},
abstract = {Enterprise Crowdsourcing has the potential to be a very powerful and disruptive paradigm for human resource deployment, project development and project management as we know them. This paper details ongoing work at TCS Innovation Labs --- Web 2.0, Tata Consultancy Services, Chennai, India to develop an Enterprise Crowdsourcing Solution to tackle the various processes involved in the development of software by leveraging the untapped human resource in the organization. Large IT organizations have a lot of untapped manpower in the form of trainees, the bench strength and people involved in roles which do not fully employ their strengths in particular technologies they are experts in. This system aims to allow untapped talent to get access to challenging tasks part of other projects and work on them while providing a disruptive way to allocate resources in a conventional software development environment.},
booktitle = {Proceedings of the 11th International Conference on Current Trends in Web Engineering},
pages = {177–180},
numpages = {4},
keywords = {collaborative work, crowdsourcing, enterprise crowdsourcing, social networking, web 2.0},
location = {Paphos, Cyprus},
series = {ICWE'11}
}

@inproceedings{10.1145/3290605.3300761,
author = {Chen, Quanze and Bragg, Jonathan and Chilton, Lydia B. and Weld, Dan S.},
title = {Cicero: Multi-Turn, Contextual Argumentation for Accurate Crowdsourcing},
year = {2019},
isbn = {9781450359702},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3290605.3300761},
doi = {10.1145/3290605.3300761},
abstract = {Traditional approaches for ensuring high quality crowdwork have failed to achieve high-accuracy on difficult problems. Aggregating redundant answers often fails on the hardest problems when the majority is confused. Argumentation has been shown to be effective in mitigating these drawbacks. However, existing argumentation systems only support limited interactions and show workers general justifications, not context-specific arguments targeted to their reasoning. This paper presents Cicero, a new workflow that improves crowd accuracy on difficult tasks by engaging workers in multi-turn, contextual discussions through real-time, synchronous argumentation. Our experiments show that compared to previous argumentation systems which only improve the average individual worker accuracy by 6.8 percentage points on the Relation Extraction domain, our workflow achieves 16.7 percentage point improvement. Furthermore, previous argumentation approaches don't apply to tasks with many possible answers; in contrast, Cicero works well in these cases, raising accuracy from 66.7\% to 98.8\% on the Codenames domain.},
booktitle = {Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems},
pages = {1–14},
numpages = {14},
keywords = {argumentation, crowdsourcing, dialog},
location = {Glasgow, Scotland Uk},
series = {CHI '19}
}

@inproceedings{10.5555/2484920.2485201,
author = {Yu, Han and Shen, Zhiqi and Miao, Chunyan and An, Bo},
title = {A reputation-aware decision-making approach for improving the efficiency of crowdsourcing systems},
year = {2013},
isbn = {9781450319935},
publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
address = {Richland, SC},
abstract = {A crowdsourcing system is a useful platform for utilizing the intelligence and skills of the mass. Nevertheless, like any open system that involves the exchange of things of value, selfish and malicious behaviors exist in crowdsourcing systems and need to be mitigated. Trust management has been proven to be a viable solution in many systems. However, a major difference between crowdsourcing systems and existing trust models designed for multi-agent systems is that human trustees have limited task processing capacity per unit time compared to an intelligent agent program. This paper recognizes a problem in current trust-aware decision-making methods for task assignment in crowdsourcing platforms. On the one hand, trust-based methods over-assign tasks to trusted workers, while on the other hand, workload-based solutions do not give sufficient guarantees on the quality of work. The proposed solution, the social welfare optimizing reputation-aware decision-making (SWORD) approach, strikes a balance between the two and is shown through extensive simulations to significantly improve social welfare of crowdsourcing platforms compared to related work.},
booktitle = {Proceedings of the 2013 International Conference on Autonomous Agents and Multi-Agent Systems},
pages = {1315–1316},
numpages = {2},
keywords = {decision-making, reputation, trust},
location = {St. Paul, MN, USA},
series = {AAMAS '13}
}

@inproceedings{10.1145/3265689.3265716,
author = {Yang, Qian and Cui, Lizhen and Zheng, Miao and Liu, Shijun and Guo, Wei and Lu, Xudong and Zheng, Yongqing and Li, Qingzhong},
title = {LBTask: A Benchmark for Spatial Crowdsourcing Platforms},
year = {2018},
isbn = {9781450365871},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3265689.3265716},
doi = {10.1145/3265689.3265716},
abstract = {The popularity of smart phones has made rapid development of crowdsourcing. The emergence of these crowdsourcing software has brought great convenience to our life. Traditional crowdsourcing platforms, such as Amazon Mechanical Turk and Crowdflower, publish some tasks on the site, Workers choose the tasks that are of interest and submit the answers to the tasks by browsing the tasks on the platform. And spatial crowdsourcing platforms (like gMission) are used to assign crowdsourcing tasks related to location. However, most crowdsourcing platforms support a small number of assignment and quality control algorithms. In this paper, a benchmark for spatial crowdsourcing platforms, called LBTask, is designed in order to adapt to the emergence of spatial crowdsourcing tasks, which focuses on solving location aware crowdsourcing tasks. Compared with other crowdsourcing platforms, LBTask can support various assignment and quality control algorithms in the architecture according to different strategies. In the distribution and assignment of tasks, the position factors of tasks and workers are taken into consideration in addition to considering the time and other factors.},
booktitle = {Proceedings of the 3rd International Conference on Crowd Science and Engineering},
articleno = {27},
numpages = {6},
keywords = {spatial crowdsourcing platform, task assignment},
location = {Singapore, Singapore},
series = {ICCSE'18}
}

@inproceedings{10.1145/1979742.1979802,
author = {Mujumdar, Dhawal and Kallenbach, Manuel and Liu, Brandon and Hartmann, Bj\"{o}rn},
title = {Crowdsourcing suggestions to programming problems for dynamic web development languages},
year = {2011},
isbn = {9781450302685},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1979742.1979802},
doi = {10.1145/1979742.1979802},
abstract = {Developers increasingly consult online examples and message boards to find solutions to common programming tasks. On the web, finding solutions to debugging problems is harder than searching for working code. Prior research introduced a social recommender system, HelpMeOut, that crowdsources debugging suggestions by presenting fixes to errors that peers have applied in the past. However, HelpMeOut only worked for statically typed, compiled programming languages like Java. We investigate how suggestions can be provided for dynamic, interpreted web development languages. Our primary insight is to instrument test-driven development to collect examples of bug fixes. We present Crowd::Debug, a tool for Ruby programmers that realizes these benefits.},
booktitle = {CHI '11 Extended Abstracts on Human Factors in Computing Systems},
pages = {1525–1530},
numpages = {6},
keywords = {debugging, recommender systems, test-driven development},
location = {Vancouver, BC, Canada},
series = {CHI EA '11}
}

@inproceedings{10.1109/HICSS.2015.341,
author = {Ghosh, Kaushik and Sen, Kabir},
title = {A Conceptual Model to Understand the Factors that Drive Individual Participation in Crowdsourcing for Medical Diagnosis},
year = {2015},
isbn = {9781479973675},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/HICSS.2015.341},
doi = {10.1109/HICSS.2015.341},
abstract = {In healthcare, assisted by collective knowledge of a large group of individuals, crowd sourcing is enabling prediction of disease outbreaks and diagnosis of rare medical conditions. This article is a 'Research-in Progress' that examines the role of Web-based platforms in driving participation of individuals in crowd sourcing services for medical diagnosis. Based on existing literature, a conceptual research model is developed that outlines factors that drive individual participation. A set of propositions based on the research model are developed. A research methodology is proposed with a plan for the empirical analysis. Contributions and implications are discussed.},
booktitle = {Proceedings of the 2015 48th Hawaii International Conference on System Sciences},
pages = {2815–2823},
numpages = {9},
keywords = {Contributor, Crowdsourcing, Medical Diagnosis, Participation, Seeker},
series = {HICSS '15}
}

@inproceedings{10.1145/2536714.2536717,
author = {Faggiani, Adriano and Gregori, Enrico and Lenzini, Luciano and Luconi, Valerio and Vecchio, Alessio},
title = {Lessons learned from the design, implementation, and management of a smartphone-based crowdsourcing system},
year = {2013},
isbn = {9781450324304},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2536714.2536717},
doi = {10.1145/2536714.2536717},
abstract = {Ubiquitousness of smartphones, when combined with the power of crowdsourcing, enables radically novel application scenarios, where a massive amount of mobile users scattered over wide geographical regions cooperate towards a single goal. Nevertheless these new possibilities come at the cost of additional complexity, such as the presence of humans in the control loop, scarce resources of mobile devices, increased management costs due the large number of users. In this paper we report and discuss the lessons learned from the design, implementation and management of Portolan, a smartphone-based crowdsourcing system aimed at monitoring large-scale networks.},
booktitle = {Proceedings of First International Workshop on Sensing and Big Data Mining},
pages = {1–6},
numpages = {6},
keywords = {Crowdsourcing, network sensing, smartphone},
location = {Roma, Italy},
series = {SENSEMINE'13}
}

@inproceedings{10.1145/3289600.3291035,
author = {Han, Lei and Roitero, Kevin and Gadiraju, Ujwal and Sarasua, Cristina and Checco, Alessandro and Maddalena, Eddy and Demartini, Gianluca},
title = {All Those Wasted Hours: On Task Abandonment in Crowdsourcing},
year = {2019},
isbn = {9781450359405},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3289600.3291035},
doi = {10.1145/3289600.3291035},
abstract = {Crowdsourcing has become a standard methodology to collect manually annotated data such as relevance judgments at scale. On crowdsourcing platforms like Amazon MTurk or FigureEight, crowd workers select tasks to work on based on different dimensions such as task reward and requester reputation. Requesters then receive the judgments of workers who self-selected into the tasks and completed them successfully. Several crowd workers, however, preview tasks, begin working on them, reaching varying stages of task completion without finally submitting their work. Such behavior results in unrewarded effort which remains invisible to requesters. In this paper, we conduct the first investigation into the phenomenon of task abandonment, the act of workers previewing or beginning a task and deciding not to complete it. We follow a three-fold methodology which includes 1) investigating the prevalence and causes of task abandonment by means of a survey over different crowdsourcing platforms, 2) data-driven analyses of logs collected during a large-scale relevance judgment experiment, and 3) controlled experiments measuring the effect of different dimensions on abandonment. Our results show that task abandonment is a widely spread phenomenon. Apart from accounting for a considerable amount of wasted human effort, this bears important implications on the hourly wages of workers as they are not rewarded for tasks that they do not complete. We also show how task abandonment may have strong implications on the use of collected data (for example, on the evaluation of IR systems).},
booktitle = {Proceedings of the Twelfth ACM International Conference on Web Search and Data Mining},
pages = {321–329},
numpages = {9},
keywords = {crowdsourcing, relevance judgments, task abandonment},
location = {Melbourne VIC, Australia},
series = {WSDM '19}
}

@inproceedings{10.1145/3184558.3191543,
author = {Zaveri, Amrapali and Serrano, Pedro Hernandez and Desai, Manisha and Dumontier, Michel},
title = {CrowdED: Guideline for Optimal Crowdsourcing Experimental Design},
year = {2018},
isbn = {9781450356404},
publisher = {International World Wide Web Conferences Steering Committee},
address = {Republic and Canton of Geneva, CHE},
url = {https://doi.org/10.1145/3184558.3191543},
doi = {10.1145/3184558.3191543},
abstract = {Crowdsourcing involves the creating of HITs (Human Intelligent Tasks), submitting them to a crowdsourcing platform and providing a monetary reward for each HIT. One of the advantages of using crowdsourcing is that the tasks can be highly parallelized, that is, the work is performed by a high number of workers in a decentralized setting. The design also offers a means to cross-check the accuracy of the answers by assigning each task to more than one person and thus relying on majority consensus as well as reward the workers according to their performance and productivity. Since each worker is paid per task, the costs can significantly increase, irrespective of the overall accuracy of the results. Thus, one important question when designing such crowdsourcing tasks that arise is how many workers to employ and how many tasks to assign to each worker when dealing with large amounts of tasks. That is, the main research questions we aim to answer is: 'Can we a-priori estimate optimal workers and tasks' assignment to obtain maximum accuracy on all tasks'. Thus, we introduce a two-staged statistical guideline, CrowdED, for optimal crowdsourcing experimental design in order to a-priori estimate optimal workers and tasks' assignment to obtain maximum accuracy on all tasks. We describe the algorithm and present preliminary results and discussions. We implement the algorithm in Python and make it openly available on Github, provide a Jupyter Notebook and a R Shiny app for users to re-use, interact and apply in their own crowdsourcing experiments.},
booktitle = {Companion Proceedings of the The Web Conference 2018},
pages = {1109–1116},
numpages = {8},
keywords = {biomedical, crowdsourcing, data quality, data science, fair, metadata, reproducibility},
location = {Lyon, France},
series = {WWW '18}
}

@inproceedings{10.1007/978-3-319-10470-6_55,
author = {Maier-Hein, Lena and Mersmann, Sven and Kondermann, Daniel and Bodenstedt, Sebastian and Sanchez, Alexandro and Stock, Christian and Kenngott, Hannes Gotz and Eisenmann, Mathias and Speidel, Stefanie},
title = {Can Masses of Non-Experts Train Highly Accurate Image Classifiers? A Crowdsourcing Approach to Instrument Segmentation in Laparoscopic Images},
year = {2022},
isbn = {978-3-319-10469-0},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-319-10470-6_55},
doi = {10.1007/978-3-319-10470-6_55},
abstract = {Machine learning algorithms are gaining increasing interest in the context of computer-assisted interventions. One of the bottlenecks so far, however, has been the availability of training data, typically generated by medical experts with very limited resources. Crowdsourcing is a new trend that is based on outsourcing cognitive tasks to many anonymous untrained individuals from an online community. In this work, we investigate the potential of crowdsourcing for segmenting medical instruments in endoscopic image data. Our study suggests that (1) segmentations computed from annotations of multiple anonymous non-experts are comparable to those made by medical experts and (2) training data generated by the crowd is of the same quality as that annotated by medical experts. Given the speed of annotation, scalability and low costs, this implies that the scientific community might no longer need to rely on experts to generate reference or training data for certain applications. To trigger further research in endoscopic image processing, the data used in this study will be made publicly available.},
booktitle = {Medical Image Computing and Computer-Assisted Intervention – MICCAI 2014},
pages = {438–445},
numpages = {8},
keywords = {Random Forest, Training Image, Majority Vote, True Positive Rate, Compute Tomography Colonography}
}

@inproceedings{10.1007/978-3-319-15168-7_53,
author = {Miglietta, Angelo and Parisi, Emanuele},
title = {Means and Roles of Crowdsourcing Vis-\`{A}-Vis CrowdFunding for the Creation of Stakeholders Collective Benefits},
year = {2015},
isbn = {978-3-319-15167-0},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-319-15168-7_53},
doi = {10.1007/978-3-319-15168-7_53},
abstract = {This work aims at assessing characteristics and roles of Crowdsourced activities vis-\`{a}-vis online CrowdFunding platforms, assessing potential collective benefits for stakeholders that arise from social media individual activities and investment decisions of users-investors. CrowdFunding platforms in fact leverage crowds and undefined pools of potential investors to screen, select and spread each CrowdFunding initiative in a detailed and thorough way – hence allowing users to perform several tasks that are traditionally carried out throughout IT models and static criteria.We identify 5 key roles played by Crowdsourcing Systems (CS) and we develop a potential model aimed at screening positive outcomes that benefit the collectivity (stakeholders). The model evaluates Crowdsourced activities as indicators for the creation of sustainable value for the enterprise and therefore for the collectivity of stakeholders. In order to test the model, we are currently deploying an Equity CrowdFunding platform embedding strong Crowdsourced tasks.In conclusion, we classify opportunities, limits and potential for a successful deployment of Crowdsourced tasks in CrowdFunding.},
booktitle = {Social Informatics: SocInfo 2014 International Workshops, Barcelona, Spain, November 11, 2014, Revised Selected Papers},
pages = {438–447},
numpages = {10},
keywords = {CrowdFunding, Human Computation, Crowdsourcing Systems, Collaborative Computing},
location = {Barcelona, Spain}
}

@inproceedings{10.1145/3178876.3186035,
author = {Almeida, Mario and Bilal, Muhammad and Finamore, Alessandro and Leontiadis, Ilias and Grunenberger, Yan and Varvello, Matteo and Blackburn, Jeremy},
title = {CHIMP: Crowdsourcing Human Inputs for Mobile Phones},
year = {2018},
isbn = {9781450356398},
publisher = {International World Wide Web Conferences Steering Committee},
address = {Republic and Canton of Geneva, CHE},
url = {https://doi.org/10.1145/3178876.3186035},
doi = {10.1145/3178876.3186035},
abstract = {While developing mobile apps is becoming easier, testing and characterizing their behavior is still hard. On the one hand, the de facto testing tool, called "Monkey," scales well due to being based on random inputs, but fails to gather inputs useful in understanding things like user engagement and attention. On the other hand, gathering inputs and data from real users requires distributing instrumented apps, or even phones with pre-installed apps, an expensive and inherently unscaleable task. To address these limitations we present CHIMP, a system that integrates automated tools and large-scale crowdsourced inputs. CHIMP is different from previous approaches in that it runs apps in a virtualized mobile environment that thousands of users all over the world can access via a standard Web browser. CHIMP is thus able to gather the full range of real-user inputs, detailed run-time traces of apps, and network traffic. We thus describe CHIMP»s design and demonstrate the efficiency of our approach by testing thousands of apps via thousands of crowdsourced users. We calibrate CHIMP with a large-scale campaign to understand how users approach app testing tasks. Finally, we show how CHIMP can be used to improve both traditional app testing tasks, as well as more novel tasks such as building a traffic classifier on encrypted network flows.},
booktitle = {Proceedings of the 2018 World Wide Web Conference},
pages = {45–54},
numpages = {10},
keywords = {crowdsourcing, mobile, testing, virtualization},
location = {Lyon, France},
series = {WWW '18}
}

@inproceedings{10.1145/1743384.1743478,
author = {Nowak, Stefanie and R\"{u}ger, Stefan},
title = {How reliable are annotations via crowdsourcing: a study about inter-annotator agreement for multi-label image annotation},
year = {2010},
isbn = {9781605588155},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1743384.1743478},
doi = {10.1145/1743384.1743478},
abstract = {The creation of golden standard datasets is a costly business. Optimally more than one judgment per document is obtained to ensure a high quality on annotations. In this context, we explore how much annotations from experts differ from each other, how different sets of annotations influence the ranking of systems and if these annotations can be obtained with a crowdsourcing approach. This study is applied to annotations of images with multiple concepts. A subset of the images employed in the latest ImageCLEF Photo Annotation competition was manually annotated by expert annotators and non-experts with Mechanical Turk. The inter-annotator agreement is computed at an image-based and concept-based level using majority vote, accuracy and kappa statistics. Further, the Kendall τ and Kolmogorov-Smirnov correlation test is used to compare the ranking of systems regarding different ground-truths and different evaluation measures in a benchmark scenario. Results show that while the agreement between experts and non-experts varies depending on the measure used, its influence on the ranked lists of the systems is rather small. To sum up, the majority vote applied to generate one annotation set out of several opinions, is able to filter noisy judgments of non-experts to some extent. The resulting annotation set is of comparable quality to the annotations of experts.},
booktitle = {Proceedings of the International Conference on Multimedia Information Retrieval},
pages = {557–566},
numpages = {10},
keywords = {crowdsourcing, inter-annotator agreement},
location = {Philadelphia, Pennsylvania, USA},
series = {MIR '10}
}

@inproceedings{10.5555/2452579.2455511,
author = {Chen, Qi and Wang, Gang and Tan, Chew Lim},
title = {Web Image Organization and Object Discovery by Actively Creating Visual Clusters through Crowdsourcing},
year = {2012},
isbn = {9780769549156},
publisher = {IEEE Computer Society},
address = {USA},
abstract = {In this paper, we propose to organize web images by actively creating visual clusters via crowd sourcing. We develop a two-phase framework to efficiently and effectively combine computers and a large number of human workers to build high quality visual clusters. The first phase partitions an image collection into multiple clusters, the second phase refines each generated cluster independently. In both phases, informative images are selected by computers and manually labeled by the crowds to learn improved models. Our method can be naturally extended to discover object categories in a collection of image segments. Experimental results on several data sets demonstrate the promise of our developed approach on both web image organization and object discovery tasks.},
booktitle = {Proceedings of the 2012 IEEE 24th International Conference on Tools with Artificial Intelligence - Volume 01},
pages = {419–427},
numpages = {9},
keywords = {active clustering, crowdsourcing, image organization, object discovery},
series = {ICTAI '12}
}

@inproceedings{10.1145/2676652.2676656,
author = {Rainer, Benjamin and Timmerer, Christian},
title = {Quality of Experience of Web-based Adaptive HTTP Streaming Clients in Real-World Environments using Crowdsourcing},
year = {2014},
isbn = {9781450332811},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2676652.2676656},
doi = {10.1145/2676652.2676656},
abstract = {Multimedia streaming over HTTP has gained momentum with the approval of the MPEG-DASH standard and many research papers evaluated various aspects thereof but mainly within controlled environments. However, the actual behaviour of a DASH client within real-world environments has not yet been evaluated. The aim of this paper is to compare the QoE performance of existing DASH-based Web clients within real-world environments using crowdsourcing. Therefore, we select Google's YouTube player and two open source implementations of the MPEG-DASH standard, namely the DASH-JS from Alpen-Adria-Universitaet Klagenfurt and the dash.js which is the official reference client of the DASH Industry Forum. Based on a predefined content configuration, which is comparable among the clients, we run a crowdsourcing campaign to determine the QoE of each implementation in order to determine the current state-of-the-art for MPEG-DASH systems within real-world environments. The gathered data and its analysis will be presented in the paper. It provides insights with respect to the QoE performance of current Web-based adaptive HTTP streaming systems.},
booktitle = {Proceedings of the 2014 Workshop on Design, Quality and Deployment of Adaptive Video Streaming},
pages = {19–24},
numpages = {6},
keywords = {crowdsourcing, dash, dynamic adaptive streaming over http, mpeg, qoe, quality of experience, subjective quality assessment},
location = {Sydney, Australia},
series = {VideoNext '14}
}

@inproceedings{10.5555/1927229.1927276,
author = {La Vecchia, Gioacchino and Cisternino, Antonio},
title = {Collaborative workforce, business process crowdsourcing as an alternative of BPO},
year = {2010},
isbn = {3642169848},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {Crowdsourcing is the act of outsourcing activities to networked people. This paper presents Business Process Crowdsourcing, an alternative to Business Process Outsourcing where crowd activities are coordinated, work force contributions not wasted and final result guaranteed. The positioning paper shows how to transform canonical business processes in crowdsourced business processes where Web 2.0, social networks, and business process management are combined to deploy business critical process to the Internet, getting the same level of quality and control of traditional outsourcing approaches with conventional workforce.},
booktitle = {Proceedings of the 10th International Conference on Current Trends in Web Engineering},
pages = {425–430},
numpages = {6},
keywords = {business process management, collaborative intelligence, crowdsourcing, social production},
location = {Vienna, Austria},
series = {ICWE'10}
}

@inproceedings{10.1109/UCC.2013.98,
author = {Schultheiss, Daniel and Blieske, Anja and Solf, Anja and Staeudtner, Saskia},
title = {How to Encourage the Crowd? A Study about User Typologies and Motivations on Crowdsourcing Platforms},
year = {2013},
isbn = {9780769551524},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/UCC.2013.98},
doi = {10.1109/UCC.2013.98},
abstract = {Studies about user participation on crowd sourcing platforms have revealed lists of different motivational factors. Reward and payment seems to be crucial at least on market places for creative ideas or workforce. However, past surveys or experiments mostly concentrated on one platform each with little theoretic background. The present exploratory study within the theoretic frame of motivation and creativity science appealed to more heterogeneity of platforms to be analyzed. Thus a broader view on user motivation was possible and a typology of crowd sourcers revealed four clusters of different platform users: female creatives, male technicians, academics and alternative all-rounders.},
booktitle = {Proceedings of the 2013 IEEE/ACM 6th International Conference on Utility and Cloud Computing},
pages = {506–509},
numpages = {4},
keywords = {crowdsourcing, extrinsic, intrinsic, motivation, socio-demographics, typology},
series = {UCC '13}
}

@inproceedings{10.1145/3372787.3389301,
author = {Saito, Shinobu and Iimura, Yukako},
title = {Hybrid sourcing: novel combination of crowdsourcing and inner-sourcing for software developments},
year = {2020},
isbn = {9781450370936},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3372787.3389301},
doi = {10.1145/3372787.3389301},
abstract = {Sourcing the right IT engineers is critical for project success. In recent years, two sourcing strategies have been grabbing attention: crowdsourcing and inner-sourcing. Each has their own good points and bad points. Crowdsourcing allows organizations to recruit IT engineers from outside on demand. However, organizations working on closed-source code with confidential information might be worried about security concerns (e.g., information leak). The other strategy, inner-sourcing, can make any IT engineer in an organization become a member of all projects by adopting open source software development practices. This improves the mobility of IT engineers between projects inside the organization. However, there is a limit to the types of IT engineers that one organization can have. In this report, we propose a hybrid sourcing approach. It integrates the two sourcing strategies to develop software - crowdsourcing and inner-sourcing. This approach distributes the development tasks to either software crowdsourcing or inner-sourcing according to task type. As a case study, we adopt hybrid sourcing approach for an industrial project. The project developed a web application system for a bus company. We evaluate the effectiveness and future issues of hybrid sourcing.},
booktitle = {Proceedings of the 15th International Conference on Global Software Engineering},
pages = {81–85},
numpages = {5},
keywords = {crowdsourcing, inner-sourcing, microtask distribution, microtasking, sourcing strategy},
location = {Seoul, Republic of Korea},
series = {ICGSE '20}
}

@inproceedings{10.1145/3364335.3364385,
author = {Rey, William P. and Balderama, Neil Anne Mae S. and Hipulan, Sebastianne L. and Salayon, Arney Azzih C.},
title = {MELDEN: An Android Based Mobile Crime Reporting Application Using Crowdsourcing},
year = {2019},
isbn = {9781450376532},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3364335.3364385},
doi = {10.1145/3364335.3364385},
abstract = {Among the most common crimes in the Philippines are theft, physical assault and robbery. Melden is an Android Based mobile crime reporting application that uses crowdsourcing to allow individuals to report various street crime with the reports directing to barangay officials. With the use of cross-referencing for validation of reports, the application aims for an easier, faster and reliable way of reporting of incidents to the barangay officials. The applications help with both the reporting of incidents and in gathering information about the incidents.},
booktitle = {Proceedings of the 5th International Conference on Industrial and Business Engineering},
pages = {198–201},
numpages = {4},
keywords = {Crime, Crime Report, Crowdsourcing, Report, Street Crimes},
location = {Hong Kong, Hong Kong},
series = {ICIBE '19}
}

@inproceedings{10.5555/1927229.1927273,
author = {Oliveira, F\'{a}bio and Ramos, Isabel and Santos, Leonel},
title = {Definition of a crowdsourcing innovation service for the European SMEs},
year = {2010},
isbn = {3642169848},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {Based on literature review and on the study of the most known and referred Crowdsourcing brokers, there's a clear trend to implement this model by large companies and mainly within the North American context. Our research team is focused in bringing this approach closer to the European culture, more specifically the cultural factors underlying the dynamics and motivation of communities available to solve the innovation challenges of Small and Medium Enterprises (SMEs), that we call Crowdsourcing Innovation. We believe that, due to the common lack of resources for innovation in these companies, a service capable of involving them in large networks filled with useful and reachable knowledge, and capable of supporting these companies through all the innovation process, is crucial to the future competitiveness of the European SMEs. Although our team is focusing on several aspects related to Crowdsourcing, my main research focuses the information services and supporting applications to create a web platform adapted to the key economical, organizational, legal and cultural differences that make current Crowdsourcing Innovation businesses less popular among European SMEs than in North America.},
booktitle = {Proceedings of the 10th International Conference on Current Trends in Web Engineering},
pages = {412–416},
numpages = {5},
keywords = {crowdsourcing innovation, european SMEs, intermediaries},
location = {Vienna, Austria},
series = {ICWE'10}
}

@inproceedings{10.1145/3126858.3126897,
author = {de Amorim, Marcello N. and Segundo, Ricardo M.C. and Santos, Celso A.S. and Tavares, Orivaldo de L.},
title = {Video Annotation by Cascading Microtasks: a Crowdsourcing Approach},
year = {2017},
isbn = {9781450350969},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3126858.3126897},
doi = {10.1145/3126858.3126897},
abstract = {This paper presents a general approach to perform crowdsourcing video annotation without requiring trained workers nor experts. It consists of dividing complex annotation tasks into simple and small microtasks and cascading them to generate a final result. Moreover, this approach allows using simple annotation tools rather than complex and expensive annotation systems. Also, it tends to avoid activities that may be tedious and time-consuming for workers. The cascade microtasks strategy is included in a workflow of three steps: Preparation, Annotation, and Presentation. A crowdsourcing video annotation process in which four different microtasks were cascaded was developed to evaluate the proposed approach. In the process, extra content such as images, text, hyperlinks and other elements are applied in the video enrichment. To support the experiment was developed a toolkit that includes Web-based annotation tools and aggregation methods, besides a presentation system for the annotated videos. This toolkit is open source and can be downloaded and used to replicate this experiment, as so to construct different crowdsourcing video annotation systems.},
booktitle = {Proceedings of the 23rd Brazillian Symposium on Multimedia and the Web},
pages = {49–56},
numpages = {8},
keywords = {crowdsourcing, human computation, microtasks, multimedia systems, video annotation, video enrichment},
location = {Gramado, RS, Brazil},
series = {WebMedia '17}
}

@inproceedings{10.1145/2384916.2384989,
author = {Hara, Kotaro and Le, Victoria and Froehlich, Jon},
title = {A feasibility study of crowdsourcing and google street view to determine sidewalk accessibility},
year = {2012},
isbn = {9781450313216},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2384916.2384989},
doi = {10.1145/2384916.2384989},
abstract = {We explore the feasibility of using crowd workers from Amazon Mechanical Turk to identify and rank sidewalk accessibility issues from a manually curated database of 100 Google Street View images. We examine the effect of three different interactive labeling interfaces (Point, Rectangle, and Outline) on task accuracy and duration. We close the paper by discussing limitations and opportunities for future work.},
booktitle = {Proceedings of the 14th International ACM SIGACCESS Conference on Computers and Accessibility},
pages = {273–274},
numpages = {2},
keywords = {accessible urban navigation, crowdsourcing accessibility, google street view, mechanical turk},
location = {Boulder, Colorado, USA},
series = {ASSETS '12}
}

