@article{10.1145/3555553,
author = {Hadi Mogavi, Reza and Haq, Ehsan-Ul and Gujar, Sujit and Hui, Pan and Ma, Xiaojuan},
title = {More Gamification Is Not Always Better: A Case Study of Promotional Gamification in a Question Answering Website},
year = {2022},
issue_date = {November 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {6},
number = {CSCW2},
url = {https://doi.org/10.1145/3555553},
doi = {10.1145/3555553},
abstract = {Community Question Answering Websites (CQAs) like Stack Overflow rely on continuous user contributions to keep their services active. Nevertheless, they often undergo a sharp decline in their user participation during the holiday season, undermining their performance. To address this issue, some CQAs have developed their own special promotional gamification schemes to incentivize users to maintain their contributions throughout the holiday season. These promotional gamification schemes are often time-limited, optional, and run alongside the default gamification schemes of their websites. However, the impact of such promotional gamification schemes on user behavior remains largely unexplored in the existing literature. This paper takes the first steps toward filling this knowledge gap by conducting a large-scale empirical study of a particular promotional gamification scheme called Winter Bash (WB) on the CQA of Stack Overflow. According to our findings, promotional gamification schemes may not be the panacea they are portrayed to be. For example, in the case of WB, we find that the scheme is not effective for improving the collective engagement of all users. Only some particular user types (i.e., experienced and reputable users) are often provoked under WB. Most novice users, who comprise the majority of Stack Overflow website's user base, seem to be indifferent to such a gamification scheme. Our research also shows the importance of studying the quantity and quality of user engagement in unison to better understand the effectiveness of a gamification scheme. Previous gamification studies in the literature have focused predominantly on studying the quantity of user engagement alone. Last but not least, we conclude our paper by presenting some practical considerations for improving the design of future promotional gamification schemes in CQAs and similar platforms.},
journal = {Proc. ACM Hum.-Comput. Interact.},
month = nov,
articleno = {452},
numpages = {32},
keywords = {user engagement, user behavior analysis, temporary gamification, promotional gamification, new gamification schemes, gamification, difference-in-differences (did), community question answering website (cqa)}
}

@article{10.1145/3607189,
author = {Yang, Yuanhang and He, Wei and Gao, Cuiyun and Xu, Zenglin and Xia, Xin and Liu, Chuanyi},
title = {TopicAns: Topic-informed Architecture for Answer Recommendation on Technical Q&amp;A Site},
year = {2023},
issue_date = {January 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {33},
number = {1},
issn = {1049-331X},
url = {https://doi.org/10.1145/3607189},
doi = {10.1145/3607189},
abstract = {Technical Q&amp;A sites, such as Stack Overflow and Ask Ubuntu, have been widely utilized by software engineers to seek support for development challenges. However, not all the raised questions get instant feedback, and the retrieved answers can vary in quality. The users can hardly avoid spending much time before solving their problems. Prior studies propose approaches to automatically recommend answers for the question posts on technical Q&amp;A sites. However, the lengthiness and the lack of background knowledge issues limit the performance of answer recommendation on these sites. The irrelevant sentences in the posts may introduce noise to the semantics learning and prevent neural models from capturing the gist of texts. The lexical gap between question and answer posts further misleads current models to make failure recommendations. From this end, we propose a novel neural network named TopicAns for answer selection on technical Q&amp;A sites. TopicAns aims at learning high-quality representations for the posts in Q&amp;A sites with a neural topic model and a pre-trained model. This involves three main steps: (1) generating topic-aware representations of Q&amp;A posts with the neural topic model, (2) incorporating the corpus-level knowledge from the neural topic model to enhance the deep representations generated by the pre-trained language model, and (3) determining the most suitable answer for a given query based on the topic-aware representation and the deep representation. Moreover, we propose a two-stage training technique to improve the stability of our model. We conduct comprehensive experiments on four benchmark datasets to verify our proposed TopicAns’s effectiveness. Experiment results suggest that TopicAns consistently outperforms state-of-the-art techniques by over 30\% in terms of Precision@1.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = nov,
articleno = {20},
numpages = {25},
keywords = {answer recommendation, neural networks, Stack overflow}
}

@article{10.1145/3603398,
author = {Ahmed, Muzamil and Khan, Hikmat Ullah and Khan, Muhammad Attique and Tariq, Usman and Kadry, Seifedine},
title = {Context-aware Answer Selection in Community Question Answering Exploiting Spatial Temporal Bidirectional Long Short-Term Memory},
year = {2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
issn = {2375-4699},
url = {https://doi.org/10.1145/3603398},
doi = {10.1145/3603398},
abstract = {Community Question Answering (CQA) sites provide knowledge sharing facility as the users can post questions and other users can share their answers. The selection of top-quality answers from the set of answers in a thread is a significant and challenging task in Natural Language Processing (NLP). To address this issue, we propose a deep learning based spatial temporal Bidirectional Long Short-Term Memory (Bi-LSTM) algorithm. The existing studies mainly focus only computing semantic similarity between questions and answers using votes given by the users. The proposed hybrid approach, based on both forward and backward, consider question to answer and answer to answer similarity. The forward LSTM captures the spatial impact of the answer to estimate the relevancy, whereas the backward LSTM learns temporal features with the answer to predict the best quality answer. Moreover, spatial Bi-LSTM captures past and future dependencies for a better understanding of context and to improve the effectiveness of answer selection. For extracting meaningful information from noisy text data, data is preprocessed following standard steps such as tokenization, parsing, lemmatization, stop words removal, part of speech tagging and entities extraction. Word embeddings-based Paragraph to vector (par2vec) has additional input nodes to represent paragraph information in vector for context understanding. The empirical analysis carried out on the SemEval CQA dataset shows that the proposed model outperforms state-of-art answer selection approaches.},
note = {Just Accepted},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = jun,
keywords = {Word Embeddings, Spatial Temporal Bi-LSTM, Deep Learning, Community Question Answering, Natural Language Processing}
}

@article{10.1145/3274399,
author = {Oliveira, Nigini and Muller, Michael and Andrade, Nazareno and Reinecke, Katharina},
title = {The Exchange in StackExchange: Divergences between Stack Overflow and its Culturally Diverse Participants},
year = {2018},
issue_date = {November 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {2},
number = {CSCW},
url = {https://doi.org/10.1145/3274399},
doi = {10.1145/3274399},
abstract = {StackExchange is a network of Question \&amp; Answer (Q&amp;A) sites that support collaborative knowledge exchange on a variety of topics. Prior research found a significant imbalance between those who contribute content to Q&amp;A sites (predominantly people from Western countries) and those who passively use the site (the so-called "lurkers"). One possible explanation for such participation differences between countries could be a mismatch between culturally related preferences of some users and the values ingrained in the design of the site. To examine this hypothesis, we conducted a value-sensitive analysis of the design of the StackExchange site Stack Overflow and contrasted our findings with those of participants from societies with varying cultural backgrounds using a series of focus groups and interviews. Our results reveal tensions between collectivist values, such as the openness for social interactions, and the performance-oriented, individualist values embedded in Stack Overflow's design and community guidelines. This finding confirms that socio-technical sites like Stack Overflow reflect the inherent values of their designers, knowledge that can be leveraged to foster participation equity.},
journal = {Proc. ACM Hum.-Comput. Interact.},
month = nov,
articleno = {130},
numpages = {22},
keywords = {value-sensitive design, stack overflow, question \&amp; answer sites, online collaboration, cross-cultural studies}
}

@article{10.1145/3550150,
author = {Gao, Zhipeng and Xia, Xin and Lo, David and Grundy, John and Zhang, Xindong and Xing, Zhenchang},
title = {I Know What You Are Searching for: Code Snippet Recommendation from Stack Overflow Posts},
year = {2023},
issue_date = {May 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {32},
number = {3},
issn = {1049-331X},
url = {https://doi.org/10.1145/3550150},
doi = {10.1145/3550150},
abstract = {Stack Overflow has been heavily used by software developers to seek programming-related information. More and more developers use Community Question and Answer forums, such as Stack Overflow, to search for code examples of how to accomplish a certain coding task. This is often considered to be more efficient than working from source documentation, tutorials, or full worked examples. However, due to the complexity of these online Question and Answer forums and the very large volume of information they contain, developers can be overwhelmed by the sheer volume of available information. This makes it hard to find and/or even be aware of the most relevant code examples to meet their needs. To alleviate this issue, in this work, we present a query-driven code recommendation tool, named Que2Code, that identifies the best code snippets for a user query from Stack Overflow posts. Our approach has two main stages: (i) semantically equivalent question retrieval and (ii) best code snippet recommendation. During the first stage, for a given query question formulated by a developer, we first generate paraphrase questions for the input query as a way of query boosting and then retrieve the relevant Stack Overflow posted questions based on these generated questions. In the second stage, we collect all of the code snippets within questions retrieved in the first stage and develop a novel scheme to rank code snippet candidates from Stack Overflow posts via pairwise comparisons. To evaluate the performance of our proposed model, we conduct a large-scale experiment to evaluate the effectiveness of the semantically equivalent question retrieval task and best code snippet recommendation task separately on Python and Java datasets in Stack Overflow. We also perform a human study to measure how real-world developers perceive the results generated by our model. Both the automatic and human evaluation results demonstrate the promising performance of our model, and we have released our code and data to assist other researchers.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = apr,
articleno = {80},
numpages = {42},
keywords = {Duplicate questions, paraphrase mining, Stack Overflow, Code Search}
}

@article{10.1145/2629445,
author = {Wang, G. Alan and Wang, Harry Jiannan and Li, Jiexun and Abrahams, Alan S. and Fan, Weiguo},
title = {An Analytical Framework for Understanding Knowledge-Sharing Processes in Online Q&amp;A Communities},
year = {2014},
issue_date = {March 2015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {5},
number = {4},
issn = {2158-656X},
url = {https://doi.org/10.1145/2629445},
doi = {10.1145/2629445},
abstract = {Online communities have become popular knowledge sources for both individuals and organizations. Computer-mediated communication research shows that communication patterns play an important role in the collaborative efforts of online knowledge-sharing activities. Existing research is mainly focused on either user egocentric positions in communication networks or communication patterns at the community level. Very few studies examine thread-level communication and process patterns and their impacts on the effectiveness of knowledge sharing. In this study, we fill this research gap by proposing an innovative analytical framework for understanding thread-level knowledge sharing in online Q&amp;A communities based on dialogue act theory, network analysis, and process mining. More specifically, we assign a dialogue act tag for each post in a discussion thread to capture its conversation purpose and then apply graph and process mining algorithms to examine knowledge-sharing processes. Our results, which are based on a real support forum dataset, show that the proposed analytical framework is effective in identifying important communication, conversation, and process patterns that lead to helpful knowledge sharing in online Q&amp;A communities.},
journal = {ACM Trans. Manage. Inf. Syst.},
month = dec,
articleno = {18},
numpages = {31},
keywords = {process mining, online community, knowledge sharing, dialogue act, communication network, Computer-mediated communication}
}

@article{10.1145/3450503,
author = {Chatterjee, Preetha and Damevski, Kostadin and Kraft, Nicholas A. and Pollock, Lori},
title = {Automatically Identifying the Quality of Developer Chats for Post Hoc Use},
year = {2021},
issue_date = {October 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {30},
number = {4},
issn = {1049-331X},
url = {https://doi.org/10.1145/3450503},
doi = {10.1145/3450503},
abstract = {Software engineers are crowdsourcing answers to their everyday challenges on Q&amp;A forums (e.g., Stack Overflow) and more recently in public chat communities such as Slack, IRC, and Gitter. Many software-related chat conversations contain valuable expert knowledge that is useful for both mining to improve programming support tools and for readers who did not participate in the original chat conversations. However, most chat platforms and communities do not contain built-in quality indicators (e.g., accepted answers, vote counts). Therefore, it is difficult to identify conversations that contain useful information for mining or reading, i.e., conversations of post hoc quality. In this article, we investigate automatically detecting developer conversations of post hoc quality from public chat channels. We first describe an analysis of 400 developer conversations that indicate potential characteristics of post hoc quality, followed by a machine learning-based approach for automatically identifying conversations of post hoc quality. Our evaluation of 2,000 annotated Slack conversations in four programming communities (python, clojure, elm, and racket) indicates that our approach can achieve precision of 0.82, recall of 0.90, F-measure of 0.86, and MCC of 0.57. To our knowledge, this is the first automated technique for detecting developer conversations of post hoc quality.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = jul,
articleno = {48},
numpages = {28},
keywords = {quality of social content, Online software developer chats}
}

@article{10.1145/3607188,
author = {Huang, Qing and Sun, Yanbang and Xing, Zhenchang and Yu, Min and Xu, Xiwei and Lu, Qinghua},
title = {API Entity and Relation Joint Extraction from Text via Dynamic Prompt-tuned Language Model},
year = {2023},
issue_date = {January 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {33},
number = {1},
issn = {1049-331X},
url = {https://doi.org/10.1145/3607188},
doi = {10.1145/3607188},
abstract = {Extraction of Application Programming Interfaces (APIs) and their semantic relations from unstructured text (e.g., Stack Overflow) is a fundamental work for software engineering tasks (e.g., API recommendation). However, existing approaches are rule based and sequence labeling based. They must manually enumerate the rules or label data for a wide range of sentence patterns, which involves a significant amount of labor overhead and is exacerbated by morphological and common-word ambiguity. In contrast to matching or labeling API entities and relations, this article formulates heterogeneous API extraction and API relation extraction task as a sequence-to-sequence generation task and proposes the API Entity-Relation Joint Extraction framework (AERJE), an API entity-relation joint extraction model based on the large pre-trained language model. After training on a small number of ambiguous but correctly labeled data, AERJE builds a multi-task architecture that extracts API entities and relations from unstructured text using dynamic prompts. We systematically evaluate AERJE on a set of long and ambiguous sentences from Stack Overflow. The experimental results show that AERJE achieves high accuracy and discrimination ability in API entity-relation joint extraction, even with zero or few-shot fine-tuning.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = nov,
articleno = {6},
numpages = {25},
keywords = {dynamic prompt, joint extraction, API relation, API entity}
}

@article{10.1145/3610098,
author = {Liaqat, Amna and Demmans Epp, Carrie and Cai, Minghao and Munteanu, Cosmin},
title = {Exploring Collaborative Culture Sharing Dynamics in Immigrant Families through Digital Crafting and Storytelling},
year = {2023},
issue_date = {October 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {CSCW2},
url = {https://doi.org/10.1145/3610098},
doi = {10.1145/3610098},
abstract = {Families strengthen bonds by collectively constructing social identity through sharing stories, language, and culture. For immigrant families, language and culture barriers disrupt the mechanisms for maintaining intergenerational connection. Immigrant grandparents and grandchildren are particularly at risk of disconnect. In this paper, we investigate existing design guidelines using a tool (StoryTapestry) to explore the storytelling and crafting process of South-Asian immigrant grandparents and grandchildren. In this exploration, pairs used culturally-relevant images to create digital visual artifacts that tell their stories. Grandparent-grandchild pairs from 10 South-Asian immigrant families participated in this exploration of how the digital process fosters positive social connection, culture sharing, and co-construction. A thematic analysis revealed how collaborative digital crafting encourages the crossing of language and culture barriers, knowledge sharing, and creativity. We contribute an understanding of interaction dynamics and socio-technical implications of intergenerational and cross-cultural collaboration by demonstrating (1) that collaborative digital crafting can reverse traditional educator and learner roles to create culture sharing opportunities, (2) that grandparents play a central role in maintaining social interaction, (3) that structure can guide grandparent-grandchild pairs to a shared goal, and (4) that flexibility encourages engagement from children. We synthesize ideas from migration and collaboration research, and we discuss how the culture, language, and generational dynamics in our study extend what is known about each of these spaces. Together, our design implications offer insight into building digital tools that promote engagement, knowledge sharing, and collaboration between immigrant grandparents and grandchildren navigating social disconnect post-migration.},
journal = {Proc. ACM Hum.-Comput. Interact.},
month = oct,
articleno = {307},
numpages = {29},
keywords = {crafting, culture, families, immigrant, learning, storytelling}
}

@article{10.1145/3456873,
author = {Tao, Yida and Tang, Shan and Liu, Yepang and Xu, Zhiwu and Qin, Shengchao},
title = {Speeding Up Data Manipulation Tasks with Alternative Implementations: An Exploratory Study},
year = {2021},
issue_date = {October 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {30},
number = {4},
issn = {1049-331X},
url = {https://doi.org/10.1145/3456873},
doi = {10.1145/3456873},
abstract = {As data volume and complexity grow at an unprecedented rate, the performance of data manipulation programs is becoming a major concern for developers. In this article, we study how alternative API choices could improve data manipulation performance while preserving task-specific input/output equivalence. We propose a lightweight approach that leverages the comparative structures in Q&amp;A sites to extracting alternative implementations. On a large dataset of Stack Overflow posts, our approach extracts 5,080&nbsp;pairs of alternative implementations that invoke different data manipulation APIs to solve the same tasks, with an accuracy of 86\%. Experiments show that for 15\% of the extracted pairs, the faster implementation achieved &gt;10x speedup over its slower alternative. We also characterize 68&nbsp;recurring alternative API pairs from the extraction results to understand the type of APIs that can be used alternatively. To put these findings into practice, we implement a tool, AlterApi7, to automatically optimize real-world data manipulation programs. In the 1,267&nbsp;optimization attempts on the Kaggle dataset, 76\% achieved desirable performance improvements with up to orders-of-magnitude speedup. Finally, we discuss notable challenges of using alternative APIs for optimizing data manipulation programs. We hope that our study offers a new perspective on API recommendation and automatic performance optimization.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = jul,
articleno = {49},
numpages = {28},
keywords = {performance optimization, mining software repository, empirical study, data manipulation, API selection}
}

@article{10.1145/3588938,
author = {Tu, Jianhong and Fan, Ju and Tang, Nan and Wang, Peng and Li, Guoliang and Du, Xiaoyong and Jia, Xiaofeng and Gao, Song},
title = {Unicorn: A Unified Multi-tasking Model for Supporting Matching Tasks in Data Integration},
year = {2023},
issue_date = {May 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {1},
number = {1},
url = {https://doi.org/10.1145/3588938},
doi = {10.1145/3588938},
abstract = {Data matching - which decides whether two data elements (e.g., string, tuple, column, or knowledge graph entity) are the "same" (a.k.a. a match) - is a key concept in data integration, such as entity matching and schema matching. The widely used practice is to build task-specific or even dataset-specific solutions, which are hard to generalize and disable the opportunities of knowledge sharing that can be learned from different datasets and multiple tasks. In this paper, we propose Unicorn, a unified model for generally supporting common data matching tasks. Unicorn can enable knowledge sharing by learning from multiple tasks and multiple datasets, and can also support zero-shot prediction for new tasks with zero labeled matching/non-matching pairs. However, building such a unified model is challenging due to heterogeneous formats of input data elements and various matching semantics of multiple tasks. To address the challenges, Unicorn employs one generic Encoder that converts any pair of data elements (a, b) into a learned representation, and uses a Matcher, which is a binary classifier, to decide whether a matches b. To align matching semantics of multiple tasks, Unicorn adopts a mixture-of-experts model that enhances the learned representation into a better representation. We conduct extensive experiments using 20 datasets on seven well-studied data matching tasks, and find that our unified model can achieve better performance on most tasks and on average, compared with the state-of-the-art specific models trained for ad-hoc tasks and datasets separately. Moreover, Unicorn can also well serve new matching tasks with zero-shot learning.},
journal = {Proc. ACM Manag. Data},
month = may,
articleno = {84},
numpages = {26},
keywords = {data integration, data matching, multi-task learning}
}

@article{10.1145/3494518,
author = {Yang, Wenhua and Zhang, Chong and Pan, Minxue and Xu, Chang and Zhou, Yu and Huang, Zhiqiu},
title = {Do Developers Really Know How to Use Git Commands? A Large-scale Study Using Stack Overflow},
year = {2022},
issue_date = {July 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {31},
number = {3},
issn = {1049-331X},
url = {https://doi.org/10.1145/3494518},
doi = {10.1145/3494518},
abstract = {Git, a cross-platform and open source distributed version control tool, provides strong support for non-linear development and is capable of handling everything from small to large projects with speed and efficiency. It has become an indispensable tool for millions of software developers and is the de facto standard of version control in software development nowadays. However, despite its widespread use, developers still frequently face difficulties when using various Git commands to manage projects and collaborate. To better help developers use Git, it is necessary to understand the issues and difficulties that they may encounter when using Git. Unfortunately, this problem has not yet been comprehensively studied. To fill this knowledge gap, in this article, we conduct a large-scale study on Stack Overflow, a popular Q&amp;A forum for developers. We extracted and analyzed 80,370 relevant questions from Stack Overflow, and reported the increasing popularity of the Git command questions. By analyzing the questions, we identified the Git commands that are frequently asked and those that are associated with difficult questions on Stack Overflow to help understand the difficulties developers may encounter when using Git commands. In addition, we conducted a survey to understand how developers learn Git commands in practice, showing that self-learning is the primary learning approach. These findings provide a range of actionable implications for researchers, educators, and developers.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = apr,
articleno = {44},
numpages = {29},
keywords = {user survey, Stack Overflow, Git commands}
}

@article{10.1145/3187011,
author = {Liu, Zhenguang and Xia, Yingjie and Liu, Qi and He, Qinming and Zhang, Chao and Zimmermann, Roger},
title = {Toward Personalized Activity Level Prediction in Community Question Answering Websites},
year = {2018},
issue_date = {April 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {14},
number = {2s},
issn = {1551-6857},
url = {https://doi.org/10.1145/3187011},
doi = {10.1145/3187011},
abstract = {Community Question Answering (CQA) websites have become valuable knowledge repositories. Millions of internet users resort to CQA websites to seek answers to their encountered questions. CQA websites provide information far beyond a search on a site such as Google due to (1) the plethora of high-quality answers, and (2) the capabilities to post new questions toward the communities of domain experts. While most research efforts have been made to identify experts or to preliminarily detect potential experts of CQA websites, there has been a remarkable shift toward investigating how to keep the engagement of experts. Experts are usually the major contributors of high-quality answers and questions of CQA websites. Consequently, keeping the expert communities active is vital to improving the lifespan of these websites. In this article, we present an algorithm termed PALP to predict the activity level of expert users of CQA websites. To the best of our knowledge, PALP is the first approach to address a personalized activity level prediction model for CQA websites. Furthermore, it takes into consideration user behavior change over time and focuses specifically on expert users. Extensive experiments on the Stack Overflow website demonstrate the competitiveness of PALP over existing methods.},
journal = {ACM Trans. Multimedia Comput. Commun. Appl.},
month = apr,
articleno = {41},
numpages = {15},
keywords = {personalized model, logistic regression, activity level prediction, Question answering website}
}

@article{10.1145/3487291,
author = {Guo, Aibo and Li, Xinyi and Pang, Ning and Zhao, Xiang},
title = {Adversarial Cross-domain Community Question Retrieval},
year = {2022},
issue_date = {May 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {21},
number = {3},
issn = {2375-4699},
url = {https://doi.org/10.1145/3487291},
doi = {10.1145/3487291},
abstract = {Community Q&amp;A forum is a special type of social media that provides a platform to raise questions and to answer them (both by forum participants), to facilitate online information sharing. Currently, community Q&amp;A forums in professional domains have attracted a large number of users by offering professional knowledge. To support information access and save users’ efforts of raising new questions, they usually come with a question retrieval function, which retrieves similar existing questions (and their answers) to a user’s query. However, it can be difficult for community Q&amp;A forums to cover all domains, especially those emerging lately with little labeled data but great discrepancy from existing domains. We refer to this scenario as cross-domain question retrieval. To handle the unique challenges of cross-domain question retrieval, we design a model based on adversarial training, namely, X-QR, which consists of two modules—a domain discriminator and a sentence matcher. The domain discriminator aims at aligning the source and target data distributions and unifying the feature space by domain-adversarial training. With the assistance of the domain discriminator, the sentence matcher is able to learn domain-consistent knowledge for the final matching prediction. To the best of our knowledge, this work is among the first to investigate the domain adaption problem of sentence matching for community Q&amp;A forums question retrieval. The experiment results suggest that the proposed X-QR model offers better performance than conventional sentence matching methods in accomplishing cross-domain community Q&amp;A tasks.},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = jan,
articleno = {61},
numpages = {22},
keywords = {adversarial training, domain adaption, community question retrieval, Community Q&amp;A}
}

@article{10.1109/TASLP.2022.3178241,
author = {Wang, Qian and Zhang, Jiajun and Zong, Chengqing},
title = {Synchronous Inference for Multilingual Neural Machine Translation},
year = {2022},
issue_date = {2022},
publisher = {IEEE Press},
volume = {30},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2022.3178241},
doi = {10.1109/TASLP.2022.3178241},
abstract = {Multilingual neural machine translation allows a single model to translate between multiple language pairs, which greatly reduces the cost of model training and receives much attention recently. Previous studies mainly focus on training stage optimization and improve positive knowledge transfer among languages with different levels of parameter sharing, but ignore the multilingual knowledge transfer during inference although the translation in one language may help the generation of other languages. This work enhances knowledge sharing among multiple target languages in the inference phase. To achieve this, we propose a synchronous inference method that can simultaneously generate translations in multiple languages. During generation, the model that predicts the next word of each language not only based on source sentence and previously predicted segments, but also based on predicted words of other target languages. To maximize the inference stage knowledge sharing, we design a cross-lingual attention module which allows the model to dynamically selects the most relevant information from multiple target languages. The synchronous inference model requires multi-way parallel training data which is scarce. We therefore propose to adopt multi-task learning to incorporate large-scale bilingual data. We evaluate our method on three multilingual translation datasets and prove that the proposed method significantly improve the translation quality and the decoding efficiency compared to strong bilingual and multilingual baselines.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = may,
pages = {1827–1839},
numpages = {13}
}

@article{10.1145/3401026,
author = {Gao, Zhipeng and Xia, Xin and Grundy, John and Lo, David and Li, Yuan-Fang},
title = {Generating Question Titles for Stack Overflow from Mined Code Snippets},
year = {2020},
issue_date = {October 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {29},
number = {4},
issn = {1049-331X},
url = {https://doi.org/10.1145/3401026},
doi = {10.1145/3401026},
abstract = {Stack Overflow has been heavily used by software developers as a popular way to seek programming-related information from peers via the internet. The Stack Overflow community recommends users to provide the related code snippet when they are creating a question to help others better understand it and offer their help. Previous studies have shown that a significant number of these questions are of low-quality and not attractive to other potential experts in Stack Overflow. These poorly asked questions are less likely to receive useful answers and hinder the overall knowledge generation and sharing process. Considering one of the reasons for introducing low-quality questions in SO is that many developers may not be able to clarify and summarize the key problems behind their presented code snippets due to their lack of knowledge and terminology related to the problem, and/or their poor writing skills, in this study we propose an approach to assist developers in writing high-quality questions by automatically generating question titles for a code snippet using a deep sequence-to-sequence learning approach. Our approach is fully data-driven and uses an attention mechanism to perform better content selection, a copy mechanism to handle the rare-words problem and a coverage mechanism to eliminate word repetition problem. We evaluate our approach on Stack Overflow datasets over a variety of programming languages (e.g., Python, Java, Javascript, C# and SQL) and our experimental results show that our approach significantly outperforms several state-of-the-art baselines in both automatic and human evaluation. We have released our code and datasets to facilitate other researchers to verify their ideas and inspire the follow up work.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = sep,
articleno = {26},
numpages = {37},
keywords = {sequence-to-sequence, question quality, question generation, Stack overflow}
}

@article{10.1145/3274302,
author = {Chen, Chunyang and Chen, Xi and Sun, Jiamou and Xing, Zhenchang and Li, Guoqiang},
title = {Data-Driven Proactive Policy Assurance of Post Quality in Community q&amp;a Sites},
year = {2018},
issue_date = {November 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {2},
number = {CSCW},
url = {https://doi.org/10.1145/3274302},
doi = {10.1145/3274302},
abstract = {To ensure the post quality, Q&amp;A sites usually develop a list of quality assurance guidelines for "dos and don'ts", and adopt collaborative editing mechanism to fix quality violations. Quality guidelines are mostly high-level principles, and many tacit and context-sensitive aspects of the expected quality cannot be easily enforced by a set of explicit rules. Collaborative editing is a reactive mechanism after low-quality posts have been posted. Our study of collaborative editing data on Stack Overflow suggests that tacit and context-sensitive quality-assurance knowledge is manifested in the editing patterns of large numbers of collaborative edits. Inspired by this observation, we develop and evaluate a Convolutional Neural Network based approach to learn editing patterns from historical post edits for predicting the need of editing a post. Our approach provides a proactive policy assurance mechanism that warns users potential quality issues in a post before it is posted.},
journal = {Proc. ACM Hum.-Comput. Interact.},
month = nov,
articleno = {33},
numpages = {22},
keywords = {quality assurance, deep learning, collaborative editing, Q&amp;A sites}
}

@article{10.1145/3434279,
author = {Zhang, Haoxiang and Wang, Shaowei and Chen, Tse-Hsun (Peter) and Hassan, Ahmed E.},
title = {Are Comments on Stack Overflow Well Organized for Easy Retrieval by Developers?},
year = {2021},
issue_date = {April 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {30},
number = {2},
issn = {1049-331X},
url = {https://doi.org/10.1145/3434279},
doi = {10.1145/3434279},
abstract = {Many Stack Overflow answers have associated informative comments that can strengthen them and assist developers. A prior study found that comments can provide additional information to point out issues in their associated answer, such as the obsolescence of an answer. By showing more informative comments (e.g., the ones with higher scores) and hiding less informative ones, developers can more effectively retrieve information from the comments that are associated with an answer. Currently, Stack Overflow prioritizes the display of comments, and, as a result, 4.4 million comments (possibly including informative comments) are hidden by default from developers. In this study, we investigate whether this mechanism effectively organizes informative comments. We find that (1) the current comment organization mechanism does not work well due to the large amount of tie-scored comments (e.g., 87\% of the comments have 0-score) and (2) in 97.3\% of answers with hidden comments, at least one comment that is possibly informative is hidden while another comment with the same score is shown (i.e., unfairly hidden comments). The longest unfairly hidden comment is more likely to be informative than the shortest one. Our findings highlight that Stack Overflow should consider adjusting the comment organization mechanism to help developers effectively retrieve informative comments. Furthermore, we build a classifier that can effectively distinguish informative comments from uninformative comments. We also evaluate two alternative comment organization mechanisms (i.e., the Length mechanism and the Random mechanism) based on text similarity and the prediction of our classifier.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = feb,
articleno = {22},
numpages = {31},
keywords = {stack overflow, crowdsourced knowledge sharing, commenting, Q8A website, Empirical software engineering}
}

@article{10.1145/3329485,
author = {Wu, Qunfang and Sang, Yisi and Huang, Yun},
title = {Danmaku: A New Paradigm of Social Interaction via Online Videos},
year = {2019},
issue_date = {June 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {2},
number = {2},
url = {https://doi.org/10.1145/3329485},
doi = {10.1145/3329485},
abstract = {Danmaku is a new commentary design for online videos. Unlike traditional forums where comments are displayed asynchronously below a video screen in order of when the comments are posted, danmaku comments are overlaid on the screen and displayed along with the video. This new design creates a pseudo-synchronous effect by displaying asynchronous comments with certain video segments in a synchronous fashion, and the links between danmaku comments and the video segments are defined by users. Danmaku is gaining popularity; however, little is known, compared to the traditional forum design, regarding how effective the new danmaku design is in promoting social interactions among online users. In this work, we collected 38,399 danmaku comments and 16,414 forum comments posted in 2017 on 30 popular videos on Bilibili.com. We compared user participation from different perspectives, e.g., number of comments, sentiment of the comments, language patterns, and ways of knowledge sharing. Our results showed that compared to the traditional linear design, the danmaku design significantly promoted user participation, i.e., there were more users and more comments in danmaku. Additionally, active users posted more positive comments, though they were anonymous; more linguistic memes were used in danmaku, suggesting that it was used to facilitate community-building. In addition to its effectiveness in promoting social interactions, our results also show that danmaku and forum designs play complementary roles in knowledge sharing, where danmaku comments involved more explicit (know-what) knowledge sharing, and forum comments exhibited more tacit (know-how) knowledge sharing. Our findings contribute to the development of social presence theory and have design implications for better social interaction via online videos.},
journal = {Trans. Soc. Comput.},
month = jun,
articleno = {7},
numpages = {24},
keywords = {tacit knowledge, synchronous, social presence, online videos, knowledge sharing, forum comments, explicit knowledge, anonymity, Danmaku}
}

@article{10.1145/3492855,
author = {Kou, Ziyi and Shang, Lanyu and Zhang, Yang and Wang, Dong},
title = {HC-COVID: A Hierarchical Crowdsource Knowledge Graph Approach to Explainable COVID-19 Misinformation Detection},
year = {2022},
issue_date = {January 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {6},
number = {GROUP},
url = {https://doi.org/10.1145/3492855},
doi = {10.1145/3492855},
abstract = {The proliferation of social media has promoted the spread of misinformation that raises many concerns in our society. This paper focuses on a critical problem of explainable COVID-19 misinformation detection that aims to accurately identify and explain misleading COVID-19 claims on social media. Motivated by the lack of COVID-19 relevant knowledge in existing solutions, we construct a novel crowdsource knowledge graph based approach to incorporate the COVID-19 knowledge facts by leveraging the collaborative efforts of expert and non-expert crowd workers. Two important challenges exist in developing our solution: i) how to effectively coordinate the crowd efforts from both expert and non-expert workers to generate the relevant knowledge facts for detecting COVID-19 misinformation; ii) How to leverage the knowledge facts from the constructed knowledge graph to accurately explain the detected COVID-19 misinformation. To address the above challenges, we develop HC-COVID, a hierarchical crowdsource knowledge graph based framework that explicitly models the COVID-19 knowledge facts contributed by crowd workers with different levels of expertise and accurately identifies the related knowledge facts to explain the detection results. We evaluate HC-COVID using two public real-world datasets on social media. Evaluation results demonstrate that HC-COVID significantly outperforms state-of-the-art baselines in terms of the detection accuracy of misleading COVID-19 claims and the quality of the explanations.},
journal = {Proc. ACM Hum.-Comput. Interact.},
month = jan,
articleno = {36},
numpages = {25},
keywords = {human-ai collaboration, explainable misinformation detection, covid19}
}

@article{10.1145/3428282,
author = {Bagherzadeh, Mehdi and Fireman, Nicholas and Shawesh, Anas and Khatchadourian, Raffi},
title = {Actor concurrency bugs: a comprehensive study on symptoms, root causes, API usages, and differences},
year = {2020},
issue_date = {November 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {4},
number = {OOPSLA},
url = {https://doi.org/10.1145/3428282},
doi = {10.1145/3428282},
abstract = {Actor concurrency is becoming increasingly important in the development of real-world software systems. Although actor concurrency may be less susceptible to some multithreaded concurrency bugs, such as low-level data races and deadlocks, it comes with its own bugs that may be different. However, the fundamental characteristics of actor concurrency bugs, including their symptoms, root causes, API usages, examples, and differences when they come from different sources are still largely unknown. Actor software development can significantly benefit from a comprehensive qualitative and quantitative understanding of these characteristics, which is the focus of this work, to foster better API documentation, development practices, testing, debugging, repairing, and verification frameworks. To conduct this study, we take the following major steps. First, we construct a set of 186 real-world Akka actor bugs from Stack Overflow and GitHub via manual analysis of 3,924 Stack Overflow questions, answers, and comments and 3,315 GitHub commits, messages, original and modified code snippets, issues, and pull requests. Second, we manually study these actor bugs and their fixes to understand and classify their symptoms, root causes, and API usages. Third, we study the differences between the commonalities and distributions of symptoms, root causes, and API usages of our Stack Overflow and GitHub actor bugs. Fourth, we discuss real-world examples of our actor bugs with these symptoms and root causes. Finally, we investigate the relation of our findings with those of previous work and discuss their implications. A few findings of our study are: (1) symptoms of our actor bugs can be classified into five categories, with Error as the most common symptom and Incorrect Exceptions as the least common, (2) root causes of our actor bugs can be classified into ten categories, with Logic as the most common root cause and Untyped Communication as the least common, (3) a small number of Akka API packages are responsible for most of API usages by our actor bugs, and (4) our Stack Overflow and GitHub actor bugs can differ significantly in commonalities and distributions of their symptoms, root causes, and API usages. While some of our findings agree with those of previous work, others sharply contrast.},
journal = {Proc. ACM Program. Lang.},
month = nov,
articleno = {214},
numpages = {32},
keywords = {Stack Overflow, GitHub, Akka actor bugs, Actor bug symptoms, Actor bug root causes, Actor bug differences, Actor bug API usages}
}

@article{10.5555/3503984.3503990,
author = {Crandall, Kalee},
title = {Knowledge sharing technology in school counseling: a literature review},
year = {2021},
issue_date = {October 2021},
publisher = {Consortium for Computing Sciences in Colleges},
address = {Evansville, IN, USA},
volume = {37},
number = {2},
issn = {1937-4771},
abstract = {School counselors are expected to perform a wide range of tasks to improve student outcomes but are oftentimes limited in the resources needed to perform these tasks. The lack of resources, including time and the knowledge needed to complete tasks effectively, may contribute to unnecessary stress and possible burnout. This research uses a modified systematic literature review process to explore knowledge sharing technology in school counseling and presents a proposed model for future research adapted from Alavi's Model of Knowledge Transfer among Individuals in a Group. The findings of this study outline the current state of research and are relevant to research and practice.},
journal = {J. Comput. Sci. Coll.},
month = oct,
pages = {61–69},
numpages = {9}
}

@article{10.1145/3604552,
author = {Qin, Chuan and Zhu, Hengshu and Shen, Dazhong and Sun, Ying and Yao, Kaichun and Wang, Peng and Xiong, Hui},
title = {Automatic Skill-Oriented Question Generation and Recommendation for Intelligent Job Interviews},
year = {2023},
issue_date = {January 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {42},
number = {1},
issn = {1046-8188},
url = {https://doi.org/10.1145/3604552},
doi = {10.1145/3604552},
abstract = {Job interviews are the most widely accepted method for companies to select suitable candidates, and a critical challenge is finding the right questions to ask job candidates. Moreover, there is a lack of integrated tools for automatically generating interview questions and recommending the right questions to interviewers. To this end, in this paper, we propose an intelligent system for assisting job interviews, namely, DuerQues. To build this system, we first investigate how to automatically generate skill-oriented interview questions in a scalable way by learning external knowledge from online knowledge-sharing communities. Along this line, we develop a novel distantly supervised skill entity recognition method to identify skill entities from large-scale search queries and web page titles with less need for human annotation. Additionally, we propose a neural generative model for generating skill-oriented interview questions. In particular, we introduce a data-driven solution to create high-quality training instances and design a learning algorithm to improve the performance of question generation. Furthermore, we exploit click-through data from query logs and design a recommender system for recommending suitable questions to interviewers. Specifically, we introduce a graph-enhanced algorithm to efficiently recommend suitable questions given a set of queried skills. Finally, extensive experiments on real-world datasets demonstrate the effectiveness of our DuerQues system in terms of the quality of generated skill-oriented questions and the performance of question recommendation.},
journal = {ACM Trans. Inf. Syst.},
month = aug,
articleno = {27},
numpages = {32},
keywords = {Job interview assessment, question generation, question recommendation}
}

@article{10.1145/3546945,
author = {Cogo, Filipe Roseiro and Xia, Xin and Hassan, Ahmed E.},
title = {Assessing the Alignment between the Information Needs of Developers and the Documentation of Programming Languages: A Case Study on Rust},
year = {2023},
issue_date = {March 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {32},
number = {2},
issn = {1049-331X},
url = {https://doi.org/10.1145/3546945},
doi = {10.1145/3546945},
abstract = {Programming language documentation refers to the set of technical documents that provide application developers with a description of the high-level concepts of a language (e.g., manuals, tutorials, and API references). Such documentation is essential to support application developers in effectively using a programming language. One of the challenges faced by documenters (i.e., personnel that design and produce documentation for a programming language) is to ensure that documentation has relevant information that aligns with the concrete needs of developers, defined as the missing knowledge that developers acquire via voluntary search. In this article, we present an automated approach to support documenters in evaluating the differences and similarities between the concrete information need of developers and the current state of documentation (a problem that we refer to as the topical alignment of a programming language documentation). Our approach leverages semi-supervised topic modelling that uses domain knowledge to guide the derivation of topics. We initially train a baseline topic model from a set of Rust-related Q&amp;A posts. We then use this baseline model to determine the distribution of topic probabilities of each document of the official Rust documentation. Afterwards, we assess the similarities and differences between the topics of the Q&amp;A posts and the official documentation. Our results show a relatively high level of topical alignment in Rust documentation. Still, information about specific topics is scarce in both the Q&amp;A websites and the documentation, particularly related topics with programming niches such as network, game, and database development. For other topics (e.g., related topics with language features such as structs, patterns and matchings, and foreign function interface), information is only available on Q&amp;A websites while lacking in the official documentation. Finally, we discuss implications for programming language documenters, particularly how to leverage our approach to prioritize topics that should be added to the documentation.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = apr,
articleno = {43},
numpages = {48},
keywords = {domain knowledge, topic models, RustForum, Stack Overflow, Q&amp;A websites, Rust, programming languages, Documentation}
}

@article{10.1109/TCBB.2022.3225234,
author = {Saadat, Hajira and Shah, Babar and Halim, Zahid and Anwar, Sajid},
title = {Knowledge Graph-Based Convolutional Network Coupled With Sentiment Analysis Towards Enhanced Drug Recommendation},
year = {2022},
issue_date = {July-Aug. 2024},
publisher = {IEEE Computer Society Press},
address = {Washington, DC, USA},
volume = {21},
number = {4},
issn = {1545-5963},
url = {https://doi.org/10.1109/TCBB.2022.3225234},
doi = {10.1109/TCBB.2022.3225234},
abstract = {Recommending appropriate drugs to patients based on their history and symptoms is a complex real-world problem. Knowing whether a drug is useful without its consumption by a variety of people followed by proper evaluation is a challenge. Modern-day recommender systems can assist in this provided they receive large data to learn. Public reviews on various drugs are available for knowledge sharing. These reviews assist in recommending the best and most appropriate option to the user. The explicit feedback underpins the entire recommender system. This work develops a novel knowledge graph-based convolutional network for recommending drugs. The knowledge graph is coupled with sentiment analysis extracted from the public reviews on drugs to enhance drug recommendations. For each drug that has been used previously, sentiments have been analyzed to determine which one has the most effective reviews. The knowledge graph effectively captures user-item relatedness by mining its associated attributes. Experiments are performed on public benchmarks and a comparison is made with closely related state-of-the-art works. Based on the obtained results, the current work performs better than the past contributions by achieving up to 98.7% Area Under Curve (AUC) score.},
journal = {IEEE/ACM Trans. Comput. Biol. Bioinformatics},
month = nov,
pages = {983–994},
numpages = {12}
}

@article{10.1145/3441302,
author = {Ghasemi, Negin and Fatourechi, Ramin and Momtazi, Saeedeh},
title = {User Embedding for Expert Finding in Community Question Answering},
year = {2021},
issue_date = {August 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {15},
number = {4},
issn = {1556-4681},
url = {https://doi.org/10.1145/3441302},
doi = {10.1145/3441302},
abstract = {The number of users who have the appropriate knowledge to answer asked questions in community question answering is lower than those who ask questions. Therefore, finding expert users who can answer the questions is very crucial and useful. In this article, we propose a framework to find experts for given questions and assign them the related questions. The proposed model benefits from users’ relations in a community along with the lexical and semantic similarities between new question and existing answers. Node embedding is applied to the community graph to find similar users. Our experiments on four different Stack Exchange datasets show that adding community relations improves the performance of expert finding models.},
journal = {ACM Trans. Knowl. Discov. Data},
month = mar,
articleno = {70},
numpages = {16},
keywords = {semantic text similarity, graph embedding, community question answering, Expert finding}
}

@article{10.1145/3301442,
author = {Der Weth, Christian Von and Abdul, Ashraf and Kashyap, Abhinav R. and Kankanhalli, Mohan S.},
title = {CloseUp—A Community-Driven Live Online Search Engine},
year = {2019},
issue_date = {August 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {19},
number = {3},
issn = {1533-5399},
url = {https://doi.org/10.1145/3301442},
doi = {10.1145/3301442},
abstract = {Search engines are still the most common way of finding information on the Web. However, they are largely unable to provide satisfactory answers to time- and location-specific queries. Such queries can best and often only be answered by humans that are currently on-site. Although online platforms for community question answering are very popular, very few exceptions consider the notion of users’ current physical locations. In this article, we present CloseUp, our prototype for the seamless integration of community-driven live search into a Google-like search experience. Our efforts focus on overcoming the defining differences between traditional Web search and community question answering, namely the formulation of search requests (keyword-based queries vs. well-formed questions) and the expected response times (milliseconds vs. minutes/hours). To this end, the system features a deep learning pipeline to analyze submitted queries and translate relevant queries into questions. Searching users can submit suggested questions to a community of mobile users. CloseUp provides a stand-alone mobile application for submitting, browsing, and replying to questions. Replies from mobile users are presented as live results in the search interface. Using a field study, we evaluated the feasibility and practicability of our approach.},
journal = {ACM Trans. Internet Technol.},
month = aug,
articleno = {39},
numpages = {21},
keywords = {social computing, query transformation, crowdsourcing, community question answering, collaborative service, Live online search}
}

@article{10.1145/3412845,
author = {Gao, Zhipeng and Xia, Xin and Lo, David and Grundy, John},
title = {Technical Q8A Site Answer Recommendation via Question Boosting},
year = {2021},
issue_date = {January 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {30},
number = {1},
issn = {1049-331X},
url = {https://doi.org/10.1145/3412845},
doi = {10.1145/3412845},
abstract = {Software developers have heavily used online question-and-answer platforms to seek help to solve their technical problems. However, a major problem with these technical Q8A sites is “answer hungriness,” i.e., a large number of questions remain unanswered or unresolved, and users have to wait for a long time or painstakingly go through the provided answers with various levels of quality. To alleviate this time-consuming problem, we propose a novel DEEPANS neural network–based approach to identify the most relevant answer among a set of answer candidates. Our approach follows a three-stage process: question boosting, label establishment, and answer recommendation. Given a post, we first generate a clarifying question as a way of question boosting. We automatically establish the positive, neutral+, neutral-, and negative training samples via label establishment. When it comes to answer recommendation, we sort answer candidates by the matching scores calculated by our neural network–based model. To evaluate the performance of our proposed model, we conducted a large-scale evaluation on four datasets, collected from the real-world technical Q8A sites (i.e., Ask Ubuntu, Super User, Stack Overflow Python, and Stack Overflow Java). Our experimental results show that our approach significantly outperforms several state-of-the-art baselines in automatic evaluation. We also conducted a user study with 50 solved/unanswered/unresolved questions. The user-study results demonstrate that our approach is effective in solving the answer-hungry problem by recommending the most relevant answers from historical archives.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = dec,
articleno = {11},
numpages = {34},
keywords = {weakly supervised learning, sequence-to-sequence, question boosting, question answering, deep neural network, CQA}
}

@article{10.1145/2934687,
author = {Srba, Ivan and Bielikova, Maria},
title = {A Comprehensive Survey and Classification of Approaches for Community Question Answering},
year = {2016},
issue_date = {August 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {10},
number = {3},
issn = {1559-1131},
url = {https://doi.org/10.1145/2934687},
doi = {10.1145/2934687},
abstract = {Community question-answering (CQA) systems, such as Yahoo! Answers or Stack Overflow, belong to a prominent group of successful and popular Web 2.0 applications, which are used every day by millions of users to find an answer on complex, subjective, or context-dependent questions. In order to obtain answers effectively, CQA systems should optimally harness collective intelligence of the whole online community, which will be impossible without appropriate collaboration support provided by information technologies. Therefore, CQA became an interesting and promising subject of research in computer science and now we can gather the results of 10 years of research. Nevertheless, in spite of the increasing number of publications emerging each year, so far the research on CQA systems has missed a comprehensive state-of-the-art survey. We attempt to fill this gap by a review of 265 articles published between 2005 and 2014, which were selected from major conferences and journals. According to this evaluation, at first we propose a framework that defines descriptive attributes of CQA approaches. Second, we introduce a classification of all approaches with respect to problems they are aimed to solve. The classification is consequently employed in a review of a significant number of representative approaches, which are described by means of attributes from the descriptive framework. As a part of the survey, we also depict the current trends as well as highlight the areas that require further attention from the research community.},
journal = {ACM Trans. Web},
month = aug,
articleno = {18},
numpages = {63},
keywords = {user modeling, online communities, knowledge sharing, exploratory studies, content modeling, adaptive collaboration support, Community question answering}
}

@article{10.1145/3604607,
author = {Ma, Suyu and Chen, Chunyang and Khalajzadeh, Hourieh and Grundy, John},
title = {A First Look at Dark Mode in Real-world Android Apps},
year = {2023},
issue_date = {January 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {33},
number = {1},
issn = {1049-331X},
url = {https://doi.org/10.1145/3604607},
doi = {10.1145/3604607},
abstract = {Android apps often have a “dark mode” option used in low-light situations, for those who find the conventional color palette problematic, or because of personal preferences. Typically developers add a dark mode option for their apps with different backgrounds, text, and sometimes iconic forms. We wanted to understand the actual provision of this dark mode in real-world Android apps through an empirical study of posts from Stack Overflow and real-world Android app analysis. Using these approaches, we identified the aspects of dark mode that developers implemented as well as the key difficulties they experienced in implementing it. We performed a quantitative analysis using open-coding of more than 300 discussion threads to create a taxonomy regarding the aspects discussed by developers with respect to dark mode in Android. Our quantitative analysis of over 6,000 Android apps highlights which dark mode features are typically provided in Android apps and which aspects developers care about during dark mode design. We also examined four app development support tools to see how well they aid Android app development for dark mode. From our analysis, we distilled some key lessons to guide further research and actions in aiding developers with supporting users who require such assistive features. For example, developers should be aware of the potential risks in using unsuitable dark mode design schema and researchers should take dark mode features into consideration when developing app development support tools.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = nov,
articleno = {3},
numpages = {26},
keywords = {Android, accessibility, dark mode, Graphical user interface}
}

@article{10.1145/3603706,
author = {Fadlallah, Hadi and Kilany, Rima and Dhayne, Houssein and El Haddad, Rami and Haque, Rafiqul and Taher, Yehia and Jaber, Ali},
title = {BIGQA: Declarative Big Data Quality Assessment},
year = {2023},
issue_date = {September 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {15},
number = {3},
issn = {1936-1955},
url = {https://doi.org/10.1145/3603706},
doi = {10.1145/3603706},
abstract = {In the big data domain, data quality assessment operations are often complex and must be implementable in a distributed and timely manner. This article tries to generalize the quality assessment operations by providing a new ISO-based declarative data quality assessment framework (BIGQA). BIGQA is a flexible solution that supports data quality assessment in different domains and contexts. It facilitates the planning and execution of big data quality assessment operations for data domain experts and data management specialists at any phase in the data life cycle. This work implements BIGQA to demonstrate its ability to produce customized data quality reports while running efficiently on parallel or distributed computing frameworks. BIGQA generates data quality assessment plans using straightforward operators designed to handle big data and guarantee a high degree of parallelism when executed. Moreover, it allows incremental data quality assessment to avoid reading the whole dataset each time the quality assessment operation is required. The result was validated using radiation wireless sensor data and Stack Overflow users’ data to show that it can be implemented within different contexts. The experiments show a 71\% performance improvement over a 1 GB flat file on a single processing machine compared with a non-parallel application and a 75\% performance improvement over a 25 GB flat file within a distributed environment compared to a non-distributed application.},
journal = {J. Data and Information Quality},
month = aug,
articleno = {27},
numpages = {30},
keywords = {data quality, big data, quality assessment, Declarative framework}
}

@article{10.1145/3274381,
author = {Lu, Zhicong and Heo, Seongkook and Wigdor, Daniel J.},
title = {StreamWiki: Enabling Viewers of Knowledge Sharing Live Streams to Collaboratively Generate Archival Documentation for Effective In-Stream and Post Hoc Learning},
year = {2018},
issue_date = {November 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {2},
number = {CSCW},
url = {https://doi.org/10.1145/3274381},
doi = {10.1145/3274381},
abstract = {Knowledge-sharing live streams are distinct from traditional educational videos, at least because of the large concurrently-viewing audience and the real-time discussions between viewers and the streamer. Though this creates unique opportunities for interactive learning, it also brings a challenge for creating a useful archive for post hoc learning. This paper presents the results of interviews with knowledge sharing streamers, their moderators, and viewers to understand current experiences and needs for sharing and learning knowledge through live streaming. Based on those findings, we built StreamWiki, a tool which leverages the viewers during live streams to produce useful archives of the interactive learning experience. On StreamWiki, moderators initiate tasks that viewers complete by conducting microtasks, such as writing a summary, commenting, and voting for informative comments. As a result, a summary document is built in real time. Through the tests of our prototype with streamers and viewers, we found that StreamWiki could help understanding the content and the context of the stream, during the stream and for post hoc learning.},
journal = {Proc. ACM Hum.-Comput. Interact.},
month = nov,
articleno = {112},
numpages = {26},
keywords = {live streaming, learning, knowledge sharing, knowledge building, collaborative documentation}
}

@article{10.1145/3618298,
author = {Yi, Zixuan and Ounis, Iadh and MacDonald, Craig},
title = {Contrastive Graph Prompt-tuning for Cross-domain Recommendation},
year = {2023},
issue_date = {March 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {42},
number = {2},
issn = {1046-8188},
url = {https://doi.org/10.1145/3618298},
doi = {10.1145/3618298},
abstract = {Recommender systems commonly suffer from the long-standing data sparsity problem where insufficient user-item interaction data limits the systems’ ability to make accurate recommendations. This problem can be alleviated using cross-domain recommendation techniques. In particular, in a cross-domain setting, knowledge sharing between domains permits improved effectiveness on the target domain. While recent cross-domain recommendation techniques used a pre-training configuration, we argue that such techniques lead to a low fine-tuning efficiency, especially when using large neural models. In recent language models, prompts have been used for parameter-efficient and time-efficient tuning of the models on the downstream tasks—these prompts represent a tunable latent vector that permits to freeze the rest of the language model’s parameters. To address the cross-domain recommendation task in an efficient manner, we propose a novel Personalised Graph Prompt-based Recommendation (PGPRec) framework, which leverages the efficiency benefits from prompt-tuning. In such a framework, we develop personalised and item-wise graph prompts based on relevant items to those items the user has interacted with. In particular, we apply Contrastive Learning to generate the pre-trained embeddings, to allow an increased generalisability in the pre-training stage, and to ensure an effective prompt-tuning stage. To evaluate the effectiveness of our PGPRec framework in a cross-domain setting, we conduct an extensive evaluation with the top-k recommendation task and perform a cold-start analysis. The obtained empirical results on four Amazon Review datasets show that our proposed PGPRec framework can reduce up to 74\% of the tuned parameters with a competitive performance and achieves an 11.41\% improved performance compared to the strongest baseline in a cold-start scenario.},
journal = {ACM Trans. Inf. Syst.},
month = dec,
articleno = {60},
numpages = {28},
keywords = {graph neural network, recommender system, Personalisation}
}

@article{10.1145/3617174,
author = {Huang, Qing and Yuan, Zhiqiang and Xing, Zhenchang and Peng, Xin and Xu, Xiwei and Lu, Qinghua},
title = {FQN Inference in Partial Code by Prompt-tuned Language Model of Code},
year = {2023},
issue_date = {February 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {33},
number = {2},
issn = {1049-331X},
url = {https://doi.org/10.1145/3617174},
doi = {10.1145/3617174},
abstract = {Partial code usually involves non-fully-qualified type names (non-FQNs) and undeclared receiving objects. Resolving the FQNs of these non-FQN types and undeclared receiving objects (referred to as type inference) is the prerequisite to effective search and reuse of partial code. Existing dictionary-lookup based methods build a symbolic knowledge base of API names and code contexts, which involve significant compilation overhead and are sensitive to unseen API names and code context variations. In this article, we propose using a prompt-tuned code masked language model (MLM) as a neural knowledge base for type inference, called POME, which is lightweight and has minimal requirements on code compilation. Unlike the existing symbol name and context matching for type inference, POME infers the FQNs syntax and usage knowledge encapsulated in prompt-tuned code MLM through a colze-style fill-in-blank strategy. POME is integrated as a plug-in into web and integrated development environments (IDE) to assist developers in inferring FQNs in the real world. We systematically evaluate POME on a large amount of source code from GitHub and Stack Overflow, and explore its generalization and hybrid capability. The results validate the effectiveness of the POME design and its applicability for partial code type inference, and they can be easily extended to different programming languages (PL). POME can also be used to generate a PL-hybrid type inference model for providing a one-for-all solution. As the first of its kind, our neural type inference method opens the door to many innovative ways of using partial code.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = dec,
articleno = {31},
numpages = {32},
keywords = {neural knowledge base, code masked language model, fully qualified names, Type inference}
}

@article{10.1145/3449215,
author = {Guo, Cheng and Caine, Kelly},
title = {Anonymity, User Engagement, Quality, and Trolling on Q&amp;A Sites},
year = {2021},
issue_date = {April 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {5},
number = {CSCW1},
url = {https://doi.org/10.1145/3449215},
doi = {10.1145/3449215},
abstract = {In online question and answer (Q&amp;A) communities, people ask questions and share answers at all levels of topic sensitivity. Identity options within these communities range from anonymity to real name. The amount of engagement, and the quality of engagement on Q&amp;A sites may differ depending on the identity options available. In this paper, we investigate the relationship between the amount of engagement, the quality of engagement, and different types of identity by analyzing three Q&amp;A sites with different identity policies. We find that highly sensitive questions are more likely to be asked anonymously. Furthermore, allowing anonymity does not affect answer quality and only has a weak, negative indirect effect on engagement. On the other hand, anonymity leads to more trolling. We suggest online communities provide a way for users to ask highly sensitive questions anonymously and pair this with moderation mechanisms to reduce trolling},
journal = {Proc. ACM Hum.-Comput. Interact.},
month = apr,
articleno = {141},
numpages = {27},
keywords = {anonymity, identity, online communities, privacy, q&amp;a sites, trolling, user engagement}
}

@article{10.14778/3587136.3587150,
author = {Chen, Tianyi and Gao, Jun and Chen, Hedui and Tu, Yaofeng},
title = {LOGER: A Learned Optimizer Towards Generating Efficient and Robust Query Execution Plans},
year = {2023},
issue_date = {March 2023},
publisher = {VLDB Endowment},
volume = {16},
number = {7},
issn = {2150-8097},
url = {https://doi.org/10.14778/3587136.3587150},
doi = {10.14778/3587136.3587150},
abstract = {Query optimization based on deep reinforcement learning (DRL) has become a hot research topic recently. Despite the achieved promising progress, DRL optimizers still face great challenges of robustly producing efficient plans, due to the vast search space for both join order and operator selection and the highly varying execution latency taken as the feedback signal. In this paper, we propose LOGER, a learned optimizer towards generating efficient and robust plans, aiming at producing both efficient join orders and operators. LOGER first utilizes Graph Transformer to capture relationships between tables and predicates. Then, the search space is reorganized, in which LOGER learns to restrict specific operators instead of directly selecting one for each join, while utilizing DBMS built-in optimizer to select physical operators under the restrictions. Such a strategy exploits expert knowledge to improve the robustness of plan generation while offering sufficient plan search flexibility. Furthermore, LOGER introduces ε-beam search, which keeps multiple search paths that preserve promising plans while performing guided exploration. Finally, LOGER introduces a loss function with reward weighting to further enhance performance robustness by reducing the fluctuation caused by poor operators, and log transformation to compress the range of rewards. We conduct experiments on Join Order Benchmark (JOB), TPC-DS and Stack Overflow, and demonstrate that LOGER can achieve a performance better than existing learned query optimizers, with a 2.07x speedup on JOB compared with PostgreSQL.},
journal = {Proc. VLDB Endow.},
month = mar,
pages = {1777–1789},
numpages = {13}
}

@article{10.1145/3134667,
author = {Chen, Chunyang and Xing, Zhenchang and Liu, Yang},
title = {By the Community \&amp; For the Community: A Deep Learning Approach to Assist Collaborative Editing in Q&amp;A Sites},
year = {2017},
issue_date = {November 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {1},
number = {CSCW},
url = {https://doi.org/10.1145/3134667},
doi = {10.1145/3134667},
abstract = {Community edits to questions and answers (called post edits) plays an important role in improving content quality in Stack Overflow. Our study of post edits in Stack Overflow shows that a large number of edits are about formatting, grammar and spelling. These post edits usually involve small-scale sentence edits and our survey of trusted contributors suggests that most of them care much or very much about such small sentence edits. To assist users in making small sentence edits, we develop an edit-assistance tool for identifying minor textual issues in posts and recommending sentence edits for correction. We formulate the sentence editing task as a machine translation problem, in which an original sentence is "translated" into an edited sentence. Our tool implements a character-level Recurrent Neural Network (RNN) encoder-decoder model, trained with about 6.8 millions original-edited sentence pairs from Stack Overflow post edits. We evaluate our edit assistance tool using a large-scale archival post edits, a field study of assisting a novice post editor, and a survey of trusted contributors. Our evaluation demonstrates the feasibility of training a deep learning model with post edits by the community and then using the trained model to assist post editing for the community.},
journal = {Proc. ACM Hum.-Comput. Interact.},
month = dec,
articleno = {32},
numpages = {21},
keywords = {deep learning, collaborative editing, Q&amp;A sites}
}

@article{10.1145/3589303,
author = {Wang, Yatong and Wu, Yuncheng and Chen, Xincheng and Feng, Gang and Ooi, Beng Chin},
title = {Incentive-Aware Decentralized Data Collaboration},
year = {2023},
issue_date = {June 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {1},
number = {2},
url = {https://doi.org/10.1145/3589303},
doi = {10.1145/3589303},
abstract = {Data collaboration enables multiple parties to pool data for deriving meaningful data insights. However, data misuse and unlawful data collection have led to precautionary measures being imposed by individual organizations to guide against data leakage and abuse. As a response, decentralized federated learning (DFL) has emerged as an attractive paradigm to facilitate data collaboration while being amenable to privacy-preserving data and knowledge sharing, cost reduction, and prediction accuracy improvement. Unfortunately, the participating parties in DFL tend to be heterogeneous with skew datasets and uneven capabilities. Inevitably, training and transmission costs, and the presence of free-riders pose challenges to the adoption and participation of DFL. The absence of centralized parameter servers further exacerbates the problem of evaluating the contribution of each individual party. Therefore, an effective incentive mechanism is essential to promote data collaboration.In this paper, we propose a novel Incentive-aware Decentralized fEderated leArning (IDEA) framework for facilitating data collaboration. Specifically, we first design a customizable reward scheme for heterogeneous parties to optimize their respective objectives such as higher model accuracy, communication efficiency, and computational efficiency. To reward fairly to deserving parties while offering flexibility, we propose a novel multi-agent reinforcement learning (MARL) incentive mechanism, which enables heterogeneous parties to learn their own optimal collaboration policy. We then design an efficient decentralized data collaboration algorithm that supports the customizable reward scheme based on individual objective-specific collaboration policy. We theoretically prove that the algorithm achieves a Nash equilibrium, which ensures the fairness of the corresponding rewards for parties. We conduct extensive experiments to evaluate the performance of our proposed framework against four baselines on five real-world datasets. The results show that IDEA outperforms state-of-the-art methods in terms of effectiveness, efficiency, and accumulated reward.},
journal = {Proc. ACM Manag. Data},
month = jun,
articleno = {158},
numpages = {27},
keywords = {data collaboration, decentralized learning, incentive mechanism}
}

@article{10.1145/2983645,
author = {Park, Sangkeun and Ackerman, Mark S. and Lee, Uichin},
title = {Localness of Location-based Knowledge Sharing: A Study of Naver KiN “Here”},
year = {2018},
issue_date = {August 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {12},
number = {3},
issn = {1559-1131},
url = {https://doi.org/10.1145/2983645},
doi = {10.1145/2983645},
abstract = {In location-based social Q8A services, people ask a question with a high expectation that local residents who have local knowledge will answer the question. However, little is known about the locality of user activities in location-based social Q8A services. This study aims to deepen our understanding of location-based knowledge sharing by investigating the following: general behavioral characteristics of users, the topical and typological patterns related to geographic characteristics, geographic locality of user activities, and motivations of local knowledge sharing. To this end, we analyzed a 12-month period Q8A dataset from Naver KiN “Here,” a location-based social Q8A mobile app, in addition to a supplementary survey dataset obtained from 285 mobile users. Our results reveal several unique characteristics of location-based social Q8A. When compared with conventional social Q8A sites, users ask and answer different topical/typological questions. In addition, those who answer have a strong spatial locality wherein they primarily have local knowledge in a few regions, in areas such as their home and work. We also find unique motivators such as ownership of local knowledge and a sense of local community. The findings reported in the article have significant implications for the design of Q8A systems, especially location-based social Q8A systems.},
journal = {ACM Trans. Web},
month = jul,
articleno = {16},
numpages = {33},
keywords = {mobile applications, Knowledge sharing}
}

@article{10.1145/3495530,
author = {Zhang, Peng and Liu, Baoxi and Lu, Tun and Ding, Xianghua and Gu, Hansu and Gu, Ning},
title = {Jointly Predicting Future Content in Multiple Social Media Sites Based on Multi-task Learning},
year = {2022},
issue_date = {October 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {40},
number = {4},
issn = {1046-8188},
url = {https://doi.org/10.1145/3495530},
doi = {10.1145/3495530},
abstract = {User-generated contents (UGC) in social media are the direct expression of users’ interests, preferences, and opinions. User behavior prediction based on UGC has increasingly been investigated in recent years. Compared to learning a person’s behavioral patterns in each social media site separately, jointly predicting user behavior in multiple social media sites and complementing each other (cross-site user behavior prediction) can be more accurate. However, cross-site user behavior prediction based on UGC is a challenging task due to the difficulty of cross-site data sampling, the complexity of UGC modeling, and uncertainty of knowledge sharing among different sites. For these problems, we propose a Cross-Site Multi-Task (CSMT) learning method to jointly predict user behavior in multiple social media sites. CSMT mainly derives from the hierarchical attention network and multi-task learning. Using this method, the UGC in each social media site can obtain fine-grained representations in terms of words, topics, posts, hashtags, and time slices as well as the relevances among them, and prediction tasks in different social media sites can be jointly implemented and complement each other. By utilizing two cross-site datasets sampled from Weibo, Douban, Facebook, and Twitter, we validate our method’s superiority on several classification metrics compared with existing related methods.},
journal = {ACM Trans. Inf. Syst.},
month = jan,
articleno = {79},
numpages = {28},
keywords = {hierarchical attention network, multi-task, behavioral analytics, user-generated contents, Social media}
}

@article{10.1145/3415184,
author = {Dubois, Patrick Marcel Joseph and Maftouni, Mahya and Chilana, Parmit K. and McGrenere, Joanna and Bunt, Andrea},
title = {Gender Differences in Graphic Design Q&amp;As: How Community and Site Characteristics Contribute to Gender Gaps in Answering Questions},
year = {2020},
issue_date = {October 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {4},
number = {CSCW2},
url = {https://doi.org/10.1145/3415184},
doi = {10.1145/3415184},
abstract = {Question and answer (Q&amp;A) sites can capture a range of user perspectives on using complex, feature-rich software. Little is known, however, on who is contributing to the sites. We look at contribution diversity from the perspective of gender in a domain with near gender parity: graphic design. Through content analysis of 330 answers from two popular Q&amp;A sites and semi-structured interviews with 24 graphic designers, we examine who is contributing, what content, how the community shows appreciation towards their answers, and perceived motivations and barriers to participation. We find that despite gender balance in the field, women contribute far less frequently than men. We also see gender differences in contribution styles and user appreciation. Our interviews shed further light on how Q&amp;A community cultures might be impacting men and women differently and how design choices made by the sites? developers might be exacerbating these differences. We suggest implications for design for improving gender inclusivity.},
journal = {Proc. ACM Hum.-Comput. Interact.},
month = oct,
articleno = {113},
numpages = {26},
keywords = {women, stack exchange, quora, q&amp;a sites, gender}
}

@article{10.1145/3359758,
author = {Tausczik, Yla and Huang, Xiaoyun},
title = {The Impact of Group Size on the Discovery of Hidden Profiles in Online Discussion Groups},
year = {2019},
issue_date = {September 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {2},
number = {3},
url = {https://doi.org/10.1145/3359758},
doi = {10.1145/3359758},
abstract = {Online discussions help individuals to gather knowledge and make important decisions in diverse areas from health and finance to computing and data science. Online discussion groups exhibit unique group dynamics not found in traditional small groups, such as staggered participation and asynchronous communication, and the effects of these features on knowledge sharing is not well understood. In this article, we focus on one such aspect: wide variation in group size. Using a controlled experiment with a hidden profile task, we evaluate online discussion groups’ capacity to share distributed knowledge when group size ranges from 4 to 32 participants. We found that individuals in medium-sized discussions performed the best, and we suggest that this represents a tradeoff in which larger groups tend to share more facts, but have more difficulty than smaller groups at resolving misunderstandings.},
journal = {Trans. Soc. Comput.},
month = nov,
articleno = {10},
numpages = {25},
keywords = {online forums, knowledge sharing, collective intelligence, collective information processing, Hidden profile}
}

@article{10.1145/3579458,
author = {Liu, Jingjing and Wang, Xun and Tolmie, Peter and Wulf, Volker},
title = {Articulation Work and the Management of Intersubjectivity Disjunctures in Offshored Production},
year = {2023},
issue_date = {April 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {CSCW1},
url = {https://doi.org/10.1145/3579458},
doi = {10.1145/3579458},
abstract = {Since the move away from integrated value chains in production in the 1980s, the outsourcing and offshoring of various aspects of manufacturing has become commonplace. This has led to global production and marketing relationships between enterprises in numerous different countries, a prominent axis being between Europe and Asia. This paper reports on a study of one such offshoring relationship between a German SME (Small and Medium Enterprise) and their Chinese subsidiary, the trials and tribulations of the collaboration between them, and the implications of this for our understanding of how articulation work might be best supported in the context of global production. We also look at how a separate entity, which we term an 'articulation hub', was established to support articulation between the sites. While a number of studies have looked at articulation work in Global Software Development (GSD), there are very few ethnographic studies of offshored manufacturing, despite the unique challenges it presents. We find here that issues arise not only because of differences in technology environments, infrastructures, and cultural expectations, but also because of differing development strategies and business philosophies. The core problem confronting articulation work in offshored production is the difficulty of arriving at a mutually-grounded intersubjectivity, where shared assumptions about working practices and their relative importance and value can be trusted to apply, leading to what we term 'intersubjectivity disjunctures'. These disjunctures have a number of important implications for accomplishing articulation work. This paper offers a number of contributions to CSCW. First, it adds to a very thin corpus of CSCW-relevant ethnographic studies of global manufacturing. Second, it finds that articulation work in offshored production is less closely-coupled than it is in other distributed settings, making conventionally promoted solutions, such as knowledge sharing and relationship building, less relevant. Third, it reveals how differences in moral reasoning can result in different sites assigning very different priorities to articulation work. Finally, while articulation work is typically seen to be invisible work in CSCW, we argue that, in this context, there are a number of ways in which using a separate hub to actively render articulation work visible may be the best solution.},
journal = {Proc. ACM Hum.-Comput. Interact.},
month = apr,
articleno = {25},
numpages = {34},
keywords = {German-Chinese collaboration, HQ-subsidiary interaction, SME, articulation work, ethnography, industrial production, intersubjectivity, offshoring}
}

@article{10.1145/3597204,
author = {Liu, Xuanzhe and Gu, Diandian and Chen, Zhenpeng and Wen, Jinfeng and Zhang, Zili and Ma, Yun and Wang, Haoyu and Jin, Xin},
title = {Rise of Distributed Deep Learning Training in the Big Model Era: From a Software Engineering Perspective},
year = {2023},
issue_date = {November 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {32},
number = {6},
issn = {1049-331X},
url = {https://doi.org/10.1145/3597204},
doi = {10.1145/3597204},
abstract = {Deep learning (DL) has become a key component of modern software. In the “big model” era, the rich features of DL-based software (i.e., DL software) substantially rely on powerful DL models, e.g., BERT, GPT-3, and the recently emerging GPT-4, which are trained on the powerful cloud with large datasets. Hence, training effective DL models has become a vital stage in the whole software lifecycle. When training deep learning models, especially those big models, developers need to parallelize and distribute the computation and memory resources amongst multiple devices (e.g., a cluster of GPUs) in the training process, which is known as distributed deep learning training, or distributed training for short. However, the unique challenges that developers encounter in distributed training process have not been studied in the software engineering community. Given the increasingly heavy dependence of current DL-based software on distributed training, this paper aims to fill in the knowledge gap and presents the first comprehensive study on developers’ issues in distributed training. To this end, we focus on popular DL frameworks that support distributed training (including TensorFlow, PyTorch, Keras, and Horovod) and analyze 1,131 real-world developers’ issues about using these frameworks reported on Stack Overflow and GitHub. We construct a fine-grained taxonomy consisting of 30 categories regarding the fault symptoms and summarize common fix patterns for different symptoms. We find that: (1) many distributed-specific faults and non-distributed-specific faults inherently share the same fault symptoms, making it challenging to debug; (2) most of the fault symptoms have frequent fix patterns; (3) about half of the faults are related to system-level configurations. Based on the results, we suggest actionable implications on research avenues that can potentially facilitate the distributed training to develop DL-based software, such as focusing on the frequent and common fix patterns when designing testing or debugging tools, developing efficient testing and debugging techniques for communication configuration along with the synthesis of network configuration analysis, designing new multi-device checkpoint-and-replay techniques to help reproduction, and designing serverless APIs for cloud platforms.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = sep,
articleno = {156},
numpages = {26},
keywords = {software engineering, distributed training, Empirical study}
}

@article{10.1145/3434168,
author = {Chen, Yan and Lasecki, Walter S. and Dong, Tao},
title = {Towards Supporting Programming Education at Scale via Live Streaming},
year = {2021},
issue_date = {December 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {4},
number = {CSCW3},
url = {https://doi.org/10.1145/3434168},
doi = {10.1145/3434168},
abstract = {Live streaming, which allows streamers to broadcast their work to live viewers, is an emerging practice for teaching and learning computer programming. Participation in live streaming is growing rapidly, despite several apparent challenges, such as a general lack of training in pedagogy among streamers and scarce signals about a stream's characteristics (e.g., difficulty, style, and usefulness) to help viewers decide what to watch. To understand why people choose to participate in live streaming for teaching or learning programming, and how they cope with both apparent and non-obvious challenges, we interviewed 14 streamers and viewers about their experience with live streaming programming. Among other results, we found that the casual and impromptu nature of live streaming makes it easier to prepare than pre-recorded videos, and viewers have the opportunity to shape the content and learning experience via real-time communication with both the streamer and each other. Nonetheless, we identified several challenges that limit the potential of live streaming as a learning medium. For example, streamers voiced privacy and harassment concerns, and existing streaming platforms do not adequately support viewer-streamer interactions, adaptive learning, and discovery and selection of streaming content. Based on these findings, we suggest specialized tools to facilitate knowledge sharing among people teaching and learning computer programming online, and we offer design recommendations that promote a healthy, safe, and engaging learning environment.},
journal = {Proc. ACM Hum.-Comput. Interact.},
month = jan,
articleno = {259},
numpages = {19},
keywords = {programming education, live streaming, live coding, informal learning}
}

@article{10.1145/3402521,
author = {Chen, Xiancong and Li, Lin and Pan, Weike and Ming, Zhong},
title = {A Survey on Heterogeneous One-class Collaborative Filtering},
year = {2020},
issue_date = {October 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {38},
number = {4},
issn = {1046-8188},
url = {https://doi.org/10.1145/3402521},
doi = {10.1145/3402521},
abstract = {Recommender systems play an important role in providing personalized services for users in the context of information overload. Generally, users’ feedback toward items often contain the most significant information reflecting their preferences, which enables accurate personalized recommendation. In real applications, users’ feedback are usually heterogeneous (rather than homogeneous) such as purchases and examinations in e-commerce, which reflects users’ preferences in different degrees. Effective modeling of such heterogeneous one-class feedback is challenging compared with that of homogeneous feedback of ratings. As a response, heterogeneous one-class collaborative filtering (HOCCF) is proposed, which often converts the heterogeneous feedback into two parts (i.e., target feedback and auxiliary feedback), aiming to care more about the target feedback (e.g., purchases) with the assistance of the auxiliary feedback (e.g., examinations). In this survey, we provide an overview of the representative HOCCF methods from the perspective of factorization-based methods, transfer learning-based methods, and deep learning-based methods. First, we review the factorization-based methods according to different strategies. Second, we describe the transfer learning-based methods with different knowledge sharing manners. Third, we discuss the deep learning-based methods according to the neural architectures. Moreover, we include some important example applications, describe the empirical studies, and discuss some promising future directions.},
journal = {ACM Trans. Inf. Syst.},
month = aug,
articleno = {35},
numpages = {54},
keywords = {transfer learning, matrix factorization, deep learning, Heterogeneous one-class collaborative filtering}
}

@article{10.1145/3565799,
author = {Wu, Di and Jing, Xiao-Yuan and Zhang, Hongyu and Feng, Yang and Chen, Haowen and Zhou, Yuming and Xu, Baowen},
title = {Retrieving API Knowledge from Tutorials and Stack Overflow Based on Natural Language Queries},
year = {2023},
issue_date = {September 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {32},
number = {5},
issn = {1049-331X},
url = {https://doi.org/10.1145/3565799},
doi = {10.1145/3565799},
abstract = {When encountering unfamiliar APIs, developers tend to seek help from API tutorials and Stack Overflow (SO). API tutorials help developers understand the API knowledge in a general context, while SO often explains the API knowledge in a specific programming task. Thus, tutorials and SO posts together can provide more API knowledge. However, it is non-trivial to retrieve API knowledge from both API tutorials and SO posts based on natural language queries. Two major problems are irrelevant API knowledge in two different resources and the lexical gap between the queries and documents. In this article, we regard a fragment in tutorials and a Question and Answering (Q&amp;A) pair in SO as a knowledge item (KI). We generate ⟨ API, FRA⟩ pairs (FRA stands for fragment) from tutorial fragments and APIs and build ⟨ API, QA⟩ pairs based on heuristic rules of SO posts. We fuse ⟨ API, FRA⟩ pairs and ⟨ API, QA⟩ pairs to generate API knowledge (AK for short) datasets, where each data item is an ⟨ API, KI⟩ pair. We propose a novel approach, called PLAN, to automatically retrieve API knowledge from both API tutorials and SO posts based on natural language queries. PLAN contains three main stages: (1) API knowledge modeling, (2) query mapping, and (3) API knowledge retrieving. It first utilizes a deep-transfer-metric-learning-based relevance identification (DTML) model to effectively find relevant ⟨ API, KI⟩ pairs containing two different knowledge items (⟨ API, QA⟩ pairs and ⟨ API, FRA⟩ pairs) simultaneously. Then, PLAN generates several potential APIs as a way to reduce the lexical gap between the query and ⟨ API, KI⟩ pairs. According to potential APIs, we can select relevant ⟨ API, KI⟩ pairs to generate potential results. Finally, PLAN returns a list of ranked ⟨ API, KI⟩ pairs that are related to the query. We evaluate the effectiveness of PLAN with 270 queries on Java and Android AK datasets containing 10,072 ⟨ API, KI⟩ pairs. Our experimental results show that PLAN is effective and outperforms the state-of-the-art approaches. Our user study further confirms the effectiveness of PLAN in locating useful API knowledge.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = jul,
articleno = {109},
numpages = {36},
keywords = {natural language queries, deep transfer metric learning, Stack Overflow, API tutorial}
}

@article{10.1145/3555124,
author = {Hadi Mogavi, Reza and Zhang, Yuanhao and Haq, Ehsan-Ul and Wu, Yongjin and Hui, Pan and Ma, Xiaojuan},
title = {What Do Users Think of Promotional Gamification Schemes? A Qualitative Case Study in a Question Answering Website},
year = {2022},
issue_date = {November 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {6},
number = {CSCW2},
url = {https://doi.org/10.1145/3555124},
doi = {10.1145/3555124},
abstract = {In recent years, studies on the user experience have emerged as an indispensable part of any gamification research. The study of user experience enables gamification designers and practitioners to design or adapt their gamification schemes in a more knowledgeable and efficacious manner. However, one popular gamification scheme that has largely remained under-researched in terms of user experience is promotional gamification, which refers to an optional and time-limited gamification program that usually mounts an already gamified platform to increase user incentive and engagement for a short span of time (e.g., during the holiday season). The current study undertakes the first steps necessary to explore users' experiences of working with a promotional gamification scheme in a large-scale online community. To this end, we conduct an extensive qualitative case study of users' experiences with a promotional gamification scheme on the Community Question Answering Website (CQA) of Stack Exchange, called Winter Bash (WB). Notably, the purpose of WB is to operate as a makeshift solution that prevents the decline in user contributions during the holiday season. However, like many other gamification schemes, WB is not devoid of issues, and our research helps identify those issues without overlooking the WB's strengths. Our study denotes not only the first (empirical) typology of users' affective responses to promotional gamification schemes but also the first classification of (de)motivational factors involved in user engagement. At its core, this study comprises two salient parts: (1) a content analysis of user-generated data regarding WB (from the past eight years), and (2) a series of semi-structured interviews with 17 international users who are familiar with WB. We triangulate our findings from (1) and (2) by performing a similar content analysis for two other promotional gamification schemes, namely "Answerathon" (from Travel Meta) and "Discussion Tournament" (from Reddit). Based on the findings of this study, we present certain guidelines for gamification designers and practitioners, enabling them to deploy or adapt their promotional gamification schemes in a more knowledgeable and effective manner. Finally, our work is concluded by highlighting a few novel research opportunities for researchers invested in the fields of Human-Computer Interaction (HCI) and Computer-Supported Cooperative Work (CSCW).},
journal = {Proc. ACM Hum.-Comput. Interact.},
month = nov,
articleno = {399},
numpages = {34},
keywords = {qualitative research, promotional gamification, motivators, gamification design, gamification, engagement, demotivators, community question answering website (CQA), affective response}
}

@article{10.1145/3274300,
author = {Chan, Joel and Chang, Joseph Chee and Hope, Tom and Shahaf, Dafna and Kittur, Aniket},
title = {SOLVENT: A Mixed Initiative System for Finding Analogies between Research Papers},
year = {2018},
issue_date = {November 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {2},
number = {CSCW},
url = {https://doi.org/10.1145/3274300},
doi = {10.1145/3274300},
abstract = {Scientific discoveries are often driven by finding analogies in distant domains, but the growing number of papers makes it difficult to find relevant ideas in a single discipline, let alone distant analogies in other domains. To provide computational support for finding analogies across domains, we introduce SOLVENT, a mixed-initiative system where humans annotate aspects of research papers that denote their background (the high-level problems being addressed), purpose (the specific problems being addressed), mechanism (how they achieved their purpose), and findings (what they learned/achieved), and a computational model constructs a semantic representation from these annotations that can be used to find analogies among the research papers. We demonstrate that this system finds more analogies than baseline information-retrieval approaches; that annotators and annotations can generalize beyond domain; and that the resulting analogies found are useful to experts. These results demonstrate a novel path towards computationally supported knowledge sharing in research communities.},
journal = {Proc. ACM Hum.-Comput. Interact.},
month = nov,
articleno = {31},
numpages = {21},
keywords = {scientific discovery, crowdsourcing, computer-supported cooperative work, analogy}
}

@article{10.1145/3468270,
author = {Zhang, Xiao and Liu, Meng and Yin, Jianhua and Ren, Zhaochun and Nie, Liqiang},
title = {Question Tagging via Graph-guided Ranking},
year = {2021},
issue_date = {January 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {40},
number = {1},
issn = {1046-8188},
url = {https://doi.org/10.1145/3468270},
doi = {10.1145/3468270},
abstract = {With the increasing prevalence of portable devices and the popularity of community Question Answering (cQA) sites, users can seamlessly post and answer many questions. To effectively organize the information for precise recommendation and easy searching, these platforms require users to select topics for their raised questions. However, due to the limited experience, certain users fail to select appropriate topics for their questions. Thereby, automatic question tagging becomes an urgent and vital problem for the cQA sites, yet it is non-trivial due to the following challenges. On the one hand, vast and meaningful topics are available yet not utilized in the cQA sites; how to model and tag them to relevant questions is a highly challenging problem. On the other hand, related topics in the cQA sites may be organized into a directed acyclic graph. In light of this, how to exploit relations among topics to enhance their representations is critical. To settle these challenges, we devise a graph-guided topic ranking model to tag questions in the cQA sites appropriately. In particular, we first design a topic information fusion module to learn the topic representation by jointly considering the name and description of the topic. Afterwards, regarding the special structure of topics, we propose an information propagation module to enhance the topic representation. As the comprehension of questions plays a vital role in question tagging, we design a multi-level context-modeling-based question encoder to obtain the enhanced question representation. Moreover, we introduce an interaction module to extract topic-aware question information and capture the interactive information between questions and topics. Finally, we utilize the interactive information to estimate the ranking scores for topics. Extensive experiments on three Chinese cQA datasets have demonstrated that our proposed model outperforms several state-of-the-art competitors.},
journal = {ACM Trans. Inf. Syst.},
month = sep,
articleno = {12},
numpages = {23},
keywords = {question tagging, community question answering, Graph-guided topic ranking}
}

@article{10.1145/3371388,
author = {Li, Hongfei and Shankar, Ramesh and Stallaert, Jan},
title = {Invested or Indebted: Ex-ante and Ex-post Reciprocity in Online Knowledge Sharing Communities},
year = {2020},
issue_date = {March 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {11},
number = {1},
issn = {2158-656X},
url = {https://doi.org/10.1145/3371388},
doi = {10.1145/3371388},
abstract = {Online communities that curate knowledge critically depend on high-quality contributions from anonymous expert users. Understanding users’ motivation to contribute knowledge helps practitioners design such websites for optimal user contribution and user benefits. Researchers have studied reciprocity as a motivation for users to share knowledge online. In this study, we focus on two different types of reciprocity as drivers of online contribution: ex-post and ex-ante reciprocity. Ex-post reciprocity refers to users who received help from others in the past and pay back by helping others at present. Using a quasi-experiment performed via the instrumental variable method as the identification strategy, we test whether users who received more answers last week answer more questions in the current week on StackOverflow.com. We find a significant positive relationship between ex-post reciprocity and knowledge contribution, and such a reciprocal motivation diminishes with time. Ex-ante reciprocity refers to people helping others in expectation of future help from others. Using data from StackOverflow.com, we take advantage of a natural experiment with a difference-in-differences analysis and find evidence supporting the existence of ex-ante reciprocity. This study offers a new taxonomy for reciprocity and new insights on how reciprocity drives online knowledge sharing.},
journal = {ACM Trans. Manage. Inf. Syst.},
month = jan,
articleno = {1},
numpages = {26},
keywords = {reciprocity, knowledge sharing, Q&amp;A website, ex-ante, Ex-post}
}

@article{10.1145/3134675,
author = {Dittus, Martin and Capra, Licia},
title = {Private Peer Feedback as Engagement Driver in Humanitarian Mapping},
year = {2017},
issue_date = {November 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {1},
number = {CSCW},
url = {https://doi.org/10.1145/3134675},
doi = {10.1145/3134675},
abstract = {Prior research suggests that public negative feedback on social knowledge sharing platforms can be powerfully demotivating to newcomers, particularly when it involves peer feedback mechanisms such as ratings and commenting systems. What is the impact on newcomer retention when feedback is private, and from a single peer reviewer? We study these effects using the example of the Humanitarian OpenStreetMap Team, a Wikipedia-style social mapping platform where the review process is closer to a teacher-learner model rather than a public peer review. We observe peer feedback for early contributions by 1,300 newcomers, and assess the impact of different classes of feedback, including performance feedback, corrective feedback, and verbal rewards. We find that verbal rewards and immediate feedback can have a powerful effect on newcomer retention. In order to better support such positive engagement effects, we recommend that system designers conceptually distinguish between mechanisms for quality control and for learner feedback.},
journal = {Proc. ACM Hum.-Comput. Interact.},
month = dec,
articleno = {40},
numpages = {18},
keywords = {rewards, retention, peer feedback, motivations, engagement, crowdsourcing, crowdmapping}
}

@article{10.5555/2392896.2392900,
author = {Davis, Matt},
title = {Sacrifice a canary upon the stack of the gods: on canaries, coal mines and stack sanity},
year = {2012},
issue_date = {October 2012},
publisher = {Belltown Media},
address = {Houston, TX},
volume = {2012},
number = {222},
issn = {1075-3583},
abstract = {Stack canaries provide a simple means of detecting stack corruption and can prevent unintended stack overflow-based execution.},
journal = {Linux J.},
month = oct,
articleno = {4}
}

@article{10.1145/3470006,
author = {Nikanjam, Amin and Braiek, Houssem Ben and Morovati, Mohammad Mehdi and Khomh, Foutse},
title = {Automatic Fault Detection for Deep Learning Programs Using Graph Transformations},
year = {2021},
issue_date = {January 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {31},
number = {1},
issn = {1049-331X},
url = {https://doi.org/10.1145/3470006},
doi = {10.1145/3470006},
abstract = {Nowadays, we are witnessing an increasing demand in both corporates and academia for exploiting Deep Learning (DL) to solve complex real-world problems. A DL program encodes the network structure of a desirable DL model and the process by which the model learns from the training dataset. Like any software, a DL program can be faulty, which implies substantial challenges of software quality assurance, especially in safety-critical domains. It is therefore crucial to equip DL development teams with efficient fault detection techniques and tools. In this article, we propose NeuraLint, a model-based fault detection approach for DL programs, using meta-modeling and graph transformations. First, we design a meta-model for DL programs that includes their base skeleton and fundamental properties. Then, we construct a graph-based verification process that covers 23 rules defined on top of the meta-model and implemented as graph transformations to detect faults and design inefficiencies in the generated models (i.e., instances of the meta-model). First, the proposed approach is evaluated by finding faults and design inefficiencies in 28 synthesized examples built from common problems reported in the literature. Then NeuraLint successfully finds 64 faults and design inefficiencies in 34 real-world DL programs extracted from Stack Overflow posts and GitHub repositories. The results show that NeuraLint effectively detects faults and design issues in both synthesized and real-world examples with a recall of 70.5\% and a precision of 100\%. Although the proposed meta-model is designed for feedforward neural networks, it can be extended to support other neural network architectures such as recurrent neural networks. Researchers can also expand our set of verification rules to cover more types of issues in DL programs.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = sep,
articleno = {14},
numpages = {27},
keywords = {fault detection, deep learning, model-based verification, Graph transformations}
}

@article{10.1145/3295460,
author = {Romero, Daniel M. and Uzzi, Brian and Kleinberg, Jon},
title = {Social Networks under Stress: Specialized Team Roles and Their Communication Structure},
year = {2019},
issue_date = {February 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {13},
number = {1},
issn = {1559-1131},
url = {https://doi.org/10.1145/3295460},
doi = {10.1145/3295460},
abstract = {Social network research has begun to take advantage of fine-grained communications regarding coordination, decision-making, and knowledge sharing. These studies, however, have not generally analyzed how external events are associated with a social network’s structure and communicative properties. Here, we study how external events are associated with a network’s change in structure and communications. Analyzing a complete dataset of millions of instant messages among the decision-makers with different roles in a large hedge fund and their network of outside contacts, we investigate the link between price shocks, network structure, and change in the affect and cognition of decision-makers embedded in the network. We also analyze the communication dynamics among specialized teams in the organization. When price shocks occur the communication network tends not to display structural changes associated with adaptiveness such as the activation of weak ties to obtain novel information. Rather, the network “turtles up.” It displays a propensity for higher clustering, strong tie interaction, and an intensification of insider vs. outsider and within-role vs. between-role communication. Further, we find changes in network structure predict shifts in cognitive and affective processes, execution of new transactions, and local optimality of transactions better than prices, revealing the important predictive relationship between network structure and collective behavior within a social network.},
journal = {ACM Trans. Web},
month = feb,
articleno = {6},
numpages = {24},
keywords = {temporal dynamics, organizations, collective behavior, Social networks}
}

@article{10.1145/3211871,
author = {Ochieng, Peter and Kyanda, Swaib},
title = {Large-Scale Ontology Matching: State-of-the-Art Analysis},
year = {2018},
issue_date = {July 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {51},
number = {4},
issn = {0360-0300},
url = {https://doi.org/10.1145/3211871},
doi = {10.1145/3211871},
abstract = {Ontologies have become a popular means of knowledge sharing and reuse. This has motivated the development of large-sized independent ontologies within the same or different domains with some overlapping information among them. To integrate such large ontologies, automatic matchers become an inevitable solution. However, the process of matching large ontologies has high space and time complexities. Therefore, for a tool to efficiently and accurately match these large ontologies within the limited computing resources, it must have techniques that can significantly reduce the high space and time complexities associated with the ontology matching process. This article provides a review of the state-of-the-art techniques being applied by ontology matching tools to achieve scalability and produce high-quality mappings when matching large ontologies. In addition, we provide a direct comparison of the techniques to gauge their effectiveness in achieving scalability. A review of the state-of-the-art ontology matching tools that employ each strategy is also provided. We also evaluate the state-of-the-art tools to gauge the progress they have made over the years in improving alignment’s quality and reduction of execution time when matching large ontologies.},
journal = {ACM Comput. Surv.},
month = jul,
articleno = {75},
numpages = {35},
keywords = {scalability, repair, ontology mapping, mapping repair, Survey}
}

@article{10.1145/3555179,
author = {Zhang, Yang and Zong, Ruohan and Kou, Ziyi and Shang, Lanyu and Wang, Dong},
title = {CrowdNAS: A Crowd-guided Neural Architecture Searching Approach to Disaster Damage Assessment},
year = {2022},
issue_date = {November 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {6},
number = {CSCW2},
url = {https://doi.org/10.1145/3555179},
doi = {10.1145/3555179},
abstract = {Disaster damage assessment (DDA) has emerged as an important application in disaster response and management, which aims to assess the damage severity of an affected area by leveraging AI (e.g., deep learning) techniques to examine the imagery data posted on social media during a disaster event. In this paper, we focus on a crowd-guided neural architecture searching (NAS) problem in DDA applications. Our goal is to leverage human intelligence from crowdsourcing systems to guide the discovery of the optimal neural network architecture in the design space to achieve the desirable damage assessment performance. Our work is motivated by the limitation that the deep neural network architectures in current DDA solutions are mainly designed by AI experts, which is known to be both time-consuming and error-prone. Two critical technical challenges exist in solving our problem: i) it is challenging to design a manageable NAS space for crowd-based solutions; ii) it is non-trivial to transfer the imperfect crowd knowledge to effective decisions in identifying the optimal neural network architecture of a DDA application. To address the above challenges, we develop CrowdNAS, a crowd-guided NAS framework that develops novel techniques inspired by AI, crowdsourcing, and estimation theory to address the NAS problem. The evaluation results from two real-world DDA applications show that CrowdNAS consistently outperforms the state-of-the-art AI-only, crowd-AI, and NAS baselines by achieving the highest classification accuracy in the damage assessment while maintaining a low computational cost under various evaluation scenarios.},
journal = {Proc. ACM Hum.-Comput. Interact.},
month = nov,
articleno = {288},
numpages = {29},
keywords = {neural architecture searching, disaster damage assessment, crowdsourcing}
}

@article{10.1145/3337799,
author = {Thukral, Deepak and Pandey, Adesh and Gupta, Rishabh and Goyal, Vikram and Chakraborty, Tanmoy},
title = {DiffQue: Estimating Relative Difficulty of Questions in Community Question Answering Services},
year = {2019},
issue_date = {July 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {10},
number = {4},
issn = {2157-6904},
url = {https://doi.org/10.1145/3337799},
doi = {10.1145/3337799},
abstract = {Automatic estimation of relative difficulty of a pair of questions is an important and challenging problem in community question answering (CQA) services. There are limited studies that addressed this problem. Past studies mostly leveraged expertise of users answering the questions and barely considered other properties of CQA services such as metadata of users and posts, temporal information, and textual content. In this article, we propose DiffQue, a novel system that maps this problem to a network-aided edge directionality prediction problem. DiffQue&nbsp;starts by constructing a novel network structure that captures different notions of difficulties among a pair of questions. It then measures the relative difficulty of two questions by predicting the direction of a (virtual) edge connecting these two questions in the network. It leverages features extracted from the network structure, metadata of users/posts, and textual description of questions and answers. Experiments on datasets obtained from two CQA sites (further divided into four datasets) with human annotated ground-truth show that DiffQue&nbsp;outperforms four state-of-the-art methods by a significant margin (28.77\% higher F1 score and 28.72\% higher AUC than the best baseline). As opposed to the other baselines, (i) DiffQue&nbsp;appropriately responds to the training noise, (ii) DiffQue&nbsp;is capable of adapting multiple domains (CQA datasets), and (iii) DiffQue&nbsp;can efficiently handle the “cold start” problem that may arise due to the lack of information for newly posted questions or newly arrived users.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = jul,
articleno = {42},
numpages = {27},
keywords = {time-evolving networks, edge directionality prediction, difficulty of questions, Community question answering}
}

@article{10.14778/3229863.3236244,
author = {Jarovsky, Ariel and Milo, Tova and Novgorodov, Slava and Tan, Wang-Chiew},
title = {GOLDRUSH: rule sharing system for fraud detection},
year = {2018},
issue_date = {August 2018},
publisher = {VLDB Endowment},
volume = {11},
number = {12},
issn = {2150-8097},
url = {https://doi.org/10.14778/3229863.3236244},
doi = {10.14778/3229863.3236244},
abstract = {Fraud detection rules, written by domain experts, are often employed by financial companies to enhance their machine learning-based mechanisms for accurate detection of fraudulent transactions. Accurate rule writing is a challenging task where domain experts spend significant effort and time. A key observation is that much of this difficulty originates from the fact that experts typically work as "lone rangers" or in isolated groups to define the rules, or work on detecting frauds in one context in isolation from frauds that occur in another context. However, in practice there is a lot of commonality in what different experts are trying to achieve.In this demo, we present the GOLDRUSH system, which facilitates knowledge sharing via effective adaptation of fraud detection rules from one context to another. GOLDRUSH abstracts the possible semantic interpretations of each of the conditions in the rules in one context and adapts them to the target context. Efficient algorithms are used to identify the most effective rule adaptations w.r.t a given cost-benefit metric. We showcase GOLDRUSH through a reenactment of a real-life fraud detection event. Our demonstration will engage the VLDB'18 audience, allowing them to play the role of experts collaborating in the fight against financial frauds.},
journal = {Proc. VLDB Endow.},
month = aug,
pages = {1998–2001},
numpages = {4}
}

@article{10.1145/3524106,
author = {Husen, Arif and Chaudary, Muhammad Hasanain and Ahmad, Farooq},
title = {A Survey on Requirements of Future Intelligent Networks: Solutions and Future Research Directions},
year = {2022},
issue_date = {April 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {55},
number = {4},
issn = {0360-0300},
url = {https://doi.org/10.1145/3524106},
doi = {10.1145/3524106},
abstract = {The context of this study examines the requirements of Future Intelligent Networks (FIN), solutions, and current research directions through a survey technique. The background of this study is hinged on the applications of Machine Learning (ML) in the networking field. Through careful analysis of literature and real-world reports, we noted that ML has significantly expedited decision-making processes, enhanced intelligent automation, and helped resolve complex problems economically in different fields of life. Various researchers have also envisioned future networks incorporating intelligent functions and operations with ML. Several efforts have been made to automate individual functions and operations in the networking domain; however, most of the existing ML models proposed in the literature lack several vital requirements. Hence, this study aims to present a comprehensive summary of the requirements of FIN and propose a taxonomy of different network functionalities that needs to be equipped with ML techniques. The core objectives of this study are to provide a taxonomy of requirements envisioned for end-to-end FIN, relevant ML techniques, and their analysis to find research gaps, open issues, and future research directions. The real benefit of ML applications in any domain can only be ensured if intelligent capabilities cover all of its components. We observed that future generations of networks are heterogeneous, multi-vendor, and multidimensional, and ML can provide optimal results only if intelligent capabilities are used on a holistic scale. Realizing intelligence on a holistic scale is only possible if the ML algorithms can solve heterogeneous problems in a multi-vendor and multidimensional environment. ML models must be reliable and efficient, support, and possess the capability to learn and share the knowledge across the network layers and administrative domains to solve issues. First, this study ascertains the requirements of the FIN and proposes their taxonomy through reviews on envisioned ideas by various researchers and articles gathered from reputed conferences and standard developing organizations using keyword queries. Second, we have reviewed existing studies on ML applications focusing on coverage, heterogeneity, distributed architecture, and cross-domain knowledge learning and sharing. Our study observed that in the past, ML applications were focused mainly on an individual/isolated level only, and aspects of global and deep holistic learning with cross-layer/cross-domain knowledge sharing with agile ML operations are not explored at large. We recommend that the issues mentioned previously be addressed with improved ML architecture and agile operations and propose an ML pipeline based architecture for FIN. The significant contribution of this study is the impetus for researchers to seek ML models suitable for a modular, distributed, multi-domain, and multi-layer environment and provide decision making on a global or holistic rather than an individual function level.},
journal = {ACM Comput. Surv.},
month = nov,
articleno = {73},
numpages = {61},
keywords = {deep holistic learning, feature sharing, cross-layer learning, knowledge sharing, cross-administrative domain learning, global learning, Future intelligent networks}
}

@article{10.1145/2063231.2063236,
author = {Convertino, Gregorio and Mentis, Helena M. and Slavkovic, Aleksandra and Rosson, Mary Beth and Carroll, John M.},
title = {Supporting common ground and awareness in emergency management planning: A design research project},
year = {2011},
issue_date = {December 2011},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {18},
number = {4},
issn = {1073-0516},
url = {https://doi.org/10.1145/2063231.2063236},
doi = {10.1145/2063231.2063236},
abstract = {We present a design research project on knowledge sharing and activity awareness in distributed emergency management planning. In three experiments we studied groups using three different prototypes, respectively: a paper-prototype in a collocated work setting, a first software prototype in a distributed setting, and a second, enhanced software prototype in a distributed setting. In this series of studies we tried to better understand the processes of knowledge sharing and activity awareness in complex cooperative work by developing and investigating new tools that can support these processes. We explicate the design rationale behind each prototype and report the results of each experiment investigating it. We discuss how the results from each prototyping phase brought us closer to defining properties of a system that facilitate the sharing and awareness of both content and process knowledge. Our designs enhanced aspects of distributed group performance, in some respects beyond that of comparable face-to-face groups.},
journal = {ACM Trans. Comput.-Hum. Interact.},
month = dec,
articleno = {22},
numpages = {34},
keywords = {emergency management planning, design, common ground, colocated and distributed geo-collaboration, CSCW, Activity awareness}
}

@article{10.1145/3320489,
author = {Dehghan, Mahdi and Abin, Ahmad Ali},
title = {Translations Diversification for Expert Finding: A Novel Clustering-based Approach},
year = {2019},
issue_date = {June 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {13},
number = {3},
issn = {1556-4681},
url = {https://doi.org/10.1145/3320489},
doi = {10.1145/3320489},
abstract = {Expert finding is the task of retrieving and ranking knowledgeable people in the subject of user’s query. It is a well-studied problem that has attracted the attention of many researchers. The most important challenge in expert finding is to determine the similarity between query words and documents authored by candidate experts. One of the most important challenges in Information Retrieval (IR) community is the issue of vocabulary gap between queries and documents. In this study, a translation model based on words clustering in two query and co-occurrence spaces is proposed to overcome this problem. First, the words that are semantically close, are clustered in a query space and then each cluster in this space are clustered again in a co-occurrence space. Representatives of each cluster in the co-occurrence space are considered as a diverse subset of the parent cluster. By this method, the query translations are expected to be diversified in the query space. Next, a probabilistic model, that is based on the belonging degree of word to cluster and similarity of cluster to query in the query space, is used to consider the problem of vocabulary gap. Finally, the corresponding translations to each query are used in conjunction with a combination model for expert finding. Experiments on Stack Overflow dataset show the effectiveness of the proposed method for expert finding.},
journal = {ACM Trans. Knowl. Discov. Data},
month = may,
articleno = {32},
numpages = {20},
keywords = {translation model, translation diversification, data clustering, Stack Overflow, Expert finding}
}

@article{10.1145/3360578,
author = {Luan, Sifei and Yang, Di and Barnaby, Celeste and Sen, Koushik and Chandra, Satish},
title = {Aroma: code recommendation via structural code search},
year = {2019},
issue_date = {October 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {3},
number = {OOPSLA},
url = {https://doi.org/10.1145/3360578},
doi = {10.1145/3360578},
abstract = {Programmers often write code that has similarity to existing code written somewhere. A tool that could help programmers to search such similar code would be immensely useful. Such a tool could help programmers to extend partially written code snippets to completely implement necessary functionality, help to discover extensions to the partial code which are commonly included by other programmers, help to cross-check against similar code written by other programmers, or help to add extra code which would fix common mistakes and errors. We propose Aroma, a tool and technique for code recommendation via structural code search. Aroma indexes a huge code corpus including thousands of open-source projects, takes a partial code snippet as input, searches the corpus for method bodies containing the partial code snippet, and clusters and intersects the results of the search to recommend a small set of succinct code snippets which both contain the query snippet and appear as part of several methods in the corpus. We evaluated Aroma on 2000 randomly selected queries created from the corpus, as well as 64 queries derived from code snippets obtained from Stack Overflow, a popular website for discussing code. We implemented Aroma for 4 different languages, and developed an IDE plugin for Aroma. Furthermore, we conducted a study where we asked 12 programmers to complete programming tasks using Aroma, and collected their feedback. Our results indicate that Aroma is capable of retrieving and recommending relevant code snippets efficiently.},
journal = {Proc. ACM Program. Lang.},
month = oct,
articleno = {152},
numpages = {28},
keywords = {structural code search, feature-based code representation, code recommendation, clustering, clone detection}
}

@article{10.1145/3162050,
author = {Khodadadi, Ali and Hosseini, Seyed Abbas and Tavakoli, Erfan and Rabiee, Hamid R.},
title = {Continuous-Time User Modeling in Presence of Badges: A Probabilistic Approach},
year = {2018},
issue_date = {June 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {12},
number = {3},
issn = {1556-4681},
url = {https://doi.org/10.1145/3162050},
doi = {10.1145/3162050},
abstract = {User modeling plays an important role in delivering customized web services to the users and improving their engagement. However, most user models in the literature do not explicitly consider the temporal behavior of users. More recently, continuous-time user modeling has gained considerable attention and many user behavior models have been proposed based on temporal point processes. However, typical point process-based models often considered the impact of peer influence and content on the user participation and neglected other factors. Gamification elements are among those factors that are neglected, while they have a strong impact on user participation in online services. In this article, we propose interdependent multi-dimensional temporal point processes that capture the impact of badges on user participation besides the peer influence and content factors. We extend the proposed processes to model user actions over the community-based question and answering websites, and propose an inference algorithm based on Variational-Expectation Maximization that can efficiently learn the model parameters. Extensive experiments on both synthetic and real data gathered from Stack Overflow show that our inference algorithm learns the parameters efficiently and the proposed method can better predict the user behavior compared to the alternatives.},
journal = {ACM Trans. Knowl. Discov. Data},
month = mar,
articleno = {37},
numpages = {30},
keywords = {variational EM, user profiling, temporal point process, stack overflow, gamification, badge, User modeling}
}

@article{10.1145/3274307,
author = {Choi, Joohee and Tausczik, Yla},
title = {Will Too Many Editors Spoil The Tag? Conflicts and Alignment in Q&amp;A Categorization},
year = {2018},
issue_date = {November 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {2},
number = {CSCW},
url = {https://doi.org/10.1145/3274307},
doi = {10.1145/3274307},
abstract = {Q&amp;A websites compile useful knowledge through user-generated questions and responses. Many Q&amp;As use collaborative tagging systems to improve search and discovery while distributing the work of categorizing and organization throughout the community. Although early work on collaborative tagging questioned whether consistent categorization schemes could emerge from large groups with little to no coordination, empirical studies have found surprising coherence among users' tags. We build on this research by testing whether coherence emerges in tag usage on Q&amp;As, a more challenging context, focusing in particular on mismatches in the specificity of tags (basic level disagreement). We found that some users shifted toward more specific tag usage over time slightly increasing conflict, but that moderators were instrumental in helping to resolve some of this conflict. This study highlights the importance of learning and moderation in the development of coherence in collaborative tagging systems.},
journal = {Proc. ACM Hum.-Comput. Interact.},
month = nov,
articleno = {38},
numpages = {19},
keywords = {moderation, distributed cognition, collaborative tagging, categorization, Q&amp;As}
}

@article{10.1109/TASLP.2016.2544661,
author = {Zhou, Guangyou and Xie, Zhiwen and He, Tingting and Zhao, Jun and Hu, Xiaohua Tony},
title = {Learning the multilingual translation representations for question retrieval in community question answering via non-negative matrix factorization},
year = {2016},
issue_date = {July 2016},
publisher = {IEEE Press},
volume = {24},
number = {7},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2016.2544661},
doi = {10.1109/TASLP.2016.2544661},
abstract = {Community question answering (CQA) has become an increasingly popular research topic. In this paper, we focus on the problem of question retrieval. Question retrieval in CQA can automatically find the most relevant and recent questions that have been solved by other users. However, the word ambiguity and word mismatch problems bring about new challenges for question retrieval in CQA. State-of-the-art approaches address these issues by implicitly expanding the queried questions with additional words or phrases using monolingual translation models. While useful, the effectiveness of these models is highly dependent on the availability of quality parallel monolingual corpora (e.g., question--answer pairs) in the absence of which they are troubled by noise issues. In this work, we propose an alternative way to address the word ambiguity and word mismatch problems by taking advantage of potentially rich semantic information drawn from other languages. Our proposed method employs statistical machine translation to improve question retrieval and enriches the question representation with the translated words from other languages via non-negative matrix factorization. Experiments conducted on real CQA data sets show that our proposed approach is promising.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = jul,
pages = {1305–1314},
numpages = {10},
keywords = {text mining, question retrieval, natural language processing, information retrieval, community question answering}
}

@article{10.1145/2579991,
author = {Bislimovska, Bojana and Bozzon, Alessandro and Brambilla, Marco and Fraternali, Piero},
title = {Textual and Content-Based Search in Repositories of Web Application Models},
year = {2014},
issue_date = {March 2014},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {8},
number = {2},
issn = {1559-1131},
url = {https://doi.org/10.1145/2579991},
doi = {10.1145/2579991},
abstract = {Model-driven engineering relies on collections of models, which are the primary artifacts for software development. To enable knowledge sharing and reuse, models need to be managed within repositories, where they can be retrieved upon users’ queries. This article examines two different techniques for indexing and searching model repositories, with a focus on Web development projects encoded in a domain-specific language. Keyword-based and content-based search (also known as query-by-example) are contrasted with respect to the architecture of the system, the processing of models and queries, and the way in which metamodel knowledge can be exploited to improve search. A thorough experimental evaluation is conducted to examine what parameter configurations lead to better accuracy and to offer an insight in what queries are addressed best by each system.},
journal = {ACM Trans. Web},
month = mar,
articleno = {11},
numpages = {47},
keywords = {search, domain-specific language, Web application, Information retrieval}
}

@article{10.1145/2948063,
author = {Molino, Piero and Aiello, Luca Maria and Lops, Pasquale},
title = {Social Question Answering: Textual, User, and Network Features for Best Answer Prediction},
year = {2016},
issue_date = {January 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {35},
number = {1},
issn = {1046-8188},
url = {https://doi.org/10.1145/2948063},
doi = {10.1145/2948063},
abstract = {Community question answering (CQA) sites use a collaborative paradigm to satisfy complex information needs. Although the task of matching questions to their best answers has been tackled for more than a decade, the social question-answering practice is a complex process. The factors influencing the accuracy of question-answer matching are many and hard to disentangle. We approach the task from an application-oriented perspective, probing the space of several dimensions relevant to this problem: features, algorithms, and topics. We gather under a learning to rank framework the most extensive feature set used in literature to date, including 225 features from five different families. We test the power of such features in predicting the best answer to a question on the largest dataset from Yahoo Answers used for this task so far (40M answers) and provide a faceted analysis of the results along different topical areas and question types. We propose a novel family of distributional semantics measures that most of the time can seamlessly replace widely used linguistic similarity features, being more than one order of magnitude faster to compute and providing greater predictive power. The best feature set reaches an improvement between 11\% and 26\% in P@1 compared to recent well-established state-of-the-art methods.},
journal = {ACM Trans. Inf. Syst.},
month = sep,
articleno = {4},
numpages = {40},
keywords = {expertise networks, expert finding, distributional semantics, best answer prediction, Yahoo Answers, Community question answering}
}

@article{10.1145/2742547,
author = {Wang, Ting-Xuan and Lu, Wen-Hsiang},
title = {Constructing Complex Search Tasks with Coherent Subtask Search Goals},
year = {2015},
issue_date = {February 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {15},
number = {2},
issn = {2375-4699},
url = {https://doi.org/10.1145/2742547},
doi = {10.1145/2742547},
abstract = {Nowadays, due to the explosive growth of web content and usage, users deal with their complex search tasks by web search engines. However, conventional search engines consider a search query corresponding only to a simple search task. In order to accomplish a complex search task, which consists of multiple subtask search goals, users usually have to issue a series of queries. For example, the complex search task “travel to Dubai” may involve several subtask search goals, including reserving hotel room, surveying Dubai landmarks, booking flights, and so forth. Therefore, a user can efficiently accomplish his or her complex search task if search engines can predict the complex search task with a variety of subtask search goals. In this work, we propose a complex search task model (CSTM) to deal with this problem. The CSTM first groups queries into complex search task clusters, and then generates subtask search goals from each complex search task cluster. To raise the performance of CSTM, we exploit four web resources including community question answering, query logs, search engine result pages, and clicked pages. Experimental results show that our CSTM is effective in identifying the comprehensive subtask search goals of a complex search task.},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = dec,
articleno = {6},
numpages = {29},
keywords = {subtask search goal, query clustering, Complex search task}
}

@article{10.1145/3439769,
author = {Uddin, Gias and Khomh, Foutse and Roy, Chanchal K.},
title = {Automatic API Usage Scenario Documentation from Technical Q&amp;A Sites},
year = {2021},
issue_date = {July 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {30},
number = {3},
issn = {1049-331X},
url = {https://doi.org/10.1145/3439769},
doi = {10.1145/3439769},
abstract = {The online technical Q&amp;A site Stack Overflow (SO) is popular among developers to support their coding and diverse development needs. To address shortcomings in API official documentation resources, several research works have thus focused on augmenting official API documentation with insights (e.g., code examples) from SO. The techniques propose to add code examples/insights about APIs into its official documentation. Recently, surveys of software developers find that developers in SO consider the combination of code examples and reviews about APIs as a form of API documentation, and that they consider such a combination to be more useful than official API documentation when the official resources can be incomplete, ambiguous, incorrect, and outdated. Reviews are opinionated sentences with positive/negative sentiments. However, we are aware of no previous research that attempts to automatically produce API documentation from SO by considering both API code examples and reviews. In this article, we present two novel algorithms that can be used to automatically produce API documentation from SO by combining code examples and reviews towards those examples. The first algorithm is called statistical documentation, which shows the distribution of positivity and negativity around the code examples of an API using different metrics (e.g., star ratings). The second algorithm is called concept-based documentation, which clusters similar and conceptually relevant usage scenarios. An API usage scenario contains a code example, a textual description of the underlying task addressed by the code example, and the reviews (i.e., opinions with positive and negative sentiments) from other developers towards the code example. We deployed the algorithms in Opiner, a web-based platform to aggregate information about APIs from online forums. We evaluated the algorithms by mining all Java JSON-based posts in SO and by conducting three user studies based on produced documentation from the posts. The first study is a survey, where we asked the participants to compare our proposed algorithms against a Javadoc-syle documentation format (called as Type-based documentation in Opiner). The participants were asked to compare along four development scenarios (e.g., selection, documentation). The participants preferred our proposed two algorithms over type-based documentation. In our second user study, we asked the participants to complete four coding tasks using Opiner and the API official and informal documentation resources. The participants were more effective and accurate while using Opiner. In a subsequent survey, more than 80\% of participants asked the Opiner documentation platform to be integrated into the formal API documentation to complement and improve the API official documentation.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = apr,
articleno = {31},
numpages = {45},
keywords = {usage scenario, documentation, crowd-sourced developer forum, API}
}

@article{10.1145/2180868.2180869,
author = {Cao, Xin and Cong, Gao and Cui, Bin and Jensen, Christian S. and Yuan, Quan},
title = {Approaches to Exploring Category Information for Question Retrieval in Community Question-Answer Archives},
year = {2012},
issue_date = {May 2012},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {30},
number = {2},
issn = {1046-8188},
url = {https://doi.org/10.1145/2180868.2180869},
doi = {10.1145/2180868.2180869},
abstract = {Community Question Answering (CQA) is a popular type of service where users ask questions and where answers are obtained from other users or from historical question-answer pairs. CQA archives contain large volumes of questions organized into a hierarchy of categories. As an essential function of CQA services, question retrieval in a CQA archive aims to retrieve historical question-answer pairs that are relevant to a query question. This article presents several new approaches to exploiting the category information of questions for improving the performance of question retrieval, and it applies these approaches to existing question retrieval models, including a state-of-the-art question retrieval model. Experiments conducted on real CQA data demonstrate that the proposed techniques are effective and efficient and are capable of outperforming a variety of baseline methods significantly.},
journal = {ACM Trans. Inf. Syst.},
month = may,
articleno = {7},
numpages = {38},
keywords = {question search, cluster-based retrieval, categorization, Question-answering services}
}

@article{10.14778/3137765.3137790,
author = {Wang, Chao and Feng, Yihao and Guo, Qi and Li, Zhaoxian and Liu, Kexin and Tang, Zijian and Tung, Anthony K. H. and Wu, Lifu and Zheng, Yuxin},
title = {ARShop: a cloud-based augmented reality system for shopping},
year = {2017},
issue_date = {August 2017},
publisher = {VLDB Endowment},
volume = {10},
number = {12},
issn = {2150-8097},
url = {https://doi.org/10.14778/3137765.3137790},
doi = {10.14778/3137765.3137790},
abstract = {ARShop is a one-stop solution for shopping in the cyber-physical world with the help of crowd knowledge and augmented reality. Its ultimate goal is to improve customers' shopping experience. When a customer enters a physical shop and snaps a shot, the enriched cyber information of the surroundings will pop up and be augmented on the screen. ARShop can also be the customer's personal shopping assistant who can show routes to the shops that the customer is interested in. In addition, ARShop provides merchants with a web-based interface to manage their shops and promote their business to customers, and provides customers with an Android App to query using images.},
journal = {Proc. VLDB Endow.},
month = aug,
pages = {1845–1848},
numpages = {4}
}

@article{10.1145/2990497,
author = {Azad, Shams and Rigby, Peter C. and Guerrouj, Latifa},
title = {Generating API Call Rules from Version History and Stack Overflow Posts},
year = {2017},
issue_date = {May 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {25},
number = {4},
issn = {1049-331X},
url = {https://doi.org/10.1145/2990497},
doi = {10.1145/2990497},
abstract = {Researchers have shown that related functions can be mined from groupings of functions found in the version history of a system. Our first contribution is to expand this approach to a community of applications and set of similar applications. Android developers use a set of application programming interface (API) calls when creating apps. These API calls are used in similar ways across multiple applications. By clustering co-changing API calls used by 230 Android apps across 12k versions, we are able to predict the API calls that individual app developers will use with an average precision of 75\% and recall of 22\%. When we make predictions from the same category of app, such as Finance, we attain precision and recall of 81\% and 28\%, respectively.Our second contribution can be characterized as “programmers who discussed these functions were also interested in these functions.” Informal discussions on Stack Overflow provide a rich source of information about related API calls as developers provide solutions to common problems. By grouping API calls contained in each positively voted answer posts, we are able to create rules that predict the calls that app developers will use in their own apps with an average precision of 66\% and recall of 13\%.For comparison purposes, we developed a baseline by clustering co-changing API calls for each individual app and generated association rules from them. The baseline predicts API calls used by app developers with a precision and recall of 36\% and 23\%, respectively.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = jan,
articleno = {29},
numpages = {22},
keywords = {version history, informal documentation, community of applications, association rule mining, Stack Overflow, API method calls}
}

@article{10.1145/3064884,
author = {Costa, Alceu Ferraz and Yamaguchi, Yuto and Traina, Agma Juci Machado and Jr., Caetano Traina and Faloutsos, Christos},
title = {Modeling Temporal Activity to Detect Anomalous Behavior in Social Media},
year = {2017},
issue_date = {November 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {11},
number = {4},
issn = {1556-4681},
url = {https://doi.org/10.1145/3064884},
doi = {10.1145/3064884},
abstract = {Social media has become a popular and important tool for human communication. However, due to this popularity, spam and the distribution of malicious content by computer-controlled users, known as bots, has become a widespread problem. At the same time, when users use social media, they generate valuable data that can be used to understand the patterns of human communication. In this article, we focus on the following important question: Can we identify and use patterns of human communication to decide whether a human or a bot controls a user? The first contribution of this article is showing that the distribution of inter-arrival times (IATs) between postings is characterized by following four patterns: (i) heavy-tails, (ii) periodic-spikes, (iii) correlation between consecutive values, and (iv) bimodallity. As our second contribution, we propose a mathematical model named Act-M (Activity Model). We show that Act-M can accurately fit the distribution of IATs from social media users. Finally, we use Act-M to develop a method that detects if users are bots based only on the timing of their postings. We validate Act-M using data from over 55 million postings from four social media services: Reddit, Twitter, Stack-Overflow, and Hacker-News. Our experiments show that Act-M provides a more accurate fit to the data than existing models for human dynamics. Additionally, when detecting bots, Act-M provided a precision higher than 93\% and 77\% with a sensitivity of 70\% for the Twitter and Reddit datasets, respectively.},
journal = {ACM Trans. Knowl. Discov. Data},
month = jul,
articleno = {49},
numpages = {23},
keywords = {inter-arrival times, communication dynamics, anomaly detection, Social media}
}

@article{10.1145/2622669,
author = {Maalej, Walid and Tiarks, Rebecca and Roehm, Tobias and Koschke, Rainer},
title = {On the Comprehension of Program Comprehension},
year = {2014},
issue_date = {August 2014},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {23},
number = {4},
issn = {1049-331X},
url = {https://doi.org/10.1145/2622669},
doi = {10.1145/2622669},
abstract = {Research in program comprehension has evolved considerably over the past decades. However, only little is known about how developers practice program comprehension in their daily work. This article reports on qualitative and quantitative research to comprehend the strategies, tools, and knowledge used for program comprehension. We observed 28 professional developers, focusing on their comprehension behavior, strategies followed, and tools used. In an online survey with 1,477 respondents, we analyzed the importance of certain types of knowledge for comprehension and where developers typically access and share this knowledge.We found that developers follow pragmatic comprehension strategies depending on context. They try to avoid comprehension whenever possible and often put themselves in the role of users by inspecting graphical interfaces. Participants confirmed that standards, experience, and personal communication facilitate comprehension. The team size, its distribution, and open-source experience influence their knowledge sharing and access behavior. While face-to-face communication is preferred for accessing knowledge, knowledge is frequently shared in informal comments.Our results reveal a gap between research and practice, as we did not observe any use of comprehension tools and developers seem to be unaware of them. Overall, our findings call for reconsidering the research agendas towards context-aware tool support.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = sep,
articleno = {31},
numpages = {37},
keywords = {program comprehension, knowledge sharing, information needs, context-aware software engineering, Empirical software engineering}
}

@article{10.1145/3134737,
author = {Vermette, Laton and Dembla, Shruti and Wang, April Y. and McGrenere, Joanna and Chilana, Parmit K.},
title = {Social CheatSheet: An Interactive Community-Curated Information Overlay for Web Applications},
year = {2017},
issue_date = {November 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {1},
number = {CSCW},
url = {https://doi.org/10.1145/3134737},
doi = {10.1145/3134737},
abstract = {Users can often find it difficult to sift through dense help pages, tutorials, Q&amp;A sites, blogs, and wikis to locate useful task-specific instructions for feature-rich applications. We present Social CheatSheet, an interactive information overlay that can appear atop any existing web application and retrieve relevant step-by-step instructions and tutorials curated by other users. Based on results of our formative study, the system offers several features for users to search, browse, filter, and bookmark community-generated help content and to ask questions and clarifications. Furthermore, Social CheatSheet includes embedded curation features for users to generate their own annotated notes and tutorials that can be kept private or shared with the user community. A weeklong deployment study with 15 users showed that users found Social CheatSheet to be useful and they were able to easily both add their own curated content and locate content generated by other users. The majority of users wanted to keep using the system beyond the deployment. We discuss the potential of Social CheatSheet as an application-independent platform driven by community curation efforts to lower the barriers in finding relevant help and instructions.},
journal = {Proc. ACM Hum.-Comput. Interact.},
month = dec,
articleno = {102},
numpages = {19},
keywords = {software learning, software help, social help, community-curated help}
}

@article{10.1007/s00778-010-0211-9,
author = {Carmel, David and Roitman, Haggai and Yom-Tov, Elad},
title = {Social bookmark weighting for search and recommendation},
year = {2010},
issue_date = {December  2010},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {19},
number = {6},
issn = {1066-8888},
url = {https://doi.org/10.1007/s00778-010-0211-9},
doi = {10.1007/s00778-010-0211-9},
abstract = {Social bookmarking enables knowledge sharing and efficient discovery on the web, where users can collaborate together by tagging documents of interests. A lot of attention was given lately for utilizing social bookmarking data to enhance traditional IR tasks. Yet, much less attention was given to the problem of estimating the effectiveness of an individual bookmark for the specific tasks. In this work, we propose a novel framework for social bookmark weighting which allows us to estimate the effectiveness of each of the bookmarks individually for several IR tasks. We show that by weighting bookmarks according to their estimated quality, we can significantly improve social search effectiveness. We further demonstrate that using the same framework, we can derive solutions to several recommendation tasks such as tag recommendation, user recommendation, and document recommendation. Empirical evaluation on real data gathered from two large bookmarking systems demonstrates the effectiveness of the new social bookmark weighting framework.},
journal = {The VLDB Journal},
month = dec,
pages = {761–775},
numpages = {15},
keywords = {Tagging, Social bookmarking, Experimentation, Bookmarks, Algorithms}
}

@article{10.14778/1920841.1920877,
author = {Neumann, Thomas and Weikum, Gerhard},
title = {x-RDF-3X: fast querying, high update rates, and consistency for RDF databases},
year = {2010},
issue_date = {September 2010},
publisher = {VLDB Endowment},
volume = {3},
number = {1–2},
issn = {2150-8097},
url = {https://doi.org/10.14778/1920841.1920877},
doi = {10.14778/1920841.1920877},
abstract = {The RDF data model is gaining importance for applications in computational biology, knowledge sharing, and social communities. Recent work on RDF engines has focused on scalable performance for querying, and has largely disregarded updates. In addition to incremental bulk loading, applications also require online updates with flexible control over multi-user isolation levels and data consistency. The challenge lies in meeting these requirements while retaining the capability for fast querying.This paper presents a comprehensive solution that is based on an extended deferred-indexing method with integrated versioning. The version store enables time-travel queries that are efficiently processed without adversely affecting queries on the current data. For flexible consistency, transactional concurrency control is provided with options for either snapshot isolation or full serializability. All methods are integrated in an extension of the RDF-3X system, and their very good performance for both queries and updates is demonstrated by measurements of multi-user workloads with real-life data as well as stress-test synthetic loads.},
journal = {Proc. VLDB Endow.},
month = sep,
pages = {256–263},
numpages = {8}
}

@article{10.1145/2180868.2180872,
author = {Pal, Aditya and Harper, F. Maxwell and Konstan, Joseph A.},
title = {Exploring Question Selection Bias to Identify Experts and Potential Experts in Community Question Answering},
year = {2012},
issue_date = {May 2012},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {30},
number = {2},
issn = {1046-8188},
url = {https://doi.org/10.1145/2180868.2180872},
doi = {10.1145/2180868.2180872},
abstract = {Community Question Answering (CQA) services enable their users to exchange knowledge in the form of questions and answers. These communities thrive as a result of a small number of highly active users, typically called experts, who provide a large number of high-quality useful answers. Expert identification techniques enable community managers to take measures to retain the experts in the community. There is further value in identifying the experts during the first few weeks of their participation as it would allow measures to nurture and retain them. In this article we address two problems: (a) How to identify current experts in CQA? and (b) How to identify users who have potential of becoming experts in future (potential experts)? In particular, we propose a probabilistic model that captures the selection preferences of users based on the questions they choose for answering. The probabilistic model allows us to run machine learning methods for identifying experts and potential experts. Our results over several popular CQA datasets indicate that experts differ considerably from ordinary users in their selection preferences; enabling us to predict experts with higher accuracy over several baseline models. We show that selection preferences can be combined with baseline measures to improve the predictive performance even further.},
journal = {ACM Trans. Inf. Syst.},
month = may,
articleno = {10},
numpages = {28},
keywords = {question selection process, community question answering, Expert identification}
}

@article{10.1145/2994599,
author = {Niu, Wei and Liu, Zhijiao and Caverlee, James},
title = {On Local Expert Discovery via Geo-Located Crowds, Queries, and Candidates},
year = {2016},
issue_date = {November 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {2},
number = {4},
issn = {2374-0353},
url = {https://doi.org/10.1145/2994599},
doi = {10.1145/2994599},
abstract = {Local experts are critical for many location-sensitive information needs, and yet there is a research gap in our understanding of the factors impacting who is recognized as a local expert and in methods for discovering local experts. Hence, in this article, we explore a geo-spatial learning-to-rank framework for identifying local experts. Three of the key features of the proposed approach are: (i) a learning-based framework for integrating multiple user-based, content-based, list-based, and crowd-based factors impacting local expertise that leverages the fine-grained GPS coordinates of millions of social media users; (ii) a location-sensitive random walk that propagates crowd knowledge of a candidate’s expertise; and (iii) a comprehensive controlled study over AMT-labeled local experts on eight topics and in four cities. We find significant improvements of local expert finding versus two state-of-the-art alternatives, as well as evidence for the generalizability of local expert ranking models to new topics and new locations.},
journal = {ACM Trans. Spatial Algorithms Syst.},
month = nov,
articleno = {14},
numpages = {24},
keywords = {geo-spatial, Twitter list, Local expert discovery}
}

@article{10.14778/1920841.1921025,
author = {Si, Xiance and Chang, Edward Y. and Gy\"{o}ngyi, Zolt\'{a}n and Sun, Maosong},
title = {Confucius and its intelligent disciples: integrating social with search},
year = {2010},
issue_date = {September 2010},
publisher = {VLDB Endowment},
volume = {3},
number = {1–2},
issn = {2150-8097},
url = {https://doi.org/10.14778/1920841.1921025},
doi = {10.14778/1920841.1921025},
abstract = {Q&amp;A sites continue to flourish as a large number of users rely on them as useful substitutes for incomplete or missing search results. In this paper, we present our experience with developing Confucius, a Google Q&amp;A service launched in 21 countries and four languages by the end of 2009. Confucius employs six data mining subroutines to harness synergy between web search and social networks. We present these subroutines' design goals, algorithms, and their effects on service quality. We also describe techniques for and experience with scaling the subroutines to mine massive data sets.},
journal = {Proc. VLDB Endow.},
month = sep,
pages = {1505–1516},
numpages = {12}
}

@article{10.1145/1870096.1870099,
author = {Bouguessa, Mohamed and Wang, Shengrui and Dumoulin, Benoit},
title = {Discovering Knowledge-Sharing Communities in Question-Answering Forums},
year = {2010},
issue_date = {December 2010},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {5},
number = {1},
issn = {1556-4681},
url = {https://doi.org/10.1145/1870096.1870099},
doi = {10.1145/1870096.1870099},
abstract = {In this article, we define a knowledge-sharing community in a question-answering forum as a set of askers and authoritative users such that, within each community, askers exhibit more homogeneous behavior in terms of their interactions with authoritative users than elsewhere. A procedure for discovering members of such a community is devised. As a case study, we focus on Yahoo! Answers, a large and diverse online question-answering service. Our contribution is twofold. First, we propose a method for automatic identification of authoritative actors in Yahoo! Answers. To this end, we estimate and then model the authority scores of participants as a mixture of gamma distributions. The number of components in the mixture is determined using the Bayesian Information Criterion (BIC), while the parameters of each component are estimated using the Expectation-Maximization (EM) algorithm. This method allows us to automatically discriminate between authoritative and nonauthoritative users. Second, we represent the forum environment as a type of transactional data such that each transaction summarizes the interaction of an asker with a specific set of authoritative users. Then, to group askers on the basis of their interactions with authoritative users, we propose a parameter-free transaction data clustering algorithm which is based on a novel criterion function. The identified clusters correspond to the communities that we aim to discover. To evaluate the suitability of our clustering algorithm, we conduct a series of experiments on both synthetic data and public real-life data. Finally, we put our approach to work using data from Yahoo! Answers which represent users’ activities over one full year.},
journal = {ACM Trans. Knowl. Discov. Data},
month = dec,
articleno = {3},
numpages = {49},
keywords = {transaction data, mixture models, Clustering}
}

@article{10.1145/2019618.2019620,
author = {Sutanto, Juliana and Kankanhalli, Atreyi and Tan, Bernard Cheng Yian},
title = {Eliciting a sense of virtual community among knowledge contributors},
year = {2008},
issue_date = {October 2011},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {2},
number = {3},
issn = {2158-656X},
url = {https://doi.org/10.1145/2019618.2019620},
doi = {10.1145/2019618.2019620},
abstract = {Member-initiated virtual communities for product knowledge sharing and commerce purposes are proliferating as useful alternatives to company information and commerce Web sites. Although such communities are easy to create with the availability of numerous tools, the challenge lies in keeping the community alive and thriving. Key to sustainability is members' Sense Of Virtual Community (SOVC) so that they feel responsible for contributing their knowledge and creating value for others. However, it is unclear what leads to the SOVC among knowledge contributors. Building on appraisal theory, we hypothesize that the fulfillment of contributors' informational, instrumental, entertainment, self-discovery, and social enhancement needs will increase their SOVC. To test the hypotheses, we surveyed knowledge contributors in a beauty-product-related community to examine the relationship between their needs' fulfillment and SOVC levels. Other than the social enhancement need, all other needs' fulfillment were found to be positively related to SOVC levels. To further understand how the SOVC of knowledge contributors changes over time, we conducted a longitudinal analysis of a panel of these members. We discovered that over time, changes in the perceived fulfillment of their instrumental, entertainment, and self-discovery needs determined the change of their SOVC. The results have implications for future research as well as for the sustainability and value generation from such virtual communities.},
journal = {ACM Trans. Manage. Inf. Syst.},
month = oct,
articleno = {14},
numpages = {17},
keywords = {needs fulfillment, longitudinal analysis, knowledge contributors, Sense of virtual community}
}

@article{10.1145/1132462.1132467,
author = {Choi, Yoonseo and Han, Hwansoo},
title = {Optimal register reassignment for register stack overflow minimization},
year = {2006},
issue_date = {March 2006},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {3},
number = {1},
issn = {1544-3566},
url = {https://doi.org/10.1145/1132462.1132467},
doi = {10.1145/1132462.1132467},
abstract = {Architectures with a register stack can implement efficient calling conventions. Using the overlapping of callers' and callees' registers, callers are able to pass parameters to callees without a memory stack. The most recent instance of a register stack can be found in the Intel Itanium architecture. A hardware component called the register stack engine (RSE) provides an illusion of an infinite-length register stack using a memory-backed process to handle overflow and underflow for a physically limited number of registers. Despite such hardware support, some applications suffer from the overhead required to handle register stack overflow and underflow. The memory latency associated with the overflow and underflow of a register stack can be reduced by generating multiple register allocation instructions within a procedure [Settle et al. 2003]. Live analysis is utilized to find a set of registers that are not required to keep their values across procedure boundaries. However, among those dead registers, only the registers that are consecutively located in a certain part of the register stack frame can be removed. We propose a compiler-supported register reassignment technique that reduces RSE overflow/underflow further. By reassigning registers based on live analysis, our technique forces as many dead registers to be removed as possible. We define the problem of optimal register reassignment, which minimizes interprocedural register stack heights considering multiple call sites within a procedure. We present how this problem is related to a path-finding problem in a graph called a sequence graph. We also propose an efficient heuristic algorithm for the problem. Finally, we present the measurement of effects of the proposed techniques on SPEC CINT2000 benchmark suite and the analysis of the results. The result shows that our approach reduces the RSE cycles by 6.4\% and total cpu cycles by 1.7\% on average.},
journal = {ACM Trans. Archit. Code Optim.},
month = mar,
pages = {90–114},
numpages = {25},
keywords = {sequence graph, register stack, register allocation, Register assignment}
}

@article{10.14778/3611540.3611544,
author = {Anneser, Christoph and Tatbul, Nesime and Cohen, David and Xu, Zhenggang and Pandian, Prithviraj and Laptev, Nikolay and Marcus, Ryan},
title = {AutoSteer: Learned Query Optimization for Any SQL Database},
year = {2023},
issue_date = {August 2023},
publisher = {VLDB Endowment},
volume = {16},
number = {12},
issn = {2150-8097},
url = {https://doi.org/10.14778/3611540.3611544},
doi = {10.14778/3611540.3611544},
abstract = {This paper presents AutoSteer, a learning-based solution that automatically drives query optimization in any SQL database that exposes tunable optimizer knobs. AutoSteer builds on the Bandit optimizer (Bao) and extends it with new capabilities (e.g., automated hint-set discovery) to minimize integration effort and facilitate usability in both monolithic and disaggregated SQL systems. We successfully applied AutoSteer on PostgreSQL, PrestoDB, Spark-SQL, MySQL, and DuckDB - five popular open-source database engines with diverse query optimizers. We then conducted a detailed experimental evaluation with public benchmarks (JOB, Stackoverflow, TPC-DS) and a production workload from Meta's PrestoDB deployments. Our evaluation shows that AutoSteer can not only outperform these engines' native query optimizers (e.g., up to 40\% improvements for PrestoDB) but can also match the performance of Bao-for-PostgreSQL with reduced human supervision and increased adaptivity, as it replaces Bao's static, expert-picked hint-sets with those that are automatically discovered. We also provide an open-source implementation of AutoSteer together with a visual tool for interactive use by query optimization experts.},
journal = {Proc. VLDB Endow.},
month = aug,
pages = {3515–3527},
numpages = {13}
}

@article{10.14778/3611479.3611534,
author = {Li, Peng and He, Yeye and Yan, Cong and Wang, Yue and Chaudhuri, Surajit},
title = {Auto-Tables: Synthesizing Multi-Step Transformations to Relationalize Tables without Using Examples},
year = {2023},
issue_date = {July 2023},
publisher = {VLDB Endowment},
volume = {16},
number = {11},
issn = {2150-8097},
url = {https://doi.org/10.14778/3611479.3611534},
doi = {10.14778/3611479.3611534},
abstract = {Relational tables, where each row corresponds to an entity and each column corresponds to an attribute, have been the standard for tables in relational databases. However, such a standard cannot be taken for granted when dealing with tables "in the wild". Our survey of real spreadsheet-tables and web-tables shows that over 30\% of such tables do not conform to the relational standard, for which complex table-restructuring transformations are needed before these tables can be queried easily using SQL-based tools. Unfortunately, the required transformations are non-trivial to program, which has become a substantial pain point for technical and non-technical users alike, as evidenced by large numbers of forum questions in places like StackOverflow and Excel/Tableau forums.We develop an Auto-Tables system that can automatically synthesize pipelines with multi-step transformations (in Python or other languages), to transform non-relational tables into standard relational forms for downstream analytics, obviating the need for users to manually program transformations. We compile an extensive benchmark for this new task, by collecting 244 real test cases from user spreadsheets and online forums. Our evaluation suggests that Auto-Tables can successfully synthesize transformations for over 70\% of test cases at interactive speeds, without requiring any input from users, making this an effective tool for both technical and non-technical users to prepare data for analytics.},
journal = {Proc. VLDB Endow.},
month = jul,
pages = {3391–3403},
numpages = {13}
}

@article{10.5555/3417639.3417673,
author = {Johnson, Jeremiah W. and Jin, Karen H.},
title = {Jupyter notebooks in education},
year = {2020},
issue_date = {April 2020},
publisher = {Consortium for Computing Sciences in Colleges},
address = {Evansville, IN, USA},
volume = {35},
number = {8},
issn = {1937-4771},
abstract = {Jupyter notebooks are widely used in industry for a range of tasks. This is particularly so in areas that involve significant amounts of data analysis or machine learning; indeed, while 5\% of Python developers surveyed in the 2018 JetBrains Python Developer Survey report using Jupyter notebooks for their primary development tool, when restricted to those working in data science roles, Jupyter notebooks tied with the PyCharm IDE as the most popular tool for Python development [1], and in the 2019 StackOverflow developer survey, 9.5\% of developers surveyed listed Jupyter notebooks as their preferred development environment [2].},
journal = {J. Comput. Sci. Coll.},
month = apr,
pages = {268–269},
numpages = {2}
}

@article{10.1145/3177732.3177735,
author = {Yaghmazadeh, Navid and Wang, Xinyu and Dillig, Isil},
title = {Automated migration of hierarchical data to relational tables using programming-by-example},
year = {2018},
issue_date = {January 2018},
publisher = {VLDB Endowment},
volume = {11},
number = {5},
issn = {2150-8097},
url = {https://doi.org/10.1145/3177732.3177735},
doi = {10.1145/3177732.3177735},
abstract = {While many applications export data in hierarchical formats like XML and JSON, it is often necessary to convert such hierarchical documents to a relational representation. This paper presents a novel programming-by-example approach, and its implementation in a tool called Mitra, for automatically migrating tree-structured documents to relational tables. We have evaluated the proposed technique using two sets of experiments. In the first experiment, we used Mitra to automate 98 data transformation tasks collected from StackOverflow. Our method can generate the desired program for 94\% of these benchmarks with an average synthesis time of 3.8 seconds. In the second experiment, we used Mitra to generate programs that can convert real-world XML and JSON datasets to full-fledged relational databases. Our evaluation shows that Mitra can automate the desired transformation for all datasets.},
journal = {Proc. VLDB Endow.},
month = oct,
pages = {580–593},
numpages = {14}
}

@article{10.1145/3187009.3177735,
author = {Yaghmazadeh, Navid and Wang, Xinyu and Dillig, Isil},
title = {Automated migration of hierarchical data to relational tables using programming-by-example},
year = {2018},
issue_date = {January 2018},
publisher = {VLDB Endowment},
volume = {11},
number = {5},
issn = {2150-8097},
url = {https://doi.org/10.1145/3187009.3177735},
doi = {10.1145/3187009.3177735},
abstract = {While many applications export data in hierarchical formats like XML and JSON, it is often necessary to convert such hierarchical documents to a relational representation. This paper presents a novel programming-by-example approach, and its implementation in a tool called Mitra, for automatically migrating tree-structured documents to relational tables. We have evaluated the proposed technique using two sets of experiments. In the first experiment, we used Mitra to automate 98 data transformation tasks collected from StackOverflow. Our method can generate the desired program for 94\% of these benchmarks with an average synthesis time of 3.8 seconds. In the second experiment, we used Mitra to generate programs that can convert real-world XML and JSON datasets to full-fledged relational databases. Our evaluation shows that Mitra can automate the desired transformation for all datasets.},
journal = {Proc. VLDB Endow.},
month = jan,
pages = {580–593},
numpages = {14}
}

@article{10.1145/3569090,
author = {Cheng, Danzhao and Ch’ng, Eugene},
title = {Harnessing Collective Differences in Crowdsourcing Behaviour for Mass Photogrammetry of 3D Cultural Heritage},
year = {2022},
issue_date = {March 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {16},
number = {1},
issn = {1556-4673},
url = {https://doi.org/10.1145/3569090},
doi = {10.1145/3569090},
abstract = {Disorganised and self-organised crowdsourcing activities that harness collective behaviours to achieve a specific level of performance and task completeness are not well understood. Such phenomena become indistinct when highly varied environments are present, particularly for crowdsourcing photogrammetry-based 3D models. Mass photogrammetry can democratise traditional close-range photogrammetry procedures by outsourcing image acquisition tasks to a crowd of non-experts to capture geographically scattered 3D objects. To improve public engagement, we need to understand how individual behaviour in collective efforts work in traditional disorganised crowdsourcing and how it can be organised for better performance. This research aims to investigate the effectiveness of disorganised and self-organised collaborative crowdsourcing. It examines the collaborative dynamics among participants and the trends we could leverage if team structures were incorporated. Two scenarios were proposed and constructed: asynchronous crowdsourcing, which implicitly aggregates isolated contributions from disorganised individuals; and synchronous collaborative crowdsourcing, which assigns participants into a crowd-based self-organised team. Our experiment demonstrated that a self-organised team working in synchrony can effectively improve crowdsourcing photogrammetric 3D models in terms of model completeness and user experience. Through our study, we demonstrated that this crowdsourcing mechanism can provide a social context where participants can exchange information via implicit communication, and collectively build a shared mental model that pertains to their responsibilities and task goals. It stimulates participants’ prosocial motivation and reinforces their commitment. With more time and effort invested, their positive sense of ownership increases, fostering higher dedication and better contribution. Our findings shed further light on the potentials of adopting team structures to encourage effective collaborations in conventionally individual-based voluntary crowdsourcing settings, especially in the digital heritage domain.},
journal = {J. Comput. Cult. Herit.},
month = dec,
articleno = {19},
numpages = {23},
keywords = {cultural heritage, task allocation, crowd behaviour, collaboration dynamics, team structures, mass photogrammetry, Crowdsourcing}
}

@article{10.1145/3603256,
author = {Fornaroli, Alessandro and Gatica-Perez, Daniel},
title = {Urban Crowdsourcing Platforms across the World: A Systematic Review},
year = {2023},
issue_date = {September 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {4},
number = {3},
url = {https://doi.org/10.1145/3603256},
doi = {10.1145/3603256},
abstract = {Urban crowdsourcing platforms are becoming increasingly important, especially considering the relevance of citizen-centricity in smart cities. This systematic review aims at analyzing existing academic literature on urban crowdsourcing platforms to gather citizen-generated data and shed light on the state of research and development of these tools. Studies describing data-gathering urban crowdsourcing platforms were selected following the PRISMA protocol, for a total of 30 studies, corresponding to 32 platforms. After analyzing the studies at large, this review then proceeds to examine and catalogue the platforms, focusing on their location, purpose, and public data availability. While providing valuable information on existing platforms, the catalogue is subject to different types of bias, including a geographical one, which derive primarily from the chosen methodology to identify platforms worldwide. The article also discusses the implications of such choices.},
journal = {Digit. Gov.: Res. Pract.},
month = sep,
articleno = {15},
numpages = {19},
keywords = {citizen sourcing, Urban crowdsourcing}
}

@article{10.1109/TNET.2022.3223367,
author = {Shi, Qi and Hao, Dong},
title = {Social Sourcing: Incorporating Social Networks Into Crowdsourcing Contest Design},
year = {2022},
issue_date = {Aug. 2023},
publisher = {IEEE Press},
volume = {31},
number = {4},
issn = {1063-6692},
url = {https://doi.org/10.1109/TNET.2022.3223367},
doi = {10.1109/TNET.2022.3223367},
abstract = {In a crowdsourcing contest, a principal holding a task posts it to a crowd. People in the crowd then compete with each other to win the rewards. Although in real life, a crowd is usually networked and people influence each other via social ties, existing crowdsourcing contest theories do not aim to answer how interpersonal relationships influence people’s incentives and behaviors and thereby affect the crowdsourcing performance. In this work, we novelly take people’s social ties as a key factor in the modeling and designing of agents’ incentives in crowdsourcing contests. We establish two contest mechanisms by which the principal can impel the agents to invite their neighbors to contribute to the task. The first mechanism has a symmetric Bayesian Nash equilibrium, and it is very simple for agents to play and easy for the principal to predict the contest performance. The second mechanism has an asymmetric Bayesian Nash equilibrium, and agents’ behaviors in equilibrium show a vast diversity which is strongly related to their social relations. The Bayesian Nash equilibrium analysis of these new mechanisms reveals that, besides agents’ intrinsic abilities, the social relations among them also play a central role in decision-making. Moreover, we design an effective algorithm to automatically compute the Bayesian Nash equilibrium of the invitation crowdsourcing contest and further adapt it to a large graph dataset. Both theoretical and empirical results show that the new invitation crowdsourcing contests can substantially enlarge the number of participants, whereby the principal can obtain significantly better solutions without a large advertisement expenditure.},
journal = {IEEE/ACM Trans. Netw.},
month = nov,
pages = {1535–1549},
numpages = {15}
}

@article{10.1145/3610044,
author = {Pei, Weiping and Likhtenshteyn, Yanina and Yue, Chuan},
title = {A Tale of Two Communities: Privacy of Third Party App Users in Crowdsourcing - The Case of Receipt Transcription},
year = {2023},
issue_date = {October 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {CSCW2},
url = {https://doi.org/10.1145/3610044},
doi = {10.1145/3610044},
abstract = {Mobile and web apps are increasingly relying on the data generated or provided by users such as from their uploaded documents and images. Unfortunately, those apps may raise significant user privacy concerns. Specifically, to train or adapt their models for accurately processing huge amounts of data continuously collected from millions of app users, app or service providers have widely adopted the approach of crowdsourcing for recruiting crowd workers to manually annotate or transcribe the sampled ever-changing user data. However, when users' data are uploaded through apps and then become widely accessible to hundreds of thousands of anonymous crowd workers, many human-in-the-loop related privacy questions arise concerning both the app user community and the crowd worker community. In this paper, we propose to investigate the privacy risks brought by this significant trend of large-scale crowd-powered processing of app users' data generated in their daily activities. We consider the representative case of receipt scanning apps that have millions of users, and focus on the corresponding receipt transcription tasks that appear popularly on crowdsourcing platforms. We design and conduct an app user survey study (n=108) to explore how app users perceive privacy in the context of using receipt scanning apps. We also design and conduct a crowd worker survey study (n=102) to explore crowd workers' experiences on receipt and other types of transcription tasks as well as their attitudes towards such tasks. Overall, we found that most app users and crowd workers expressed strong concerns about the potential privacy risks to receipt owners, and they also had a very high level of agreement with the need for protecting receipt owners' privacy. Our work provides insights on app users' potential privacy risks in crowdsourcing, and highlights the need and challenges for protecting third party users' privacy on crowdsourcing platforms. We have responsibly disclosed our findings to the related crowdsourcing platform and app providers.},
journal = {Proc. ACM Hum.-Comput. Interact.},
month = oct,
articleno = {253},
numpages = {43},
keywords = {app user, crowdsourcing, privacy, receipt transcription}
}

@article{10.1145/3544018,
author = {Miao, Xiaoye and Peng, Huanhuan and Gao, Yunjun and Zhang, Zongfu and Yin, Jianwei},
title = {On Dynamically Pricing Crowdsourcing Tasks},
year = {2023},
issue_date = {February 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {17},
number = {2},
issn = {1556-4681},
url = {https://doi.org/10.1145/3544018},
doi = {10.1145/3544018},
abstract = {Crowdsourcing techniques have been extensively explored in the past decade, including task allocation, quality assessment, and so on. Most of professional crowdsourcing platforms adopt the fixed pricing scheme to offer a fixed price for crowd tasks. It is neither incentive for crowd workers to produce good performance, nor profitable for the requester to gain high utility with low budget. In this article, we study the problem of pricing crowdsourcing tasks with optional bonuses. We propose a dynamic pricing mechanism, named CrowdPricer for incentively delivering bonuses to the crowd workers of completing tasks, in addition to offering a base payment for completing a task. We leverage a deep time sequence model to learn the effect of bonuses on workers’ quality for crowd tasks. CrowdPricer makes decisions on whether to provide bonuses on workers, so as to maximize the requester’s utility in expectation. We present an efficient bonus delivery algorithm under the help of beam search technique, in order to efficiently solve the decision making problem. Extensive experiments using both a real crowdsourcing platform and simulations demonstrate that CrowdPricer yields the higher utility for the requester. It also obtains more correct crowd answers than the state-of-the-art pricing methods.},
journal = {ACM Trans. Knowl. Discov. Data},
month = feb,
articleno = {30},
numpages = {27},
keywords = {optimization, deep learning model, pricing mechanism, Crowdsourcing}
}

@article{10.1145/3604940,
author = {Maddalena, Eddy and Ib\'{a}\~{n}ez, Luis-Daniel and Reeves, Neal and Simperl, Elena},
title = {Qrowdsmith: Enhancing Paid Microtask Crowdsourcing with Gamification and Furtherance Incentives},
year = {2023},
issue_date = {October 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {14},
number = {5},
issn = {2157-6904},
url = {https://doi.org/10.1145/3604940},
doi = {10.1145/3604940},
abstract = {Microtask crowdsourcing platforms are social intelligence systems in which volunteers, called crowdworkers, complete small, repetitive tasks in return for a small fee. Beyond payments, task requesters are considering non-monetary incentives such as points, badges, and other gamified elements to increase performance and improve crowdworker experience. In this article, we present Qrowdsmith, a platform for gamifying microtask crowdsourcing. To design the system, we explore empirically a range of gamified and financial incentives and analyse their impact on how efficient, effective, and reliable the results are. To maintain participation over time and save costs, we propose furtherance incentives, which are offered to crowdworkers to encourage additional contributions in addition to the fee agreed upfront. In a series of controlled experiments, we find that while gamification can work as furtherance incentives, it impacts negatively on crowdworkers’ performance, both in terms of the quantity and quality of work, as compared to a baseline where they can continue to contribute voluntarily. Gamified incentives are also less effective than paid bonus equivalents. Our results contribute to the understanding of how best to encourage engagement in microtask crowdsourcing activities and design better crowd intelligence systems.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = sep,
articleno = {86},
numpages = {26},
keywords = {Qrowdsmith: Enhancing paid microtask crowdsourcing with gamification and furtherance incentives}
}

@article{10.14778/3231751.3231766,
author = {He, Yeye and Chu, Xu and Ganjam, Kris and Zheng, Yudian and Narasayya, Vivek and Chaudhuri, Surajit},
title = {Transform-data-by-example (TDE): an extensible search engine for data transformations},
year = {2018},
issue_date = {June 2018},
publisher = {VLDB Endowment},
volume = {11},
number = {10},
issn = {2150-8097},
url = {https://doi.org/10.14778/3231751.3231766},
doi = {10.14778/3231751.3231766},
abstract = {Today, business analysts and data scientists increasingly need to clean, standardize and transform diverse data sets, such as name, address, date time, and phone number, before they can perform analysis. This process of data transformation is an important part of data preparation, and is known to be difficult and time-consuming for end-users.Traditionally, developers have dealt with these longstanding transformation problems using custom code libraries. They have built vast varieties of custom logic for name parsing and address standardization, etc., and shared their source code in places like GitHub. Data transformation would be a lot easier for end-users if they can discover and reuse such existing transformation logic.We developed Transform-Data-by-Example (TDE), which works like a search engine for data transformations. TDE "indexes" vast varieties of transformation logic in source code, DLLs, web services and mapping tables, so that users only need to provide a few input/output examples to demonstrate a desired transformation, and TDE can interactively find relevant functions to synthesize new programs consistent with all examples. Using an index of 50K functions crawled from GitHub and Stackoverflow, TDE can already handle many common transformations not currently supported by existing systems. On a benchmark with over 200 transformation tasks, TDE generates correct transformations for 72\% tasks, which is considerably better than other systems evaluated. A beta version of TDE for Microsoft Excel is available via Office store1. Part of the TDE technology also ships in Microsoft Power BI.},
journal = {Proc. VLDB Endow.},
month = jun,
pages = {1165–1177},
numpages = {13}
}

@article{10.1145/3133910,
author = {Madsen, Magnus and Lhot\'{a}k, Ond\v{r}ej and Tip, Frank},
title = {A model for reasoning about JavaScript promises},
year = {2017},
issue_date = {October 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {1},
number = {OOPSLA},
url = {https://doi.org/10.1145/3133910},
doi = {10.1145/3133910},
abstract = {In JavaScript programs, asynchrony arises in situations such as web-based user-interfaces, communicating with servers through HTTP requests, and non-blocking I/O. Event-based programming is the most popular approach for managing asynchrony, but suffers from problems such as lost events and event races, and results in code that is hard to understand and debug. Recently, ECMAScript 6 has added support for promises, an alternative mechanism for managing asynchrony that enables programmers to chain asynchronous computations while supporting proper error handling. However, promises are complex and error-prone in their own right, so programmers would benefit from techniques that can reason about the correctness of promise-based code. Since the ECMAScript 6 specification is informal and intended for implementers of JavaScript engines, it does not provide a suitable basis for formal reasoning. This paper presents λp, a core calculus that captures the essence of ECMAScript 6 promises. Based on λp, we introduce the promise graph, a program representation that can assist programmers with debugging of promise-based code. We then report on a case study in which we investigate how the promise graph can be helpful for debugging errors related to promises in code fragments posted to the StackOverflow website.},
journal = {Proc. ACM Program. Lang.},
month = oct,
articleno = {86},
numpages = {24},
keywords = {Promises, Promise Graph, JavaScript, Formal Semantics, EcmaScript 6}
}

@article{10.1109/TASLP.2022.3233468,
author = {Mart\'{\i}n-Morat\'{o}, Irene and Mesaros, Annamaria},
title = {Strong Labeling of Sound Events Using Crowdsourced Weak Labels and Annotator Competence Estimation},
year = {2023},
issue_date = {2023},
publisher = {IEEE Press},
volume = {31},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2022.3233468},
doi = {10.1109/TASLP.2022.3233468},
abstract = {Crowdsourcing is a popular tool for collecting large amounts of annotated data, but the specific format of the strong labels necessary for sound event detection is not easily obtainable through crowdsourcing. In this work, we propose a novel annotation workflow that leverages the efficiency of crowdsourcing weak labels, and uses a high number of annotators to produce reliable and objective strong labels. The weak labels are collected in a highly redundant setup, to allow reconstruction of the temporal information. To obtain reliable labels, the annotators' competence is estimated using MACE (Multi-Annotator Competence Estimation) and incorporated into the strong labels estimation through weighing of individual opinions. We show that the proposed method produces consistently reliable strong annotations not only for synthetic audio mixtures, but also for audio recordings of real everyday environments. While only a maximum 80% coincidence with the complete and correct reference annotations was obtained for synthetic data, these results are explained by an extended study of how polyphony and SNR levels affect the identification rate of the sound events by the annotators. On real data, even though the estimated annotators' competence is significantly lower and the coincidence with reference labels is under 69%, the proposed majority opinion approach produces reliable aggregated strong labels in comparison with the more difficult task of crowdsourcing directly strong labels.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = jan,
pages = {902–914},
numpages = {13}
}

@article{10.1145/3479531,
author = {Ram\'{\i}rez, Jorge and Sayin, Burcu and Baez, Marcos and Casati, Fabio and Cernuzzi, Luca and Benatallah, Boualem and Demartini, Gianluca},
title = {On the State of Reporting in Crowdsourcing Experiments and a Checklist to Aid Current Practices},
year = {2021},
issue_date = {October 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {5},
number = {CSCW2},
url = {https://doi.org/10.1145/3479531},
doi = {10.1145/3479531},
abstract = {Crowdsourcing is being increasingly adopted as a platform to run studies with human subjects. Running a crowdsourcing experiment involves several choices and strategies to successfully port an experimental design into an otherwise uncontrolled research environment, e.g., sampling crowd workers, mapping experimental conditions to micro-tasks, or ensure quality contributions. While several guidelines inform researchers in these choices, guidance of how and what to report from crowdsourcing experiments has been largely overlooked. If under-reported, implementation choices constitute variability sources that can affect the experiment's reproducibility and prevent a fair assessment of research outcomes. In this paper, we examine the current state of reporting of crowdsourcing experiments and offer guidance to address associated reporting issues. We start by identifying sensible implementation choices, relying on existing literature and interviews with experts, to then extensively analyze the reporting of 171 crowdsourcing experiments. Informed by this process, we propose a checklist for reporting crowdsourcing experiments.},
journal = {Proc. ACM Hum.-Comput. Interact.},
month = oct,
articleno = {387},
numpages = {34},
keywords = {crowdsourcing, crowdsourcing experiments, reporting, reproducibility}
}

@article{10.1145/3556545,
author = {Wu, Gongqing and Zhou, Liangzhu and Xia, Jiazhu and Li, Lei and Bao, Xianyu and Wu, Xindong},
title = {Crowdsourcing Truth Inference Based on Label Confidence Clustering},
year = {2023},
issue_date = {May 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {17},
number = {4},
issn = {1556-4681},
url = {https://doi.org/10.1145/3556545},
doi = {10.1145/3556545},
abstract = {Truth inference can help solve some difficult problems of data integration in crowdsourcing. Crowdsourced workers are not experts and their labeling ability varies greatly; therefore, in practical applications, it is difficult to determine whether the labels collected from a crowdsourcing platform are correct. This article proposes a novel algorithm called truth inference based on label confidence clustering (TILCC) to improve the quality of integrated labels for the single-choice classification problem in crowdsourcing labeling tasks. We obtain the label confidence via worker reliability, which is calculated from multiple noise labels using a truth discovery method, and then we generate the clustering features and use the K-means algorithm to cluster all the tasks into K different clusters. Each cluster corresponds to a specific class, and the tasks in the cluster are assigned a label. Compared with the performances of six state-of-the-art methods, MV, ZenCrowd, PM, CATD, GLAD, and GTIC, on 12 randomly selected real-world datasets, the performance of our algorithm showed many advantages: no need to set complex parameters, faster running speed, and significantly higher accuracy.},
journal = {ACM Trans. Knowl. Discov. Data},
month = feb,
articleno = {46},
numpages = {20},
keywords = {single-choice classification, label confidence, clustering, crowdsourcing truth inference, Truth discovery}
}

@article{10.1145/3487580,
author = {Xu, Jia and Zhou, Yuanhang and Chen, Gongyu and Ding, Yuqing and Yang, Dejun and Liu, Linfeng},
title = {Topic-aware Incentive Mechanism for Task Diffusion in Mobile Crowdsourcing through Social Network},
year = {2021},
issue_date = {February 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {22},
number = {1},
issn = {1533-5399},
url = {https://doi.org/10.1145/3487580},
doi = {10.1145/3487580},
abstract = {Crowdsourcing has become an efficient paradigm to utilize human intelligence to perform tasks that are challenging for machines. Many incentive mechanisms for crowdsourcing systems have been proposed. However, most of existing incentive mechanisms assume that there are sufficient participants to perform crowdsourcing tasks. In large-scale crowdsourcing scenarios, this assumption may be not applicable. To address this issue, we diffuse the crowdsourcing tasks in social network to increase the number of participants. To make the task diffusion more applicable to crowdsourcing system, we enhance the classic Independent Cascade model so the influence is strongly connected with both the types and topics of tasks. Based on the tailored task diffusion model, we formulate the Budget Feasible Task Diffusion (BFTD) problem for maximizing the value function of platform with constrained budget. We design a parameter estimation algorithm based on Expectation Maximization algorithm to estimate the parameters in proposed task diffusion model. Benefitting from the submodular property of the objective function, we apply the budget-feasible incentive mechanism, which satisfies desirable properties of computational efficiency, individual rationality, budget-feasible, truthfulness, and guaranteed approximation, to stimulate the task diffusers. The simulation results based on two real-world datasets show that our incentive mechanism can improve the number of active users and the task completion rate by 9.8\% and 11\%, on average.},
journal = {ACM Trans. Internet Technol.},
month = dec,
articleno = {23},
numpages = {23},
keywords = {EM algorithm, reverse auction, incentive mechanism, social network, Mobile crowdsourcing}
}

@article{10.1145/3555649,
author = {Tsvetkova, Milena and M\"{u}ller, Sebastian and Vuculescu, Oana and Ham, Haylee and Sergeev, Rinat A.},
title = {Relative Feedback Increases Disparities in Effort and Performance in Crowdsourcing Contests: Evidence from a Quasi-Experiment on Topcoder},
year = {2022},
issue_date = {November 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {6},
number = {CSCW2},
url = {https://doi.org/10.1145/3555649},
doi = {10.1145/3555649},
abstract = {Rankings and leaderboards are often used in crowdsourcing contests and online communities to motivate individual contributions but feedback based on social comparison can also have negative effects. Here, we study the unequal effects of such feedback on individual effort and performance for individuals of different ability. We hypothesize that the effects of social comparison differ for top performers and bottom performers in a way that the inequality between the two increases. We use a quasi-experimental design to test our predictions with data from Topcoder, a large online crowdsourcing platform that publishes computer programming contests. We find that in contests where the submitted code is evaluated against others' submissions, rather than using an absolute scale, top performers increase their effort while bottom performers decrease it. As a result, relative scoring leads to better outcomes for those at the top but lower engagement for bottom performers. Our findings expose an important but overlooked drawback from using gamified competitions, rankings, and relative evaluations, with potential implications for crowdsourcing markets, online learning environments, online communities, and organizations in general.},
journal = {Proc. ACM Hum.-Comput. Interact.},
month = nov,
articleno = {536},
numpages = {27},
keywords = {task performance, task effort, feedback giving, engagement, crowdsourcing contests}
}

@article{10.1109/TASLP.2023.3325135,
author = {Chen, Huiyao and Sun, Yueheng and Zhang, Meishan and Zhang, Min},
title = {Automatic Noise Generation and Reduction for Text Classification},
year = {2023},
issue_date = {2024},
publisher = {IEEE Press},
volume = {32},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2023.3325135},
doi = {10.1109/TASLP.2023.3325135},
abstract = {Label noise is an important issue in machine learning, which might lead to negative influences on various tasks. Given that real benchmarks for evaluation of noise reduction methods are limited, plenty of studies construct pseudo noisy data to verify their proposed methods. However, very few works have realized the rationality of the noise generation strategies. If the generated pseudo datasets are biased, their final conclusions might also be problematic. In this work, we focus on text classification of natural language processing (NLP) to investigate various pseudo noise generation methods, which is the first work of this line for NLP. In particular, we compare the noise generated with crowdsourcing noise, a kind of real noise as gold-standard, to evaluate these noise generation methods. After then, we measure and compare the performance of representative noise reduction methods respectively based on the data of crowdsourcing and our top-ranked pseudo noisy generation strategies. We conduct experiments on five text classification datasets, offering detailed comparison results as well as discussions.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = oct,
pages = {139–150},
numpages = {12}
}

@article{10.1145/3555595,
author = {Huang, Keman and Zhou, Jilei and Chen, Shao},
title = {Being a Solo Endeavor or Team Worker in Crowdsourcing Contests? It is a Long-term Decision You Need to Make},
year = {2022},
issue_date = {November 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {6},
number = {CSCW2},
url = {https://doi.org/10.1145/3555595},
doi = {10.1145/3555595},
abstract = {Workers in crowdsourcing are evolving from one-off, independent micro-workers to on-demand collaborators with a long-term orientation. They were expected to collaborate as transient teams to solve more complex, non-trivial tasks. However, collaboration as a team may not be as prevalent as possible, given the lack of support for synchronous collaboration and the "competition, collaboration but transient" nature of crowdsourcing. Aiming at unfolding how individuals collaborate as a transient team and how such teamwork can affect an individual's long-term success, this study investigates the individuals' collaborations on Kaggle, a crowdsourcing contest platform for data analysis. The analysis reveals a growing trend of collaborating as a transient team, which is influenced by contest designs like complexity and reward. However, compared with working independently, the surplus of teamwork in a contest varies over time. Furthermore, the teamwork experience is beneficial for individuals in the short term and long term. Our study distinguishes the team-related intellectual capital and solo-related intellectual capital, and finds a path dependency effect for the individual to work solely or collectively. These findings allow us to contribute insights into the collaborative strategies for crowd workers, contest designers, and platform operators like Kaggle.},
journal = {Proc. ACM Hum.-Comput. Interact.},
month = nov,
articleno = {494},
numpages = {32},
keywords = {transient team, team worker, solo endeavor, long-term aspect, crowdsourcing contests, crowdsourcing, Kaggle}
}

@article{10.1145/3494522,
author = {Hettiachchi, Danula and Kostakos, Vassilis and Goncalves, Jorge},
title = {A Survey on Task Assignment in Crowdsourcing},
year = {2022},
issue_date = {March 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {55},
number = {3},
issn = {0360-0300},
url = {https://doi.org/10.1145/3494522},
doi = {10.1145/3494522},
abstract = {Quality improvement methods are essential to gathering high-quality crowdsourced data, both for research and industry applications. A popular and broadly applicable method is task assignment that dynamically adjusts crowd workflow parameters. In this survey, we review task assignment methods that address: heterogeneous task assignment, question assignment, and plurality problems in crowdsourcing. We discuss and contrast how these methods estimate worker performance, and highlight potential challenges in their implementation. Finally, we discuss future research directions for task assignment methods, and how crowdsourcing platforms and other stakeholders can benefit from them.},
journal = {ACM Comput. Surv.},
month = feb,
articleno = {49},
numpages = {35},
keywords = {worker attributes, question assignment, plurality problem, heterogeneous task assignment, data quality, Crowdsourcing}
}

@article{10.1145/3600228,
author = {Mirishkar, Ganesh S. and Raju V, Vishnu Vidyadhara and Naroju, Meher Dinesh and Maity, Sudhamay and Yalla, Prakash and Vuppala, Anil Kumar},
title = {IIITH-CSTD Corpus: Crowdsourced Strategies for the Collection of a Large-scale Telugu Speech Corpus},
year = {2023},
issue_date = {July 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {22},
number = {7},
issn = {2375-4699},
url = {https://doi.org/10.1145/3600228},
doi = {10.1145/3600228},
abstract = {Due to the lack of a large annotated speech corpus, many low-resource Indian languages struggle to utilize recent advancements in deep neural network architectures for Automatic Speech Recognition (ASR) tasks. Collecting large-scale databases is an expensive and time-consuming task. Current approaches lack extensive traditional expert-based data acquisition guidelines, as they are tedious and complex. In this work, we present the International Institute of Information Technology Hyderabad-Crowd Sourced Telugu Database (IIITH-CSTD), a Telugu corpus collected through crowdsourcing. In particular, our main objective is to mitigate the low-resource problem for Telugu. We also present the sources, crowdsourcing pipeline, and the protocols used to collect the corpus for a low-resource language, namely, Telugu. Data of approximately 2,000 hours of transcribed audio is presented and released in this article, covering three major regional dialects of the Telugu language in three different (i.e., read, conversational and spontaneous) speaking styles on topics such as politics, sports, and arts, science, and so on.1 We also present the experimental results of the collected corpus on ASR tasks. We hope this work will motivate researchers to curate large-scale annotated speech data for other low-resource Indic languages.},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = jul,
articleno = {195},
numpages = {26},
keywords = {low-resource languages, resource creation, End-to-End, TDNN, dialects, Speech recognition}
}

@article{10.1145/3492854,
author = {Sun, Yuling and Ma, Xiaojuan and Ye, Kai and He, Liang},
title = {Investigating Crowdworkers' Identify, Perception and Practices in Micro-Task Crowdsourcing},
year = {2022},
issue_date = {January 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {6},
number = {GROUP},
url = {https://doi.org/10.1145/3492854},
doi = {10.1145/3492854},
abstract = {Crowdsourcing is rapidly gaining popularity among academic and business communities. Yet, our understanding of this work way is still in its incipient stage, in particular regarding the increasingly large and diverse crowdworkers. As such, we aim to understand crowdworkers' perception and experience to themselves and their work from their own perspective. We explore this by a mix-methods study of crowdworkers in Ali, one of prominent micro-task crowdsourcing platforms in China. Our findings highlight crowdworker in Ali is not only a coded name, but also an identity with some positive attitudes and beliefs towards work and life. In particular, this identity provides many socio-psychological benefits for crowdworkers, which further contributes to their consistent engagement in Ali and proactive practices to improve crowdworker communities and Ali platform collaboratively. We according suggest that taking crowdworker identity as a lens for crowdsourcing research, and turning attention towards construction and expressions of crowdworkers' identity and values in their own context.},
journal = {Proc. ACM Hum.-Comput. Interact.},
month = jan,
articleno = {35},
numpages = {20},
keywords = {proactive practices, perception, micro-task crowdsourcing, identity, group identity, crowdsourcing in china, ali crowdsourcing}
}

@article{10.1145/3597419,
author = {Chatzivasili, Loukas and Charalambous, Georgia and Kapitsaki, Georgia and Papoutsoglou, Maria and Markakis, Konstantinos and Harasim, Ioustina and Plevraki, Maria and Magoutis, Kostas},
title = {SocioCoast: Design and Implementation of a Data-Driven Platform for Citizen Science in Coastal Areas},
year = {2023},
issue_date = {September 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {4},
number = {3},
url = {https://doi.org/10.1145/3597419},
doi = {10.1145/3597419},
abstract = {Due to the rapid urbanization and globalization that has occurred in recent decades, cities have undergone significant changes and transformations. The smart city concept can lead to improvements in the life quality of citizens and contribute to the city’s sustainability by collecting humans’ real-time observations. More specifically in smart coastal areas, citizen’s science data can be utilized not only to increase the effectiveness of its services like smart tourism and smart transit but also to raise awareness about environmental issues. Emphasizing on coastal areas under the Blue Flag program and the Natura 2000 network, in this work, we introduce a system that aims to collect crowdsourcing data from citizens and tourists and combine or compare them with existing knowledge for these areas. We describe the design and implementation: the back-end, its data sources and also the front-end outputs, a web knowledge platform, and a crowdsourcing mobile application. Geospatial information as well as historical or real-time data collected will be provided to users for further exploitation and creation of their own applications. A demonstration of the system is also provided to showcase its usefulness in increasing attractiveness and raising awareness for coastal areas by offering users rich and updated information.},
journal = {Digit. Gov.: Res. Pract.},
month = sep,
articleno = {13},
numpages = {21},
keywords = {data visualization, data collection, crowdsourcing, smart beaches, coastal areas, Citizen science}
}

@article{10.1145/3479586,
author = {Pei, Weiping and Yang, Zhiju and Chen, Monchu and Yue, Chuan},
title = {Quality Control in Crowdsourcing based on Fine-Grained Behavioral Features},
year = {2021},
issue_date = {October 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {5},
number = {CSCW2},
url = {https://doi.org/10.1145/3479586},
doi = {10.1145/3479586},
abstract = {Crowdsourcing is popular for large-scale data collection and labeling, but a major challenge is on detecting low-quality submissions. Recent studies have demonstrated that behavioral features of workers are highly correlated with data quality and can be useful in quality control. However, these studies primarily leveraged coarsely extracted behavioral features, and did not further explore quality control at the fine-grained level, i.e., the annotation unit level. In this paper, we investigate the feasibility and benefits of using fine-grained behavioral features, which are the behavioral features finely extracted from a worker's individual interactions with each single unit in a subtask, for quality control in crowdsourcing. We design and implement a framework named Fine-grained Behavior-based Quality Control (FBQC) that specifically extracts fine-grained behavioral features to provide three quality control mechanisms: (1) quality prediction for objective tasks, (2) suspicious behavior detection for subjective tasks, and (3) unsupervised worker categorization. Using the FBQC framework, we conduct two real-world crowdsourcing experiments and demonstrate that using fine-grained behavioral features is feasible and beneficial in all three quality control mechanisms. Our work provides clues and implications for helping job requesters or crowdsourcing platforms to further achieve better quality control.},
journal = {Proc. ACM Hum.-Comput. Interact.},
month = oct,
articleno = {442},
numpages = {28},
keywords = {behavior analysis, crowdsourcing, quality control}
}

@article{10.1145/3597201,
author = {Roitero, Kevin and Barbera, David La and Soprano, Michael and Demartini, Gianluca and Mizzaro, Stefano and Sakai, Tetsuya},
title = {How Many Crowd Workers Do I Need? On Statistical Power when Crowdsourcing Relevance Judgments},
year = {2023},
issue_date = {January 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {42},
number = {1},
issn = {1046-8188},
url = {https://doi.org/10.1145/3597201},
doi = {10.1145/3597201},
abstract = {To scale the size of Information Retrieval collections, crowdsourcing has become a common way to collect relevance judgments at scale. Crowdsourcing experiments usually employ 100–10,000 workers, but such a number is often decided in a heuristic way. The downside is that the resulting dataset does not have any guarantee of meeting predefined statistical requirements as, for example, have enough statistical power to be able to distinguish in a statistically significant way between the relevance of two documents.We propose a methodology adapted from literature on sound topic set size design, based on t-test and ANOVA, which aims at guaranteeing the resulting dataset to meet a predefined set of statistical requirements. We validate our approach on several public datasets.Our results show that we can reliably estimate the recommended number of workers needed to achieve statistical power, and that such estimation is dependent on the topic, while the effect of the relevance scale is limited. Furthermore, we found that such estimation is dependent on worker features such as agreement. Finally, we describe a set of practical estimation strategies that can be used to estimate the worker set size, and we also provide results on the estimation of document set sizes.},
journal = {ACM Trans. Inf. Syst.},
month = aug,
articleno = {21},
numpages = {26},
keywords = {relevance judgments, statistical analysis, Crowdsourcing}
}

@article{10.1145/3555223,
author = {Chhibber, Nalin and Goh, Joslin and Law, Edith},
title = {Teachable Conversational Agents for Crowdwork: Effects on Performance and Trust},
year = {2022},
issue_date = {November 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {6},
number = {CSCW2},
url = {https://doi.org/10.1145/3555223},
doi = {10.1145/3555223},
abstract = {Traditional crowdsourcing has mostly been viewed as requester-worker interaction where requesters publish tasks to solicit input from human crowdworkers. While most of this research area is catered towards the interest of requesters, we view this workflow as a teacher-learner interaction scenario where one or more human-teachers solve Human Intelligence Tasks to train machine learners. In this work, we explore how teachable machine learners can impact their human-teachers, and whether they form a trustable relation that can be relied upon for task delegation in the context of crowdsourcing. Specifically, we focus our work on teachable agents that learn to classify news articles while also guiding the teaching process through conversational interventions. In a two-part study, where several crowd workers individually teach the agent, we investigate whether this learning by teaching approach benefits human-machine collaboration, and whether it leads to trustworthy AI agents that crowd workers would delegate tasks to. Results demonstrate the benefits of the learning by teaching approach, in terms of perceived usefulness for crowdworkers, and the dynamics of trust built through the teacher-learner interaction.},
journal = {Proc. ACM Hum.-Comput. Interact.},
month = nov,
articleno = {331},
numpages = {21},
keywords = {trusting conversational AI, learning by teaching, interactive machine learning, human-AI interaction, conversational interactions}
}

@article{10.14778/3561261.3561266,
author = {Yang, Yi and Cheng, Yurong and Yuan, Ye and Wang, Guoren and Chen, Lei and Sun, Yongjiao},
title = {Privacy-preserving cooperative online matching over spatial crowdsourcing platforms},
year = {2022},
issue_date = {September 2022},
publisher = {VLDB Endowment},
volume = {16},
number = {1},
issn = {2150-8097},
url = {https://doi.org/10.14778/3561261.3561266},
doi = {10.14778/3561261.3561266},
abstract = {With the continuous development of spatial crowdsourcing platform, online task assignment problem has been widely studied as a typical problem in spatial crowdsourcing. Most of the existing studies are based on a single-platform task assignment to maximize the platform's revenue. Recently, cross online task assignment has been proposed, aiming at increasing the mutual benefit through cooperations. However, existing methods fail to consider the data privacy protection in the process of cooperation and cause the leakage of sensitive data such as the location of a request and the historical data of cooperative platforms. In this paper, we propose Privacy-preserving Cooperative Online Matching (PCOM), which protects the privacy of the users and workers on their respective platforms. We design a PCOM framework and provide theoretical proof that the framework satisfies the differential privacy property. We then propose two PCOM algorithms based on two different privacy-preserving strategies. Extensive experiments on real and synthetic datasets confirm the effectiveness and efficiency of our algorithms.},
journal = {Proc. VLDB Endow.},
month = sep,
pages = {51–63},
numpages = {13}
}

@article{10.1007/s00778-019-00568-7,
author = {Tong, Yongxin and Zhou, Zimu and Zeng, Yuxiang and Chen, Lei and Shahabi, Cyrus},
title = {Spatial crowdsourcing: a survey},
year = {2019},
issue_date = {Jan 2020},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {29},
number = {1},
issn = {1066-8888},
url = {https://doi.org/10.1007/s00778-019-00568-7},
doi = {10.1007/s00778-019-00568-7},
abstract = {Crowdsourcing is a computing paradigm where humans are actively involved in a computing task, especially for tasks that are intrinsically easier for humans than for computers. Spatial crowdsourcing is an increasing popular category of crowdsourcing in the era of mobile Internet and sharing economy, where tasks are spatiotemporal and must be completed at a specific location and time. In fact, spatial crowdsourcing has stimulated a series of recent industrial successes including sharing economy for urban services (Uber and Gigwalk) and spatiotemporal data collection (OpenStreetMap and Waze). This survey dives deep into the challenges and techniques brought by the unique characteristics of spatial crowdsourcing. Particularly, we identify four core algorithmic issues in spatial crowdsourcing: (1) task assignment, (2) quality control, (3) incentive mechanism design, and (4) privacy protection. We conduct a comprehensive and systematic review of existing research on the aforementioned four issues. We also analyze representative spatial crowdsourcing applications and explain how they are enabled by these four technical issues. Finally, we discuss open questions that need to be addressed for future spatial crowdsourcing research and applications.},
journal = {The VLDB Journal},
month = aug,
pages = {217–250},
numpages = {34},
keywords = {Privacy protection, Incentive mechanism, Quality control, Task assignment, Spatial crowdsourcing}
}

@article{10.1145/3479519,
author = {Matsubara, Masaki and Borromeo, Ria Mae and Amer-Yahia, Sihem and Morishima, Atsuyuki},
title = {Task Assignment Strategies for Crowd Worker Ability Improvement},
year = {2021},
issue_date = {October 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {5},
number = {CSCW2},
url = {https://doi.org/10.1145/3479519},
doi = {10.1145/3479519},
abstract = {Workers are the most important resource in crowdsourcing. However, only investing in worker-centric needs, such as skill improvement, often conflicts with short-term platform-centric needs, such as task throughput. This paper studies learning strategies in task assignment in crowdsourcing and their impact on platform-centric needs. We formalize learning potential of individual tasks and collaborative tasks, and devise an iterative task assignment and completion approach that implements strategies grounded in learning theories. We conduct experiments to compare several learning strategies in terms of skill improvement, and in terms of task throughput and contribution quality. We discuss how our findings open new research directions in learning and collaboration.},
journal = {Proc. ACM Hum.-Comput. Interact.},
month = oct,
articleno = {375},
numpages = {20},
keywords = {crowdsourcing for learning, formalization, task assignment}
}

@article{10.1145/3555627,
author = {Bragg, Danielle and Glasser, Abraham and Minakov, Fyodor and Caselli, Naomi and Thies, William},
title = {Exploring Collection of Sign Language Videos through Crowdsourcing},
year = {2022},
issue_date = {November 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {6},
number = {CSCW2},
url = {https://doi.org/10.1145/3555627},
doi = {10.1145/3555627},
abstract = {Inadequate sign language data currently impedes advancement of sign language ML and AI. Training on existing datasets results in limited models due to small size, and lack of diverse signers in real-world settings. Complex labeling problems in particular often limit scale. In this work, we explore the potential for crowdsourcing to help overcome these barriers. To do this, we ran a user study with exploratory crowdsourcing tasks designed to support scalability: 1) to record videos of specific content -- thereby enabling automatic, scalable labeling -- and 2) to perform quality control checks for execution consistency -- further reducing post-processing requirements. We also provided workers with a searchable view of the crowdsourced dataset, to boost engagement and transparency and align with Deaf community values. Our user study included 29 participants using our exploratory tasks to record 1906 videos and perform 2331 quality control checks. Our results suggest that a crowd of signers may be able to generate high-quality recordings and perform reliable quality control, and that the signing community values visibility into the resulting dataset.},
journal = {Proc. ACM Hum.-Comput. Interact.},
month = nov,
articleno = {514},
numpages = {24},
keywords = {sign language, machine learning, education, dataset, data, crowdsourcing, corpus, citizen science}
}

@article{10.1109/TNET.2020.3018448,
author = {Xia, Hui and Zhang, Rui and Cheng, Xiangguo and Qiu, Tie and Wu, Dapeng Oliver},
title = {Two-Stage Game Design of Payoff Decision-Making Scheme for Crowdsourcing Dilemmas},
year = {2020},
issue_date = {Dec. 2020},
publisher = {IEEE Press},
volume = {28},
number = {6},
issn = {1063-6692},
url = {https://doi.org/10.1109/TNET.2020.3018448},
doi = {10.1109/TNET.2020.3018448},
abstract = {Crowdsourcing uses collective intelligence to finish complicated tasks and is widely applied in many fields. However, the crowdsourcing dilemmas between the task requester and the task completer restrict the efficiency of system severely, e.g., the cooperation dilemma leads to the failure in the interactions and the quality of service dilemma results in the inability of task completer to provide high-quality service. Current research usually focuses on solving only one aforementioned dilemma and fails to integrate perfectly with the service architectural pattern of crowdsourcing systems. In this article, combined with the crowdsourcing interaction phase, we limit the objects that cause dilemma and propose a &lt;inline-formula&gt; &lt;tex-math notation="LaTeX"&gt;$boldsymbol {t}$ &lt;/tex-math&gt;&lt;/inline-formula&gt;wo-stage &lt;inline-formula&gt; &lt;tex-math notation="LaTeX"&gt;$boldsymbol {g}$ &lt;/tex-math&gt;&lt;/inline-formula&gt;ame &lt;inline-formula&gt; &lt;tex-math notation="LaTeX"&gt;$boldsymbol {p}$ &lt;/tex-math&gt;&lt;/inline-formula&gt;ayoff &lt;inline-formula&gt; &lt;tex-math notation="LaTeX"&gt;$boldsymbol {d}$ &lt;/tex-math&gt;&lt;/inline-formula&gt;ecision-making scheme (&lt;italic&gt;TGPD&lt;/italic&gt;) to overcome these shortcomings. To solve the cooperation dilemma between the requester and the crowdsourcing platform, we first propose a dynamic payment method based on the reputation-quality rules for the task requester, and then develop a cos-evaluation algorithm to estimate platform’s cost, last design a co-determine algorithm to determine whether the platform adopts a cooperative strategy. To address the quality of service dilemma between the crowdsourcing platform and the workers, we first present an auction-screening method to estimate the reasonable recruitment range of workers which can be optimized by the result of cos-evaluation algorithm, and then use a reward distribution method to motivate workers to complete tasks with high quality and on time. The experimental results indicate that our new scheme successfully increases the worker’s and platforms’ payoffs at the same time, improves the accuracy of screening workers, enhances the worker’s quality of service, and decreases the platform’s cost.},
journal = {IEEE/ACM Trans. Netw.},
month = dec,
pages = {2741–2754},
numpages = {14}
}

@article{10.1145/3431806,
author = {Gama, Kiev},
title = {Successful Models of Hackathons and Innovation Contests to Crowdsource Rapid Responses to COVID-19},
year = {2020},
issue_date = {April 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {2},
number = {2},
url = {https://doi.org/10.1145/3431806},
doi = {10.1145/3431806},
abstract = {To help fighting the many adversarial effects that the COVID-19 pandemic brought, many initiatives supported by public, private and non-governmental organizations are crowdsourcing the development of solutions through hackathons and innovation contests. This article discusses the models and common aspects of two successful initiatives led by governmental institutions that used a crowdsourcing approach to foster solutions aiming at pandemic-related issues: the EUVsVirus (Europe) and the MPLabs COVID-19 Challenge (Brazil).},
journal = {Digit. Gov.: Res. Pract.},
month = dec,
articleno = {20},
numpages = {7},
keywords = {open innovation, online hackathons, innovation contests, crowdsourcing, COVID-19}
}

@article{10.1145/3569470,
author = {Xie, Zhiqing and Luo, Haiyong and Zhang, Xiaotian and Xiong, Hao and Zhao, Fang and Li, Zhaohui and Ye, Qi and Rong, Bojie and Gao, Jiuchong},
title = {TransFloor: Transparent Floor Localization for Crowdsourcing Instant Delivery},
year = {2023},
issue_date = {December 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {6},
number = {4},
url = {https://doi.org/10.1145/3569470},
doi = {10.1145/3569470},
abstract = {Smart on-demand delivery services require accurate indoor localization to enhance the system-human synergy experience of couriers in complex multi-story malls and platform construction. Floor localization is an essential part of indoor positioning, which can provide floor/altitude data support for upper-level 3D indoor navigation services (e.g., delivery route planning) to improve delivery efficiency and optimize order dispatching strategies. We argue that due to label dependence and device dependence, the existing floor localization methods cannot be flexibly deployed on a large scale in numerous multi-story malls across the country, nor can they apply to all couriers/users on the platform. This paper proposes a novel self-evolving and user-transparent floor localization system named TransFloor, based on crowdsourcing delivery data (e.g., order status and sensors data) without additional label investment and specialized equipment constraints. TransFloor consists of an unsupervised barometer-based module--IOD-TKPD and an NLP-inspired Wi-Fi-based module--Wifi2Vec, and Self-Labeling is a perfect bridge between both to completely achieve label-free and device-independent floor positioning. In addition, TransFloor is designed as a lightweight plugin embedded into the platform without refactoring the existing architecture, and it has been deployed nationwide to adaptively launch real-time accurate 3D/floor positioning services for numerous crowdsourcing couriers. We evaluate TransFloor on real-world records from an instant delivery platform (involving 672,282 orders, 7,390 couriers, and 6,206 merchants in 388 malls during two months). It can achieve an average accuracy of 94.61\% and demonstrate good robustness to device heterogeneity and adaptive durability, outperforming existing state-of-the-art methods. Crucially, it can effectively improve user satisfaction and reduce overdue delivery by providing accurate floor navigation information in complex multi-story malls. As a case study, the platform reduces erroneous order scheduling by 60\% and overdue delivery by 2.7\%, and increases delivery efficiency by reducing courier arrival time by 12.27 seconds accounting for 7.29\%. We believe that the key ideas of TransFloor can be extended to other crowdsourcing scenarios for the public further.},
journal = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
month = jan,
articleno = {189},
numpages = {30},
keywords = {Instant delivery, Floor localization, Crowdsourcing, 3D indoor localization}
}

@article{10.1145/3434175,
author = {Anjum, Samreen and Lin, Chi and Gurari, Danna},
title = {CrowdMOT: Crowdsourcing Strategies for Tracking Multiple Objects in Videos},
year = {2021},
issue_date = {December 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {4},
number = {CSCW3},
url = {https://doi.org/10.1145/3434175},
doi = {10.1145/3434175},
abstract = {Crowdsourcing is a valuable approach for tracking objects in videos in a more scalable manner than possible with domain experts. However, existing frameworks do not produce high quality results with non-expert crowdworkers, especially for scenarios where objects split. To address this shortcoming, we introduce a crowdsourcing platform called CrowdMOT, and investigate two micro-task design decisions: (1) whether to decompose the task so that each worker is in charge of annotating all objects in a sub-segment of the video versus annotating a single object across the entire video, and (2) whether to show annotations from previous workers to the next individuals working on the task. We conduct experiments on a diversity of videos which show both familiar objects (aka - people) and unfamiliar objects (aka - cells). Our results highlight strategies for efficiently collecting higher quality annotations than observed when using strategies employed by today's state-of-art crowdsourcing system.},
journal = {Proc. ACM Hum.-Comput. Interact.},
month = jan,
articleno = {266},
numpages = {25},
keywords = {video annotation, crowdsourcing, computer vision}
}

@article{10.1145/3478117,
author = {Ding, Yi and Guo, Baoshen and Zheng, Lin and Lu, Mingming and Zhang, Desheng and Wang, Shuai and Son, Sang Hyuk and He, Tian},
title = {A City-Wide Crowdsourcing Delivery System with Reinforcement Learning},
year = {2021},
issue_date = {Sept 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {5},
number = {3},
url = {https://doi.org/10.1145/3478117},
doi = {10.1145/3478117},
abstract = {The revolution of online shopping in recent years demands corresponding evolution in delivery services in urban areas. To cater to this trend, delivery by the crowd has become an alternative to the traditional delivery services thanks to the advances in ubiquitous computing. Notably, some studies use public transportation for crowdsourcing delivery, given its low-cost delivery network with millions of passengers as potential couriers. However, multiple practical impact factors are not considered in existing public-transport-based crowdsourcing delivery studies due to a lack of data and limited ubiquitous computing infrastructures in the past. In this work, we design a crowdsourcing delivery system based on public transport, considering the practical factors of time constraints, multi-hop delivery, and profits. To incorporate the impact factors, we build a reinforcement learning model to learn the optimal order dispatching strategies from massive passenger data and package data. The order dispatching problem is formulated as a sequential decision making problem for the packages routing, i.e., select the next station for the package. A delivery time estimation module is designed to accelerate the training process and provide statistical delivery time guarantee. Three months of real-world public transportation data and one month of package delivery data from an on-demand delivery platform in Shenzhen are used in the evaluation. Compared with existing crowdsourcing delivery algorithms and widely used baselines, we achieve a 40\% increase in profit rates and a 29\% increase in delivery rates. Comparison with other reinforcement learning algorithms shows that we can improve the profit rate and the delivery rate by 9\% and 8\% by using time estimation in action filtering. We share the data used in the project to the community for other researchers to validate our results and conduct further research.1 [1].},
journal = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
month = sep,
articleno = {97},
numpages = {22},
keywords = {Sharing Economy, Reinforcement Learning, Crowdsourcing, Crowdsourced Labor}
}

@article{10.1145/3291933,
author = {Gummidi, Srinivasa Raghavendra Bhuvan and Xie, Xike and Pedersen, Torben Bach},
title = {A Survey of Spatial Crowdsourcing},
year = {2019},
issue_date = {June 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {44},
number = {2},
issn = {0362-5915},
url = {https://doi.org/10.1145/3291933},
doi = {10.1145/3291933},
abstract = {Widespread use of advanced mobile devices has led to the emergence of a new class of crowdsourcing called spatial crowdsourcing. Spatial crowdsourcing advances the potential of a crowd to perform tasks related to real-world scenarios involving physical locations, which were not feasible with conventional crowdsourcing methods. The main feature of spatial crowdsourcing is the presence of spatial tasks that require workers to be physically present at a particular location for task fulfillment. Research related to this new paradigm has gained momentum in recent years, necessitating a comprehensive survey to offer a bird’s-eye view of the current state of spatial crowdsourcing literature. In this article, we discuss the spatial crowdsourcing infrastructure and identify the fundamental differences between spatial and conventional crowdsourcing. Furthermore, we provide a comprehensive view of the existing literature by introducing a taxonomy, elucidate the issues/challenges faced by different components of spatial crowdsourcing, and suggest potential research directions for the future.},
journal = {ACM Trans. Database Syst.},
month = mar,
articleno = {8},
numpages = {46},
keywords = {task scheduling, task matching, task assignment, spatial databases, spatial crowdsourcing, rewards, quality assurance, location privacy, incentive mechanism, Algorithms}
}

@article{10.1145/3571816,
author = {Xu, Long and Park, Su Jin and Lee, Sangwon},
title = {Color2Vec: Web-Based Modeling of Word-Color Association with Sociocultural Contexts},
year = {2023},
issue_date = {August 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {30},
number = {4},
issn = {1073-0516},
url = {https://doi.org/10.1145/3571816},
doi = {10.1145/3571816},
abstract = {Color design has long benefited from the statistical analysis of public taste and, more recently, from crowdsourcing to discover fresh and popular ideas. However, the current color dictionary is considerably restricted in terms of the scope of expressible design concepts and the control of target demographics. We propose a search-engine-based color palette generator inspired by Natural Language Processing algorithms that filter and cluster semantically related words. The post-evaluation reveals that our results not only faithfully realize the given keywords but are notable indicators of inter-group dynamics; the differential recognition of the other group's identity colors reflects the direction of historic, geographic, or cultural influence.},
journal = {ACM Trans. Comput.-Hum. Interact.},
month = sep,
articleno = {51},
numpages = {29},
keywords = {data-driven design, Word Color Association, Natural Language Processing, Color emotion}
}

@article{10.1145/2873060,
author = {Walk, Simon and Helic, Denis and Geigl, Florian and Strohmaier, Markus},
title = {Activity Dynamics in Collaboration Networks},
year = {2016},
issue_date = {May 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {10},
number = {2},
issn = {1559-1131},
url = {https://doi.org/10.1145/2873060},
doi = {10.1145/2873060},
abstract = {Many online collaboration networks struggle to gain user activity and become self-sustaining due to the ramp-up problem or dwindling activity within the system. Prominent examples include online encyclopedias such as (Semantic) MediaWikis, Question and Answering portals such as StackOverflow, and many others. Only a small fraction of these systems manage to reach self-sustaining activity, a level of activity that prevents the system from reverting to a nonactive state. In this article, we model and analyze activity dynamics in synthetic and empirical collaboration networks. Our approach is based on two opposing and well-studied principles: (i) without incentives, users tend to lose interest to contribute and thus, systems become inactive, and (ii) people are susceptible to actions taken by their peers (social or peer influence). With the activity dynamics model that we introduce in this article we can represent typical situations of such collaboration networks. For example, activity in a collaborative network, without external impulses or investments, will vanish over time, eventually rendering the system inactive. However, by appropriately manipulating the activity dynamics and/or the underlying collaboration networks, we can jump-start a previously inactive system and advance it toward an active state. To be able to do so, we first describe our model and its underlying mechanisms. We then provide illustrative examples of empirical datasets and characterize the barrier that has to be breached by a system before it can become self-sustaining in terms of critical mass and activity dynamics. Additionally, we expand on this empirical illustration and introduce a new metric p—the Activity Momentum—to assess the activity robustness of collaboration networks.},
journal = {ACM Trans. Web},
month = may,
articleno = {11},
numpages = {32},
keywords = {network science, critical mass, collaboration networks, activity momentum, activity dynamics, Dynamical systems}
}

@article{10.1109/TNET.2021.3105427,
author = {Shi, Zhiguo and Yang, Guang and Gong, Xiaowen and He, Shibo and Chen, Jiming},
title = {Quality-Aware Incentive Mechanisms Under Social Influences in Data Crowdsourcing},
year = {2021},
issue_date = {Feb. 2022},
publisher = {IEEE Press},
volume = {30},
number = {1},
issn = {1063-6692},
url = {https://doi.org/10.1109/TNET.2021.3105427},
doi = {10.1109/TNET.2021.3105427},
abstract = {Incentive mechanism design and quality control are two key challenges in data crowdsourcing, because of the need for recruitment of crowd users and their limited capabilities. Without considering users’ social influences, existing mechanisms often result in low efficiency in terms of the platform’s cost. In this paper, we exploit social influences among users as incentives to motivate users’ participation, in order to reduce the cost of recruiting users. Based on social influences, we design incentive mechanisms with the goal of achieving high quality of crowdsourced data and low cost of incentivizing users’ participation. Specifically, we consider three scenarios. In the full information scenario, we design task assignment and user recruitment mechanisms to optimize the data quality while reducing the incentive cost. In the partial information scenario, users’ qualities and costs are unknown. We exploit the correlation between tasks to overcome the information asymmetry, for both cases of opportunistic crowdsourcing and participatory crowdsourcing. Further, in the dynamic social influence scenario, we investigate the dynamics of users’ social influences and design extra rewards for users to make full use of the social influence and achieve maximum cost saving. We evaluate the incentive mechanisms using numerical results, which demonstrate their effectiveness.},
journal = {IEEE/ACM Trans. Netw.},
month = aug,
pages = {176–189},
numpages = {14}
}

@article{10.1145/3476063,
author = {Qiu, Sihang and Bozzon, Alessandro and Birk, Max V. and Gadiraju, Ujwal},
title = {Using Worker Avatars to Improve Microtask Crowdsourcing},
year = {2021},
issue_date = {October 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {5},
number = {CSCW2},
url = {https://doi.org/10.1145/3476063},
doi = {10.1145/3476063},
abstract = {The future of crowd work has been identified to depend on worker satisfaction, but we lack a thorough understanding of how worker satisfaction can be increased in microtask crowdsourcing. Prior work has shown that one solution is to build tasks that are engaging. To facilitate engagement, two methods that have received attention in recent HCI literature are the use of video games and conversational interfaces. While these are largely different techniques, they aim for the same goal of reducing worker burden and increasing engagement in a task. On one hand, video games have huge motivation potential and translating game design elements for motivational purposes has shown positive effects. Recent work in games research has shown that the use of player avatars is effective in fostering interest, enjoyment, and other aspects pertaining to intrinsic motivation. On the other hand, conversational interfaces have been argued to have advantages over traditional GUIs due to facilitating a more human-like interaction. Conversational microtasking has recently been proposed to improve worker engagement in microtask marketplaces. The contexts of games and crowd work are underlined by the need to motivate and engage participants, yet the potential of using worker avatars to promote self-identification and improve worker satisfaction in microtask crowdsourcing has remained unexplored. Addressing this knowledge gap, we carried out a between-subject study involving 360 crowd workers. We investigated how worker avatars influence quality related outcomes of workers and their perceived experience, in conventional web and novel conversational interfaces. We equipped workers with the functionality of customizing their avatars, and selecting characterizations for their avatars, to understand whether identifying with an avatar can increase the motivation of workers. We found that using worker avatars with conversational interfaces can effectively reduce cognitive workload and increase worker retention. Our results indicate the occurrence of similarity and wishful avatar identification in crowdsourcing. Our findings have important implications in alleviating workers' perceived workload and on the design of crowdsourcing microtasks.},
journal = {Proc. ACM Hum.-Comput. Interact.},
month = oct,
articleno = {322},
numpages = {28},
keywords = {avatar, chatbot, crowdsourcing, motivation, perceived workload}
}

@article{10.1145/3226028,
author = {Jiang, Jiuchuan and An, Bo and Jiang, Yichuan and Lin, Donghui and Bu, Zhan and Cao, Jie and Hao, Zhifeng},
title = {Understanding Crowdsourcing Systems from a Multiagent Perspective and Approach},
year = {2018},
issue_date = {June 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {13},
number = {2},
issn = {1556-4665},
url = {https://doi.org/10.1145/3226028},
doi = {10.1145/3226028},
abstract = {Crowdsourcing has recently been significantly explored. Although related surveys have been conducted regarding this subject, each has mainly consisted of a review of a single aspect of crowdsourcing systems or on the application of crowdsourcing in a specific application domain. A crowdsourcing system is a comprehensive set of multiple entities, including various elements and processes. Multiagent computing has already been widely envisioned as a powerful paradigm for modeling autonomous multi-entity systems with adaptation to dynamic environments. Therefore, this article presents a novel multiagent perspective and approach to understanding crowdsourcing systems, which can be used to correlate the research on crowdsourcing and multiagent systems and inspire possible interdisciplinary research between the two areas. This article mainly discusses the following two aspects: (1) The multiagent perspective can be used for conducting a comprehensive survey on the state of the art of crowdsourcing, and (2) the multiagent approach can bring about concrete enhancements for crowdsourcing technology and inspire future research directions that enable crowdsourcing research to overcome the typical challenges in crowdsourcing technology. Finally, this article discusses the advantages and disadvantages of the multiagent perspective by comparing it with two other popular perspectives on crowdsourcing: the business perspective and the technical perspective.},
journal = {ACM Trans. Auton. Adapt. Syst.},
month = jul,
articleno = {8},
numpages = {32},
keywords = {multiagent systems, crowdsourcing processes, crowdsourcing elements, Crowdsourcing systems}
}

@article{10.1145/3555542,
author = {Nishal, Sachita and Diakopoulos, Nicholas},
title = {From Crowd Ratings to Predictive Models of Newsworthiness to Support Science Journalism},
year = {2022},
issue_date = {November 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {6},
number = {CSCW2},
url = {https://doi.org/10.1145/3555542},
doi = {10.1145/3555542},
abstract = {The scale of scientific publishing continues to grow, creating overload on science journalists who are inundated with choices for what would be most interesting, important, and newsworthy to cover in their reporting. Our work addresses this problem by considering the viability of creating a predictive model of newsworthiness of scientific articles that is trained using crowdsourced evaluations of newsworthiness. We proceed by first evaluating the potential of crowd-sourced evaluations of newsworthiness by assessing their alignment with expert ratings of newsworthiness, analyzing both quantitative correlations and qualitative rating rationale to understand limitations. We then demonstrate and evaluate a predictive model trained on these crowd ratings together with arXiv article metadata, text, and other computed features. Based on the crowdsourcing protocol we developed, we find that while crowdsourced ratings of newsworthiness often align moderately with expert ratings, there are also notable differences and divergences which limit the approach. Yet despite these limitations we also find that the predictive model we built provides a reasonably precise set of rankings when validated against expert evaluations (P@10 = 0.8, P@15 = 0.67), suggesting that a viable signal can be learned from crowdsourced evaluations of newsworthiness. Based on these findings we discuss opportunities for future work to leverage crowdsourcing and predictive approaches to support journalistic work in discovering and filtering newsworthy information.},
journal = {Proc. ACM Hum.-Comput. Interact.},
month = nov,
articleno = {441},
numpages = {28},
keywords = {science journalism, newsworthiness, news values, crowdsourcing}
}

@article{10.1145/3555611,
author = {Kaufman, Robert A. and Haupt, Michael Robert and Dow, Steven P.},
title = {Who's in the Crowd Matters: Cognitive Factors and Beliefs Predict Misinformation Assessment Accuracy},
year = {2022},
issue_date = {November 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {6},
number = {CSCW2},
url = {https://doi.org/10.1145/3555611},
doi = {10.1145/3555611},
abstract = {Misinformation runs rampant on social media and has been tied to adverse health behaviors such as vaccine hesitancy. Crowdsourcing can be a means to detect and impede the spread of misinformation online. However, past studies have not deeply examined the individual characteristics - such as cognitive factors and biases - that predict crowdworker accuracy at identifying misinformation. In our study (n = 265), Amazon Mechanical Turk (MTurk) workers and university students assessed the truthfulness and sentiment of COVID-19 related tweets as well as answered several surveys on personal characteristics. Results support the viability of crowdsourcing for assessing misinformation and content stance (i.e., sentiment) related to ongoing and politically-charged topics like the COVID-19 pandemic, however, alignment with experts depends on who is in the crowd. Specifically, we find that respondents with high Cognitive Reflection Test (CRT) scores, conscientiousness, and trust in medical scientists are more aligned with experts while respondents with high Need for Cognitive Closure (NFCC) and those who lean politically conservative are less aligned with experts. We see differences between recruitment platforms as well, as our data shows university students are on average more aligned with experts than MTurk workers, most likely due to overall differences in participant characteristics on each platform. Results offer transparency into how crowd composition affects misinformation and stance assessment and have implications on future crowd recruitment and filtering practices.},
journal = {Proc. ACM Hum.-Comput. Interact.},
month = nov,
articleno = {553},
numpages = {18},
keywords = {social media, misinformation, crowdsourcing, bias}
}

@article{10.14778/3554821.3554848,
author = {Xi, Yihai and Wang, Ning and Chen, Xinyu and Zhang, Yiyi and Wang, Zilong and Xu, Zhihong and Wang, Yue},
title = {EasyDR: a human-in-the-loop error detection&amp;repair platform for holistic table cleaning},
year = {2022},
issue_date = {August 2022},
publisher = {VLDB Endowment},
volume = {15},
number = {12},
issn = {2150-8097},
url = {https://doi.org/10.14778/3554821.3554848},
doi = {10.14778/3554821.3554848},
abstract = {Many tables on the web suffer from multi-level and multi-type quality problems, but existing cleaning systems cannot provide a comprehensive quality improvement for them. Most of these systems are designed for solving a specific type of error, so that we need to resort to a number of different cleaning tools (one per error type) to get a high quality table. In this demonstration, we propose a human-in-the-loop cleaning platform EasyDR for detecting and repairing multi-level&amp;multi-type errors in tables. The attendees will experience the following features of EasyDR: 1) Holistic error detection&amp;repair. Users are able to perform a holistic table cleaning in EasyDR where machine algorithms are responsible for error detection while human intelligence is leveraged for error repairing. 2) Human-in-the-loop table cleaning. EasyDR performs an all-round quality diagnosis for the table, and automatically generates crowdsourcing cleaning tasks for the detected errors. To simplify cleaning tasks for crowdsourcing workers, EasyDR provides two task optimization techniques including domain-aware table summarization and difficulty-aware task order optimization. 3) Customizable cleaning mode. EasyDR provides a declarative language for users to customize cleaning tasks flexibly, e.g., selecting target errors, restricting the cleaning scope, defining the cooperation mode for machine and crowd.},
journal = {Proc. VLDB Endow.},
month = aug,
pages = {3578–3581},
numpages = {4}
}

@article{10.14778/3407790.3407839,
author = {Chen, Zhao and Cheng, Peng and Chen, Lei and Lin, Xuemin and Shahabi, Cyrus},
title = {Fair task assignment in spatial crowdsourcing},
year = {2020},
issue_date = {August 2020},
publisher = {VLDB Endowment},
volume = {13},
number = {12},
issn = {2150-8097},
url = {https://doi.org/10.14778/3407790.3407839},
doi = {10.14778/3407790.3407839},
abstract = {With the pervasiveness of mobile devices, wireless broadband and sharing economy, spatial crowdsourcing is becoming part of our daily life. Existing studies on spatial crowdsourcing usually focus on enhancing the platform interests and customer experiences. In this work, however, we study the fair assignment of tasks to workers in spatial crowdsourcing. That is, we aim to assign tasks, considered as a resource in short supply, to individual spatial workers in a fair manner. In this paper, we first formally define an online bi-objective matching problem, namely the Fair and Effective Task Assignment (FETA) problem, with its special cases/variants of it to capture most typical spatial crowdsourcing scenarios. We propose corresponding solutions for each variant of FETA. Particularly, we show that the dynamic sequential variant, which is a generalization of an existing fairness scheduling problem, can be solved with an O(n) fairness cost bound (n is the total number of workers), and give an O(n/m) fairness cost bound for the m-sized general batch case (m is the minimum batch size). Finally, we evaluate the effectiveness and efficiency of our algorithm on both synthetic and real data sets.},
journal = {Proc. VLDB Endow.},
month = jul,
pages = {2479–2492},
numpages = {14}
}

@article{10.1007/s00778-022-00740-6,
author = {Nikookar, Sepideh and Esfandiari, Mohammadreza and Borromeo, Ria Mae and Sakharkar, Paras and Amer-Yahia, Sihem and Basu Roy, Senjuti},
title = {Diversifying recommendations on sequences of sets},
year = {2022},
issue_date = {Mar 2023},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {32},
number = {2},
issn = {1066-8888},
url = {https://doi.org/10.1007/s00778-022-00740-6},
doi = {10.1007/s00778-022-00740-6},
abstract = {Diversifying recommendations on a sequence of sets (or sessions) of items captures a variety of applications. Notable examples include recommending online music playlists, where a session is a channel and multiple channels are listened to in sequence, or recommending tasks in crowdsourcing, where a session is a set of tasks and multiple task sessions are completed in sequence. Item diversity can be defined in more than one way, e.g., as a genre diversity for music, or as a function of reward in crowdsourcing. A user who engages in multiple sessions may intend to experience diversity within and/or across sessions. Intra session diversity is set-based, whereas Inter session diversity is naturally sequence-based. This novel formulation gives rise to four bi-objective problems with the goal of minimizing or maximizing Inter and Intra diversities. We prove hardness and develop efficient algorithms with theoretical guarantees. Our experiments with human subjects on two real datasets show that our diversity formulations do serve different user needs and yield high user satisfaction. Our large-scale experiments on real and synthetic data empirically demonstrate that our solutions satisfy our theoretical bounds and are highly scalable, compared to baselines.},
journal = {The VLDB Journal},
month = may,
pages = {283–304},
numpages = {22},
keywords = {Data management models, Combinatorial optimization, Diversity algorithms, Recommendation systems}
}

@article{10.1145/3328906,
author = {Acer, Utku G\"{u}nay and Broeck, Marc van den and Forlivesi, Claudio and Heller, Florian and Kawsar, Fahim},
title = {Scaling Crowdsourcing with Mobile Workforce: A Case Study with Belgian Postal Service},
year = {2019},
issue_date = {June 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {3},
number = {2},
url = {https://doi.org/10.1145/3328906},
doi = {10.1145/3328906},
abstract = {Traditional urban-scale crowdsourcing approaches suffer from three caveats - lack of complete spatiotemporal coverage, lack of accurate information and lack of sustained engagement of crowd workers. In this paper, we argue that these caveats can be addressed by embedding crowdsourcing tasks into the daily routine of mobile workforces that roam around an urban area. As a use case, we take the bpost who deliver the letters and parcels to the citizens across entire Belgium. We present a study that explores the behavioural attributes of these mobile postal workers both quantitatively (6.3K) and qualitatively (6) to assess the opportunity of leveraging them for crowdsourcing tasks. We report their mobility pattern, workflow, and behavioural traits which collectively inform the design of a purpose-built crowdsourcing solution. In particular, our solution operates on two key techniques - route augmentation, and on-wearable interruptibility management. Together, these mechanisms enhance the spatial coverage, response accuracy and increase workers' engagement with crowdsourcing tasks. We describe these principal components in a wearable smartwatch application supported by a data management infrastructure. Finally, we report a first-of-its-kind real-world trial with ten postal workers for two weeks to assess the quality of road signs at the city centre of Antwerp. Our findings suggest that our solution was effective in achieving 89\% spatial coverage and increasing response rate (83.6\%) and accuracy (100\%) of the crowdsourcing tasks. Although limited in scale, these and the rest of our findings highlight the way of building an efficient and purposeful crowdsourcing solution of the future.},
journal = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
month = jun,
articleno = {35},
numpages = {32},
keywords = {wearable computing, mobile crowdsourcing, interruptibility, behaviour modelling}
}

@article{10.14778/3554821.3554876,
author = {Wu, Qingshun and Li, Yafei and Li, Huiling and Zhang, Di and Zhu, Guanglei},
title = {AMRAS: a visual analysis system for spatial crowdsourcing},
year = {2022},
issue_date = {August 2022},
publisher = {VLDB Endowment},
volume = {15},
number = {12},
issn = {2150-8097},
url = {https://doi.org/10.14778/3554821.3554876},
doi = {10.14778/3554821.3554876},
abstract = {The wide adoption of GPS-enabled smart devices has greatly promoted spatial crowdsourcing, where the core issue is how to assign tasks to workers efficiently and with high quality. In this paper, we build a novel visual analysis system for spatial crowdsourcing, namely AMRAS, which can not only intuitively present the task allocation for workers under different time window scales to users (e.g., data analysts and managers) in real-time, but also help users analyze task assignment decision model and its learning process. AMRAS has the following novel features. First, AMRAS provides two user-friendly interfaces that allow users to employ simple and easy-to-use console to perform statistical analysis. Secondly, AMRAS provides three powerful visualization tools, such as the visualization of assignment results, assignment process, and assignment decision model, which not only allow users to intuitively analyze the whole process of task assignment, but also help users discover the computational bottleneck of their task assignment solution. Finally, AMRAS enables online access to real-time data, providing users with instant assignment and instant analysis. We have implemented and deployed AMRAS on Alibaba Cloud and demonstrated its usability and efficiency in real-world datasets. The demonstration video of AMRAS has been uploaded to Google Drive.},
journal = {Proc. VLDB Endow.},
month = aug,
pages = {3690–3693},
numpages = {4}
}

@article{10.14778/3236187.3236196,
author = {Cheng, Peng and Jian, Xun and Chen, Lei},
title = {An experimental evaluation of task assignment in spatial crowdsourcing},
year = {2018},
issue_date = {July 2018},
publisher = {VLDB Endowment},
volume = {11},
number = {11},
issn = {2150-8097},
url = {https://doi.org/10.14778/3236187.3236196},
doi = {10.14778/3236187.3236196},
abstract = {Recently, with the rapid development of mobile devices and the crowdsourcing platforms, the spatial crowdsourcing has attracted much attention from the database community. Specifically, spatial crowdsourcing refers to sending a location-based request to workers according to their positions, and workers need to physically move to specified locations to conduct tasks. Many works have studied task assignment problems in spatial crowdsourcing, however, their problem settings are different from each other. Thus, it is hard to compare the performances of existing algorithms on task assignment in spatial crowdsourcing. In this paper, we present a comprehensive experimental comparison of most existing algorithms on task assignment in spatial crowdsourcing. Specifically, we first give general definitions about spatial workers and spatial tasks based on definitions in the existing works such that the existing algorithms can be applied on the same synthetic and real data sets. Then, we provide a uniform implementation for all the tested algorithms of task assignment problems in spatial crowdsourcing (open sourced). Finally, based on the results on both synthetic and real data sets, we discuss the strengths and weaknesses of tested algorithms, which can guide future research on the same area and practical implementations of spatial crowdsourcing systems.},
journal = {Proc. VLDB Endow.},
month = jul,
pages = {1428–1440},
numpages = {13}
}

@article{10.1007/s00778-021-00713-1,
author = {Zheng, Libin and Chen, Lei and Cheng, Peng},
title = {Privacy-preserving worker allocation in crowdsourcing},
year = {2022},
issue_date = {Jul 2022},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {31},
number = {4},
issn = {1066-8888},
url = {https://doi.org/10.1007/s00778-021-00713-1},
doi = {10.1007/s00778-021-00713-1},
abstract = {Crowdsourcing has been a prevalent way to obtain answers for tasks that need human intelligence. In general, a crowdsourcing platform is responsible for allocating workers to each received task, with high-quality workers in priority. However, the allocation results can in turn yield knowledge about workers’ quality. For example, those unallocated workers are supposed to be less-qualified. They can be upset if such information is known by the public, which is an invasion of their privacy. To alleviate such concerns, we study the privacy-preserving worker allocation problem in this paper, aiming to properly allocate the workers while protecting their privacy. We propose worker allocation methods with the property of differential privacy, which proceed by first computing weights for each potential allocation and then sampling according to the weights. The Markov Chain Monte Carlo-based method is shown in our experiments to improve over the trivial random allocation method by 18.9\% in terms of worker quality on synthetic data. On the real data, it realizes differential privacy with less than 20\% loss on quality even when ϵ=13.},
journal = {The VLDB Journal},
month = jan,
pages = {733–751},
numpages = {19},
keywords = {Worker privacy, Worker allocation, Crowdsourcing, Differential privacy}
}

@article{10.1109/TNET.2022.3141069,
author = {Han, Yanyan and Wu, Hongyi},
title = {Privacy-Aware Participant Recruitment in Opportunistic Device to Device Networks},
year = {2022},
issue_date = {June 2022},
publisher = {IEEE Press},
volume = {30},
number = {3},
issn = {1063-6692},
url = {https://doi.org/10.1109/TNET.2022.3141069},
doi = {10.1109/TNET.2022.3141069},
abstract = {In most of the existing mobile applications for data collection and data analytics, either the privacy issue is frequently neglected or the privacy options are not configurable by the participants. This paper proposes configurable privacy level by potential crowdsourcing participants who are able to choose their desirable privacy level and get paid based on the quality of data they provide to the originator in Device to Device network (D2D). Combining with the encryption technique, not only the user’s privacy is protected, but also the encryption complexity is reduced. We first formulate the participant recruitment process into an optimization problem from the participants’ perspective by considering the competition and collaboration among existing and candidate participants in order to achieve the best utility. Then we design a distributed approximate scheme that relies on participants’ local knowledge to complete the overall recruitment task. We implement the approximate approach in Dell Streak tablets and carry out a campus-scale experiment for 21 days, plus run simulations for more extensive and detailed evaluation under various task settings. The results demonstrate the efficiency of the proposed approaches and disclose valuable insights for practical considerations in D2D based crowdsourcing.},
journal = {IEEE/ACM Trans. Netw.},
month = jan,
pages = {1340–1351},
numpages = {12}
}

@article{10.1145/3415181,
author = {Hettiachchi, Danula and van Berkel, Niels and Kostakos, Vassilis and Goncalves, Jorge},
title = {CrowdCog: A Cognitive Skill based System for Heterogeneous Task Assignment and Recommendation in Crowdsourcing},
year = {2020},
issue_date = {October 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {4},
number = {CSCW2},
url = {https://doi.org/10.1145/3415181},
doi = {10.1145/3415181},
abstract = {While crowd workers typically complete a variety of tasks in crowdsourcing platforms, there is no widely accepted method to successfully match workers to different types of tasks. Researchers have considered using worker demographics, behavioural traces, and prior task completion records to optimise task assignment. However, optimum task assignment remains a challenging research problem due to limitations of proposed approaches, which in turn can have a significant impact on the future of crowdsourcing. We present 'CrowdCog', an online dynamic system that performs both task assignment and task recommendations, by relying on fast-paced online cognitive tests to estimate worker performance across a variety of tasks. Our work extends prior work that highlights the effect of workers' cognitive ability on crowdsourcing task performance. Our study, deployed on Amazon Mechanical Turk, involved 574 workers and 983 HITs that span across four typical crowd tasks (Classification, Counting, Transcription, and Sentiment Analysis). Our results show that both our assignment method and recommendation method result in a significant performance increase (5\% to 20\%) as compared to a generic or random task assignment. Our findings pave the way for the use of quick cognitive tests to provide robust recommendations and assignments to crowd workers.},
journal = {Proc. ACM Hum.-Comput. Interact.},
month = oct,
articleno = {110},
numpages = {22},
keywords = {dynamic task assignment, crowdsourcing, cognitive abilities}
}

@article{10.1145/3392837,
author = {Qiu, Sihang and Gadiraju, Ujwal and Bozzon, Alessandro},
title = {Estimating Conversational Styles in Conversational Microtask Crowdsourcing},
year = {2020},
issue_date = {May 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {4},
number = {CSCW1},
url = {https://doi.org/10.1145/3392837},
doi = {10.1145/3392837},
abstract = {Crowdsourcing marketplaces have provided a large number of opportunities for online workers to earn a living. To improve satisfaction and engagement of such workers, who are vital for the sustainability of the marketplaces, recent works have used conversational interfaces to support the execution of a variety of crowdsourcing tasks. The rationale behind using conversational interfaces stems from the potential engagement that conversation can stimulate. Prior works in psychology have also shown that conversational styles can play an important role in communication. There are unexplored opportunities to estimate a worker's conversational style with an end goal of improving worker satisfaction, engagement and quality. Addressing this knowledge gap, we investigate the role of conversational styles in conversational microtask crowdsourcing. To this end, we design a conversational interface which supports task execution, and we propose methods to estimate the conversational style of a worker. Our experimental setup was designed to empirically observe how conversational styles of workers relate with quality-related outcomes. Results show that even a naive supervised classifier can predict the conversation style with high accuracy (80\%), and crowd workers with an Involvement conversational style provided a significantly higher output quality, exhibited a higher user engagement and perceived less cognitive task load in comparison to their counterparts. Our findings have important implications on task design with respect to improving worker performance and their engagement in microtask crowdsourcing.},
journal = {Proc. ACM Hum.-Comput. Interact.},
month = may,
articleno = {32},
numpages = {23},
keywords = {work outcomes, user engagement, microtask crowdsourcing, conversational style, cognitive task load.}
}

@article{10.1145/3457216,
author = {Yang, Keyu and Gao, Yunjun and Liang, Lei and Bian, Song and Chen, Lu and Zheng, Baihua},
title = {CrowdTC: Crowd-powered Learning for Text Classification},
year = {2021},
issue_date = {February 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {16},
number = {1},
issn = {1556-4681},
url = {https://doi.org/10.1145/3457216},
doi = {10.1145/3457216},
abstract = {Text classification is a fundamental task in content analysis. Nowadays, deep learning has demonstrated promising performance in text classification compared with shallow models. However, almost all the existing models do not take advantage of the wisdom of human beings to help text classification. Human beings are more intelligent and capable than machine learning models in terms of understanding and capturing the implicit semantic information from text. In this article, we try to take guidance from human beings to classify text. We propose Crowd-powered learning for Text Classification (CrowdTC for short). We design and post the questions on a crowdsourcing platform to extract keywords in text. Sampling and clustering techniques are utilized to reduce the cost of crowdsourcing. Also, we present an attention-based neural network and a hybrid neural network to incorporate the extracted keywords as human guidance into deep neural networks. Extensive experiments on public datasets confirm that CrowdTC improves the text classification accuracy of neural networks by using the crowd-powered keyword guidance.},
journal = {ACM Trans. Knowl. Discov. Data},
month = jul,
articleno = {15},
numpages = {23},
keywords = {neural networks, keyword extraction, crowdsourcing, Text classification}
}

@article{10.1007/s00778-018-0516-7,
author = {Rahman, Habibur and Roy, Senjuti Basu and Thirumuruganathan, Saravanan and Amer-Yahia, Sihem and Das, Gautam},
title = {Optimized group formation for solving collaborative tasks},
year = {2019},
issue_date = {February  2019},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {28},
number = {1},
issn = {1066-8888},
url = {https://doi.org/10.1007/s00778-018-0516-7},
doi = {10.1007/s00778-018-0516-7},
abstract = {Many popular applications, such as collaborative document editing, sentence translation, or citizen science, resort to collaborative crowdsourcing, a special form of human-based computing, where, crowd workers with appropriate skills and expertise are required to form groups to solve complex tasks. While there has been extensive research on workers' task assignment for traditional microtask-based crowdsourcing, they often ignore the critical aspect of collaboration. Central to any collaborative crowdsourcing process is the aspect of solving collaborative tasks that requires successful collaboration among the workers. Our formalism considers two main collaboration-related factors--affinity and upper critical mass--appropriately adapted from organizational science and social theories. Our contributions are threefold. First, we formalize the notion of collaboration among crowd workers and propose a comprehensive optimization model for task assignment in a collaborative crowdsourcing environment. Next, we study the hardness of the task assignment optimization problem and propose a series of efficient exact and approximation algorithms with provable theoretical guarantees. Finally, we present a detailed set of experimental results stemming from two real-world collaborative crowdsourcing application using Amazon Mechanical Turk.},
journal = {The VLDB Journal},
month = feb,
pages = {1–23},
numpages = {23},
keywords = {Group formation, Crowdsourcing, Collaboration, Algorithms}
}

@article{10.1145/3476060,
author = {Toxtli, Carlos and Suri, Siddharth and Savage, Saiph},
title = {Quantifying the Invisible Labor in Crowd Work},
year = {2021},
issue_date = {October 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {5},
number = {CSCW2},
url = {https://doi.org/10.1145/3476060},
doi = {10.1145/3476060},
abstract = {Crowdsourcing markets provide workers with a centralized place to find paid work. What may not be obvious at first glance is that, in addition to the work they do for pay, crowd workers also have to shoulder a variety of unpaid invisible labor in these markets, which ultimately reduces workers' hourly wages. Invisible labor includes finding good tasks, messaging requesters, or managing payments. However, we currently know little about how much time crowd workers actually spend on invisible labor or how much it costs them economically. To ensure a fair and equitable future for crowd work, we need to be certain that workers are being paid fairly for all of the work they do. In this paper, we conduct a field study to quantify the invisible labor in crowd work. We build a plugin to record the amount of time that 100 workers on Amazon Mechanical Turk dedicate to invisible labor while completing 40,903 tasks. If we ignore the time workers spent on invisible labor, workers' median hourly wage was $3.76. But, we estimated that crowd workers in our study spent 33\% of their time daily on invisible labor, dropping their median hourly wage to $2.83. We found that the invisible labor differentially impacts workers depending on their skill level and workers' demographics. The invisible labor category that took the most time and that was also the most common revolved around workers having to manage their payments. The second most time-consuming invisible labor category involved hyper-vigilance, where workers vigilantly watched over requesters' profiles for newly posted work or vigilantly searched for labor. We hope that through our paper, the invisible labor in crowdsourcing becomes more visible, and our results help to reveal the larger implications of the continuing invisibility of labor in crowdsourcing.},
journal = {Proc. ACM Hum.-Comput. Interact.},
month = oct,
articleno = {319},
numpages = {26},
keywords = {browser plugin, crowdsourcing, ghost work, invisible labor, online work, productivity, research methodology, system, time measurement, tool}
}

@article{10.1145/3617128,
author = {Lin, Yuanhui and Gatica-Perez, Daniel},
title = {Characterizing Swiss Alpine Lakes: from Wikipedia to Citizen Science},
year = {2023},
issue_date = {December 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {1},
number = {2},
url = {https://doi.org/10.1145/3617128},
doi = {10.1145/3617128},
abstract = {Within the scope of a citizen science project that aims at understanding the ecological impact of climate change on bacteria communities in Swiss alpine lakes, we designed and implemented an interactive information platform using data collected from Wikipedia, project-specific data, and other sources. By presenting information about Swiss alpine lakes in an interactive way, the goal of the platform is to raise awareness among the public about the state of Swiss alpine lakes, and ultimately to contribute to the conservation of these ecosystems by engaging citizens. Volunteers were invited to use and assess the platform, by answering questions about alpine lake facts and platform usability. The results show that users can accurately extract factual information from the platform. User feedback was also used to improve the platform functionalities. Finally, an online crowdsourcing activity for lake polygon drawing was conducted to enrich the Swiss alpine lake database with this information. The results show that users can implement this task with high quality.},
journal = {ACM J. Comput. Sustain. Soc.},
month = dec,
articleno = {13},
numpages = {24},
keywords = {crowdsourcing, web-based learning, interactive platform, citizen science, climate change, Swiss alpine lakes}
}

@article{10.1145/3397180,
author = {Miao, Xin and Kang, Yanrong and Ma, Qiang and Liu, Kebin and Chen, Lei},
title = {Quality-aware Online Task Assignment in Mobile Crowdsourcing},
year = {2020},
issue_date = {August 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {16},
number = {3},
issn = {1550-4859},
url = {https://doi.org/10.1145/3397180},
doi = {10.1145/3397180},
abstract = {In recent years, mobile crowdsourcing has emerged as a powerful computation paradigm to harness human power to perform spatial tasks such as collecting real-time traffic information and checking product prices in a specific supermarket. A fundamental problem of mobile crowdsourcing is: When both tasks and crowd workers appear in the platforms dynamically, how to assign an appropriate set of tasks to each worker. Most existing studies focus on efficient assignment algorithms based on bipartite graph matching. However, they overlook an important fact that crowd workers might be unreliable. Thus, their task assignment schemes cannot ensure the overall quality. In this article, we investigate the Quality-aware Online Task Assignment (QAOTA) problem in mobile crowdsourcing. We propose a probabilistic model to measure the quality of tasks and a hitchhiking model to characterize workers’ behavior patterns. We model task assignment as a quality maximization problem and derive a polynomial-time online assignment algorithm. Through rigorous analysis, we prove that the proposed algorithm approximates the offline optimal solution with a competitive ratio of 10/7. Finally, we demonstrate the efficiency and effectiveness of our solution through intensive experiments.},
journal = {ACM Trans. Sen. Netw.},
month = jul,
articleno = {30},
numpages = {21},
keywords = {task assignment, Crowdsourcing}
}

@article{10.1145/3375194,
author = {de Souza, Cleidson R. B. and Machado, Leticia S. and Melo, Ricardo Rodrigo M.},
title = {On Moderating Software Crowdsourcing Challenges},
year = {2020},
issue_date = {January 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {4},
number = {GROUP},
url = {https://doi.org/10.1145/3375194},
doi = {10.1145/3375194},
abstract = {Crowdsourcing divides a task into small pieces that are carried out by the crowd. In Software Engineering, crowdsourcing divides the software development tasks of to be carried out online by the crow and is simply called Software Crowdsourcing (SW CS). Most SW CS platforms support this emerging software development strategy and operate within a framework of competition among the crowd. Competitive SW CS platforms intentionally minimize communication and collaboration among the parties involved (customer, platform, and crowd) while they compete in the software development tasks. The goal of this paper is to investigate platform moderators in SW CS challenges. Platform moderators are individuals who work for the SW CS platforms to mediate customer and crowd. A qualitative analysis of the content of the communication forums hosted on the TopCoder platform was performed to analyze the messages exchanged by the platform moderators and the crowd. Our empirical results indicate that co-pilots enforce and, at the same time, extend the limitations of the documentation associated with the tasks to support crowd members, provide technical help to crowd members during the competitions, and engage the crowd in the challenges. Co-pilots are organized, work diligently, worrying about being fair, and, at the same time, seeking to find a balance between autonomy and dependency on the customer. We conclude by providing insights to improve the design of software crowdsourcing platforms.},
journal = {Proc. ACM Hum.-Comput. Interact.},
month = jan,
articleno = {14},
numpages = {22},
keywords = {topcoder, software crowdsourcing, platform moderators, crowd, communication, collaboration, challenge}
}

@article{10.1145/3463932,
author = {Machado, Leticia S. and Melo, Ricardo Rodrigo M. and de Souza, Cleidson R. B. and Prikladnicki, Rafael},
title = {Collaborative Behavior and Winning Challenges in Competitive Software Crowdsourcing},
year = {2021},
issue_date = {July 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {5},
number = {GROUP},
url = {https://doi.org/10.1145/3463932},
doi = {10.1145/3463932},
abstract = {Software Crowdsourcing (SW CS) allows a requester to increase the speed of its software development efforts by submitting a task to be performed by the crowd. SW CS is usually structured around software platforms, which are used by crowd members to identify a task suited for them, gather information about this task, and finally, submit a solution for it. In competitive software crowdsourcing, members of the crowd independently create solutions while competing against each other by monetary rewards for task completion. While competition usually reduces collaboration, in this paper, we investigated how crowd members create a collaborative behavior during programming challenges using online forums to help each other, share useful information, and discuss important documents and artifacts. We also investigated different collaborative behaviours by crowd members and and how this collaboration is associated with crowd members' improved outcome in the challenges. These results are based on analysis of the online forums from Topcoder, one of the largest competitive SW CS platforms},
journal = {Proc. ACM Hum.-Comput. Interact.},
month = jul,
articleno = {220},
numpages = {25},
keywords = {topcoder, software crowdsourcing, communication, collaboration}
}

@article{10.1145/3555137,
author = {Rechkemmer, Amy and Yin, Ming},
title = {Understanding the Microtask Crowdsourcing Experience for Workers with Disabilities: A Comparative View},
year = {2022},
issue_date = {November 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {6},
number = {CSCW2},
url = {https://doi.org/10.1145/3555137},
doi = {10.1145/3555137},
abstract = {Microtask crowdsourcing holds great potential as an employment opportunity with the flexibility and anonymity that individuals with disability may require. Though prior research has explored the accessibility of crowd work, the lived crowd work experiences of the broader community of workers with disability are still largely under-explored, especially when it comes to how their experiences are similar to or different from the experiences of workers without disability. In this work, we aim to obtain a deeper understanding of the microtask crowdsourcing experience for people with disabilities, especially regarding their financial and social experiences of participating in crowd work, along with the benefits and challenges that they encounter through this work. Specifically, we first surveyed 1,200 crowd workers both with and without disability about their experiences using the Amazon Mechanical Turk platform, and the differences we found inspired the design of a follow-up survey to gain greater understanding of the crowd work experience for workers with disability. Our findings reveal that workers with disability receive unique benefits from performing crowd work, such as a greater sense of purpose, but also encounter many challenges, such as completing tasks on time and earning a livable wage, causing them to turn to online communities for assistance. Although many of the challenges they face are not unique to crowd workers with disability, workers with disability may be disproportionately impacted by these challenges. From our findings, we provide implications for crowd platforms, as well as the gig economy as a whole, that seek to promote greater consideration of workers with a diverse range of conditions to create a more valuable work experience for them.},
journal = {Proc. ACM Hum.-Comput. Interact.},
month = nov,
articleno = {412},
numpages = {30},
keywords = {survey, mechanical turk, crowdsourcing, accessibility}
}

@article{10.1145/3396863,
author = {Shah, Nihar B. and Zhou, Dengyong},
title = {Approval Voting and Incentives in Crowdsourcing},
year = {2020},
issue_date = {August 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {8},
number = {3},
issn = {2167-8375},
url = {https://doi.org/10.1145/3396863},
doi = {10.1145/3396863},
abstract = {The growing need for labeled training data has made crowdsourcing a vital tool for developing machine learning applications. Here, workers on a crowdsourcing platform are typically shown a list of unlabeled items, and for each of these items, are asked to choose a label from one of the provided options. The workers in crowdsourcing platforms are not experts, thereby making it essential to judiciously elicit the information known to the workers. With respect to this goal, there are two key shortcomings of current systems: (i) the incentives of the workers are not aligned with those of the requesters; and (ii) the interface does not allow workers to convey their knowledge accurately by forcing them to make a single choice among a set of options. In this article, we address these issues by introducing approval voting to utilize the expertise of workers who have partial knowledge of the true answer and coupling it with two strictly proper scoring rules. We additionally establish attractive properties of optimality and uniqueness of our scoring rules. We also conduct preliminary empirical studies on Amazon Mechanical Turk, and the results of these experiments validate our approach.},
journal = {ACM Trans. Econ. Comput.},
month = jun,
articleno = {13},
numpages = {40},
keywords = {Proper scoring rules, incentives, labeling}
}

@article{10.1145/3479572,
author = {Fogliato, Riccardo and Chouldechova, Alexandra and Lipton, Zachary},
title = {The Impact of Algorithmic Risk Assessments on Human Predictions and its Analysis via Crowdsourcing Studies},
year = {2021},
issue_date = {October 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {5},
number = {CSCW2},
url = {https://doi.org/10.1145/3479572},
doi = {10.1145/3479572},
abstract = {As algorithmic risk assessment instruments (RAIs) are increasingly adopted to assist decision makers, their predictive performance and potential to promote inequity have come under scrutiny. However, while most studies examine these tools in isolation, researchers have come to recognize that assessing their impact requires understanding the behavior of their human interactants. In this paper, building off of several recent crowdsourcing works focused on criminal justice, we conduct a vignette study in which laypersons are tasked with predicting future re-arrests. Our key findings are as follows: (1) Participants often predict that an offender will be rearrested even when they deem the likelihood of re-arrest to be well below 50\%; (2) Participants do not anchor on the RAI's predictions; (3) The time spent on the survey varies widely across participants and most cases are assessed in less than 10 seconds; (4) Judicial decisions, unlike participants' predictions, depend in part on factors that are orthogonal to the likelihood of re-arrest. These results highlight the influence of several crucial but often overlooked design decisions and concerns around generalizability when constructing crowdsourcing studies to analyze the impacts of RAI},
journal = {Proc. ACM Hum.-Comput. Interact.},
month = oct,
articleno = {428},
numpages = {24},
keywords = {algorithm-assisted decision-making, algorithmic risk assessment instruments, human in-the-loop, user study}
}

@article{10.1145/3555536,
author = {Zhang, Yang and Zong, Ruohan and Shang, Lanyu and Kou, Ziyi and Zeng, Huimin and Wang, Dong},
title = {CrowdOptim: A Crowd-driven Neural Network Hyperparameter Optimization Approach to AI-based Smart Urban Sensing},
year = {2022},
issue_date = {November 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {6},
number = {CSCW2},
url = {https://doi.org/10.1145/3555536},
doi = {10.1145/3555536},
abstract = {AI-based smart urban sensing (ASUS) has emerged as a scalable and pervasive application paradigm in smart city planning and management that aims to automatically assess the physical status of the urban environments by leveraging AI techniques and massive urban sensing data. In this paper, we focus on a crowd-driven neural network (NN) hyperparameter optimization problem in ASUS applications. Our goal is to utilize the human intelligence from crowdsourcing systems to identify the optimal NN hyperparameter configuration for an ASUS model. Our work is motivated by the observation that the hyperparameters of current ASUS models are often manually configured by the AI specialists, which is known to be both error-prone and suboptimal. Two key technical challenges exist in solving our problem: i) it is challenging to effectively translate the highly complex NN hyperparameter optimization problem in AI to a simplified problem that can be solved by crowd workers without extensive AI expertise; ii) it is difficult to identify the optimal hyperparameter configuration in the large hyperparameter search space given the blackbox nature of the AI model. To address the above challenges, we develop CrowdOptim, a crowd-AI collaborative learning framework that integrates the techniques from crowdsourcing, hyperparameter optimization, and estimation theory to address the crowd-driven NN hyperparameter optimization problem in ASUS applications. The evaluation results from two real-world ASUS applications (i.e., smart city infrastructure monitoring (SCIM) and urban environment cleanliness assessment (UECA)) show that CrowdOptim consistently outperforms the state-of-the-art deep convolutional networks, crowd-AI, and hyperparameter optimization baselines in achieving the ASUS application objectives under various evaluation scenarios.},
journal = {Proc. ACM Hum.-Comput. Interact.},
month = nov,
articleno = {435},
numpages = {27},
keywords = {smart urban sensing, hyperparameter optimization, crowd-AI collaborative system}
}

@article{10.1145/3415237,
author = {Burghardt, Keith and Hogg, Tad and D'Souza, Raissa and Lerman, Kristina and Posfai, Marton},
title = {Origins of Algorithmic Instabilities in Crowdsourced Ranking},
year = {2020},
issue_date = {October 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {4},
number = {CSCW2},
url = {https://doi.org/10.1145/3415237},
doi = {10.1145/3415237},
abstract = {Crowdsourcing systems aggregate decisions of many people to help users quickly identify high-quality options, such as the best answers to questions or interesting news stories. A long-standing issue in crowdsourcing is how option quality and human judgement heuristics interact to affect collective outcomes, such as the perceived popularity of options. We address this limitation by conducting a controlled experiment where subjects choose between two ranked options whose quality can be independently varied. We use this data to construct a model that quantifies how judgement heuristics and option quality combine when deciding between two options. The model reveals popularity-ranking can be unstable: unless the quality difference between the two options is sufficiently high, the higher quality option is not guaranteed to be eventually ranked on top. To rectify this instability, we create an algorithm that accounts for judgement heuristics to infer the best option and rank it first. This algorithm is guaranteed to be optimal if data matches the model. When the data does not match the model, however, simulations show that in practice this algorithm performs better or at least as well as popularity-based and recency-based ranking for any two-choice question. Our work suggests that algorithms relying on inference of mathematical models of user behavior can substantially improve outcomes in crowdsourcing systems.},
journal = {Proc. ACM Hum.-Comput. Interact.},
month = oct,
articleno = {166},
numpages = {20},
keywords = {decision-formation, crowd-wisdom, algorithmic instability, algorithmic bias}
}

@article{10.1145/3415176,
author = {Simons, Rachel N. and Gurari, Danna and Fleischmann, Kenneth R.},
title = {"I Hope This Is Helpful": Understanding Crowdworkers' Challenges and Motivations for an Image Description Task},
year = {2020},
issue_date = {October 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {4},
number = {CSCW2},
url = {https://doi.org/10.1145/3415176},
doi = {10.1145/3415176},
abstract = {AI image captioning challenges encourage broad participation in designing algorithms that automatically create captions for a variety of images and users. To create large datasets necessary for these challenges, researchers typically employ a shared crowdsourcing task design for image captioning. This paper discusses findings from our thematic analysis of 1,064 comments left by Amazon Mechanical Turk workers using this task design to create captions for images taken by people who are blind. Workers discussed difficulties in understanding how to complete this task, provided suggestions of how to improve the task, gave explanations or clarifications about their work, and described why they found this particular task rewarding or interesting. Our analysis provides insights both into this particular genre of task as well as broader considerations for how to employ crowdsourcing to generate large datasets for developing AI algorithms.},
journal = {Proc. ACM Hum.-Comput. Interact.},
month = oct,
articleno = {105},
numpages = {26},
keywords = {image captioning, crowdsourcing, computer vision, artificial intelligence, amazon mechanical turk, accessibility}
}

@article{10.1145/3432928,
author = {Feldman, Molly Q. and McInnis, Brian James},
title = {How We Write with Crowds},
year = {2021},
issue_date = {December 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {4},
number = {CSCW3},
url = {https://doi.org/10.1145/3432928},
doi = {10.1145/3432928},
abstract = {Writing is a common task for crowdsourcing researchers exploring complex and creative work. To better understand how we write with crowds, we conducted both a literature review of crowd-writing systems and structured interviews with designers of such systems. We argue that the cognitive process theory of writing described by Flower and Hayes (1981), originally proposed as a theory of how solo writers write, offers a useful analytic lens for examining the design of crowd-writing systems. This lens enabled us to identify system design challenges that are inherent to the process of writing as well as design challenges that are introduced by crowdsourcing. The findings present both similarities and differences between how solo writers write versus how we write with crowds. To conclude, we discuss how the research community might apply and transcend the cognitive process model to identify opportunities for future research in crowd-writing systems.},
journal = {Proc. ACM Hum.-Comput. Interact.},
month = jan,
articleno = {229},
numpages = {31},
keywords = {writing theory, writing, crowdsourcing, crowd-writing, collaborative writing}
}

@article{10.14778/3137765.3137827,
author = {Tong, Yongxin and Chen, Lei and Shahabi, Cyrus},
title = {Spatial crowdsourcing: challenges, techniques, and applications},
year = {2017},
issue_date = {August 2017},
publisher = {VLDB Endowment},
volume = {10},
number = {12},
issn = {2150-8097},
url = {https://doi.org/10.14778/3137765.3137827},
doi = {10.14778/3137765.3137827},
abstract = {Crowdsourcing is a new computing paradigm where humans are actively enrolled to participate in the procedure of computing, especially for tasks that are intrinsically easier for humans than for computers. The popularity of mobile computing and sharing economy has extended conventional web-based crowdsourcing to spatial crowdsourcing (SC), where spatial data such as location, mobility and the associated contextual information, plays a central role. In fact, spatial crowdsourcing has stimulated a series of recent industrial successes including Citizen Sensing (Waze), P2P ride-sharing (Uber) and Real-time Online-To-Offline (O2O) services (Instacart and Postmates).In this tutorial, we review the paradigm shift from web-based crowdsourcing to spatial crowdsourcing. We dive deep into the challenges and techniques brought by the unique spatio-temporal characteristics of spatial crowdsourcing. Particularly, we survey new designs in task assignment, quality control, incentive mechanism design and privacy protection on spatial crowdsourcing platforms, as well as the new trend to incorporate crowdsourcing to enhance existing spatial data processing techniques. We also discuss case studies of representative spatial crowdsourcing systems and raise open questions and current challenges for the audience to easily comprehend the tutorial and to advance this important research area.},
journal = {Proc. VLDB Endow.},
month = aug,
pages = {1988–1991},
numpages = {4}
}

@article{10.1145/3415203,
author = {Fan, Shaoyang and Gadiraju, Ujwal and Checco, Alessandro and Demartini, Gianluca},
title = {CrowdCO-OP: Sharing Risks and Rewards in Crowdsourcing},
year = {2020},
issue_date = {October 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {4},
number = {CSCW2},
url = {https://doi.org/10.1145/3415203},
doi = {10.1145/3415203},
abstract = {Paid micro-task crowdsourcing has gained in popularity partly due to the increasing need for large-scale manually labelled datasets which are often used to train and evaluate Artificial Intelligence systems. Modern paid crowdsourcing platforms use a piecework approach to rewards, meaning that workers are paid for each task they complete, given that their work quality is considered sufficient by the requester or the platform. Such an approach creates risks for workers; their work may be rejected without being rewarded, and they may be working on poorly rewarded tasks, in light of the disproportionate time required to complete them. As a result, recent research has shown that crowd workers may tend to choose specific, simple, and familiar tasks and avoid new requesters to manage these risks. In this paper, we propose a novel crowdsourcing reward mechanism that allows workers to share these risks and achieve a standardized hourly wage equal for all participating workers. Reward-focused workers can thereby take up challenging and complex HITs without bearing the financial risk of not being rewarded for completed work. We experimentally compare different crowd reward schemes and observe their impact on worker performance and satisfaction. Our results show that 1) workers clearly perceive the benefits of the proposed reward scheme, 2) work effectiveness and efficiency are not impacted as compared to those of the piecework scheme, and 3) the presence of slow workers is limited and does not disrupt the proposed cooperation-based approaches.},
journal = {Proc. ACM Hum.-Comput. Interact.},
month = oct,
articleno = {132},
numpages = {24},
keywords = {worker behavior, reward sharing, human computation, fairness, crowdsourcing}
}

@article{10.14778/3407790.3407842,
author = {Krivosheev, Evgeny and Bykau, Siarhei and Casati, Fabio and Prabhakar, Sunil},
title = {Detecting and preventing confused labels in crowdsourced data},
year = {2020},
issue_date = {August 2020},
publisher = {VLDB Endowment},
volume = {13},
number = {12},
issn = {2150-8097},
url = {https://doi.org/10.14778/3407790.3407842},
doi = {10.14778/3407790.3407842},
abstract = {Crowdsourcing is a challenging activity for many reasons, from task design to workers' training, identification of low-quality annotators, and many more. A particularly subtle form of error is due to confusion of observations, that is, crowd workers (including diligent ones) that confuse items of a class i with items of a class j, either because they are similar or because the task description has failed to explain the differences.In this paper we show that confusion of observations can be a frequent occurrence in many tasks, and that such confusions cause a significant loss in accuracy. As a consequence, confusion detection is of primary importance for crowdsourced data labeling and classification. To address this problem we introduce an algorithm for confusion detection that leverages an inference procedure based on Markov Chain Monte Carlo (MCMC) sampling. We evaluate the algorithm via both synthetic datasets and crowdsourcing experiments and show that it has high accuracy in confusion detection (up to 99\%). We experimentally show that quality is significantly improved without sacrificing efficiency. Finally, we show that detecting confusion is important as it can alert task designers early in the crowdsourcing process and lead designers to modify the task or add specific training and information to reduce the occurrence of workers' confusion. We show that even simple modifications, such as alerting workers of the risk of confusion, can improve performance significantly.},
journal = {Proc. VLDB Endow.},
month = jul,
pages = {2522–2535},
numpages = {14}
}

@article{10.1145/2910575,
author = {Easley, David and Ghosh, Arpita},
title = {Incentives, Gamification, and Game Theory: An Economic Approach to Badge Design},
year = {2016},
issue_date = {June 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {4},
number = {3},
issn = {2167-8375},
url = {https://doi.org/10.1145/2910575},
doi = {10.1145/2910575},
abstract = {Gamification is growing increasingly prevalent as a means to incentivize user engagement of social media sites that rely on user contributions. Badges, or equivalent rewards, such as top-contributor lists that are used to recognize a user's contributions on a site, clearly appear to be valued by users who actively pursue and compete for them. However, different sites use different badge designs, varying how, and for what, badges are awarded. Some sites, such as StackOverflow, award badges for meeting fixed levels of contribution. Other sites, such as Amazon and Y! Answers, reward users for being among some top set of contributors on the site, corresponding to a competitive standard of performance. Given that users value badges, and that contributing to a site requires effort, how badges are designed will affect the incentives—therefore the participation and effort—elicited from strategic users on a site.We take a game-theoretic approach to badge design, analyzing the incentives created by widely used badge designs in a model in which winning a badge is valued, effort is costly, and potential contributors to the site endogenously decide whether or not to participate, and how much total effort to put into their contributions to the site. We analyze equilibrium existence, as well as equilibrium participation and effort, in an absolute standards mechanism Mα in which badges are awarded for meeting some absolute level of (observed) effort, and a relative standards mechanism Mρ corresponding to competitive standards, as in a top-ρ contributor badge. We find that equilibria always exist in both mechanisms, even when the value from winning a badge depends endogenously on the number of other winners. However, Mα has zero-participation equilibria for standards that are too high, whereas all equilibria in Mρ elicit nonzero participation for all possible ρ, provided that ρ is specified as a fixed number rather than as a fraction of actual contributors (note that the two are not equivalent in a setting with endogenous participation). Finally, we ask whether or not a site should explicitly announce the number of users winning a badge. The answer to this question is determined by the curvature of the value of winning the badge as a function of the number of other winners.},
journal = {ACM Trans. Econ. Comput.},
month = jun,
articleno = {16},
numpages = {26},
keywords = {user-generated content, social media, incentives, game theory, badges, Gamification}
}

@article{10.1145/3432222,
author = {Mairittha, Nattaya and Mairittha, Tittaya and Lago, Paula and Inoue, Sozo},
title = {CrowdAct: Achieving High-Quality Crowdsourced Datasets in Mobile Activity Recognition},
year = {2021},
issue_date = {March 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {5},
number = {1},
url = {https://doi.org/10.1145/3432222},
doi = {10.1145/3432222},
abstract = {In this study, we propose novel gamified active learning and inaccuracy detection for crowdsourced data labeling for an activity recognition system using mobile sensing (CrowdAct). First, we exploit active learning to address the lack of accurate information. Second, we present the integration of gamification into active learning to overcome the lack of motivation and sustained engagement. Finally, we introduce an inaccuracy detection algorithm to minimize inaccurate data. To demonstrate the capability and feasibility of the proposed model in realistic settings, we developed and deployed the CrowdAct system to a crowdsourcing platform. For our experimental setup, we recruited 120 diverse workers. Additionally, we gathered 6,549 activity labels from 19 activity classes by using smartphone sensors and user engagement information. We empirically evaluated the quality of CrowdAct by comparing it with a baseline using techniques such as machine learning and descriptive and inferential statistics. Our results indicate that CrowdAct was effective in improving activity accuracy recognition, increasing worker engagement, and reducing inaccurate data in crowdsourced data labeling. Based on our findings, we highlight critical and promising future research directions regarding the design of efficient activity data collection with crowdsourcing.},
journal = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
month = mar,
articleno = {50},
numpages = {32},
keywords = {inaccuracy detection, gamified active learning, crowdsourced labeling, activity recognition}
}

@article{10.1145/3555534,
author = {Shi, Yang and Chen, Siji and Liu, Pei and Long, Jiang and Cao, Nan},
title = {ColorCook: Augmenting Color Design for Dashboarding with Domain-Associated Palettes},
year = {2022},
issue_date = {November 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {6},
number = {CSCW2},
url = {https://doi.org/10.1145/3555534},
doi = {10.1145/3555534},
abstract = {Visualization dashboards serve as an information presentation that uses a tiled layout of key metrics visualized in charts for collaborative decision-making. Existing work has developed tools and techniques for computational color design. Much of these efforts have focused on selecting effective color palettes for independent charts while few attempts have been made to support the expressive color design of multiple coordinated charts in dashboards. In this work, we describe ColorCook, an interactive system that helps design expressive and effective dashboard colorings using domain-associated palettes. ColorCook employs an integrated color workflow for dashboarding, consisting of color selection, assignment, and adjustment. We evaluated ColorCook through a crowdsourcing experiment and a user study. The results of our evaluation indicated that ColorCook is useful for effective and expressive color design.},
journal = {Proc. ACM Hum.-Comput. Interact.},
month = nov,
articleno = {433},
numpages = {25},
keywords = {visualization dashboards, design support tools, crowdsourcing, color design}
}

@article{10.1109/TNET.2018.2811736,
author = {Chatterjee, Avhishek and Borokhovich, Michael and Varshney, Lav R. and Vishwanath, Sriram and Varshney, Lav R. and Chatterjee, Avhishek and Vishwanath, Sriram and Borokhovich, Michael},
title = {Efficient and Flexible Crowdsourcing of Specialized Tasks With Precedence Constraints},
year = {2018},
issue_date = {April 2018},
publisher = {IEEE Press},
volume = {26},
number = {2},
issn = {1063-6692},
url = {https://doi.org/10.1109/TNET.2018.2811736},
doi = {10.1109/TNET.2018.2811736},
abstract = {Many companies now use crowdsourcing to leverage external as well as internal crowds to perform specialized work, and so methods of improving efficiency are critical. Tasks in crowdsourcing systems with specialized work have multiple steps and each step requires multiple skills. Steps may have different flexibilities in terms of obtaining service from one or multiple agents due to varying levels of dependency among parts of steps. Steps of a task may have precedence constraints among them. Moreover, there are variations in loads of different types of tasks requiring different skill sets and availabilities of agents with different skill sets. Considering these constraints together necessitate the design of novel schemes to allocate steps to agents. In addition, large crowdsourcing systems require allocation schemes that are simple, fast, decentralized, and offer customers task requesters the freedom to choose agents. In this paper, we study the performance limits of such crowdsourcing systems and propose efficient allocation schemes that provably meet the performance limits under these additional requirements. We demonstrate our algorithms on data from a crowdsourcing platform run by a nonprofit company and show significant improvements over current practice.},
journal = {IEEE/ACM Trans. Netw.},
month = apr,
pages = {879–892},
numpages = {14}
}

@article{10.1145/3274428,
author = {Skorupska, Kinga and Nunez, Manuel and Kopec, Wieslaw and Nielek, Radoslaw},
title = {Older Adults and Crowdsourcing: Android TV App for Evaluating TEDx Subtitle Quality},
year = {2018},
issue_date = {November 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {2},
number = {CSCW},
url = {https://doi.org/10.1145/3274428},
doi = {10.1145/3274428},
abstract = {In this paper we describe the insights from an exploratory qualitative pilot study testing the feasibility of a solution that would encourage older adults to participate in online crowdsourcing tasks in a non-computer scenario. Therefore, we developed an Android TV application using Amara API to retrieve subtitles for TEDx talks which allows the participants to detect and categorize errors to support the quality of the translation and transcription processes. It relies on the older adults' innate skills as long-time native language users and the motivating factors of this socially and personally beneficial task. The study allowed us to verify the underlying concept of using Smart TVs as interfaces for crowdsourcing, as well as possible barriers, including the interface, configuration issues, topics and the process itself. We have also assessed the older adults' interaction and engagement with this TV-enabled online crowdsourcing task and we are convinced that the design of our setup addresses some key barriers to crowdsourcing by older adults. It also validates avenues for further research in this area focused on such considerations as autonomy and freedom of choice, familiarity, physical and cognitive comfort as well as building confidence and the edutainment value.},
journal = {Proc. ACM Hum.-Comput. Interact.},
month = nov,
articleno = {159},
numpages = {23},
keywords = {volunteering, subtitling, software engineering, social inclusion, smart tv, older adults, edutainment, crowdsourcing, application development, android tv}
}

@article{10.1145/3579593,
author = {Bi, Nanyi and Huang, Yi-Ching (Janet) and Han, Chao-Chun and Hsu, Jane Yung-jen},
title = {You Know What I Meme: Enhancing People's Understanding and Awareness of Hateful Memes Using Crowdsourced Explanations},
year = {2023},
issue_date = {April 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {CSCW1},
url = {https://doi.org/10.1145/3579593},
doi = {10.1145/3579593},
abstract = {Good explanations help people understand hateful memes and mitigate sharing. While AI-enabled automatic detection has proliferated, we argue that quality-controlled crowdsourcing can be an effective strategy to offer good explanations for hateful memes. This paper proposes a Generate-Annotate-Revise workflow to crowdsource explanations and presents the results from two user studies. Study 1 evaluated the objective quality of the explanation with three measurements: detailedness, completeness, and accuracy, and suggested that the proposed workflow generated higher quality explanations than the ones from a single-stage workflow without quality control. Study 2 used an online experiment to examine how different explanations affect users' perception. The results from 127 participants demonstrated that people without prior cultural knowledge gained significant perceived understanding and awareness of hateful memes when presented with explanations generated by the proposed multi-stage workflow as opposed to single-stage or machine-generated explanations.},
journal = {Proc. ACM Hum.-Comput. Interact.},
month = apr,
articleno = {117},
numpages = {27},
keywords = {crowdsourcing, explainable artificial intelligence, hateful memes, hateful memes explanation, user study}
}

@article{10.1145/3347514,
author = {Chen, Yanjiao and Wang, Xu and Li, Baochun and Zhang, Qian},
title = {An Incentive Mechanism for Crowdsourcing Systems with Network Effects},
year = {2019},
issue_date = {November 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {19},
number = {4},
issn = {1533-5399},
url = {https://doi.org/10.1145/3347514},
doi = {10.1145/3347514},
abstract = {In a crowdsourcing system, it is important for the crowdsourcer to engineer extrinsic rewards to incentivize the participants. With mobile social networking, a user enjoys an intrinsic benefit when she aligns her behavior with the behavior of others. Referred to as network effects, such an intrinsic benefit becomes more significant as more users join and contribute to the crowdsourcing system. But should a crowdsourcer design her extrinsic rewards differently when such network effects are taken into consideration? In this article, we incorporate network effects as a contributing factor to intrinsic rewards, and study its influence on the design of extrinsic rewards. We show that the number of participating users and their contributions to the crowdsourcing system evolve to a steady equilibrium, thanks to subtle interactions between intrinsic rewards due to network effects and extrinsic rewards offered by the crowdsourcer. Taken network effects into consideration, we design progressively more sophisticated extrinsic reward mechanisms, and propose new and optimal strategies for a crowdsourcer to obtain a higher utility. Through simulations and examples, we demonstrate that with our new strategies, a crowdsourcer is able to attract more participants with higher contributed efforts; and the participants gain higher utilities from both intrinsic and extrinsic rewards.},
journal = {ACM Trans. Internet Technol.},
month = sep,
articleno = {49},
numpages = {21},
keywords = {network effects, intrinsic rewards, incentive mechanism, Crowdsourcing}
}

@article{10.1145/3629139,
author = {Peroni, Leonardo and Gorinsky, Sergey and Tashtarian, Farzad and Timmerer, Christian},
title = {Empowerment of Atypical Viewers via Low-Effort Personalized Modeling of Video Streaming Quality},
year = {2023},
issue_date = {December 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {1},
number = {CoNEXT3},
url = {https://doi.org/10.1145/3629139},
doi = {10.1145/3629139},
abstract = {Quality of Experience (QoE) and QoE models are of an increasing importance to networked systems. The traditional QoE modeling for video streaming applications builds a one-size-fits-all QoE model that underserves atypical viewers who perceive QoE differently. To address the problem of atypical viewers, this paper proposes iQoE (individualized QoE), a method that employs explicit, expressible, and actionable feedback from a viewer to construct a personalized QoE model for this viewer. The iterative iQoE design exercises active learning and combines a novel sampler with a modeler. The chief emphasis of our paper is on making iQoE sample-efficient and accurate. By leveraging the Microworkers crowdsourcing platform, we conduct studies with 120 subjects who provide 14,400 individual scores. According to the subjective studies, a session of about 22 minutes empowers a viewer to construct a personalized QoE model that, compared to the best of the 10 baseline models, delivers the average accuracy improvement of at least 42\% for all viewers and at least 85\% for the atypical viewers. The large-scale simulations based on a new technique of synthetic profiling expand the evaluation scope by exploring iQoE design choices, parameter sensitivity, and generalizability.},
journal = {Proc. ACM Netw.},
month = nov,
articleno = {17},
numpages = {27},
keywords = {accuracy, modeling, perception dataset, personalization, personalized QoE model, quality of experience, sample efficiency, subjective study, video streaming}
}

@article{10.1145/3301003,
author = {Difallah, Djellel and Checco, Alessandro and Demartini, Gianluca and Cudr\'{e}-Mauroux, Philippe},
title = {Deadline-Aware Fair Scheduling for Multi-Tenant Crowd-Powered Systems},
year = {2019},
issue_date = {March 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {2},
number = {1},
url = {https://doi.org/10.1145/3301003},
doi = {10.1145/3301003},
abstract = {Crowdsourcing has become an integral part of many systems and services that deliver high-quality results for complex tasks such as data linkage, schema matching, and content annotation. A standard function of such crowd-powered systems is to publish a batch of tasks on a crowdsourcing platform automatically and to collect the results once the workers complete them. Currently, these systems provide limited guarantees over the execution time, which is problematic for many applications. Timely completion may even be impossible to guarantee due to factors specific to the crowdsourcing platform, such as the availability of workers and concurrent tasks. In our previous work, we presented the architecture of a crowd-powered system that reshapes the interaction mechanism with the crowd. Specifically, we studied a push-crowdsourcing model whereby the workers receive tasks instead of selecting them from a portal. Based on this interaction model, we employed scheduling techniques similar to those found in distributed computing infrastructures to automate the task assignment process. In this work, we first devise a generic scheduling strategy that supports both fairness and deadline-awareness. Second, to complement the proof-of-concept experiments previously performed with the crowd, we present an extensive set of simulations meant to analyze the properties of the proposed scheduling algorithms in an environment with thousands of workers and tasks. Our experimental results show that, by accounting for human factors, micro-task scheduling can achieve fairness for best-effort batches and boosts production batches.},
journal = {Trans. Soc. Comput.},
month = feb,
articleno = {3},
numpages = {29},
keywords = {task scheduling, priority, human factors, deadline, Crowdsourcing systems}
}

@article{10.1049/iet-com.2018.5356,
author = {Hou, Jian and Luo, Shuyun and Xu, Weiqiang and Wang, Lili},
title = {Fairness‐based multi‐task reward allocation in mobile crowdsourcing system},
year = {2019},
issue_date = {October 2019},
publisher = {John Wiley \&amp; Sons, Inc.},
address = {USA},
volume = {13},
number = {16},
url = {https://doi.org/10.1049/iet-com.2018.5356},
doi = {10.1049/iet-com.2018.5356},
abstract = {Mobile crowdsourcing‐based applications, widely popular, exploit the sensing data crowdsourced from smartphone users without putting any burden on the extra cost of data sensing and collection. However, user participation in crowdsourcing incurs resource cost, such as battery, bandwidth, thus it is critical to design incentive mechanisms for propelling user's participation. Previous diverse incentive mechanisms designed for crowdsourcing applications only focus on users' contribution for reward allocation, while ignore another important property, i.e. fairness, users' reward should be corresponding with their cost. In this study, the authors first introduce a new concept called rate of return (RoR), defined as the ratio of received reward and incurred cost for each user, to demonstrate the property of fairness. With the goal of guarantee, the fairness of reward allocation for each user in a multiple‐task system, three algorithms, consensus‐based reward allocation, consensus‐based balanced topology reward allocation and Gossip‐based reward allocation are proposed for the demands of various scenarios, in which the RoR values are synchronised by optimising the fairness function in either centralised or decentralised manner. Through rigorous theoretical analysis and extensive simulations, it is finally demonstrated that the proposed reward allocation algorithms have the good property of fairness with quick convergence.},
journal = {IET Communications},
month = oct,
pages = {2506–2511},
numpages = {6},
keywords = {diverse incentive mechanisms, resource cost, user participation, data sensing, smartphone users, sensing data, mobile crowdsourcing‐based applications, mobile crowdsourcing system, fairness‐based multitask reward allocation, reward allocation algorithms, fairness function, Gossip‐based reward allocation, consensus‐based balanced topology reward allocation, consensus‐based reward allocation, multiple‐task system, received reward, optimisation, resource allocation, incentive schemes, mobile computing, smart phones}
}

@article{10.1145/3134724,
author = {Retelny, Daniela and Bernstein, Michael S. and Valentine, Melissa A.},
title = {No Workflow Can Ever Be Enough: How Crowdsourcing Workflows Constrain Complex Work},
year = {2017},
issue_date = {November 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {1},
number = {CSCW},
url = {https://doi.org/10.1145/3134724},
doi = {10.1145/3134724},
abstract = {The dominant crowdsourcing infrastructure today is the workflow, which decomposes goals into small independent tasks. However, complex goals such as design and engineering have remained stubbornly difficult to achieve with crowdsourcing workflows. Is this due to a lack of imagination, or a more fundamental limit? This paper explores this question through in-depth case studies of 22 workers across six workflow-based crowd teams, each pursuing a complex and interdependent web development goal. We used an inductive mixed method approach to analyze behavior trace data, chat logs, survey responses and work artifacts to understand how workers enacted and adapted the crowdsourcing workflows. Our results indicate that workflows served as useful coordination artifacts, but in many cases critically inhibited crowd workers from pursuing real-time adaptations to their work plans. However, the CSCW and organizational behavior literature argues that all sufficiently complex goals require open-ended adaptation. If complex work requires adaptation but traditional static crowdsourcing workflows can't support it, our results suggest that complex work may remain a fundamental limitation of workflow-based crowdsourcing infrastructures.},
journal = {Proc. ACM Hum.-Comput. Interact.},
month = dec,
articleno = {89},
numpages = {23},
keywords = {workflows, human computation, crowdsourcing}
}

@article{10.1145/3583689,
author = {Fr\"{a}nti, Pasi and Fazal, Nancy},
title = {Design Principles for Content Creation in Location-Based Games},
year = {2023},
issue_date = {October 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {19},
number = {5s},
issn = {1551-6857},
url = {https://doi.org/10.1145/3583689},
doi = {10.1145/3583689},
abstract = {Location-based games have been around since 2000 across various fields, including education, health, and entertainment. The main challenge facing such games is content generation. In contrast to normal games, content in location-based games is inherently dependent on location. The biggest challenge is the availability of the content globally. Other challenges include player engagement, enjoyable interactions with the real-world environment, safety, and customizability based on player performance and preference. While crowdsourcing has often been adopted as a tool for content creation, this approach requires quality control. Designing high-quality content requires detailed guidelines. In this paper, we introduce design principles for the creation of high-quality content that can survive for long periods of time. These principles are derived from ten years of experience running our in-house orienteering treasure-hunt game called O-Mopsi, which represents a case study in this paper. O-Mopsi allows players to visit pre-defined locations. The design principles are expected to be generalizable to other location-based games as well as to the creation of sightseeing tours more generally.},
journal = {ACM Trans. Multimedia Comput. Commun. Appl.},
month = jun,
articleno = {165},
numpages = {30},
keywords = {O-Mopsi, treasure hunting, orienteering, location-based game (LBG), user-generated media (UGM), user-generated content (UGC), quality evaluation, Content creation}
}

@article{10.1145/3480965,
author = {Moshfeghi, Yashar and Huertas-Rosero, Alvaro Francisco},
title = {A Game Theory Approach for Estimating Reliability of Crowdsourced Relevance Assessments},
year = {2021},
issue_date = {July 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {40},
number = {3},
issn = {1046-8188},
url = {https://doi.org/10.1145/3480965},
doi = {10.1145/3480965},
abstract = {In this article, we propose an approach to improve quality in crowdsourcing (CS) tasks using Task Completion Time (TCT) as a source of information about the reliability of workers in a game-theoretical competitive scenario. Our approach is based on the hypothesis that some workers are more risk-inclined and tend to gamble with their use of time when put to compete with other workers. This hypothesis is supported by our previous simulation study. We test our approach with 35 topics from experiments on the TREC-8 collection being assessed as relevant or non-relevant by crowdsourced workers both in a competitive (referred to as “Game”) and non-competitive (referred to as “Base”) scenario. We find that competition changes the distributions of TCT, making them sensitive to the quality (i.e., wrong or right) and outcome (i.e., relevant or non-relevant) of the assessments. We also test an optimal function of TCT as weights in a weighted majority voting scheme. From probabilistic considerations, we derive a theoretical upper bound for the weighted majority performance of cohorts of 2, 3, 4, and 5 workers, which we use as a criterion to evaluate the performance of our weighting scheme. We find our approach achieves a remarkable performance, significantly closing the gap between the accuracy of the obtained relevance judgements and the upper bound. Since our approach takes advantage of TCT, which is an available quantity in any CS tasks, we believe it is cost-effective and, therefore, can be applied for quality assurance in crowdsourcing for micro-tasks.},
journal = {ACM Trans. Inf. Syst.},
month = nov,
articleno = {60},
numpages = {29},
keywords = {relevance assessment, crowdsourcing, Game theory}
}

@article{10.1145/3231934,
author = {Safran, Mejdl and Che, Dunren},
title = {Efficient Learning-Based Recommendation Algorithms for Top-N Tasks and Top-N Workers in Large-Scale Crowdsourcing Systems},
year = {2018},
issue_date = {January 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {37},
number = {1},
issn = {1046-8188},
url = {https://doi.org/10.1145/3231934},
doi = {10.1145/3231934},
abstract = {The task and worker recommendation problems in crowdsourcing systems have brought up unique characteristics that are not present in traditional recommendation scenarios, i.e., the huge flow of tasks with short lifespans, the importance of workers’ capabilities, and the quality of the completed tasks. These unique features make traditional recommendation approaches no longer satisfactory for task and worker recommendation in crowdsourcing systems. In this article, we propose a two-tier data representation scheme (defining a worker--category suitability score and a worker--task attractiveness score) to support personalized task and worker recommendations. We also extend two optimization methods, namely least mean square error and Bayesian personalized rank, to better fit the characteristics of task/worker recommendation in crowdsourcing systems. We then integrate the proposed representation scheme and the extended optimization methods along with the two adapted popular learning models, i.e., matrix factorization and kNN, and result in two lines of top-N recommendation algorithms for crowdsourcing systems: (1) Top-N-Tasks recommendation algorithms for discovering the top-N most suitable tasks for a given worker and (2) Top-N-Workers recommendation algorithms for identifying the top-N best workers for a task requester. An extensive experimental study is conducted that validates the effectiveness and efficiency of a broad spectrum of algorithms, accompanied by our analysis and the insights gained.},
journal = {ACM Trans. Inf. Syst.},
month = oct,
articleno = {2},
numpages = {46},
keywords = {task recommendation, ranking algorithms, machine learning, crowdsourcing, Crowd computing}
}

@article{10.1145/3375187,
author = {Wang, Yihong and Papangelis, Konstantinos and Lykourentzou, Ioanna and Liang, Hai-Ning and Sadien, Irwyn and Demerouti, Evangelia and Khan, Vassilis-Javed},
title = {In Their Shoes: A Structured Analysis of Job Demands, Resources, Work Experiences, and Platform Commitment of Crowdworkers in China},
year = {2020},
issue_date = {January 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {4},
number = {GROUP},
url = {https://doi.org/10.1145/3375187},
doi = {10.1145/3375187},
abstract = {Despite the growing interest in crowdsourcing, this new labor model has recently received severe criticism. The most important point of this criticism is that crowdworkers are often underpaid and overworked. This severely affects job satisfaction and productivity. Although there is a growing body of evidence exploring the work experiences of crowdworkers in various countries, there have been a very limited number of studies to the best of our knowledge exploring the work experiences of Chinese crowdworkers. In this paper we aim to address this gap. Based on a framework of well-established approaches, namely the Job Demands-Resources model, the Work Design Questionnaire, the Oldenburg Burnout Inventory, the Utrecht Work Engagement Scale, and the Organizational Commitment Questionnaire, we systematically study the work experiences of 289 crowdworkers who work for ZBJ.com - the most popular Chinese crowdsourcing platform. Our study examines these crowdworker experiences along four dimensions: (1) crowdsourcing job demands, (2) job resources available to the workers, (3) crowdwork experiences, and (4) platform commitment. Our results indicate significant differences across the four dimensions based on crowdworkers' gender, education, income, job nature, and health condition. Further, they illustrate that different crowdworkers have different needs and threshold of demands and resources and that this plays a significant role in terms of moderating the crowdwork experience and platform commitment. Overall, our study sheds light to the work experiences of the Chinese crowdworkers and at the same time contributes to furthering understandings related to the work experiences of crowdworkers.},
journal = {Proc. ACM Hum.-Comput. Interact.},
month = jan,
articleno = {07},
numpages = {40},
keywords = {work experience, work design questionnaire, utrecht work engagement scale, oldenburg burnout inventory, job-demands resources model, crowdsourcing, china}
}

@article{10.1145/3391707,
author = {Huang, Yapei and Tian, Yun and Liu, Zhijie and Jin, Xiaowei and Liu, Yanan and Zhao, Shifeng and Tian, Daxin},
title = {A Traffic Density Estimation Model Based on Crowdsourcing Privacy Protection},
year = {2020},
issue_date = {August 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {11},
number = {4},
issn = {2157-6904},
url = {https://doi.org/10.1145/3391707},
doi = {10.1145/3391707},
abstract = {Acquiring traffic condition information is of great significance in transportation guidance, urban planning, and route recommendation. To date, traffic density data are generally acquired by road sound analysis, video data analysis, or in-vehicle network communication, which are usually financially or temporally expensive. Another way to get traffic conditions is to collect track data by crowdsourcing. However, this way lead to a greater risk of leaking users’ privacy. To avoid the risk, this article proposes a traffic density estimation model based on crowdsourcing privacy protection. First, in the acquisition process of the track data by crowdsourcing, dual servers are employed for transmission, and homomorphic encryption is carried out to encrypt the data to protect the data from being leaked during transmission. Second, sampling is implemented for randomization and anonymization to reduce the spatial continuity and temporal continuity of position data. In this way, the intermediate server cannot acquire users’ original data, and the main server cannot obtain users’ personal information. Finally, before data transmission, Laplace noising is performed on the users’ local position data to further protect the original location information. The proposed algorithm in this study realizes that only users have their original track data, and the servers involved in the work cannot infer the original track data, which ensures the real security of user privacy. The proposed algorithm was verified with the track data from the Didi Gaia Data Opening Plan. The experimental results showed that the proposed algorithm could still maintain the validity of data analysis results and the security of user data privacy after homomorphic encryption, noise addition, and sample collection, and displayed good robustness and scalability.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = may,
articleno = {46},
numpages = {18},
keywords = {traffic flow, sample, encryption, crowdsourcing, Differential privacy}
}

@article{10.1145/3134748,
author = {Xia, Huichuan and Wang, Yang and Huang, Yun and Shah, Anuj},
title = {"Our Privacy Needs to be Protected at All Costs": Crowd Workers' Privacy Experiences on Amazon Mechanical Turk},
year = {2017},
issue_date = {November 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {1},
number = {CSCW},
url = {https://doi.org/10.1145/3134748},
doi = {10.1145/3134748},
abstract = {Crowdsourcing platforms such as Amazon Mechanical Turk (MTurk) are widely used by organizations, researchers, and individuals to outsource a broad range of tasks to crowd workers. Prior research has shown that crowdsourcing can pose privacy risks (e.g., de-anonymization) to crowd workers. However, little is known about the specific privacy issues crowd workers have experienced and how they perceive the state of privacy in crowdsourcing. In this paper, we present results from an online survey of 435 MTurk crowd workers from the US, India, and other countries and areas. Our respondents reported different types of privacy concerns (e.g., data aggregation, profiling, scams), experiences of privacy losses (e.g., phishing, malware, stalking, targeted ads), and privacy expectations on MTurk (e.g., screening tasks). Respondents from multiple countries and areas reported experiences with the same privacy issues, suggesting that these problems may be endemic to the whole MTurk platform. We discuss challenges, high-level principles and concrete suggestions in protecting crowd workers'; privacy on MTurk and in crowdsourcing more broadly.},
journal = {Proc. ACM Hum.-Comput. Interact.},
month = dec,
articleno = {113},
numpages = {22},
keywords = {privacy, crowdsourcing, amazon mechanical turk (mturk)}
}

@article{10.1007/s00778-020-00613-w,
author = {Yang, Jingru and Fan, Ju and Wei, Zhewei and Li, Guoliang and Liu, Tongyu and Du, Xiaoyong},
title = {A game-based framework for crowdsourced data labeling},
year = {2020},
issue_date = {Nov 2020},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {29},
number = {6},
issn = {1066-8888},
url = {https://doi.org/10.1007/s00778-020-00613-w},
doi = {10.1007/s00778-020-00613-w},
abstract = {Data labeling, which assigns data with multiple classes, is indispensable for many applications, such as machine learning and data integration. However, existing labeling solutions either incur expensive cost for large datasets or produce noisy results. This paper introduces a cost-effective labeling approach and focuses on the labeling rule generation problem that aims to generate high-quality rules to largely reduce the labeling cost while preserving quality. To address the problem, we first generate candidate rules and then devise a game-based crowdsourcing approach CrowdGame to select high-quality rules by considering coverage and accuracy. CrowdGame employs two groups of crowd workers: One group answers rule validation tasks (whether a rule is valid) to play a role of rule generator, while the other group answers tuple checking tasks (whether the label of a data tuple is correct) to play a role of rule refuter. We let the two groups play a two-player game: Rule generator identifies high-quality rules with large coverage, while rule refuter tries to refute its opponent rule generator by checking some tuples that provide enough evidence to reject rules with low accuracy. This paper studies the challenges in CrowdGame. The first is to balance the trade-off between coverage and accuracy. We define the loss of a rule by considering the two factors. The second is rule accuracy estimation. We utilize Bayesian estimation to combine both rule validation and tuple checking tasks. The third is to select crowdsourcing tasks to fulfill the game-based framework for minimizing the loss. We introduce a minimax strategy and develop efficient task selection algorithms. We also develop a hybrid crowd-machine method for effective label assignment under budget-constrained crowdsourcing settings. We conduct experiments on entity matching and relation extraction, and the results show that our method outperforms state-of-the-art solutions.},
journal = {The VLDB Journal},
month = may,
pages = {1311–1336},
numpages = {26},
keywords = {Labeling rules, Data labeling, Crowdsourcing}
}

@article{10.1145/3148148,
author = {Daniel, Florian and Kucherbaev, Pavel and Cappiello, Cinzia and Benatallah, Boualem and Allahbakhsh, Mohammad},
title = {Quality Control in Crowdsourcing: A Survey of Quality Attributes, Assessment Techniques, and Assurance Actions},
year = {2018},
issue_date = {January 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {51},
number = {1},
issn = {0360-0300},
url = {https://doi.org/10.1145/3148148},
doi = {10.1145/3148148},
abstract = {Crowdsourcing enables one to leverage on the intelligence and wisdom of potentially large groups of individuals toward solving problems. Common problems approached with crowdsourcing are labeling images, translating or transcribing text, providing opinions or ideas, and similar—all tasks that computers are not good at or where they may even fail altogether. The introduction of humans into computations and/or everyday work, however, also poses critical, novel challenges in terms of quality control, as the crowd is typically composed of people with unknown and very diverse abilities, skills, interests, personal objectives, and technological resources. This survey studies quality in the context of crowdsourcing along several dimensions, so as to define and characterize it and to understand the current state of the art. Specifically, this survey derives a quality model for crowdsourcing tasks, identifies the methods and techniques that can be used to assess the attributes of the model, and the actions and strategies that help prevent and mitigate quality problems. An analysis of how these features are supported by the state of the art further identifies open issues and informs an outlook on hot future research directions.},
journal = {ACM Comput. Surv.},
month = jan,
articleno = {7},
numpages = {40},
keywords = {quality model, attributes, assurance, assessment, Crowdsourcing}
}

@article{10.1145/3625303,
author = {Mohanty, Vikram and Luther, Kurt},
title = {DoubleCheck: Designing Community-based Assessability for Historical Person Identification},
year = {2023},
issue_date = {December 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {16},
number = {4},
issn = {1556-4673},
url = {https://doi.org/10.1145/3625303},
doi = {10.1145/3625303},
abstract = {Historical photos are valuable for their cultural and economic significance, but can be difficult to identify accurately due to various challenges such as low-quality images, lack of corroborating evidence, and limited research resources. Misidentified photos can have significant negative consequences, including lost economic value, incorrect historical records, and the spread of misinformation that can lead to perpetuating conspiracy theories. To accurately assess the credibility of a photo identification (ID), it may be necessary to conduct investigative research, use domain knowledge, and consult experts. In this article, we introduce DoubleCheck, a quality assessment framework for verifying historical photo IDs on Civil War Photo Sleuth (CWPS), a popular online platform for identifying American Civil War-era photos using facial recognition and crowdsourcing. DoubleCheck focuses on improving CWPS’s user experience and system architecture to display information useful for assessing the quality of historical photo IDs on CWPS. In a mixed-methods evaluation of DoubleCheck, we found that users contributed a wide diversity of sources for photo IDs, which helped facilitate the community’s assessment of these IDs through DoubleCheck’s provenance visualizations. Further, DoubleCheck’s quality assessment badges and visualizations supported users in making accurate assessments of photo IDs, even in cases involving ID conflicts.},
journal = {J. Comput. Cult. Herit.},
month = nov,
articleno = {79},
numpages = {27},
keywords = {information assessability, provenance, assessable systems, digital humanities, cultural heritage, history, person identification, online communities, Crowdsourcing}
}

@article{10.1145/3359227,
author = {d'Eon, Greg and Goh, Joslin and Larson, Kate and Law, Edith},
title = {Paying Crowd Workers for Collaborative Work},
year = {2019},
issue_date = {November 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {3},
number = {CSCW},
url = {https://doi.org/10.1145/3359227},
doi = {10.1145/3359227},
abstract = {Collaborative crowdsourcing tasks allow crowd workers to solve problems that they could not handle alone, but worker motivation in these tasks is not well understood. In this paper, we study how to motivate groups of workers by paying them equitably. To this end, we characterize existing collaborative tasks based on the types of information available to crowd workers. Then, we apply concepts from equity theory to show how fair payments relate to worker motivation, and we propose two theoretically grounded classes of fair payments. Finally, we run two experiments using an audio transcription task on Amazon Mechanical Turk to understand how workers perceive these payments. Our results show that workers recognize fair and unfair payment divisions, but are biased toward payments that reward them more. Additionally, our data suggests that fair payments could lead to a small increase in worker effort. These results inform the design of future collaborative crowdsourcing tasks.},
journal = {Proc. ACM Hum.-Comput. Interact.},
month = nov,
articleno = {125},
numpages = {24},
keywords = {incentives, equity theory, crowdsourcing, cooperative game theory}
}

@article{10.1145/3610055,
author = {Tazi, Faiza and Shrestha, Sunny and Das, Sanchari},
title = {Cybersecurity, Safety, \&amp; Privacy Concerns of Student Support Structure for Information and Communication Technologies in Online Education},
year = {2023},
issue_date = {October 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {CSCW2},
url = {https://doi.org/10.1145/3610055},
doi = {10.1145/3610055},
abstract = {COVID-19 has created a dramatic paradigm shift in education methods, which forced schools and universities to abandon the usual in-person education in favor of online education modules. Such a shift has extended the time and use of internet communication technologies (ICTs) by most, making online education platforms primary cyberattack targets. In this context, this study aims to explore parents, educators, and other caregivers' concerns about online education and the cybersecurity of their children and students. Thus, we conducted a survey-based study with 983 participants recruited through popular crowdsourcing platforms: MTurk and Prolific. Our results indicate a lack of technical support following cyber safety that the students received with the sudden transition to online education. Over 31\% of our participants claimed that they never or rarely receive any communication related to cyber safety from the students' educational institutions. Additionally, our analysis shows that the student support structure needs to be trained and informed on the threats faced by children online and on the ways to mitigate these threats. Finally, we find a statistically significant difference between parents, educators, and other caregivers regarding their perceptions of children's online privacy and cyber safety. We conclude this work by providing actionable recommendations to promote privacy-preserving and digitally secure online education.},
journal = {Proc. ACM Hum.-Comput. Interact.},
month = oct,
articleno = {264},
numpages = {40},
keywords = {COVID-19, ICTs, children cyber safety, online education, online education privacy, remote learning, remote learning cybersecurity}
}

@article{10.1145/3130916,
author = {Goncalves, Jorge and Hosio, Simo and van Berkel, Niels and Ahmed, Furqan and Kostakos, Vassilis},
title = {CrowdPickUp: Crowdsourcing Task Pickup in the Wild},
year = {2017},
issue_date = {September 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {1},
number = {3},
url = {https://doi.org/10.1145/3130916},
doi = {10.1145/3130916},
abstract = {We develop and evaluate a new ubiquitous crowdsourcing platform called CrowdPickUp, that combines the advantages of mobile and situated crowdsourcing to overcome their respective limitations. In a 19-day long field study with 70 participants, we evaluate the quality of work that CrowdPickUp produces. In particular, we measure quality in terms of worker performance in a variety of tasks (requiring local knowledge, location-based, general) while using a number of different quality control mechanisms, and also capture workers’ perceptions of the platform. Our findings show that workers of CrowdPickUp contributed data of comparable quality to previously presented crowdsourcing deployments while at the same time allowing for a wide breadth of tasks to be deployed. Finally, we offer insights towards the continued exploration of this research agenda.},
journal = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
month = sep,
articleno = {51},
numpages = {22},
keywords = {worker performance, ubiquitous crowdsourcing, tasks, situated crowdsourcing, mobile crowdsourcing, location-based, local knowledge, crowdsourcing}
}

@article{10.1145/3594721,
author = {Daquino, Marilena and Wigham, Mari and Daga, Enrico and Giagnolini, Lucia and Tomasi, Francesca},
title = {CLEF. A Linked Open Data Native System for Crowdsourcing},
year = {2023},
issue_date = {September 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {16},
number = {3},
issn = {1556-4673},
url = {https://doi.org/10.1145/3594721},
doi = {10.1145/3594721},
abstract = {Collaborative data collection initiatives are increasingly becoming pivotal to cultural institutions and scholars, to boost the population of born-digital archives. For over a decade, organisations have been leveraging Semantic Web technologies to design their workflows, ensure data quality, and a means for sharing and reusing (Linked Data). Crucially, scholarly projects that leverage cultural heritage data to collaboratively develop new resources would benefit from agile solutions to simplify the Linked Data production workflow via user-friendly interfaces. To date, only a few pioneers have abandoned legacy cataloguing and archiving systems to fully embrace the Linked Open Data (LOD) paradigm and manage their catalogues or research products through LOD-native management systems. In this article we present Crowdsourcing Linked Entities via web Form (CLEF), an agile LOD-native platform for collaborative data collection, peer-review, and publication. We detail design choices as motivated by two case studies, from the Cultural Heritage and scholarly domains respectively, and we discuss benefits of our solution in the light of prior works. In particular, the strong focus on user-friendly interfaces for producing FAIR data, the provenance-aware editorial process, and the integration with consolidated data management workflows, distinguish CLEF as a novel attempt to develop Linked Data platforms for cultural heritage.},
journal = {J. Comput. Cult. Herit.},
month = aug,
articleno = {41},
numpages = {17},
keywords = {Wikidata, Linked Open Data, provenance, Crowdsourcing}
}

@article{10.1145/3546916,
author = {Qu, Yunke and Roitero, Kevin and Barbera, David La and Spina, Damiano and Mizzaro, Stefano and Demartini, Gianluca},
title = {Combining Human and Machine Confidence in Truthfulness Assessment},
year = {2022},
issue_date = {March 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {15},
number = {1},
issn = {1936-1955},
url = {https://doi.org/10.1145/3546916},
doi = {10.1145/3546916},
abstract = {Automatically detecting online misinformation at scale is a challenging and interdisciplinary problem. Deciding what is to be considered truthful information is sometimes controversial and also difficult for educated experts. As the scale of the problem increases, human-in-the-loop approaches to truthfulness that combine both the scalability of machine learning (ML) and the accuracy of human contributions have been considered.In this work, we look at the potential to automatically combine machine-based systems with human-based systems. The former exploit superviseds ML approaches; the latter involve either crowd workers (i.e., human non-experts) or human experts. Since both ML and crowdsourcing approaches can produce a score indicating the level of confidence on their truthfulness judgments (either algorithmic or self-reported, respectively), we address the question of whether it is feasible to make use of such confidence scores to effectively and efficiently combine three approaches: (i) machine-based methods, (ii) crowd workers, and (iii) human experts. The three approaches differ significantly, as they range from available, cheap, fast, scalable, but less accurate to scarce, expensive, slow, not scalable, but highly accurate.},
journal = {J. Data and Information Quality},
month = dec,
articleno = {5},
numpages = {17},
keywords = {hybrid intelligence, crowdsourcing, Misinformation}
}

@article{10.1145/3582273,
author = {Fulcini, Tommaso and Coppola, Riccardo and Ardito, Luca and Torchiano, Marco},
title = {A Review on Tools, Mechanics, Benefits, and Challenges of Gamified Software Testing},
year = {2023},
issue_date = {December 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {55},
number = {14s},
issn = {0360-0300},
url = {https://doi.org/10.1145/3582273},
doi = {10.1145/3582273},
abstract = {Gamification is an established practice in Software Engineering to increase effectiveness and engagement in many practices. This manuscript provides a characterization of the application of gamification to the Software Testing area. Such practice in fact reportedly suffers from low engagement by both personnel in industrial contexts and learners in educational contexts. Our goal is to identify the application areas and utilized gamified techniques and mechanics, the provided benefits and drawbacks, as well as the open challenges in the field. To this purpose, we conducted a Multivocal Literature Review to identify white and grey literature sources addressing gamified software testing.We analyzed 73 contributions and summarized the most common gamified mechanics, concepts, tools, and domains where they are mostly applied. We conclude that gamification in software testing is mostly applied to the test creation phase with simple white-box unit or mutation testing tools and is mostly used to foster good behaviors by promoting the testers’ accomplishment. Key research areas and main challenges in the field are: careful design of tailored gamified mechanics for specific testing techniques; the need for technological improvements to enable crowdsourcing, cooperation, and concurrency; the necessity for empirical and large-scale evaluation of the benefits delivered by gamification mechanics.},
journal = {ACM Comput. Surv.},
month = jul,
articleno = {310},
numpages = {37},
keywords = {Multivocal Literature Review, Systematic Literature Review, Software Engineering, software testing, testing and debugging gamification, Software/program verification}
}

@article{10.1145/3570615,
author = {Cui, Shuang and Han, Kai and Tang, Jing and Huang, He and Li, Xueying and Li, Zhiyu},
title = {Streaming Algorithms for Constrained Submodular Maximization},
year = {2022},
issue_date = {December 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {6},
number = {3},
url = {https://doi.org/10.1145/3570615},
doi = {10.1145/3570615},
abstract = {It is of great importance to design streaming algorithms for submodular maximization, as many applications (e.g., crowdsourcing) have large volume of data satisfying the well-known ''diminishing returns'' property, which cannot be handled by offline algorithms requiring full access to the whole dataset. However, streaming submodular maximization has been less studied than the offline algorithms due to the hardness brought by more stringent requirements on memory consumption. In this paper, we consider the fundamental problem of Submodular Maximization under k-System and d-Knapsack constraints (SMSK), which has only been successfully addressed by offline algorithms in previous studies, and we propose the first streaming algorithm for it with provable performance bounds. Our approach adopts a novel algorithmic framework dubbed MultiplexGreedy, making it also perform well under a single k-system constraint. For the special case of SMSK with only d-knapsack constraints, we further propose a streaming algorithm with better performance ratios than the state-of-the-art algorithms. As the SMSK problem generalizes most of the major problems studied in submodular maximization, our algorithms have wide applications in big data processing.},
journal = {Proc. ACM Meas. Anal. Comput. Syst.},
month = dec,
articleno = {54},
numpages = {32},
keywords = {optimization, machine learning, big data}
}

@article{10.1145/3626956,
author = {Shangqi, Lu and Martens, Wim and Niewerth, Matthias and Tao, Yufei},
title = {Partial Order Multiway Search},
year = {2023},
issue_date = {December 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {48},
number = {4},
issn = {0362-5915},
url = {https://doi.org/10.1145/3626956},
doi = {10.1145/3626956},
abstract = {Partial order multiway search (POMS) is a fundamental problem that finds applications in crowdsourcing, distributed file systems, software testing, and more. This problem involves an interaction between an algorithm 𝒜 and an oracle, conducted on a directed acyclic graph 𝒢 known to both parties. Initially, the oracle selects a vertex t in 𝒢 called the target. Subsequently, 𝒜 must identify the target vertex by probing reachability. In each probe, 𝒜 selects a set Q of vertices in 𝒢, the number of which is limited by a pre-agreed value k. The oracle then reveals, for each vertex q ∈ Q, whether q can reach the target in 𝒢. The objective of 𝒜 is to minimize the number of probes. We propose an algorithm to solve POMS in  (O(log _{1+k} n + frac{d}{k} log _{1+d} n))  probes, where n represents the number of vertices in 𝒢, and d denotes the largest out-degree of the vertices in 𝒢. The probing complexity is asymptotically optimal. Our study also explores two new POMS variants: The first one, named taciturn POMS, is similar to classical POMS but assumes a weaker oracle, and the second one, named EM POMS, is a direct extension of classical POMS to the external memory (EM) model. For both variants, we introduce algorithms whose performance matches or nearly matches the corresponding theoretical lower bounds.},
journal = {ACM Trans. Database Syst.},
month = nov,
articleno = {10},
numpages = {31},
keywords = {lower bounds, data structures, graph algorithms, Partial order}
}

@article{10.14778/3067421.3067431,
author = {Jain, Ayush and Sarma, Akash Das and Parameswaran, Aditya and Widom, Jennifer},
title = {Understanding workers, developing effective tasks, and enhancing marketplace dynamics: a study of a large crowdsourcing marketplace},
year = {2017},
issue_date = {March 2017},
publisher = {VLDB Endowment},
volume = {10},
number = {7},
issn = {2150-8097},
url = {https://doi.org/10.14778/3067421.3067431},
doi = {10.14778/3067421.3067431},
abstract = {We conduct an experimental analysis of a dataset comprising over 27 million microtasks performed by over 70,000 workers issued to a large crowdsourcing marketplace between 2012--2016. Using this data---never before analyzed in an academic context---we shed light on three crucial aspects of crowdsourcing: (1) Task design---helping requesters understand what constitutes an effective task, and how to go about designing one; (2) Marketplace dynamics --- helping marketplace administrators and designers understand the interaction between tasks and workers, and the corresponding marketplace load; and (3) Worker behavior --- understanding worker attention spans, lifetimes, and general behavior, for the improvement of the crowdsourcing ecosystem as a whole.},
journal = {Proc. VLDB Endow.},
month = mar,
pages = {829–840},
numpages = {12}
}

@article{10.1109/TNET.2017.2766280,
author = {Chatterjee, Avhishek and Varshney, Lav R. and Vishwanath, Sriram},
title = {Work Capacity of Regulated Freelance Platforms: Fundamental Limits and Decentralized Schemes},
year = {2017},
issue_date = {December 2017},
publisher = {IEEE Press},
volume = {25},
number = {6},
issn = {1063-6692},
url = {https://doi.org/10.1109/TNET.2017.2766280},
doi = {10.1109/TNET.2017.2766280},
abstract = {Crowdsourcing of jobs to online freelance platforms is rapidly gaining popularity. Most crowdsourcing platforms are uncontrolled and offer freedom to customers and freelancers to choose each other. This works well for unskilled jobs e.g., image classification with no specific quality requirement since freelancers are functionally identical. For skilled jobs e.g., software development with specific quality requirements, however, this does not ensure that the maximum number of job requests is satisfied. In this paper, we determine the capacity of regulated freelance systems, in terms of maximum satisfied job requests, and propose centralized schemes that achieve capacity. To ensure decentralized operation and freedom for customers and freelancers, we propose simple schemes compatible with the operation of current crowdsourcing platforms that approximately achieve capacity. Furthermore, for settings where the number of job requests exceeds capacity, we propose a scheme that is agnostic of that information, but is optimal and fair in declining jobs without wait.},
journal = {IEEE/ACM Trans. Netw.},
month = dec,
pages = {3641–3654},
numpages = {14}
}

@article{10.1145/3434421,
author = {Jalaly, Pooya and Tardos, \'{E}va},
title = {Simple and Efficient Budget Feasible Mechanisms for Monotone Submodular Valuations},
year = {2021},
issue_date = {March 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {9},
number = {1},
issn = {2167-8375},
url = {https://doi.org/10.1145/3434421},
doi = {10.1145/3434421},
abstract = {We study the problem of a budget limited buyer who wants to buy a set of items, each from a different seller, to maximize her value. The budget feasible mechanism design problem requires the design a mechanism that incentivizes the sellers to truthfully report their cost and maximizes the buyer’s value while guaranteeing that the total payment does not exceed her budget. Such budget feasible mechanisms can model a buyer in a crowdsourcing market interested in recruiting a set of workers (sellers) to accomplish a task for her.This budget feasible mechanism design problem was introduced by Singer in 2010. We consider the general case where the buyer’s valuation is a monotone submodular function. There are a number of truthful mechanisms known for this problem. We offer two general frameworks for simple mechanisms, and by combining these frameworks, we significantly improve on the best known results, while also simplifying the analysis. For example, we improve the approximation guarantee for the general monotone submodular case from 7.91 to 5 and for the case of large markets (where each individual item has negligible value) from 3 to 2.58. More generally, given an r approximation algorithm for the optimization problem (ignoring incentives), our mechanism is a r + 1 approximation mechanism for large markets, an improvement from 2r2. We also provide a mechanism without the large market assumption, where we achieve a 4r + 1 approximation guarantee. We also show how our results can be used for the problem of a principal hiring in a Crowdsourcing Market to select a set of tasks subject to a total budget.},
journal = {ACM Trans. Econ. Comput.},
month = jan,
articleno = {4},
numpages = {20},
keywords = {submodular valuations, algorithmic mechanism design, algorithmic game theory, Budget feasible mechanisms}
}

@article{10.1145/3130914,
author = {Gadiraju, Ujwal and Checco, Alessandro and Gupta, Neha and Demartini, Gianluca},
title = {Modus Operandi of Crowd Workers: The Invisible Role of Microtask Work Environments},
year = {2017},
issue_date = {September 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {1},
number = {3},
url = {https://doi.org/10.1145/3130914},
doi = {10.1145/3130914},
abstract = {The ubiquity of the Internet and the widespread proliferation of electronic devices has resulted in flourishing microtask crowdsourcing marketplaces, such as Amazon MTurk. An aspect that has remained largely invisible in microtask crowdsourcing is that of work environments; defined as the hardware and software affordances at the disposal of crowd workers which are used to complete microtasks on crowdsourcing platforms. In this paper, we reveal the significant role of work environments in the shaping of crowd work. First, through a pilot study surveying the good and bad experiences workers had with UI elements in crowd work, we revealed the typical issues workers face. Based on these findings, we then deployed over 100 distinct microtasks on CrowdFlower, addressing workers in India and USA in two identical batches. These tasks emulate the good and bad UI element designs that characterize crowdsourcing microtasks. We recorded hardware specifics such as CPU speed and device type, apart from software specifics including the browsers used to complete tasks, operating systems on the device, and other properties that define the work environments of crowd workers. Our findings indicate that crowd workers are embedded in a variety of work environments which influence the quality of work produced. To confirm and validate our data-driven findings we then carried out semi-structured interviews with a sample of Indian and American crowd workers from this platform. Depending on the design of UI elements in microtasks, we found that some work environments support crowd workers more than others. Based on our overall findings resulting from all the three studies, we introduce ModOp, a tool that helps to design crowdsourcing microtasks that are suitable for diverse crowd work environments. We empirically show that the use of ModOp results in reducing the cognitive load of workers, thereby improving their user experience without affecting the accuracy or task completion time.},
journal = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
month = sep,
articleno = {49},
numpages = {29},
keywords = {Work Environment, User Interface, Performance, Microtasks, Human Factors, Design, Crowdsourcing, Crowd Workers}
}

@article{10.1145/3570347,
author = {Chen, Dayin and Shi, Xiaodan and Song, Xuan and Chen, Zhiheng and Zhang, Haoran},
title = {Phone-based Ambient Temperature Measurement with a New Confidence-based Truth Inference Model},
year = {2023},
issue_date = {December 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {6},
number = {4},
url = {https://doi.org/10.1145/3570347},
doi = {10.1145/3570347},
abstract = {Monitoring the indoor temperature can help saving of energy and improve the comfort level. Smartphone, as a ubiquitous device, can be an additional data source to provide the ambient temperature estimation. However, the estimation results sometimes can be unreliable due to the different phone using states. How to integrate multiple estimation results in one area to get a more accurate prediction result is still a challenge. In this work, we proposed one phone-based ambient temperature measurement system which contains two models. The first temperature prediction model takes easily accessible phone state features as inputs and outputs ambient temperature prediction with a confidence value. The second truth inference model takes multiple prediction results with confidences as inputs and outputs a referred final prediction result. Our temperature prediction model reaches 0.253°C with MAE in our testing set. We also proved by transfer learning our model can be used in other new type of phones. We evaluate the truth inference model in our testing dataset and it reaches 0.128°C, which outperforms the state-of-the-art truth inference algorithms. We believe this work can contribute to energy conservation and provide new ideas for crowdsourcing.},
journal = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
month = jan,
articleno = {207},
numpages = {18},
keywords = {truth inference, phone, energy conservation, crowdsourcing, attention}
}

@article{10.1145/3421712,
author = {Tu, Jinzheng and Yu, Guoxian and Wang, Jun and Domeniconi, Carlotta and Guo, Maozu and Zhang, Xiangliang},
title = {CrowdWT: Crowdsourcing via Joint Modeling of Workers and Tasks},
year = {2020},
issue_date = {February 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {15},
number = {1},
issn = {1556-4681},
url = {https://doi.org/10.1145/3421712},
doi = {10.1145/3421712},
abstract = {Crowdsourcing is a relatively inexpensive and efficient mechanism to collect annotations of data from the open Internet. Crowdsourcing workers are paid for the provided annotations, but the task requester usually has a limited budget. It is desirable to wisely assign the appropriate task to the right workers, so the overall annotation quality is maximized while the cost is reduced. In this article, we propose a novel task assignment strategy (CrowdWT) to capture the complex interactions between tasks and workers, and properly assign tasks to workers. CrowdWT first develops a Worker Bias Model (WBM) to jointly model the worker’s bias, the ground truths of tasks, and the task features. WBM constructs a mapping between task features and worker annotations to dynamically assign the task to a group of workers, who are more likely to give correct annotations for the task. CrowdWT further introduces a Task Difficulty Model (TDM), which builds a Kernel ridge regressor based on task features to quantify the intrinsic difficulty of tasks and thus to assign the difficult tasks to more reliable workers. Finally, CrowdWT combines WBM and TDM into a unified model to dynamically assign tasks to a group of workers and recall more reliable and even expert workers to annotate the difficult tasks. Our experimental results on two real-world datasets and two semi-synthetic datasets show that CrowdWT achieves high-quality answers within a limited budget, and has the best performance against competitive methods.&lt;?vsp -1.5pt?&gt;},
journal = {ACM Trans. Knowl. Discov. Data},
month = dec,
articleno = {12},
numpages = {24},
keywords = {worker bias model, task difficulty model, task assignment, annotation quality, Crowdsourcing}
}

@article{10.1145/3274373,
author = {Lee, Sang Won and Krosnick, Rebecca and Park, Sun Young and Keelean, Brandon and Vaidya, Sach and O'Keefe, Stephanie D. and Lasecki, Walter S.},
title = {Exploring Real-Time Collaboration in Crowd-Powered Systems Through a UI Design Tool},
year = {2018},
issue_date = {November 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {2},
number = {CSCW},
url = {https://doi.org/10.1145/3274373},
doi = {10.1145/3274373},
abstract = {Real-time collaboration between a requester and crowd workers expands the scope of tasks that crowdsourcing can be used for by letting requesters and crowd workers interactively create various artifacts (e.g., a sketch prototype, writing, or program code). In such systems, it is increasingly common to allow requesters to verbally describe their requests, receive responses from workers, and provide immediate and continuous feedback to enhance the overall outcome of the two groups' real-time collaboration. This work is motivated by the lack of a deep understanding of the challenges that end users of such systems face in their communication with workers and the need of design implications that can address such challenges for other similar systems. In this paper, we investigate how requesters verbally communicate and collaborate with crowd workers to solve a complex task. Using a crowd-powered UI design tool, we conducted a qualitative user study to explore how requesters with varying expertise communicate and collaborate with crowd workers. Our work also identifies the unique challenges that collaborative crowdsourcing systems pose: potential expertise differences between requesters and crowd workers, the asymmetry of two-way communication (e.g., speech versus text), and the shared artifact's concurrent modification by two disparate groups. Finally, we make design recommendations that can inform the design of future real-time collaboration processes in crowdsourcing systems.},
journal = {Proc. ACM Hum.-Comput. Interact.},
month = nov,
articleno = {104},
numpages = {23},
keywords = {user interface design, prototyping, human computation, crowdsourcing, communication, collaborative design}
}

@article{10.1145/3321700,
author = {Feyisetan, Oluwaseyi and Simperl, Elena},
title = {Beyond Monetary Incentives: Experiments in Paid Microtask Contests},
year = {2019},
issue_date = {June 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {2},
number = {2},
url = {https://doi.org/10.1145/3321700},
doi = {10.1145/3321700},
abstract = {In this article, we aim to gain a better understanding into how paid microtask crowdsourcing could leverage its appeal and scaling power by using contests to boost crowd performance and engagement. We introduce our microtask-based annotation platform Wordsmith, which features incentives such as points, leaderboards, and badges on top of financial remuneration. Our analysis focuses on a particular type of incentive, contests, as a means to apply crowdsourcing in near-real-time scenarios, in which requesters need labels quickly. We model crowdsourcing contests as a continuous-time Markov chain with the objective to maximise the output of the crowd workers, while varying a parameter that determines whether a worker is eligible for a reward based on their present rank on the leaderboard. We conduct empirical experiments in which crowd workers recruited from CrowdFlower carry out annotation microtasks on Wordsmith—in our case, to identify named entities in a stream of Twitter posts. In the experimental conditions, we test different reward spreads and record the total number of annotations received. We compare the results against a control condition in which the same annotation task was completed on CrowdFlower without a time or contest constraint. The experiments show that rewarding only the best contributors in a live contest could be a viable model to deliver results faster, though quality might suffer for particular types of annotation tasks. Increasing the reward spread leads to more work being completed, especially by the top contestants. Overall, the experiments shed light on possible design improvements of paid microtasks platforms to boost task performance and speed and make the overall experience more fair and interesting for crowd workers.},
journal = {Trans. Soc. Comput.},
month = jun,
articleno = {6},
numpages = {31},
keywords = {paid microtasks, incentives, gamification, crowd computing, contests, Markov chains, Crowdsourcing}
}

@article{10.1109/TNET.2022.3228933,
author = {Xie, Hong and Lui, John C. S.},
title = {Cooperation Preference Aware Shapley Value: Modeling, Algorithms and Applications},
year = {2022},
issue_date = {Dec. 2023},
publisher = {IEEE Press},
volume = {31},
number = {6},
issn = {1063-6692},
url = {https://doi.org/10.1109/TNET.2022.3228933},
doi = {10.1109/TNET.2022.3228933},
abstract = {The Shapley value is a cornerstone in cooperative game theory and has been widely applied in networking, data science, etc. The classical Shapley value assumes that each player has an equal preference to cooperate with each other. Since the cooperation preference is an important factor of a variety of networking applications, we first generalize the classical Shapley value to allow general degree of the cooperation preference. In particular, we develop mathematical models to solicit two types of cooperation preferences, i.e., (1) group-wise preferences and (2) pair-wise preferences, and extend the classical Shapley value to capture this feature. Our second contribution is tackling the intrinsic computational challenge because even for the classical Shapley value, it is computationally expensive to evaluate. We design computationally efficient randomized algorithms with theoretical guarantees to fully cover the computational space of our generalized Shapley value. We also extend our models and algorithms to divide payoffs for multiple coalitions with dynamic preferences. We demonstrate the versatility of our framework by applying it to divide the revenue among ISPs in deploying new Internet architectures, as well as to divide the reward among workers in crowdsourcing systems.},
journal = {IEEE/ACM Trans. Netw.},
month = dec,
pages = {2439–2453},
numpages = {15}
}

@article{10.1145/3502720,
author = {Lo, Pei-Chi and Lim, Ee-Peng},
title = {Contextual Path Retrieval: A Contextual Entity Relation Embedding-based Approach},
year = {2023},
issue_date = {January 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {41},
number = {1},
issn = {1046-8188},
url = {https://doi.org/10.1145/3502720},
doi = {10.1145/3502720},
abstract = {Contextual path retrieval (CPR) refers to the task of finding contextual path(s) between a pair of entities in a knowledge graph that explains the connection between them in a given context. For this novel retrieval task, we propose the Embedding-based Contextual Path Retrieval (ECPR) framework. ECPR is based on a three-component structure that includes a context encoder and path encoder that encode query context and path, respectively, and a path ranker that assigns a ranking score to each candidate path to determine the one that should be the contextual path. For context encoding, we propose two novel context encoding methods, i.e., context-fused entity embeddings and contextualized embeddings. For path encoding, we propose PathVAE, an inductive embedding approach to generate path representations. Finally, we explore two path-ranking approaches. In our evaluation, we construct a synthetic dataset from Wikipedia and two real datasets of Wikinews articles constructed through crowdsourcing. Our experiments show that methods based on ECPR framework outperform baseline methods, and that our two proposed context encoders yield significantly better performance than baselines. We also analyze a few case studies to show the distinct features of ECPR-based methods.},
journal = {ACM Trans. Inf. Syst.},
month = jan,
articleno = {1},
numpages = {38},
keywords = {embedding learning, information retrieval, reasoning, Knowledge base}
}

@article{10.14778/3007263.3007293,
author = {Ikeda, Kosetsu and Morishima, Atsuyuki and Rahman, Habibur and Roy, Senjuti Basu and Thirumuruganathan, Saravanan and Amer-Yahia, Sihem and Das, Gautam},
title = {Collaborative crowdsourcing with crowd4U},
year = {2016},
issue_date = {September 2016},
publisher = {VLDB Endowment},
volume = {9},
number = {13},
issn = {2150-8097},
url = {https://doi.org/10.14778/3007263.3007293},
doi = {10.14778/3007263.3007293},
abstract = {Collaborative crowdsourcing is an emerging paradigm where a set of workers, often with diverse and complementary skills, form groups and work together to complete complex tasks. While crowdsourcing has been used successfully in many applications, collaboration is essential for achieving a high quality outcome for a number of emerging applications such as text translation, citizen journalism and surveillance tasks. However, no crowdsourcing platform today enables the end-to-end deployment of collaborative tasks. We demonstrate Crowd4U, a volunteer-based system that enables the deployment of diverse crowdsourcing tasks with complex data-flows, in a declarative manner. In addition to treating workers and tasks as rich entities, Crowd4U also provides an easy-to-use form-based task UI. Crowd4U implements worker-to-task assignment algorithms that are appropriate for each kind of task. Once workers are assigned to tasks, appropriate worker collaboration schemes are enforced in order to enable effective result coordination.},
journal = {Proc. VLDB Endow.},
month = sep,
pages = {1497–1500},
numpages = {4}
}

@article{10.5555/3122009.3242050,
author = {Vaughan, Jennifer Wortman},
title = {Making better use of the crowd: how crowdsourcing can advance machine learning research},
year = {2017},
issue_date = {January 2017},
publisher = {JMLR.org},
volume = {18},
number = {1},
issn = {1532-4435},
abstract = {This survey provides a comprehensive overview of the landscape of crowdsourcing research, targeted at the machine learning community. We begin with an overview of the ways in which crowdsourcing can be used to advance machine learning research, focusing on four application areas: 1) data generation, 2) evaluation and debugging of models, 3) hybrid intelligence systems that leverage the complementary strengths of humans and machines to expand the capabilities of AI, and 4) crowdsourced behavioral experiments that improve our understanding of how humans interact with machine learning systems and technology more broadly. We next review the extensive literature on the behavior of crowdworkers themselves. This research, which explores the prevalence of dishonesty among crowdworkers, how workers respond to both monetary incentives and intrinsic forms of motivation, and how crowdworkers interact with each other, has immediate implications that we distill into best practices that researchers should follow when using crowdsourcing in their own research. We conclude with a discussion of additional tips and best practices that are crucial to the success of any project that uses crowdsourcing, but rarely mentioned in the literature.},
journal = {J. Mach. Learn. Res.},
month = jan,
pages = {7026–7071},
numpages = {46},
keywords = {model evaluation, mechanical turk, incentives, hybrid intelligence, data generation, crowdsourcing, behavioral experiments}
}

@article{10.1007/s00778-017-0484-3,
author = {Hung, Nguyen Quoc and Thang, Duong Chi and Tam, Nguyen Thanh and Weidlich, Matthias and Aberer, Karl and Yin, Hongzhi and Zhou, Xiaofang},
title = {Answer validation for generic crowdsourcing tasks with minimal efforts},
year = {2017},
issue_date = {December  2017},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {26},
number = {6},
issn = {1066-8888},
url = {https://doi.org/10.1007/s00778-017-0484-3},
doi = {10.1007/s00778-017-0484-3},
abstract = {Crowdsourcing has been established as an essential means to scale human computation in diverse Web applications, reaching from data integration to information retrieval. Yet, crowd workers have wide-ranging levels of expertise. Large worker populations are heterogeneous and comprise a significant amount of faulty workers. As a consequence, quality insurance for crowd answers is commonly seen as the Achilles heel of crowdsourcing. Although various techniques for quality control have been proposed in recent years, a post-processing phase in which crowd answers are validated is still required. Such validation, however, is typically conducted by experts, whose availability is limited and whose work incurs comparatively high costs. This work aims at guiding an expert in the validation of crowd answers. We present a probabilistic model that helps to identify the most beneficial validation questions in terms of both improvement in result correctness and detection of faulty workers. By seeking expert feedback on the most problematic cases, we are able to obtain a set of high-quality answers, even if the expert does not validate the complete answer set. Our approach is applicable for a broad range of crowdsourcing tasks, including classification and counting. Our comprehensive evaluation using both real-world and synthetic datasets demonstrates that our techniques save up to 60\% of expert efforts compared to baseline methods when striving for perfect result correctness. In absolute terms, for most cases, we achieve close to perfect correctness after expert input has been sought for only 15\% of the crowdsourcing tasks.},
journal = {The VLDB Journal},
month = dec,
pages = {855–880},
numpages = {26},
keywords = {Validation, Probabilistic model, Guiding user feedback, Generic tasks, Crowdsourcing}
}

@article{10.1145/3512955,
author = {Norris, Wendy and Voida, Amy and Voida, Stephen},
title = {People Talk in Stories. Responders Talk in Data: A Framework for Temporal Sensemaking in Time- and Safety-critical Work},
year = {2022},
issue_date = {April 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {6},
number = {CSCW1},
url = {https://doi.org/10.1145/3512955},
doi = {10.1145/3512955},
abstract = {Global crowdsourcing teams who conduct humanitarian response use temporal narratives as a sensemaking device when time is a critical element of the data story. In dynamic situations in which the flow of online information is rapid, fluid, and disordered, the process of how distributed teams construct a temporal narrative is not well understood nor well supported by information and communication technologies (ICTs). Here, we examine an intense need for temporal sensemaking: time- and safety-critical information work during the 2017 Hurricane Maria crisis response in Puerto Rico. Our analysis of semi-structured interviews reveals how members of a global digital humanitarian group, The Standby Task Force (SBTF), use a process of triage, evaluation, negotiation, and synchronization to construct collective temporal narratives in their high-tempo, distributed information work. Informed by these empirical insights, we reflect on the design implications for cloud-based, collaborative ICTs used in time- and safety-critical remote work.},
journal = {Proc. ACM Hum.-Comput. Interact.},
month = apr,
articleno = {108},
numpages = {23},
keywords = {temporal coordination, sociotemporality, sensemaking, distributed work, digital humanitarian, crisis informatics}
}

@article{10.1145/3449193,
author = {Yang, Kexin Bella and Nagashima, Tomohiro and Yao, Junhui and Williams, Joseph Jay and Holstein, Kenneth and Aleven, Vincent},
title = {Can Crowds Customize Instructional Materials with Minimal Expert Guidance? Exploring Teacher-guided Crowdsourcing for Improving Hints in an AI-based Tutor},
year = {2021},
issue_date = {April 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {5},
number = {CSCW1},
url = {https://doi.org/10.1145/3449193},
doi = {10.1145/3449193},
abstract = {AI-based educational technologies may be most welcome in classrooms when they align with teachers' goals, preferences, and instructional practices. Teachers, however, have scarce time to make such customizations themselves. How might the crowd be leveraged to help time-strapped teachers? Crowdsourcing pipelines have traditionally focused on content generation. It is an open question how a pipeline might be designed so the crowd can succeed in a revision/customization task. In this paper, we explore an initial version of a teacher-guided crowdsourcing pipeline designed to improve the adaptive math hints of an AI-based tutoring system so they fit teachers' preferences, while requiring minimal expert guidance. In two experiments involving 144 math teachers and 481 crowdworkers, we found that such an expert-guided revision pipeline could save experts' time and produce better crowd-revised hints (in terms of teacher satisfaction) than two comparison conditions. The revised hints however, did not improve on the existing hints in the AI tutor, which were carefully-written but still have room for improvement and customization. Further analysis revealed that the main challenge for crowdworkers may lie in understanding teachers' brief written comments and implementing them in the form of effective edits, without introducing new problems. We also found that teachers preferred their own revisions over other sources of hints, and exhibited varying preferences for hints. Overall, the results confirm that there is a clear need for customizing hints to individual teachers' preferences. They also highlight the need for more elaborate scaffolds so the crowd can have specific knowledge of the requirements that teachers have for hints. The study represents a first exploration in the literature of how to support crowds with minimal expert guidance in revising and customizing instructional materials.},
journal = {Proc. ACM Hum.-Comput. Interact.},
month = apr,
articleno = {119},
numpages = {24},
keywords = {ai in education, expert-facilitated crowdsourcing, human computation, learning at scale, teachersourcing}
}

@article{10.1613/jair.1.13304,
author = {Liu, Chong and Wang, Yu-Xiang},
title = {Doubly Robust Crowdsourcing},
year = {2022},
issue_date = {May 2022},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {73},
issn = {1076-9757},
url = {https://doi.org/10.1613/jair.1.13304},
doi = {10.1613/jair.1.13304},
abstract = {Large-scale labeled dataset is the indispensable fuel that ignites the AI revolution as we see today. Most such datasets are constructed using crowdsourcing services such as Amazon Mechanical Turk which provides noisy labels from non-experts at a fair price. The sheer size of such datasets mandates that it is only feasible to collect a few labels per data point. We formulate the problem of test-time label aggregation as a statistical estimation problem of inferring the expected voting score. By imitating workers with supervised learners and using them in a doubly robust estimation framework, we prove that the variance of estimation can be substantially reduced, even if the learner is a poor approximation. Synthetic and real-world experiments show that by combining the doubly robust approach with adaptive worker/item selection rules, we often need much lower label cost to achieve nearly the same accuracy as in the ideal world where all workers label all data points.},
journal = {J. Artif. Int. Res.},
month = may,
numpages = {21}
}

@article{10.1145/3460865,
author = {Wu, Hanlu and Ma, Tengfei and Wu, Lingfei and Xu, Fangli and Ji, Shouling},
title = {Exploiting Heterogeneous Graph Neural Networks with Latent Worker/Task Correlation Information for Label Aggregation in Crowdsourcing},
year = {2021},
issue_date = {April 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {16},
number = {2},
issn = {1556-4681},
url = {https://doi.org/10.1145/3460865},
doi = {10.1145/3460865},
abstract = {Crowdsourcing has attracted much attention for its convenience to collect labels from non-expert workers instead of experts. However, due to the high level of noise from the non-experts, a label aggregation model that infers the true label from noisy crowdsourced labels is required. In this article, we propose a novel framework based on graph neural networks for aggregating crowd labels. We construct a heterogeneous graph between workers and tasks and derive a new graph neural network to learn the representations of nodes and the true labels. Besides, we exploit the unknown latent interaction between the same type of nodes (workers or tasks) by adding a homogeneous attention layer in the graph neural networks. Experimental results on 13 real-world datasets show superior performance over state-of-the-art models.},
journal = {ACM Trans. Knowl. Discov. Data},
month = sep,
articleno = {27},
numpages = {18},
keywords = {label aggregation, graph neural network, Crowdsourcing}
}

@article{10.1145/3512946,
author = {Kornfield, Rachel and Mohr, David C. and Ranney, Rachel and Lattie, Emily G. and Meyerhoff, Jonah and Williams, Joseph J. and Reddy, Madhu},
title = {Involving Crowdworkers with Lived Experience in Content-Development for Push-Based Digital Mental Health Tools: Lessons Learned from Crowdsourcing Mental Health Messages},
year = {2022},
issue_date = {April 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {6},
number = {CSCW1},
url = {https://doi.org/10.1145/3512946},
doi = {10.1145/3512946},
abstract = {Digital tools can support individuals managing mental health concerns, but delivering sufficiently engaging content is challenging. This paper seeks to clarify how individuals with mental health concerns can contribute content to improve push-based mental health messaging tools. We recruited crowdworkers with mental health symptoms to evaluate and revise expert-composed content for an automated messaging tool, and to generate new topics and messages. A second wave of crowdworkers evaluated expert and crowdsourced content. Crowdworkers generated topics for messages that had not been prioritized by experts, including self-care, positive thinking, inspiration, relaxation, and reassurance. Peer evaluators rated messages written by experts and peers similarly. Our findings also suggest the importance of personalization, particularly when content adaptation occurs over time as users interact with example messages. These findings demonstrate the potential of crowdsourcing for generating diverse and engaging content for push-based tools, and suggest the need to support users in meaningful content customization.},
journal = {Proc. ACM Hum.-Comput. Interact.},
month = apr,
articleno = {99},
numpages = {30},
keywords = {personalization, peer-to-peer support, mixed-methods research, mental health, digital health interventions, crowdsourcing}
}

@article{10.1145/3589325,
author = {Drien, Osnat and Freiman, Matanya and Amarilli, Antoine and Amsterdamer, Yael},
title = {Query-Guided Resolution in Uncertain Databases},
year = {2023},
issue_date = {June 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {1},
number = {2},
url = {https://doi.org/10.1145/3589325},
doi = {10.1145/3589325},
abstract = {We present a novel framework for uncertain data management. We start with a database whose tuple correctness is uncertain and an oracle that can resolve the uncertainty, i.e., decide if a tuple is correct or not. Such an oracle may correspond, e.g., to a data expert or to a crowdsourcing platform. We wish to use the oracle to clean the database with the goal of ensuring the correct answer for specific mission-critical queries. To avoid the prohibitive cost of cleaning the entire database and to minimize the expected number of calls to the oracle, we must carefully select tuples whose resolution would suffice to resolve the uncertainty in query results. In other words, we need a query-guided process for the resolution of uncertain data.We develop an end-to-end solution to this problem, based on the derivation of query answers and on correctness probabilities for the uncertain data. At a high level, we first track Boolean provenance to identify which input tuples contribute to the derivation of each output tuple, and in what ways. We then design an active learning solution for iteratively choosing tuples to resolve, based on the provenance structure and on an evolving estimation of tuple correctness probabilities. We conduct an extensive experimental study to validate our framework in different use cases.},
journal = {Proc. ACM Manag. Data},
month = jun,
articleno = {180},
numpages = {27},
keywords = {active learning, provenance, uncertain databases}
}

@article{10.1145/3555561,
author = {Miceli, Milagros and Posada, Julian},
title = {The Data-Production Dispositif},
year = {2022},
issue_date = {November 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {6},
number = {CSCW2},
url = {https://doi.org/10.1145/3555561},
doi = {10.1145/3555561},
abstract = {Machine learning (ML) depends on data to train and verify models. Very often, organizations outsource processes related to data work (i.e., generating and annotating data and evaluating outputs) through business process outsourcing (BPO) companies and crowdsourcing platforms. This paper investigates outsourced ML data work in Latin America by studying three platforms in Venezuela and a BPO in Argentina. We lean on the Foucauldian notion of dispositif to define the data-production dispositif as an ensemble of discourses, actions, and objects strategically disposed to (re)produce power/knowledge relations in data and labor. Our dispositif analysis comprises the examination of 210 data work instruction documents, 55 interviews with data workers, managers, and requesters, and participant observation. Our findings show that discourses encoded in instructions reproduce and normalize the worldviews of requesters. Precarious working conditions and economic dependency alienate workers, making them obedient to instructions. Furthermore, discourses and social contexts materialize in artifacts, such as interfaces and performance metrics, limiting workers' agency and normalizing specific ways of interpreting data. We conclude by stressing the importance of counteracting the data-production dispositif by fighting alienation and precarization, and empowering data workers to become assets in the quest for high-quality data.},
journal = {Proc. ACM Hum.-Comput. Interact.},
month = nov,
articleno = {460},
numpages = {37},
keywords = {platform labor, machine learning, data work, data production, data labeling, crowdsourcing}
}

@article{10.1145/3274403,
author = {Park, Sangkeun and Kwon, Sujin and Lee, Uichin},
title = {CampusWatch: Exploring Communitysourced Patrolling with Pervasive Mobile Technology},
year = {2018},
issue_date = {November 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {2},
number = {CSCW},
url = {https://doi.org/10.1145/3274403},
doi = {10.1145/3274403},
abstract = {Community policing to collaboratively maintain community safety and order in conjunction with law enforcement is becoming increasingly popular and efficient with the use of mobile technologies. Beyond sharing information about local problems such as crime via online discussion forums, there has been an increased focus on the impact of mobile crowdsourcing systems on community policing. In this study, we designed a novel crowdsourced patrolling campaign in which community members schedule their own patrol times and routes, then perform bike-based patrolling with video capturing using their smartphones. We conducted a four-week field study (n=20) on a university campus to verify the campaign's feasibility and observe users' behavior. Our results show key findings about users' task scheduling, event capturing and reporting behaviors, factors affecting task selection and execution and user motivation and engagement. Finally, we discuss several practical design implications in building crowdsourcing systems for community policing.},
journal = {Proc. ACM Hum.-Comput. Interact.},
month = nov,
articleno = {134},
numpages = {25},
keywords = {mobile sensing, crowdsourcing, crowdsensing, communitysourcing, community policing, community}
}

@article{10.1145/3274293,
author = {Berenberg, Daniel and Bagrow, James P.},
title = {Efficient Crowd Exploration of Large Networks: The Case of Causal Attribution},
year = {2018},
issue_date = {November 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {2},
number = {CSCW},
url = {https://doi.org/10.1145/3274293},
doi = {10.1145/3274293},
abstract = {Accurately and efficiently crowdsourcing complex, open-ended tasks can be difficult, as crowd participants tend to favor short, repetitive "microtasks". We study the crowdsourcing of large networks where the crowd provides the network topology via microtasks. Crowds can explore many types of social and information networks, but we focus on the network of causal attributions, an important network that signifies cause-and-effect relationships. We conduct experiments on Amazon Mechanical Turk (AMT) testing how workers can propose and validate individual causal relationships and introduce a method for independent crowd workers to explore large networks. The core of the method, Iterative Pathway Refinement, is a theoretically-principled mechanism for efficient exploration via microtasks. We evaluate the method using synthetic networks and apply it on AMT to extract a large-scale causal attribution network. Worker interactions reveal important characteristics of causal perception and the generated network data can help improve our understanding of causality and causal inference.},
journal = {Proc. ACM Hum.-Comput. Interact.},
month = nov,
articleno = {24},
numpages = {25},
keywords = {self-avoiding walks, networks, network motifs, microtasks, crowdwork, crowdsourcing, causality, causal attribution, amazon mechanical turk}
}

@article{10.1145/3586998,
author = {Li, Huiru and Jiang, Liangxiao and Xue, Siqing},
title = {Neighborhood Weighted Voting-Based Noise Correction for Crowdsourcing},
year = {2023},
issue_date = {August 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {17},
number = {7},
issn = {1556-4681},
url = {https://doi.org/10.1145/3586998},
doi = {10.1145/3586998},
abstract = {In crowdsourcing scenarios, we can obtain each instance’s multiple noisy labels set from different crowd workers and then use a ground truth inference algorithm to infer its integrated label. Despite the effectiveness of ground truth inference algorithms, a certain level of noise still remains in the integrated labels. To reduce the impact of noise, many noise correction algorithms have been proposed in recent years. To the best of our knowledge, however, nearly all existing noise correction algorithms only exploit each instance’s own multiple noisy label sets but ignore the multiple noisy label sets of its neighbors. Here neighbors refer to the nearest instances found in the feature space based on the distance metric learning. In this article, we propose neighborhood weighted voting-based noise correction (NWVNC). In NWVNC, we at first take advantage of the multiple noisy label sets of each instance’s neighbors (including itself) to estimate the probability that it belongs to its integrated label. Then, we use the estimated probability to identify and filter noise instances and thus obtain a clean set and a noise set. Finally, we train three heterogeneous classifiers on the clean set and correct the noise instances by the consensus voting of three trained classifiers. The experimental results on 34 simulated and two real-world crowdsourced datasets show that NWVNC significantly outperforms all the other state-of-the-art noise correction algorithms used for comparison.},
journal = {ACM Trans. Knowl. Discov. Data},
month = apr,
articleno = {96},
numpages = {18},
keywords = {consensus voting, neighborhood weighted voting, noise correction, Crowdsourcing learning}
}

@article{10.1145/3309543,
author = {Zhan, Xueying and Wang, Yaowei and Rao, Yanghui and Li, Qing},
title = {Learning from Multi-annotator Data: A Noise-aware Classification Framework},
year = {2019},
issue_date = {April 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {37},
number = {2},
issn = {1046-8188},
url = {https://doi.org/10.1145/3309543},
doi = {10.1145/3309543},
abstract = {In the field of sentiment analysis and emotion detection in social media, or other tasks such as text classification involving supervised learning, researchers rely more heavily on large and accurate labelled training datasets. However, obtaining large-scale labelled datasets is time-consuming and high-quality labelled datasets are expensive and scarce. To deal with these problems, online crowdsourcing systems provide us an efficient way to accelerate the process of collecting training data via distributing the enormous tasks to various annotators to help create large amounts of labelled data at an affordable cost. Nowadays, these crowdsourcing platforms are heavily needed in dealing with social media text, since the social network platforms (e.g., Twitter) generate huge amounts of data in textual form everyday. However, people from different social and knowledge backgrounds have different views on various texts, which may lead to noisy labels. The existing noisy label aggregation/refinement algorithms mostly focus on aggregating labels from noisy annotations, which would not guarantee their effectiveness on the subsequent classification/ranking tasks. In this article, we propose a noise-aware classification framework that integrates the steps of noisy label aggregation and classification. The aggregated noisy crowd labels are fed into a classifier for training, while the predicted labels are employed as feedback for adjusting the parameters at the label aggregating stage. The classification framework is suitable for directly running on crowdsourcing datasets and applies to various kinds of classification algorithms. The feedback strategy makes it possible for us to find optimal parameters instead of using known data for parameter selection. Simulation experiments demonstrate that our method provide significant label aggregation performance for both binary and multiple classification tasks under various noisy environments. Experimenting on real-world data validates the feasibility of our framework in real noise data and helps us verify the reasonableness of the simulated experiment settings.},
journal = {ACM Trans. Inf. Syst.},
month = feb,
articleno = {26},
numpages = {28},
keywords = {sentiment analysis, emotion detection, crowdsourcing, Social media}
}

@article{10.1007/s00778-023-00802-3,
author = {Zhao, Yan and Zheng, Kai and Wang, Ziwei and Deng, Liwei and Yang, Bin and Pedersen, Torben Bach and Jensen, Christian S. and Zhou, Xiaofang},
title = {Coalition-based task assignment with priority-aware fairness in spatial crowdsourcing},
year = {2023},
issue_date = {Jan 2024},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {33},
number = {1},
issn = {1066-8888},
url = {https://doi.org/10.1007/s00778-023-00802-3},
doi = {10.1007/s00778-023-00802-3},
abstract = {With the widespread use of networked and geo-positioned mobile devices, e.g., smartphones, Spatial Crowdsourcing (SC), which refers to the assignment of location-based tasks to moving workers, is drawing increasing attention. One of the critical issues in SC is task assignment that allocates tasks to appropriate workers. We propose and study a novel SC problem, namely Coalition-based Task Assignment (CTA), where the spatial tasks (e.g., home improvement and furniture installation) may require more than one worker (forming a coalition) to cooperate to maximize the overall rewards of workers. We design a greedy and an equilibrium-based CTA approach. The greedy approach forms a set of worker coalitions greedily for performing tasks and uses an acceptance probability to identify high-value task assignments. In the equilibrium-based approach, workers form coalitions in sequence and update their strategies (i.e., selecting a best-response task), to maximize their own utility (i.e., the reward of the coalition they belong to) until a Nash equilibrium is reached. Since the equilibrium obtained is not unique and optimal in terms of total rewards, we further propose a simulated annealing scheme to find a better Nash equilibrium. To achieve fair task assignments, we optimize the framework to distribute rewards fairly among workers in a coalition based on their marginal contributions and give workers who arrive first at the SC platform highest priority. Extensive experiments demonstrate the efficiency and effectiveness of the proposed methods on real and synthetic data.},
journal = {The VLDB Journal},
month = jul,
pages = {163–184},
numpages = {22},
keywords = {Coalition, Task assignment, Spatial crowdsourcing, Priority-aware fairness}
}

@article{10.14778/3538598.3538609,
author = {Liang, Yihuai and Li, Yan and Shin, Byeong-Seok},
title = {Decentralized crowdsourcing for human intelligence tasks with efficient on-chain cost},
year = {2022},
issue_date = {May 2022},
publisher = {VLDB Endowment},
volume = {15},
number = {9},
issn = {2150-8097},
url = {https://doi.org/10.14778/3538598.3538609},
doi = {10.14778/3538598.3538609},
abstract = {Crowdsourcing for Human Intelligence Tasks (HIT) has been widely used to crowdsource human knowledge, such as image annotation for machine learning. We use a public blockchain to play the role of traditional centralized HIT systems, such that the blockchain deals with cryptocurrency payments and acts as a trustworthy judge to resolve disputes between a worker and a requester in a decentralized setting, preventing false-reporting and free-riding. Our approach neither uses expensive cryptographic tools, such as zero-knowledge proofs, nor sends the worker's answers to the blockchain. Compared with prior works, our approach significantly reduces on-chain cost: it only requires O(1) on-chain storage and O(logN) smart contract computation, where N is the question number of a HIT. Additionally, our approach uses known answers or gold standards to determine the worker's answer quality. To motivate the requester to use honest known answers, the requester cannot learn the worker's answers if the answer quality does not meet the requirement. We further provide formal security definitions for our decentralized HIT and prove security of our construction.},
journal = {Proc. VLDB Endow.},
month = may,
pages = {1875–1888},
numpages = {14}
}

@article{10.14778/3229863.3236226,
author = {Li, Guoliang and Chai, Chengliang and Fan, Ju and Weng, Xueping and Li, Jian and Zheng, Yudian and Li, Yuanbing and Yu, Xiang and Zhang, Xiaohang and Yuan, Haitao},
title = {CDB: a crowd-powered database system},
year = {2018},
issue_date = {August 2018},
publisher = {VLDB Endowment},
volume = {11},
number = {12},
issn = {2150-8097},
url = {https://doi.org/10.14778/3229863.3236226},
doi = {10.14778/3229863.3236226},
abstract = {Crowd-powered database systems can leverage the crowd's ability to address machine-hard problems, e.g., data integration. Existing crowdsourcing systems adopt the traditional tree model to select a good query plan. However, the tree model can optimize the I/O cost but cannot optimize the monetary cost, latency and quality, which are three important optimization goals in crowdsourcing. To address this limitation, we demonstrate CDB, a crowd-powered database system. CDB proposes a new graph-based model that adopts a fine-grained tuple-level optimization model which significantly outperforms existing coarse-grained tree-based optimization models. Moreover, CDB provides a unified framework to simultaneously optimize the monetary cost, quality and latency. We have deployed CDB on well-known crowd-sourcing platforms and users can easily use our system to deploy their applications. We will demonstrate how to use CDB to address real-world applications, including web table integration and entity collection.},
journal = {Proc. VLDB Endow.},
month = aug,
pages = {1926–1929},
numpages = {4}
}

@article{10.1145/3007900,
author = {Goncalves, Jorge and Hosio, Simo and Kostakos, Vassilis},
title = {Eliciting Structured Knowledge from Situated Crowd Markets},
year = {2017},
issue_date = {May 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {17},
number = {2},
issn = {1533-5399},
url = {https://doi.org/10.1145/3007900},
doi = {10.1145/3007900},
abstract = {We present a crowdsourcing methodology to elicit highly structured knowledge for arbitrary questions. The method elicits potential answers (“options”), criteria against which those options should be evaluated, and a ranking of the top “options.” Our study shows that situated crowdsourcing markets can reliably elicit/moderate knowledge to generate a ranking of options based on different criteria that correlate with established online platforms. Our evaluation also shows that local crowds can generate knowledge that is missing from online platforms and on how a local crowd perceives a certain issue. Finally, we discuss the benefits and challenges of eliciting structured knowledge from local crowds.},
journal = {ACM Trans. Internet Technol.},
month = mar,
articleno = {14},
numpages = {21},
keywords = {structured knowledge, situated, questions, quality, performance, options, local crowds, criteria, accuracy, Crowdsourcing}
}

@article{10.1145/3078853,
author = {Tran, Luan and To, Hien and Fan, Liyue and Shahabi, Cyrus},
title = {A Real-Time Framework for Task Assignment in Hyperlocal Spatial Crowdsourcing},
year = {2018},
issue_date = {May 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {9},
number = {3},
issn = {2157-6904},
url = {https://doi.org/10.1145/3078853},
doi = {10.1145/3078853},
abstract = {Spatial Crowdsourcing (SC) is a novel platform that engages individuals in the act of collecting various types of spatial data. This method of data collection can significantly reduce cost and turnover time and is particularly useful in urban environmental sensing, where traditional means fail to provide fine-grained field data. In this study, we introduce hyperlocal spatial crowdsourcing, where all workers who are located within the spatiotemporal vicinity of a task are eligible to perform the task (e.g., reporting the precipitation level at their area and time). In this setting, there is often a budget constraint, either for every time period or for the entire campaign, on the number of workers to activate to perform tasks. The challenge is thus to maximize the number of assigned tasks under the budget constraint despite the dynamic arrivals of workers and tasks. We introduce a taxonomy of several problem variants, such as budget-per-time-period vs. budget-per-campaign and binary-utility vs. distance-based-utility. We study the hardness of the task assignment problem in the offline setting and propose online heuristics which exploit the spatial and temporal knowledge acquired over time. Our experiments are conducted with spatial crowdsourcing workloads generated by the SCAWG tool, and extensive results show the effectiveness and efficiency of our proposed solutions.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = jan,
articleno = {37},
numpages = {26},
keywords = {participatory sensing, online task assignment, crowdsensing, budget constraints, Spatial crowdsourcing, GIS}
}

@article{10.1145/3140459,
author = {Schmitz, Heinz and Lykourentzou, Ioanna},
title = {Online Sequencing of Non-Decomposable Macrotasks in Expert Crowdsourcing},
year = {2018},
issue_date = {March 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {1},
number = {1},
url = {https://doi.org/10.1145/3140459},
doi = {10.1145/3140459},
abstract = {We introduce the problem of Task Assignment and Sequencing, which models online optimization in expert crowdsourcing settings that involve non-decomposable macrotasks. Non-decomposition is a property of certain types of complex problems, like the formulation of an R&amp;D approach or the definition of a research methodology, which cannot be handled through the "divide-and-conquer" approach typically used in microtask crowdsourcing. In contrast to splitting the macrotask to multiple microtasks and allocating them to several workers in parallel, our model supports the sequential improvement of the macrotask one worker at a time, across distinct time slots of a given timeline, until a sufficient quality level is achieved. Our model assumes an online environment where expert workers are available only at specific time slots and worker/task arrivals are not known a priori. With respect to this setting, we propose TAS-ONLINE, an online algorithm that aims to complete as many tasks as possible within budget, required quality, and a given timeline, without any future input information regarding job release dates or worker availabilities. Experimental results comparing TAS-ONLINE to five benchmarks show that it achieves more completed jobs, lower flow times, and higher job quality. This work bears practical implications for providing performance and quality guarantees to expert crowdsourcing platforms that wish to integrate non-decomposable macrotasks into their offered services.},
journal = {Trans. Soc. Comput.},
month = jan,
articleno = {1},
numpages = {33},
keywords = {online scheduling decisions, macrotask scheduling, cooperative social computing, Crowdsourcing optimization}
}

@article{10.1145/3134697,
author = {Kou, Yubo and Gui, Xinning and Zhang, Shaozeng and Nardi, Bonnie},
title = {Managing Disruptive Behavior through Non-Hierarchical Governance: Crowdsourcing in League of Legends and Weibo},
year = {2017},
issue_date = {November 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {1},
number = {CSCW},
url = {https://doi.org/10.1145/3134697},
doi = {10.1145/3134697},
abstract = {Disruptive behaviors such as flaming and vandalism have been part of the Internet since its beginning. Various models of hierarchical governance have been established and managed in different online venues, with both successes and failures. Recently, a new model of non-hierarchical governance has emerged using crowdsourcing technology to allow an online community to manage itself. How do people view and work with non-hierarchical governance? In this paper, we present an interview study with people from two sites: the video game League of Legends and Weibo, a microblogging site in China. We found that people were passionate about participation in crowdsourcing, but at the same time, struggled with the system, and acted beyond their designated role within the system. We derive implications for designing online non-hierarchical governance from our research.},
journal = {Proc. ACM Hum.-Comput. Interact.},
month = dec,
articleno = {62},
numpages = {17},
keywords = {weibo, troll, social media, online game, online community, non-hierarchical governance, league of legends, harassment, governance, deviant behavior, crowdsourced governance, big data, anti-social behavior}
}

@article{10.1145/3524065,
author = {Fisher, Michael A. and Milliken, Lindsay K.},
title = {Crowdsourcing Science and Technology Expertise to Empower Legislative Branch Oversight and Policymaking},
year = {2022},
issue_date = {April 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {3},
number = {2},
url = {https://doi.org/10.1145/3524065},
doi = {10.1145/3524065},
abstract = {As the U.S. and the world struggle through the COVID-19 pandemic, the need for science and technology (S&amp;T) expertise in governance has become even more stark. But in the U.S., the vast majority of legislators and their staffs is generalist, and they have limited resources for engaging with S&amp;T. This can be a barrier to both legislative branch oversight of S&amp;T-related issues and the development of evidence-based public policies, two functions that are especially critical in times of crisis. The obstacles to addressing these gaps have, however, created opportunities for innovations in how lawmakers connect with S&amp;T resources, resulting in new models for scientist-policymaker engagement. Our program, the Congressional Science Policy Initiative, experiments with models for connecting crowdsourced S&amp;T expertise with policymakers, namely, by enriching key congressional hearings with contributions gathered from the science community, organizing advisory councils of scientists and engineers that brief lawmakers, and crowdsourcing technical assistance from the S&amp;T community for legislative initiatives. These activities, which rely on the collective intelligence of the S&amp;T community and have been readily applied to supporting lawmakers’ decision-making processes during the pandemic, have bolstered legislative branch oversight of the executive branch, fact-finding into corporate practices, and evidence-based policymaking.},
journal = {Digit. Gov.: Res. Pract.},
month = sep,
articleno = {11},
numpages = {8},
keywords = {expertise, legislatures, democratization, Crowdsourcing}
}

@article{10.1145/3274311,
author = {Davis, Dan and Hauff, Claudia and Houben, Geert-Jan},
title = {Evaluating Crowdworkers as a Proxy for Online Learners in Video-Based Learning Contexts},
year = {2018},
issue_date = {November 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {2},
number = {CSCW},
url = {https://doi.org/10.1145/3274311},
doi = {10.1145/3274311},
abstract = {Crowdsourcing has emerged as an effective method of scaling-up tasks previously reserved for a small set of experts. Accordingly, researchers in the large-scale online learning space have begun to employ crowdworkers to conduct research about large-scale, open online learning. We here report results from a crowdsourcing study (N=135) to evaluate the extent to which crowdworkers and MOOC learners behave comparably on lecture viewing and quiz tasks---the most utilized learning activities in MOOCs. This serves to (i) validate the assumption of previous research that crowdworkers are indeed reliable proxies of online learners and (ii) address the potential of employing crowdworkers as a means of online learning environment testing. Overall, we observe mixed results---in certain contexts (quiz performance and video watching behavior) crowdworkers appear to behave comparably to MOOC learners, and in other situations (interactions with in-video quizzes), their behaviors appear to be disparate. We conclude that future research should be cautious if employing crowdworkers to carry out learning tasks, as the two populations do not behave comparably on all learning-related activities.},
journal = {Proc. ACM Hum.-Comput. Interact.},
month = nov,
articleno = {42},
numpages = {16},
keywords = {replication, moocs, learning analytics, crowdwork}
}

@article{10.1145/3119930,
author = {Gadiraju, Ujwal and Fetahu, Besnik and Kawase, Ricardo and Siehndel, Patrick and Dietze, Stefan},
title = {Using Worker Self-Assessments for Competence-Based Pre-Selection in Crowdsourcing Microtasks},
year = {2017},
issue_date = {August 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {24},
number = {4},
issn = {1073-0516},
url = {https://doi.org/10.1145/3119930},
doi = {10.1145/3119930},
abstract = {Paid crowdsourcing platforms have evolved into remarkable marketplaces where requesters can tap into human intelligence to serve a multitude of purposes, and the workforce can benefit through monetary returns for investing their efforts. In this work, we focus on individual crowd worker competencies. By drawing from self-assessment theories in psychology, we show that crowd workers often lack awareness about their true level of competence. Due to this, although workers intend to maintain a high reputation, they tend to participate in tasks that are beyond their competence. We reveal the diversity of individual worker competencies, and make a case for competence-based pre-selection in crowdsourcing marketplaces. We show the implications of flawed self-assessments on real-world microtasks, and propose a novel worker pre-selection method that considers accuracy of worker self-assessments. We evaluated our method in a sentiment analysis task and observed an improvement in the accuracy by over 15\%, when compared to traditional performance-based worker pre-selection. Similarly, our proposed method resulted in an improvement in accuracy of nearly 6\% in an image validation task. Our results show that requesters in crowdsourcing platforms can benefit by considering worker self-assessments in addition to their performance for pre-selection.},
journal = {ACM Trans. Comput.-Hum. Interact.},
month = aug,
articleno = {30},
numpages = {26},
keywords = {worker behaviour, self-assessment, pre-selection, pre-screening, performance, microtasks, Crowdsourcing}
}

@article{10.1145/3532670,
author = {Mart\'{\i}, Jos\'{e}},
title = {Crowdsourcing Crisis Management and Democratic Legitimacy},
year = {2022},
issue_date = {April 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {3},
number = {2},
url = {https://doi.org/10.1145/3532670},
doi = {10.1145/3532670},
abstract = {In situations of crisis, governments must make decisions under a great uncertainty, complexity, urgency, high social pressure, and great scarcity of resources. We need to maximize the quality of decisions and ensure that we find practical and effective solutions for our problems. The stakes are simply too high. The risk, however, is to give up democratic legitimacy for, in the best case, some form of output legitimacy or technocracy. On the other hand, decisions made in times of emergency are tremendously consequential for our citizens, affecting their fundamental rights and welfare, and it is for decisions like this that democratic legitimacy seems to be crucial. We may be in the apparent dilemma of having to choose between the quality of public decision-making and its democratic legitimacy. This article claims that this is a false dilemma, at least if we take it as an either/or choice. Rather than choosing one value or the other, what we need is to find a proper balance between them on a case-by-case basis. And the article argues that one very promising way to do it is by applying the ideas of crowdlaw and crowdsourcing crisis management based on the potential of collective intelligence, which in turn grounds an ideal of participatory, deliberative, and collaborative democracy.},
journal = {Digit. Gov.: Res. Pract.},
month = nov,
articleno = {15},
numpages = {15},
keywords = {quality of decision-making, citizen participation, citizen engagement, collective intelligence, democratic legitimacy, democracy, collaborative governance, crisis management, crowdsourcing, CrowdLaw}
}

@article{10.1145/3579456,
author = {Haug, Saskia and Benke, Ivo and Maedche, Alexander},
title = {Aligning Crowdworker Perspectives and Feedback Outcomes in Crowd-Feedback System Design},
year = {2023},
issue_date = {April 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {CSCW1},
url = {https://doi.org/10.1145/3579456},
doi = {10.1145/3579456},
abstract = {Leveraging crowdsourcing in software development has received growing attention in research and practice. Crowd feedback offers a scalable and flexible way to evaluate software design solutions and the potential of crowd-feedback systems has been demonstrated in different contexts by existing research studies. However, previous research lacks a deep understanding of the effects of individual design features of crowd-feedback systems on feedback quality and quantity. Additionally, existing studies primarily focused on understanding the requirements of feedback requesters but have not fully explored the qualitative perspectives of crowd-based feedback providers. In this paper, we address these research gaps with two research studies. In study 1, we conducted a feature analysis (N=10) and concluded that from a user perspective, a crowd-feedback system should have five core features (scenario, speech-to-text, markers, categories, and star rating). In the second study, we analyzed the effects of the design features on crowdworkers' perceptions and feedback outcomes (N=210). We learned that offering feedback providers scenarios as the context of use is perceived as most important. Regarding the resulting feedback quality, we discovered that more features are not always better as overwhelming feedback providers might decrease feedback quality. Offering feedback providers categories as inspiration can increase the feedback quantity. With our work, we contribute to research on crowd-feedback systems by aligning crowdworker perspectives and feedback outcomes and thereby making the software evaluation not only more scalable but also more human-centered.},
journal = {Proc. ACM Hum.-Comput. Interact.},
month = apr,
articleno = {23},
numpages = {28},
keywords = {crowd-feedback system, crowdsourcing, design, experimental study, feedback, qualitative interviews}
}

@article{10.1145/2910586,
author = {Wu, Ou and You, Qiang and Xia, Fen and Ma, Lei and Hu, Weiming},
title = {Listwise Learning to Rank from Crowds},
year = {2016},
issue_date = {February 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {11},
number = {1},
issn = {1556-4681},
url = {https://doi.org/10.1145/2910586},
doi = {10.1145/2910586},
abstract = {Learning to rank has received great attention in recent years as it plays a crucial role in many applications such as information retrieval and data mining. The existing concept of learning to rank assumes that each training instance is associated with a reliable label. However, in practice, this assumption does not necessarily hold true as it may be infeasible or remarkably expensive to obtain reliable labels for many learning to rank applications. Therefore, a feasible approach is to collect labels from crowds and then learn a ranking function from crowdsourcing labels. This study explores the listwise learning to rank with crowdsourcing labels obtained from multiple annotators, who may be unreliable. A new probabilistic ranking model is first proposed by combining two existing models. Subsequently, a ranking function is trained by proposing a maximum likelihood learning approach, which estimates ground-truth labels and annotator expertise, and trains the ranking function iteratively. In practical crowdsourcing machine learning, valuable side information (e.g., professional grades) about involved annotators is normally attainable. Therefore, this study also investigates learning to rank from crowd labels when side information on the expertise of involved annotators is available. In particular, three basic types of side information are investigated, and corresponding learning algorithms are consequently introduced. Further, the top-k learning to rank from crowdsourcing labels are explored to deal with long training ranking lists. The proposed algorithms are tested on both synthetic and real-world data. Results reveal that the maximum likelihood estimation approach significantly outperforms the average approach and existing crowdsourcing regression methods. The performances of the proposed algorithms are comparable to those of the learning model in consideration reliable labels. The results of the investigation further indicate that side information is helpful in inferring both ranking functions and expertise degrees of annotators.},
journal = {ACM Trans. Knowl. Discov. Data},
month = jul,
articleno = {4},
numpages = {39},
keywords = {side information, probabilistic ranking model, multiple annotators, crowdsourcing, Listwise learning to rank}
}

@article{10.14778/2794367.2794372,
author = {Cheng, Peng and Lian, Xiang and Chen, Zhao and Fu, Rui and Chen, Lei and Han, Jinsong and Zhao, Jizhong},
title = {Reliable diversity-based spatial crowdsourcing by moving workers},
year = {2015},
issue_date = {June 2015},
publisher = {VLDB Endowment},
volume = {8},
number = {10},
issn = {2150-8097},
url = {https://doi.org/10.14778/2794367.2794372},
doi = {10.14778/2794367.2794372},
abstract = {With the rapid development of mobile devices and the crowdsourcing platforms, the spatial crowdsourcing has attracted much attention from the database community, specifically, spatial crowdsourcing refers to sending a location-based request to workers according to their positions. In this paper, we consider an important spatial crowdsourcing problem, namely reliable diversity-based spatial crowdsourcing (RDB-SC), in which spatial tasks (such as taking videos/photos of a landmark or firework shows, and checking whether or not parking spaces are available) are time-constrained, and workers are moving towards some directions. Our RDB-SC problem is to assign workers to spatial tasks such that the completion reliability and the spatial/temporal diversities of spatial tasks are maximized. We prove that the RDB-SC problem is NP-hard and intractable. Thus, we propose three effective approximation approaches, including greedy, sampling, and divide-and-conquer algorithms. In order to improve the efficiency, we also design an effective cost-model-based index, which can dynamically maintain moving workers and spatial tasks with low cost, and efficiently facilitate the retrieval of RDB-SC answers. Through extensive experiments, we demonstrate the efficiency and effectiveness of our proposed approaches over both real and synthetic datasets.},
journal = {Proc. VLDB Endow.},
month = jun,
pages = {1022–1033},
numpages = {12}
}

@article{10.14778/2733004.2733047,
author = {Chen, Zhao and Fu, Rui and Zhao, Ziyuan and Liu, Zheng and Xia, Leihao and Chen, Lei and Cheng, Peng and Cao, Caleb Chen and Tong, Yongxin and Zhang, Chen Jason},
title = {gMission: a general spatial crowdsourcing platform},
year = {2014},
issue_date = {August 2014},
publisher = {VLDB Endowment},
volume = {7},
number = {13},
issn = {2150-8097},
url = {https://doi.org/10.14778/2733004.2733047},
doi = {10.14778/2733004.2733047},
abstract = {As one of the successful forms of using Wisdom of Crowd, crowdsourcing, has been widely used for many human intrinsic tasks, such as image labeling, natural language understanding, market predication and opinion mining. Meanwhile, with advances in pervasive technology, mobile devices, such as mobile phones and tablets, have become extremely popular. These mobile devices can work as sensors to collect multimedia data(audios, images and videos) and location information. This power makes it possible to implement the new crowdsourcing mode: spatial crowdsourcing. In spatial crowdsourcing, a requester can ask for resources related a specific location, the mobile users who would like to take the task will travel to that place and get the data. Due to the rapid growth of mobile device uses, spatial crowdsourcing is likely to become more popular than general crowdsourcing, such as Amazon Turk and Crowdflower. However, to implement such a platform, effective and efficient solutions for worker incentives, task assignment, result aggregation and data quality control must be developed.In this demo, we will introduce gMission, a general spatial crowdsourcing platform, which features with a collection of novel techniques, including geographic sensing, worker detection, and task recommendation. We introduce the sketch of system architecture and illustrate scenarios via several case analysis.},
journal = {Proc. VLDB Endow.},
month = aug,
pages = {1629–1632},
numpages = {4}
}

@article{10.1145/3363450,
author = {Li, Wei and Chen, Haiquan and Ku, Wei-Shinn and Qin, Xiao},
title = {Turbo-GTS: A Fast Framework of Optimizing Task Throughput for Large-Scale Mobile Crowdsourcing},
year = {2020},
issue_date = {March 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {6},
number = {1},
issn = {2374-0353},
url = {https://doi.org/10.1145/3363450},
doi = {10.1145/3363450},
abstract = {In mobile crowdsourcing, workers are financially motivated to perform as many self-selected tasks as possible to maximize their revenue. Unfortunately, the existing task scheduling approaches in mobile crowdsourcing fail to consider task execution duration and do not scale for massive tasks and large geographic areas. In this article, we propose a novel framework, Turbo-GTS, in support of large-scale geo-task scheduling, with the objective of identifying an optimal task assignment for each worker to maximize the total number of tasks that can be completed for an entire worker group, given the geographic locations of each task and each worker. Since the exact solution to the geo-task scheduling problem is computationally intractable, we first propose two sub-optimal approaches (least cost neighbor with particle filtering and non-urgency degree particle filtering with iterative clustering) based on particle filtering and DBSCAN for the single-worker geo-task scheduling problem. We then extend our work to solve the multi-worker geo-task scheduling problem by proposing two space partitioning-based methods (QT-NNH and QT-NUD), which leverage point-region quadtree to ensure workload balancing. The effectiveness and efficiency of the four proposed approximate solutions are verified by our extensive experiments using both real and synthetic data. Compared to state-of-the-art approaches, our proposed solutions are able to return a higher number of completed tasks for the worker group while reducing the computation cost by up to three orders of magnitude when coping with massive tasks distributed in large geographic areas.},
journal = {ACM Trans. Spatial Algorithms Syst.},
month = jan,
articleno = {1},
numpages = {29},
keywords = {quadtree, particle filtering, Mobile crowdsourcing}
}

@article{10.1145/3565576,
author = {Wu, Gongqing and Zhuo, Xingrui and Bao, Xianyu and Hu, Xuegang and Hong, Richang and Wu, Xindong},
title = {Crowdsourcing Truth Inference via Reliability-Driven Multi-View Graph Embedding},
year = {2023},
issue_date = {June 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {17},
number = {5},
issn = {1556-4681},
url = {https://doi.org/10.1145/3565576},
doi = {10.1145/3565576},
abstract = {Crowdsourcing truth inference aims to assign a correct answer to each task from candidate answers that are provided by crowdsourced workers. A common approach is to generate workers’ reliabilities to represent the quality of answers. Although crowdsourced triples can be converted into various crowdsourced relationships, the available related methods are not effective in capturing these relationships to alleviate the harm to inference that is caused by conflicting answers. In this research, we propose a Reliability-driven Multi-view Graph Embedding framework for Truth inference (TiReMGE), which explores multiple crowdsourced relationships by organically integrating worker reliabilities into a graph space that is constructed from crowdsourced triples. Specifically, to create an interactive environment, we propose a reliability-driven initialization criterion for initializing vectors of tasks and workers as interactive carriers of reliabilities. From the perspective of multiple crowdsourced relationships, a multi-view graph embedding framework is proposed for reliability information interaction on a task-worker graph, which encodes latent crowdsourced relationships into vectors of workers and tasks for reliability update and truth inference. A heritable reliability updating method based on the Lagrange multiplier method is proposed to obtain reliabilities that match the quality of workers for interaction by a novel constraint law. Our ultimate goal is to minimize the Euclidean distance between the encoded task vector and the answer that is provided by a worker with high reliability. Extensive experimental results on nine real-world datasets demonstrate that TiReMGE significantly outperforms the nine state-of-the-art baselines.},
journal = {ACM Trans. Knowl. Discov. Data},
month = feb,
articleno = {65},
numpages = {26},
keywords = {worker quality match, reliability interaction, graph embedding, Crowdsourcing truth inference}
}

@article{10.1145/3503156,
author = {Manerkar, Sanjana and Asnani, Kavita and Khorjuvenkar, Preeti Ravindranath and Desai, Shilpa and Pawar, Jyoti D.},
title = {Konkani WordNet: Corpus-Based Enhancement using Crowdsourcing},
year = {2022},
issue_date = {July 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {21},
number = {4},
issn = {2375-4699},
url = {https://doi.org/10.1145/3503156},
doi = {10.1145/3503156},
abstract = {Konkani is one of the languages included in the eighth schedule of the Indian constitution. It is the official language of Goa and is spoken mainly in Goa and some places in Karnataka and Kerala. Konkani WordNet or Konkani Shabdamalem (k\={o}undefinedkanundefined \'{s}abdamundefinedlundefinedundefined) as it has been referred to, was developed under the Indradhanush WordNet Project Consortium during the period from August 2010 to October 2013. This project was funded by Technology Development for Indian Languages (TDIL), Department of Electronics \&amp; Information Technology (Deity), and Ministry of Communication and Information Technology (MCIT). The work on Konkani WordNet has halted since the end of the project. Currently, the Konkani WordNet contains around 32,370 synsets. However, to make it a powerful resource for NLP applications in the Konkani language, a need is felt for research work toward enhancement of the Konkani WordNet via community involvement. Crowdsourcing is a technique in which the knowledge of the crowd is utilized to accomplish a particular task.In this article, we have presented the details of the crowdsourcing platform named “Konkani Shabdarth” (k\={o}undefinedkanundefined \'{s}abdundefinedrth). Konkani Shabdarth attempts to use the knowledge of Konkani speaking people for creating new synsets and perform the quantitative enhancement of the wordnet. It also intends to work toward enhancing the overall quality of the Konkani WordNet by validating the existing synsets, and adding the missing words to the existing synsets. A text corpus named “Konkani Shabdarth Corpus”, has been created from the Konkani literature while implementing the Konkani Shabdarth tool. Using this corpus, 572 root words that are missing from the Konkani WordNet have been identified which are given as input to Konkani Shabdarth. As of now, total 94 users have registered on the platform, out of which 25 users have actually played the game. Currently, 71 new synsets have been obtained for 21 words. For some of the words, multiple entries for the concept definition have been received. This overlap is essential for automating the process of validating the synsets. Due to the pandemic period, it has been difficult to train and get players to actually play the game and contribute. We studied the impact of adding missing words from other existing Konkani text corpus on the coverage of Konkani WordNet. The expected increase in the percentage coverage of Konkani WordNet has been found to be in the range 20–27 after adding the missing words from the Konkani Shabdarth corpus in comparison to the other corpora for which the increase is in the range 1–10.},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = mar,
articleno = {80},
numpages = {18},
keywords = {crowdsourcing, Konkani Shabdarth, Konkani Wordnet, WordNet}
}

@article{10.1145/3309994,
author = {Shao, Yunqiu and Liu, Yiqun and Zhang, Fan and Zhang, Min and Ma, Shaoping},
title = {On Annotation Methodologies for Image Search Evaluation},
year = {2019},
issue_date = {July 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {37},
number = {3},
issn = {1046-8188},
url = {https://doi.org/10.1145/3309994},
doi = {10.1145/3309994},
abstract = {Image search engines differ significantly from general web search engines in the way of presenting search results. The difference leads to different interaction and examination behavior patterns, and therefore requires changes in evaluation methodologies. However, evaluation of image search still utilizes the methods for general web search. In particular, offline metrics are calculated based on coarse-fine topical relevance judgments with the assumption that users examine results in a sequential manner.In this article, we investigate annotation methods via crowdsourcing for image search evaluation based on a lab-based user study. Using user satisfaction as the golden standard, we make several interesting findings. First, instead of item-based annotation, annotating relevance in a row-based way is more efficient without hurting performance. Second, besides topical relevance, image quality plays a crucial role when evaluating the image search results, and the importance of image quality changes with search intent. Third, compared to traditional four-level scales, the fine-grain annotation method outperforms significantly. To our best knowledge, our work is the first to systematically study how diverse factors in data annotation impact image search evaluation. Our results suggest different strategies for exploiting the crowdsourcing to get data annotated under different conditions.},
journal = {ACM Trans. Inf. Syst.},
month = mar,
articleno = {29},
numpages = {32},
keywords = {user satisfaction, offline evaluation, crowdsourcing annotation, Image search}
}

@article{10.14778/3229863.3236225,
author = {Ke, Xiangyu and Teo, Michelle and Khan, Arijit and Yalavarthi, Vijaya Krishna},
title = {A demonstration of PERC: probabilistic entity resolution with crowd errors},
year = {2018},
issue_date = {August 2018},
publisher = {VLDB Endowment},
volume = {11},
number = {12},
issn = {2150-8097},
url = {https://doi.org/10.14778/3229863.3236225},
doi = {10.14778/3229863.3236225},
abstract = {This paper demonstrates PERC --- our system for crowdsourced entity resolution with human errors. Entity Resolution (ER) is a critical step in data cleaning and analytics. Although many machine-based methods existed for ER task, crowdsourcing is becoming increasingly important since humans can provide more insightful information for complex tasks, e.g., clustering of images and natural language processing. However, human workers still make mistakes due to lack of domain expertise or seriousness, ambiguity, or even malicious intent. To this end, we present a system, called PERC (&lt;u&gt;p&lt;/u&gt;robabilistic &lt;u&gt;e&lt;/u&gt;ntity &lt;u&gt;r&lt;/u&gt;esolution with &lt;u&gt;c&lt;/u&gt;rowd errors), which adopts an uncertain graph model to address the entity resolution problem with noisy crowd answers. Using our framework, the problem of ER becomes equivalent to finding the maximum-likelihood clustering. In particular, we propose a novel metric called "reliability" to measure the quality of a clustering, which takes into account both the connected-ness inside and across all clusters. PERC then automatically selects the next question to ask the crowd that maximally increases the "reliability" of the current clustering.This demonstration highlights (1) a reliability-based next crowd-sourcing framework for crowdsourced ER, which does not require any user-defined threshold, and no apriori information about the error rate of the crowd workers, (2) it improves the ER quality by 15\% and reduces the crowdsourcing cost by 50\% compared to state-of-the-art methods, and (3) its GUI can interact with users to help them compare different crowdsourced ER algorithms, their intermediate ER results as they progress, and their selected next crowdsourcing questions in a user-friendly manner. Our demonstration video is at: https://www.youtube.com/watch?v=rQ7nu3b8zXY.},
journal = {Proc. VLDB Endow.},
month = aug,
pages = {1922–1925},
numpages = {4}
}

@article{10.1145/3310227,
author = {Zhou, Yao and Ying, Lei and He, Jingrui},
title = {Multi-task Crowdsourcing via an Optimization Framework},
year = {2019},
issue_date = {June 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {13},
number = {3},
issn = {1556-4681},
url = {https://doi.org/10.1145/3310227},
doi = {10.1145/3310227},
abstract = {The unprecedented amounts of data have catalyzed the trend of combining human insights with machine learning techniques, which facilitate the use of crowdsourcing to enlist label information both effectively and efficiently. One crucial challenge in crowdsourcing is the diverse worker quality, which determines the accuracy of the label information provided by such workers. Motivated by the observations that same set of tasks are typically labeled by the same set of workers, we studied their behaviors across multiple related tasks and proposed an optimization framework for learning from task and worker dual heterogeneity. The proposed method uses a weight tensor to represent the workers’ behaviors across multiple tasks, and seeks to find the optimal solution of the tensor by exploiting its structured information. Then, we propose an iterative algorithm to solve the optimization problem and analyze its computational complexity. To infer the true label of an example, we construct a worker ensemble based on the estimated tensor, whose decisions will be weighted using a set of entropy weight. We also prove that the gradient of the most time-consuming updating block is separable with respect to the workers, which leads to a randomized algorithm with faster speed. Moreover, we extend the learning framework to accommodate to the multi-class setting. Finally, we test the performance of our framework on several datasets, and demonstrate its superiority over state-of-the-art techniques.},
journal = {ACM Trans. Knowl. Discov. Data},
month = may,
articleno = {27},
numpages = {26},
keywords = {tensor representation, optimization, entropy ensemble, crowdsourcing, Multi-task learning}
}

@article{10.1145/3539659,
author = {Rizk, Hamada and Yamaguchi, Hirozumi and Youssef, Moustafa and Higashino, Teruo},
title = {Laser Range Scanners for Enabling Zero-overhead WiFi-based Indoor Localization System},
year = {2023},
issue_date = {March 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {9},
number = {1},
issn = {2374-0353},
url = {https://doi.org/10.1145/3539659},
doi = {10.1145/3539659},
abstract = {Robust and accurate indoor localization has been the goal of several research efforts over the past decade. Toward achieving this goal, WiFi fingerprinting-based indoor localization systems have been proposed. However, fingerprinting involves significant effort—especially when done at high density—and needs to be repeated with any change in the deployment area. While a number of recent systems have been introduced to reduce the calibration effort, these still trade overhead with accuracy. This article presents LiPhi++, an accurate system for enabling fingerprinting-based indoor localization systems without the associated data collection overhead. This is achieved by leveraging the sensing capability of transportable laser range scanners to automatically label WiFi scans, which can subsequently be used to build (and maintain) a fingerprint database. As part of its design, LiPhi++ leverages this database to train a deep long short-term memory network utilizing the signal strength history from the detected access points. LiPhi++ also has provisions for handling practical deployment issues, including the noisy wireless environment, heterogeneous devices, among others. Evaluation of LiPhi++ using Android phones in two realistic testbeds shows that it can match the performance of manual fingerprinting techniques under the same deployment conditions without the overhead associated with the traditional fingerprinting process. In addition, LiPhi++ improves upon the median localization accuracy obtained from crowdsourcing-based and fingerprinting-based systems by 284\% and 418\%, respectively, when tested with data collected a few months later.},
journal = {ACM Trans. Spatial Algorithms Syst.},
month = jan,
articleno = {4},
numpages = {25},
keywords = {deep learning, laser range scanners, WiFi, fingerprinting, Indoor localization}
}

@article{10.1145/3289183,
author = {Liu, Yang and Jiang, Yonghang and Li, Zhenjiang and Wang, Jianping},
title = {Rulers on Our Arms: Waving to Measure Object Size through Contactless Sensing},
year = {2019},
issue_date = {February 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {15},
number = {1},
issn = {1550-4859},
url = {https://doi.org/10.1145/3289183},
doi = {10.1145/3289183},
abstract = {In this article, we propose a mobile system, Aware, which turns our wearable or mobile device into a ruler. It can estimate the size of objects that could be large in size and not directly touchable by the user. Such a design will enable a rich set of applications that count on the size information of surrounding environments/objects. Aware purely utilizes the motion sensors on the device for object size measures. It can also integrate with the crowdsourcing feature for both performance improvement and result sharing. We propose a series of key techniques to address three major challenges in the Aware design: (1) user’s angle of line-of-sights to the object is used in the size measure but motion sensors track only the angle of arm’s waving, (2) motion sensors are noisy that require novel and effective data processing techniques, otherwise the errors could easily overwhelm the final result, and (3) in the crowdsourcing mode, Aware needs to identify vicinal objects of similar sizes and effectively fuse the measured sizes that correspond to the same object. We consolidate the above designs and implement Aware on Android platforms. Extensive experiments with four users show that Aware can achieve accurate measurement performance for the objects of various sizes in both indoor and outdoor environments.},
journal = {ACM Trans. Sen. Netw.},
month = feb,
articleno = {14},
numpages = {25},
keywords = {wearables, mobiles, contactless sensing, Object size measure}
}

@article{10.1145/3287037,
author = {Chen, Huijie and Li, Fan and Hei, Xiaojun and Wang, Yu},
title = {CrowdX: Enhancing Automatic Construction of Indoor Floorplan with Opportunistic Encounters},
year = {2018},
issue_date = {December 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {2},
number = {4},
url = {https://doi.org/10.1145/3287037},
doi = {10.1145/3287037},
abstract = {The lack of floorplan limits the spread of pervasive indoor location-based services. Existing crowdsourcing based approaches mostly rely on identifying, locating landmarks in the environment and utilizing the spatial relationship between the landmarks and traces for efficiently constructing fine-grained floorplan. However, these methods are always restricted by the sparse landmark distribution or may cause privacy leakage. In this paper, we propose CrowdX, a crowdsourcing system for accurate, low-cost indoor floorplan construction enhanced with opportunistic encounters among mobile users. The key insight is that the spatial relation (i.e., the displacement of each user and the distance between each other during the encounter) will be extracted from the audio and inertia data, which are aligned by the proposed vibration event-based method. Such information can be used to calibrate the drift of encounter position. The calibrated encounter position is beneficial to most of the floorplan generation steps, such as trace drift elimination, landmark positioning, hallway assembling, and room area estimation. Our experiments in three shopping malls show that CrowdX achieves an average F-measure around 89.4\%. In addition, the average estimated room area error within about 20\%. The evaluation results demonstrate a significant improvement of accuracy enhanced with opportunistic encounters.},
journal = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
month = dec,
articleno = {159},
numpages = {21},
keywords = {Peer Assisted Localization, Mobile CrowdSensing, Automatic Floorplan Construction}
}

@article{10.1145/3224182,
author = {Santani, Darshan and Ruiz-Correa, Salvador and Gatica-Perez, Daniel},
title = {Looking South: Learning Urban Perception in Developing Cities},
year = {2018},
issue_date = {September 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {1},
number = {3},
url = {https://doi.org/10.1145/3224182},
doi = {10.1145/3224182},
abstract = {Mobile and social technologies are providing new opportunities to document, characterize, and gather impressions of urban environments. In this article, we present a study that examines urban perceptions of three cities in central Mexico; the study integrates a mobile crowdsourcing framework to collect geo-localized images of urban environments by a local youth community, an online crowdsourcing platform to gather impressions of urban environments along 12 physical and psychological dimensions, and a deep learning framework to automatically infer human impressions of outdoor urban scenes. Our study resulted in a collection of 7,000 geo-localized images containing outdoor scenes and views of each city’s built environment, including touristic, historical, and residential neighborhoods, and 144,000 individual judgments from Amazon Mechanical Turk. Statistical analyses show that outdoor environments can be assessed in terms of interrater agreement for most of the urban dimensions by the observers of crowdsourced images. Furthermore, we proposed a methodology to automatically infer human perceptions of outdoor scenes using a variety of low-level image features and generic deep learning (CNN) features. We found that CNN features consistently outperformed all the individual low-level image features for all the studied urban dimensions. We obtained a maximum R2 of 0.49 using CNN features; for 9 out of 12 labels, the obtained R2 values exceeded 0.44.},
journal = {Trans. Soc. Comput.},
month = dec,
articleno = {13},
numpages = {23},
keywords = {urban perception, outdoor places, deep learning, collective action, Mobile crowdsourcing, Mexico, ICTD}
}

@article{10.1145/3489141,
author = {Lee, Lung-Hao and Li, Jian-Hong and Yu, Liang-Chih},
title = {Chinese EmoBank: Building Valence-Arousal Resources for Dimensional Sentiment Analysis},
year = {2022},
issue_date = {July 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {21},
number = {4},
issn = {2375-4699},
url = {https://doi.org/10.1145/3489141},
doi = {10.1145/3489141},
abstract = {An increasing amount of research has recently focused on dimensional sentiment analysis that represents affective states as continuous numerical values on multiple dimensions, such as valence-arousal (VA) space. Compared to the categorical approach that represents affective states as distinct classes (e.g., positive and negative), the dimensional approach can provide more fine-grained (real-valued) sentiment analysis. However, dimensional sentiment resources with valence-arousal ratings are very rare, especially for the Chinese language. Therefore, this study aims to: (1) Build a Chinese valence-arousal resource called Chinese EmoBank, the first Chinese dimensional sentiment resource featuring various levels of text granularity including 5,512 single words, 2,998 multi-word phrases, 2,582 single sentences, and 2,969 multi-sentence texts. The valence-arousal ratings are annotated by crowdsourcing based on the Self-Assessment Manikin (SAM) rating scale. A corpus cleanup procedure is then performed to improve annotation quality by removing outlier ratings and improper texts. (2) Evaluate the proposed resource using different categories of classifiers such as lexicon-based, regression-based, and neural-network-based methods, and comparing their performance to a similar evaluation of an English dimensional sentiment resource.},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = jan,
articleno = {65},
numpages = {18},
keywords = {affective computing, dimensional sentiment analysis, Valence-arousal prediction}
}

@article{10.5555/3322706.3322725,
author = {Rodrigo, Enrique G. and Aledo, Juan A. and G\'{a}mez, Jos\'{e} A.},
title = {spark-crowd: a spark package for learning from crowdsourced big data},
year = {2019},
issue_date = {January 2019},
publisher = {JMLR.org},
volume = {20},
number = {1},
issn = {1532-4435},
abstract = {As the data sets increase in size, the process of manually labeling data becomes unfeasible by small groups of experts. Thus, it is common to rely on crowdsourcing platforms which provide inexpensive, but noisy, labels. Although implementations of algorithms to tackle this problem exist, none of them focus on scalability, limiting the area of application to relatively small data sets. In this paper, we present spark-crowd, an Apache Spark package for learning from crowdsourced data with scalability in mind.},
journal = {J. Mach. Learn. Res.},
month = jan,
pages = {680–684},
numpages = {5},
keywords = {spark, multiple noisy labeling, learning from crowds, crowdsourcing, crowdsourced data, big data}
}

@article{10.1145/3593582,
author = {Zhao, Yan and Deng, Liwei and Zheng, Kai},
title = {AdaTaskRec: An Adaptive Task Recommendation Framework in Spatial Crowdsourcing},
year = {2023},
issue_date = {October 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {41},
number = {4},
issn = {1046-8188},
url = {https://doi.org/10.1145/3593582},
doi = {10.1145/3593582},
abstract = {Spatial crowdsourcing is one of the prime movers for the orchestration of location-based tasks, and task recommendation is a crucial means to help workers discover attractive tasks. While a number of existing studies have focused on modeling workers’ geographical preferences in task recommendation, they ignore the phenomenon of workers’ travel intention drifts across geographical areas, i.e., workers tend to have different intentions when they travel in different areas, which discounts the task recommendation quality of existing methods especially for workers that travel in unfamiliar out-of-town areas. To address this problem, we propose an Adaptive Task Recommendation (AdaTaskRec) framework. Specifically, we first give a novel two-module worker preference learning architecture that can calculate workers’ preferences for POIs (that tasks are associated with) in different areas adaptively based on workers’ current locations. If we detect that a worker is in the hometown area, then we apply the hometown preference learning module, which hybrids different strategies to aggregate workers’ travel intentions into their preferences while considering the transition and the sequence patterns among locations. Otherwise, we invoke the out-of-town preference learning module, which is to capture workers’ preferences by learning their travel intentions and transferring their hometown preferences into their out-of-town ones. Additionally, to improve task recommendation effectiveness, we propose a dynamic top-k recommendation method that sets different k values dynamically according to the numbers of neighboring workers and tasks. We also give an extra-reward-based and a fair top-k recommendation method, which introduce the extra rewards for tasks based on their recommendation rounds and consider exposure-based fairness of tasks, respectively. Extensive experiments offer insight into the effectiveness of the proposed framework.},
journal = {ACM Trans. Inf. Syst.},
month = jul,
articleno = {95},
numpages = {32},
keywords = {spatial crowdsourcing, travel intention, Task recommendation}
}

@article{10.1109/TASLP.2016.2621659,
author = {Hasegawa-Johnson, Mark A. and Jyothi, Preethi and McCloy, Daniel and Mirbagheri, Majid and Liberto, Giovanni M. di and Das, Amit and Ekin, Bradley and Liu, Chunxi and Manohar, Vimal and Tang, Hao and Lalor, Edmund C. and Chen, Nancy F. and Hager, Paul and Kekona, Tyler and Sloan, Rose and Lee, Adrian K. C. and Hasegawa-Johnson, Mark A. and Jyothi, Preethi and McCloy, Daniel and Mirbagheri, Majid and Di Liberto, Giovanni M. and Das, Amit and Ekin, Bradley and Chunxi Liu and Manohar, Vimal and Hao Tang and Lalor, Edmund C. and Chen, Nancy F. and Hager, Paul and Kekona, Tyler and Sloan, Rose and Lee, Adrian K. C. and Chen, Nancy F. and Manohar, Vimal and McCloy, Daniel and Liu, Chunxi and Hager, Paul and Lee, Adrian K. C. and Mirbagheri, Majid and Sloan, Rose and Kekona, Tyler and Lalor, Edmund C. and Tang, Hao and Ekin, Bradley and Liberto, Giovanni M. di and Das, Amit and Hasegawa-Johnson, Mark A. and Jyothi, Preethi},
title = {ASR for Under-Resourced Languages From Probabilistic Transcription},
year = {2017},
issue_date = {January 2017},
publisher = {IEEE Press},
volume = {25},
number = {1},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2016.2621659},
doi = {10.1109/TASLP.2016.2621659},
abstract = {In many under-resourced languages it is possible to find text, and it is possible to find speech, but transcribed speech suitable for training automatic speech recognition ASR is unavailable. In the absence of native transcripts, this paper proposes the use of a probabilistic transcript: A probability mass function over possible phonetic transcripts of the waveform. Three sources of probabilistic transcripts are demonstrated. First, self-training is a well-established semisupervised learning technique, in which a cross-lingual ASR first labels unlabeled speech, and is then adapted using the same labels. Second, mismatched crowdsourcing is a recent technique in which nonspeakers of the language are asked to write what they hear, and their nonsense transcripts are decoded using noisy channel models of second-language speech perception. Third, EEG distribution coding is a new technique in which nonspeakers of the language listen to it, and their electrocortical response signals are interpreted to indicate probabilities. ASR was trained in four languages without native transcripts. Adaptation using mismatched crowdsourcing significantly outperformed self-training, and both significantly outperformed a cross-lingual baseline. Both EEG distribution coding and text-derived phone language models were shown to improve the quality of probabilistic transcripts derived from mismatched crowdsourcing.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = jan,
pages = {50–63},
numpages = {14}
}

@article{10.5555/2946645.3053447,
author = {Shah, Nihar B. and Zhou, Dengyong},
title = {Double or nothing: multiplicative incentive mechanisms for crowdsourcing},
year = {2016},
issue_date = {January 2016},
publisher = {JMLR.org},
volume = {17},
number = {1},
issn = {1532-4435},
abstract = {Crowdsourcing has gained immense popularity in machine learning applications for obtaining large amounts of labeled data. Crowdsourcing is cheap and fast, but suffers from the problem of low-quality data. To address this fundamental challenge in crowdsourcing, we propose a simple payment mechanism to incentivize workers to answer only the questions that they are sure of and skip the rest. We show that surprisingly, under a mild and natural "no-free-lunch" requirement, this mechanism is the one and only incentive-compatible payment mechanism possible. We also show that among all possible incentive-compatible mechanisms (that may or may not satisfy no-free-lunch), our mechanism makes the smallest possible payment to spammers. We further extend our results to a more general setting in which workers are required to provide a quantized confidence for each question. Interestingly, this unique mechanism takes a "multiplicative" form. The simplicity of the mechanism is an added benefit. In preliminary experiments involving over 900 worker-task pairs, we observe a significant drop in the error rates under this unique mechanism for the same or lower monetary expenditure.},
journal = {J. Mach. Learn. Res.},
month = jan,
pages = {5725–5776},
numpages = {52},
keywords = {supervised learning, proper scoring rules, mechanism design, high-quality labels, crowdsourcing}
}

@article{10.1145/2897368,
author = {Kim, Yubin and Collins-Thompson, Kevyn and Teevan, Jaime},
title = {Using the Crowd to Improve Search Result Ranking and the Search Experience},
year = {2016},
issue_date = {July 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {4},
issn = {2157-6904},
url = {https://doi.org/10.1145/2897368},
doi = {10.1145/2897368},
abstract = {Despite technological advances, algorithmic search systems still have difficulty with complex or subtle information needs. For example, scenarios requiring deep semantic interpretation are a challenge for computers. People, on the other hand, are well suited to solving such problems. As a result, there is an opportunity for humans and computers to collaborate during the course of a search in a way that takes advantage of the unique abilities of each. While search tools that rely on human intervention will never be able to respond as quickly as current search engines do, recent research suggests that there are scenarios where a search engine could take more time if it resulted in a much better experience. This article explores how crowdsourcing can be used at query time to augment key stages of the search pipeline. We first explore the use of crowdsourcing to improve search result ranking. When the crowd is used to replace or augment traditional retrieval components such as query expansion and relevance scoring, we find that we can increase robustness against failure for query expansion and improve overall precision for results filtering. However, the gains that we observe are limited and unlikely to make up for the extra cost and time that the crowd requires. We then explore ways to incorporate the crowd into the search process that more drastically alter the overall experience. We find that using crowd workers to support rich query understanding and result processing appears to be a more worthwhile way to make use of the crowd during search. Our results confirm that crowdsourcing can positively impact the search experience but suggest that significant changes to the search process may be required for crowdsourcing to fulfill its potential in search systems.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = jul,
articleno = {50},
numpages = {24},
keywords = {information retrieval, crowdsourcing, Slow search}
}

@article{10.1145/3451161,
author = {Clarke, Charles L. A. and Vtyurina, Alexandra and Smucker, Mark D.},
title = {Assessing Top- Preferences},
year = {2021},
issue_date = {July 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {39},
number = {3},
issn = {1046-8188},
url = {https://doi.org/10.1145/3451161},
doi = {10.1145/3451161},
abstract = {Assessors make preference judgments faster and more consistently than graded judgments. Preference judgments can also recognize distinctions between items that appear equivalent under graded judgments. Unfortunately, preference judgments can require more than linear effort to fully order a pool of items, and evaluation measures for preference judgments are not as well established as those for graded judgments, such as NDCG. In this article, we explore the assessment process for partial preference judgments, with the aim of identifying and ordering the top items in the pool, rather than fully ordering the entire pool. To measure the performance of a ranker, we compare its output to this preferred ordering by applying a rank similarity measure. We demonstrate the practical feasibility of this approach by crowdsourcing partial preferences for the TREC 2019 Conversational Assistance Track, replacing NDCG with a new measure named compatibility. This new measure has its most striking impact when comparing modern neural rankers, where it is able to recognize significant improvements in quality that would otherwise be missed by NDCG.},
journal = {ACM Trans. Inf. Syst.},
month = may,
articleno = {33},
numpages = {21},
keywords = {question answering, Preference judgments}
}

@article{10.1145/2870649,
author = {Han, Shuguang and Dai, Peng and Paritosh, Praveen and Huynh, David},
title = {Crowdsourcing Human Annotation on Web Page Structure: Infrastructure Design and Behavior-Based Quality Control},
year = {2016},
issue_date = {July 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {4},
issn = {2157-6904},
url = {https://doi.org/10.1145/2870649},
doi = {10.1145/2870649},
abstract = {Parsing the semantic structure of a web page is a key component of web information extraction. Successful extraction algorithms usually require large-scale training and evaluation datasets, which are difficult to acquire. Recently, crowdsourcing has proven to be an effective method of collecting large-scale training data in domains that do not require much domain knowledge. For more complex domains, researchers have proposed sophisticated quality control mechanisms to replicate tasks in parallel or sequential ways and then aggregate responses from multiple workers. Conventional annotation integration methods often put more trust in the workers with high historical performance; thus, they are called performance-based methods. Recently, Rzeszotarski and Kittur have demonstrated that behavioral features are also highly correlated with annotation quality in several crowdsourcing applications. In this article, we present a new crowdsourcing system, called Wernicke, to provide annotations for web information extraction. Wernicke collects a wide set of behavioral features and, based on these features, predicts annotation quality for a challenging task domain: annotating web page structure. We evaluate the effectiveness of quality control using behavioral features through a case study where 32 workers annotate 200 Q&amp;A web pages from five popular websites. In doing so, we discover several things: (1) Many behavioral features are significant predictors for crowdsourcing quality. (2) The behavioral-feature-based method outperforms performance-based methods in recall prediction, while performing equally with precision prediction. In addition, using behavioral features is less vulnerable to the cold-start problem, and the corresponding prediction model is more generalizable for predicting recall than precision for cross-website quality analysis. (3) One can effectively combine workers’ behavioral information and historical performance information to further reduce prediction errors.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = apr,
articleno = {56},
numpages = {25},
keywords = {worker performance, quality control, behavioral features, Crowdsourcing}
}

@article{10.5555/3586589.3586814,
author = {Rastogi, Charvi and Balakrishnan, Sivaraman and Shah, Nihar B. and Singh, Aarti},
title = {Two-sample testing on ranked preference data and the role of modeling assumptions},
year = {2022},
issue_date = {January 2022},
publisher = {JMLR.org},
volume = {23},
number = {1},
issn = {1532-4435},
abstract = {A number of applications require two-sample testing on ranked preference data. For instance, in crowdsourcing, there is a long-standing question of whether pairwise-comparison data provided by people is distributed identically to ratings-converted-to-comparisons. Other applications include sports data analysis and peer grading. In this paper, we design two-sample tests for pairwise-comparison data and ranking data. For our two-sample test for pairwise-comparison data, we establish an upper bound on the sample complexity required to correctly test whether the distributions of the two sets of samples are identical. Our test requires essentially no assumptions on the distributions. We then prove complementary lower bounds showing that our results are tight (in the minimax sense) up to constant factors. We investigate the role of modeling assumptions by proving lower bounds for a range of pairwise-comparison models (WST, MST, SST, parameter-based such as BTL and Thurstone). We also provide tests and associated sample complexity bounds for partial (or total) ranking data. Furthermore, we empirically evaluate our results via extensive simulations as well as three real-world data sets consisting of pairwise-comparisons and rankings. By applying our two-sample test on real-world pairwise-comparison data, we conclude that ratings and rankings provided by people are indeed distributed differently.},
journal = {J. Mach. Learn. Res.},
month = jan,
articleno = {225},
numpages = {48},
keywords = {social choice models, minimax theory, ranking, pairwise-comparisons, two-sample testing}
}

@article{10.1145/3491048,
author = {Shin, Suho and Choi, Hoyong and Yi, Yung and Ok, Jungseul},
title = {Power of Bonus in Pricing for Crowdsourcing},
year = {2021},
issue_date = {December 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {5},
number = {3},
url = {https://doi.org/10.1145/3491048},
doi = {10.1145/3491048},
abstract = {We consider a simple form of pricing for a crowdsourcing system, where pricing policy is published a priori, and workers then decide their task acceptance. Such a pricing form is widely adopted in practice for its simplicity, e.g., Amazon Mechanical Turk, although additional sophistication to pricing rule can enhance budget efficiency. With the goal of designing efficient and simple pricing rules, we study the impact of the following two design features in pricing policies: (i) personalization tailoring policy worker-by-worker and (ii) bonus payment to qualified task completion. In the Bayesian setting, where the only prior distribution of workers' profiles is available, we first study the Price of Agnosticism (PoA) that quantifies the utility gap between personalized and common pricing policies. We show that PoA is bounded within a constant factor under some mild conditions, and the impact of bonus is essential in common pricing. These analytic results imply that complex personalized pricing can be replaced by simple common pricing once it is equipped with a proper bonus payment. To provide insights on efficient common pricing, we then study the efficient mechanisms of bonus payment for several profile distribution regimes which may exist in practice. We provide primitive experiments on Amazon Mechanical Turk, which support our analytical findings.},
journal = {Proc. ACM Meas. Anal. Comput. Syst.},
month = dec,
articleno = {36},
numpages = {25},
keywords = {crowdsourcing, posted price mechanism, price discrimination, quality-based pricing}
}

@article{10.1145/2729713,
author = {To, Hien and Shahabi, Cyrus and Kazemi, Leyla},
title = {A Server-Assigned Spatial Crowdsourcing Framework},
year = {2015},
issue_date = {August 2015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {1},
number = {1},
issn = {2374-0353},
url = {https://doi.org/10.1145/2729713},
doi = {10.1145/2729713},
abstract = {With the popularity of mobile devices, spatial crowdsourcing is rising as a new framework that enables human workers to solve tasks in the physical world. With spatial crowdsourcing, the goal is to crowdsource a set of spatiotemporal tasks (i.e., tasks related to time and location) to a set of workers, which requires the workers to physically travel to those locations in order to perform the tasks. In this article, we focus on one class of spatial crowdsourcing, in which the workers send their locations to the server and thereafter the server assigns to every worker tasks in proximity to the worker’s location with the aim of maximizing the overall number of assigned tasks. We formally define this maximum task assignment (MTA) problem in spatial crowdsourcing, and identify its challenges. We propose alternative solutions to address these challenges by exploiting the spatial properties of the problem space, including the spatial distribution and the travel cost of the workers. MTA is based on the assumptions that all tasks are of the same type and all workers are equally qualified in performing the tasks. Meanwhile, different types of tasks may require workers with various skill sets or expertise. Subsequently, we extend MTA by taking the expertise of the workers into consideration. We refer to this problem as the maximum score assignment (MSA) problem and show its practicality and generality. Extensive experiments with various synthetic and two real-world datasets show the applicability of our proposed framework.},
journal = {ACM Trans. Spatial Algorithms Syst.},
month = jul,
articleno = {2},
numpages = {28},
keywords = {spatial task assignment, spatial crowdsourcing, participatory sensing, mobile crowdsourcing, Crowdsourcing}
}

@article{10.14778/2735479.2735482,
author = {Wu, Ting and Chen, Lei and Hui, Pan and Zhang, Chen Jason and Li, Weikai},
title = {Hear the whole story: towards the diversity of opinion in crowdsourcing markets},
year = {2015},
issue_date = {January 2015},
publisher = {VLDB Endowment},
volume = {8},
number = {5},
issn = {2150-8097},
url = {https://doi.org/10.14778/2735479.2735482},
doi = {10.14778/2735479.2735482},
abstract = {The recent surge in popularity of crowdsourcing has brought with it a new opportunity for engaging human intelligence in the process of data analysis. Crowdsourcing provides a fundamental mechanism for enabling online workers to participate in tasks that are either too difficult to be solved solely by a computer or too expensive to employ experts to perform. In the field of social science, four elements are required to form a wise crowd - Diversity of Opinion, Independence, Decentralization and Aggregation. However, while the other three elements are already studied and implemented in current crowdsourcing platforms, the 'Diversity of Opinion' has not been functionally enabled. In this paper, we address the algorithmic optimizations towards the diversity of opinion of crowdsourcing marketplaces.From a computational perspective, in order to build a wise crowd, we need to quantitatively modeling the diversity, and take it into consideration for constructing the crowd. In a crowdsourcing marketplace, we usually encounter two basic paradigms for worker selection: building a crowd to wait for tasks to come and selecting workers for a given task. Therefore, we propose our Similarity-driven Model (S-Model) and Task-driven Model (T-Model) for both of the paradigms. Under both of the models, we propose efficient and effective algorithms to enlist a budgeted number of workers, which have the optimal diversity. We have verified our solutions with extensive experiments on both synthetic datasets and real data sets.},
journal = {Proc. VLDB Endow.},
month = jan,
pages = {485–496},
numpages = {12}
}

@article{10.1145/3487607,
author = {Reynante, Brandon and Dow, Steven P. and Mahyar, Narges},
title = {A Framework for Open Civic Design: Integrating Public Participation, Crowdsourcing, and Design Thinking},
year = {2021},
issue_date = {October 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {2},
number = {4},
url = {https://doi.org/10.1145/3487607},
doi = {10.1145/3487607},
abstract = {Civic problems are often too complex to solve through traditional top-down strategies. Various governments and civic initiatives have explored more community-driven strategies where citizens get involved with defining problems and innovating solutions. While certain people may feel more empowered, the public at large often does not have accessible, flexible, and meaningful ways to engage. Prior theoretical frameworks for public participation typically offer a one-size-fits-all model based on face-to-face engagement and fail to recognize the barriers faced by even the most engaged citizens. In this article, we explore a vision for open civic design where we integrate theoretical frameworks from public engagement, crowdsourcing, and design thinking to consider the role technology can play in lowering barriers to large-scale participation, scaffolding problem-solving activities, and providing flexible options that cater to individuals’ skills, availability, and interests. We describe our novel theoretical framework and analyze the key goals associated with this vision: (1) to promote inclusive and sustained participation in civics; (2) to facilitate effective management of large-scale participation; and (3) to provide a structured process for achieving effective solutions. We present case studies of existing civic design initiatives and discuss challenges, limitations, and future work related to operationalizing, implementing, and testing this framework.},
journal = {Digit. Gov.: Res. Pract.},
month = dec,
articleno = {31},
numpages = {22},
keywords = {conceptual framework, design thinking, crowdsourcing, public participation, digital civics, Civic design}
}

@article{10.1145/3191741,
author = {Gleason, Cole and Ahmetovic, Dragan and Savage, Saiph and Toxtli, Carlos and Posthuma, Carl and Asakawa, Chieko and Kitani, Kris M. and Bigham, Jeffrey P.},
title = {Crowdsourcing the Installation and Maintenance of Indoor Localization Infrastructure to Support Blind Navigation},
year = {2018},
issue_date = {March 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {2},
number = {1},
url = {https://doi.org/10.1145/3191741},
doi = {10.1145/3191741},
abstract = {Indoor navigation systems can make unfamiliar buildings more accessible for people with vision impairments, but their adoption is hampered by the effort of installing infrastructure and maintaining it over time. Most solutions in this space require augmenting the environment with add-ons, such as Bluetooth beacons. Installing and calibrating such infrastructure requires time and expertise. Once installed, localization accuracy often degrades over time as batteries die, beacons go missing, or otherwise stop working. Even localization systems installed by experts can become unreliable weeks, months, or years after the installation. To address this problem, we created LuzDeploy: a physical crowdsourcing system that organizes non-experts for the installation and long-term maintenance of a Bluetooth-based navigation system. LuzDeploy simplifies the tasks required to install and maintain the localization infrastructure, thus making a crowdsourcing approach feasible for non-experts. We report on a field deployment where 127 participants installed and maintained a blind navigation system over several months in a 7-story building, completing 455 tasks in total. We compare the accuracy of the system installed by participants to an installation completed by experts with specialized equipment. LuzDeploy aims to improve the sustainability of indoor navigation systems to encourage widespread adoption outside of research settings.},
journal = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
month = mar,
articleno = {9},
numpages = {25},
keywords = {Individuals with visual impairments, Indoor navigation assistance, Physical crowdsourcing, Real-world accessibility, Volunteer crowd work}
}

@article{10.1145/2814573,
author = {Amor, Iheb Ben and Benbernou, Salima and Ouziri, Mourad and Malik, Zaki and Medjahed, Brahim},
title = {Discovering Best Teams for Data Leak-Aware Crowdsourcing in Social Networks},
year = {2016},
issue_date = {February 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {10},
number = {1},
issn = {1559-1131},
url = {https://doi.org/10.1145/2814573},
doi = {10.1145/2814573},
abstract = {Crowdsourcing is emerging as a powerful paradigm to help perform a wide range of tedious tasks in various enterprise applications. As such applications become more complex, crowdsourcing systems often require the collaboration of several experts connected through professional/social networks and organized in various teams. For instance, a well-known car manufacturer asked fans to contribute ideas for the kinds of technologies that should be incorporated into one of its cars. For that purpose, fans needed to collaborate and form teams competing with each others to come up with the best ideas. However, once teams are formed, each one would like to provide the best solution and treat that solution as a “trade secret,” hence preventing any data leak to its competitors (i.e., the other teams). In this article, we propose a data leak--aware crowdsourcing system called SocialCrowd. We introduce a clustering algorithm that uses social relationships between crowd workers to discover all possible teams while avoiding interteam data leakage. We also define a ranking mechanism to select the “best” team configurations. Our mechanism is based on the semiring approach defined in the area of soft constraints programming. Finally, we present experiments to assess the efficiency of the proposed approach.},
journal = {ACM Trans. Web},
month = feb,
articleno = {2},
numpages = {27},
keywords = {social networks, data leakage, clustering, Crowdsourcing}
}

@article{10.1145/3237188,
author = {Song, Jean Y. and Fok, Raymond and Kim, Juho and Lasecki, Walter S.},
title = {FourEyes: Leveraging Tool Diversity as a Means to Improve Aggregate Accuracy in Crowdsourcing},
year = {2019},
issue_date = {March 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {10},
number = {1},
issn = {2160-6455},
url = {https://doi.org/10.1145/3237188},
doi = {10.1145/3237188},
abstract = {Crowdsourcing is a common means of collecting image segmentation training data for use in a variety of computer vision applications. However, designing accurate crowd-powered image segmentation systems is challenging, because defining object boundaries in an image requires significant fine motor skills and hand-eye coordination, which makes these tasks error-prone. Typically, special segmentation tools are created and then answers from multiple workers are aggregated to generate more accurate results. However, individual tool designs can bias how and where people make mistakes, resulting in shared errors that remain even after aggregation. In this article, we introduce a novel crowdsourcing approach that leverages tool diversity as a means of improving aggregate crowd performance. Our idea is that given a diverse set of tools, answer aggregation done across tools can help improve the collective performance by offsetting systematic biases induced by the individual tools themselves. To demonstrate the effectiveness of the proposed approach, we design four different tools and present FourEyes, a crowd-powered image segmentation system that uses aggregation across different tools. We then conduct a series of studies that evaluate different aggregation conditions and show that using multiple tools can significantly improve aggregate accuracy. Furthermore, we investigate the idea of applying post-processing for multi-tool aggregation in terms of correction mechanism. We introduce a novel region-based method for synthesizing more accurate bounds for image segmentation tasks through averaging surrounding annotations. In addition, we explore the effect of adjusting the threshold parameter of an EM-based aggregation method. Our results suggest that not only the individual tool’s design, but also the correction mechanism, can affect the performance of multi-tool aggregation. This article extends a work presented at ACM IUI 2018&nbsp;[46] by providing a novel region-based error-correction method and additional in-depth evaluation of the proposed approach.},
journal = {ACM Trans. Interact. Intell. Syst.},
month = aug,
articleno = {3},
numpages = {30},
keywords = {tool diversity, semantic image segmentation, multi-tool aggregation, human computation, computer vision, Crowdsourcing}
}

@article{10.1145/3476073,
author = {Hettiachchi, Danula and Schaekermann, Mike and McKinney, Tristan J. and Lease, Matthew},
title = {The Challenge of Variable Effort Crowdsourcing and How Visible Gold Can Help},
year = {2021},
issue_date = {October 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {5},
number = {CSCW2},
url = {https://doi.org/10.1145/3476073},
doi = {10.1145/3476073},
abstract = {We consider a class of variable effort human annotation tasks in which the number of labels required per item can greatly vary (e.g., finding all faces in an image, named entities in a text, bird calls in an audio recording, etc.). In such tasks, some items require far more effort than others to annotate. Furthermore, the per-item annotation effort is not known until after each item is annotated since determining the number of labels required is an implicit part of the annotation task itself. On an image bounding-box task with crowdsourced annotators, we show that annotator accuracy and recall consistently drop as effort increases. We hypothesize reasons for this drop and investigate a set of approaches to counteract it. Firstly, we benchmark on this task a set of general best-practice methods for quality crowdsourcing. Notably, only one of these methods actually improves quality: the use of visible gold questions that provide periodic feedback to workers on their accuracy as they work. Given these promising results, we then investigate and evaluate variants of the visible gold approach, yielding further improvement. Final results show a 7\% improvement in bounding-box accuracy over the baseline. We discuss the generality of the visible gold approach and promising directions for future research.},
journal = {Proc. ACM Hum.-Comput. Interact.},
month = oct,
articleno = {332},
numpages = {26},
keywords = {crowdsourcing, data quality, gold standards, object detection, worker training}
}

@article{10.1145/3569092,
author = {Azizifard, Narges and Gelauff, Lodewijk and Gransard-Desmond, Jean-Olivier and Redi, Miriam and Schifanella, Rossano},
title = {Wiki Loves Monuments: Crowdsourcing the Collective Image of the Worldwide Built Heritage},
year = {2022},
issue_date = {March 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {16},
number = {1},
issn = {1556-4673},
url = {https://doi.org/10.1145/3569092},
doi = {10.1145/3569092},
abstract = {The wide adoption of digital technologies in the cultural heritage sector has promoted the emergence of new, distributed ways of working, communicating, and investigating cultural products and services. In particular, collaborative online platforms and crowdsourcing mechanisms have been widely adopted in the effort to solicit input from the community and promote engagement. In this work, we provide an extensive analysis of the Wiki Loves Monuments initiative, an annual, international photography contest in which volunteers are invited to take pictures of the built cultural heritage and upload them to Wikimedia Commons. We explore the geographical, temporal, and topical dimensions across the 2010–2021 editions. We first adopt a set of CNN-based artificial systems that allow the learning of deep scene features for various scene recognition tasks, exploring cross-country (dis)similarities. To overcome the rigidity of the framework based on scene descriptors, we train a deep convolutional neural network model to label a photo with its country of origin. The resulting model captures the best representation of a heritage site uploaded in a country, and it allows the domain experts to explore the complexity of cross-national architectural styles. Finally, as a validation step, we explore the link between architectural heritage and intangible cultural values, operationalized using the framework developed within the World Value Survey research program. We observe that cross-country cultural similarities match to a fair extent the interrelations emerging in the architectural domain. We think this study contributes to highlighting the richness and the potential of the Wikimedia data and tools ecosystem to act as a scientific object for art historians, iconologists, and archaeologists.},
journal = {J. Comput. Cult. Herit.},
month = dec,
articleno = {20},
numpages = {27},
keywords = {Wiki Loves Monuments, cross-cultural study, Cultural heritage}
}

@article{10.1145/3086686,
author = {Brade\v{s}ko, Luka and Witbrock, Michael and Starc, Janez and Herga, Zala and Grobelnik, Marko and Mladeni\'{c}, Dunja},
title = {Curious Cat--Mobile, Context-Aware Conversational Crowdsourcing Knowledge Acquisition},
year = {2017},
issue_date = {October 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {35},
number = {4},
issn = {1046-8188},
url = {https://doi.org/10.1145/3086686},
doi = {10.1145/3086686},
abstract = {Scaled acquisition of high-quality structured knowledge has been a longstanding goal of Artificial Intelligence research. Recent advances in crowdsourcing, the sheer number of Internet and mobile users, and the commercial availability of supporting platforms offer new tools for knowledge acquisition. This article applies context-aware knowledge acquisition that simultaneously satisfies users’ immediate information needs while extending its own knowledge using crowdsourcing. The focus is on knowledge acquisition on a mobile device, which makes the approach practical and scalable; in this context, we propose and implement a new KA approach that exploits an existing knowledge base to drive the KA process, communicate with the right people, and check for consistency of the user-provided answers. We tested the viability of the approach in experiments using our platform with real users around the world, and an existing large source of common-sense background knowledge. These experiments show that the approach is promising: the knowledge is estimated to be true and useful for users 95\% of the time. Using context to proactively drive knowledge acquisition increased engagement and effectiveness (the number of new assertions/day/user increased for 175\%). Using pre-existing and newly acquired knowledge also proved beneficial.},
journal = {ACM Trans. Inf. Syst.},
month = aug,
articleno = {33},
numpages = {46},
keywords = {reasoning, location and context based knowledge acquisition, knowledge systems, dialogue systems, crowdsourcing, chatbots, Sensor and location mining}
}

@article{10.1145/3555178,
author = {Kou, Ziyi and Zhang, Yang and Zhang, Daniel and Wang, Dong},
title = {CrowdGraph: A Crowdsourcing Multi-modal Knowledge Graph Approach to Explainable Fauxtography Detection},
year = {2022},
issue_date = {November 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {6},
number = {CSCW2},
url = {https://doi.org/10.1145/3555178},
doi = {10.1145/3555178},
abstract = {Human-centric fauxtography is a category of multi-modal posts that spread misleading information on online information distribution and sharing platforms such as online social media. The reason of a human-centric post being fauxtography is closely related to its multi-modal content that consists of diversified human and non-human subjects with complex and implicit relationships. In this paper, we focus on an explainable fauxtography detection problem where the goal is to accurately identify and explain why a human-centric social media post is fauxtography (or not). Our problem is motivated by the limitations of current fauxtography detection solutions that focus primarily on the detection task but ignore the important aspect of explaining their results (e.g., why a certain component of the post delivers the misinformation). Two important challenges exist in solving our problem: 1) it is difficult to capture the implicit relations and attributions of different subjects in a fauxtography post given the fact that many of such knowledge is shared between different crowd workers; 2) it is not a trivial task to create a multi-modal knowledge graph from crowd workers to identify and explain human-centric fauxtography posts with multi-modal contents. To address the above challenges, we develop CrowdGraph, a crowdsourcing based multi-modal knowledge graph approach to address the explainable fauxtography detection problem. We evaluate the performance of CrowdGraph by creating a real-world dataset that consists of human-centric fauxtography posts from Twitter and Reddit. The results show that CrowdGraph not only detects the fauxtography posts more accurately than the state-of-the-arts but also provides well-justified explanations to the detection results with convincing evidence.},
journal = {Proc. ACM Hum.-Comput. Interact.},
month = nov,
articleno = {287},
numpages = {28},
keywords = {social media, multi modal information, crowdsourcing}
}

@article{10.1145/3009924,
author = {Jahanian, Ali and Keshvari, Shaiyan and Vishwanathan, S. V. N. and Allebach, Jan P.},
title = {Colors -- Messengers of Concepts: Visual Design Mining for Learning Color Semantics},
year = {2017},
issue_date = {February 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {24},
number = {1},
issn = {1073-0516},
url = {https://doi.org/10.1145/3009924},
doi = {10.1145/3009924},
abstract = {We study the concept of color semantics by modeling a dataset of magazine cover designs, evaluating the model via crowdsourcing, and demonstrating several prototypes that facilitate color-related design tasks. We investigate a probabilistic generative modeling framework that expresses semantic concepts as a combination of color and word distributions -- color-word topics. We adopt an extension to Latent Dirichlet Allocation (LDA) topic modeling, called LDA-dual, to infer a set of color-word topics over a corpus of 2,654 magazine covers spanning 71 distinct titles and 12 genres. Although LDA models text documents as distributions over word topics, we model magazine covers as distributions over color-word topics. The results of our crowdsourcing experiments confirm that the model is able to successfully discover the associations between colors and linguistic concepts. Finally, we demonstrate several prototype applications that use the learned model to enable more meaningful interactions in color palette recommendation, design example retrieval, pattern recoloring, image retrieval, and image color selection.},
journal = {ACM Trans. Comput.-Hum. Interact.},
month = jan,
articleno = {2},
numpages = {39},
keywords = {visual design mining, visual design language, topic modeling, pattern recoloring, interaction design, image retrieval, image color selection, generative models, design example retrieval, color palette recommendation, aesthetics, Color semantics}
}

@article{10.1145/2837029,
author = {Luo, Tie and Das, Sajal K. and Tan, Hwee Pink and Xia, Lirong},
title = {Incentive Mechanism Design for Crowdsourcing: An All-Pay Auction Approach},
year = {2016},
issue_date = {April 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {3},
issn = {2157-6904},
url = {https://doi.org/10.1145/2837029},
doi = {10.1145/2837029},
abstract = {Crowdsourcing can be modeled as a principal-agent problem in which the principal (crowdsourcer) desires to solicit a maximal contribution from a group of agents (participants) while agents are only motivated to act according to their own respective advantages. To reconcile this tension, we propose an all-pay auction approach to incentivize agents to act in the principal’s interest, i.e., maximizing profit, while allowing agents to reap strictly positive utility. Our rationale for advocating all-pay auctions is based on two merits that we identify, namely all-pay auctions (i) compress the common, two-stage “bid-contribute” crowdsourcing process into a single “bid-cum-contribute” stage, and (ii) eliminate the risk of task nonfulfillment. In our proposed approach, we enhance all-pay auctions with two additional features: an adaptive prize and a general crowdsourcing environment. The prize or reward adapts itself as per a function of the unknown winning agent’s contribution, and the environment or setting generally accommodates incomplete and asymmetric information, risk-averse (and risk-neutral) agents, and a stochastic (and deterministic) population. We analytically derive this all-pay auction-based mechanism and extensively evaluate it in comparison to classic and optimized mechanisms. The results demonstrate that our proposed approach remarkably outperforms its counterparts in terms of the principal’s profit, agent’s utility, and social welfare.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = feb,
articleno = {35},
numpages = {26},
keywords = {shading effect, risk aversion, participatory sensing, incomplete information, Mobile crowd sensing, Bayesian Nash equilibrium}
}

@article{10.1145/3512935,
author = {M\'{e}ndez M\'{e}ndez, Ana Elisa and Cartwright, Mark and Bello, Juan Pablo and Nov, Oded},
title = {Eliciting Confidence for Improving Crowdsourced Audio Annotations},
year = {2022},
issue_date = {April 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {6},
number = {CSCW1},
url = {https://doi.org/10.1145/3512935},
doi = {10.1145/3512935},
abstract = {In this work we explore confidence elicitation methods for crowdsourcing "soft" labels, e.g., probability estimates, to reduce the annotation costs for domains with ambiguous data. Machine learning research has shown that such "soft" labels are more informative and can reduce the data requirements when training supervised machine learning models. By reducing the number of required labels, we can reduce the costs of slow annotation processes such as audio annotation. In our experiments we evaluated three confidence elicitation methods: 1) "No Confidence" elicitation, 2) "Simple Confidence" elicitation, and 3) "Betting" mechanism for confidence elicitation, at both individual (i.e., per participant) and aggregate (i.e., crowd) levels. In addition, we evaluated the interaction between confidence elicitation methods, annotation types (binary, probability, and z-score derived probability), and "soft" versus "hard" (i.e., binarized) aggregate labels. Our results show that both confidence elicitation mechanisms result in higher annotation quality than the "No Confidence" mechanism for binary annotations at both participant and recording levels. In addition, when aggregating labels at the recording level, results indicate that we can achieve comparable results to those with 10-participant aggregate annotations using fewer annotators if we aggregate "soft" labels instead of "hard" labels. These results suggest that for binary audio annotation using a confidence elicitation mechanism and aggregating continuous labels we can obtain higher annotation quality, more informative labels, with quality differences more pronounced with fewer participants. Finally, we propose a way of integrating these confidence elicitation methods into a two-stage, multi-label annotation pipeline.},
journal = {Proc. ACM Hum.-Comput. Interact.},
month = apr,
articleno = {88},
numpages = {25},
keywords = {machine learning, crowdsourcing, audio annotation}
}

@article{10.1145/3449148,
author = {Bandy, Jack},
title = {Problematic Machine Behavior: A Systematic Literature Review of Algorithm Audits},
year = {2021},
issue_date = {April 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {5},
number = {CSCW1},
url = {https://doi.org/10.1145/3449148},
doi = {10.1145/3449148},
abstract = {While algorithm audits are growing rapidly in commonality and public importance, relatively little scholarly work has gone toward synthesizing prior work and strategizing future research in the area. This systematic literature review aims to do just that, following PRISMA guidelines in a review of over 500 English articles that yielded 62 algorithm audit studies. The studies are synthesized and organized primarily by behavior (discrimination, distortion, exploitation, and misjudgement), with codes also provided for domain (e.g. search, vision, advertising, etc.), organization (e.g. Google, Facebook, Amazon, etc.), and audit method (e.g. sock puppet, direct scrape, crowdsourcing, etc.). The review shows how previous audit studies have exposed public-facing algorithms exhibiting problematic behavior, such as search algorithms culpable of distortion and advertising algorithms culpable of discrimination. Based on the studies reviewed, it also suggests some behaviors (e.g. discrimination on the basis of intersectional identities), domains (e.g. advertising algorithms), methods (e.g. code auditing), and organizations (e.g. Twitter, TikTok, LinkedIn) that call for future audit attention. The paper concludes by offering the common ingredients of successful audits, and discussing algorithm auditing in the context of broader research working toward algorithmic justice.},
journal = {Proc. ACM Hum.-Comput. Interact.},
month = apr,
articleno = {74},
numpages = {34},
keywords = {algorithm auditing, algorithmic accountability, algorithmic authority, algorithmic bias, algorithmic harm, critical algorithm studies, ethics, literature review, policy}
}

@article{10.1145/3108935,
author = {Peng, Xin and Gu, Jingxiao and Tan, Tian Huat and Sun, Jun and Yu, Yijun and Nuseibeh, Bashar and Zhao, Wenyun},
title = {CrowdService: Optimizing Mobile Crowdsourcing and Service Composition},
year = {2018},
issue_date = {May 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {18},
number = {2},
issn = {1533-5399},
url = {https://doi.org/10.1145/3108935},
doi = {10.1145/3108935},
abstract = {Some user needs can only be met by leveraging the capabilities of others to undertake particular tasks that require intelligence and labor. Crowdsourcing such capabilities is one way to achieve this. But providing a service that leverages crowd intelligence and labor is a challenge, since various factors need to be considered to enable reliable service provisioning. For example, the selection of an optimal set of workers from those who bid to perform a task needs to be made based on their reliability, expected reward, and distance to the target locations. Moreover, for an application involving multiple services, the overall cost and time constraints must be optimally allocated to each involved service. In this article, we develop a framework, named CrowdService, that supplies crowd intelligence and labor as publicly accessible crowd services via mobile crowdsourcing. The article extends our earlier work by providing an approach for constraints synthesis and worker selection. It employs a genetic algorithm to dynamically synthesize and update near-optimal cost and time constraints for each crowd service involved in a composite service and selects a near-optimal set of workers for each crowd service to be executed. We implement the proposed framework on Android platforms and evaluate its effectiveness, scalability, and usability in both experimental and user studies.},
journal = {ACM Trans. Internet Technol.},
month = jan,
articleno = {19},
numpages = {25},
keywords = {service composition, reliability, collaboration, Mobile crowdsourcing}
}

@article{10.1145/3442698,
author = {Peng, Chaoqun and Zhang, Xinglin and Ou, Zhaojing and Zhang, Junna},
title = {Task Planning Considering Location Familiarity in Spatial Crowdsourcing},
year = {2021},
issue_date = {May 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {17},
number = {2},
issn = {1550-4859},
url = {https://doi.org/10.1145/3442698},
doi = {10.1145/3442698},
abstract = {Spatial crowdsourcing (SC) is a popular distributed problem-solving paradigm that harnesses the power of mobile workers (e.g., smartphone users) to perform location-based tasks (e.g., checking product placement or taking landmark photos). Typically, a worker needs to travel physically to the target location to finish the assigned task. Hence, the worker’s familiarity level on the target location directly influences the completion quality of the task. In addition, from the perspective of the SC server, it is desirable to finish all tasks with a low recruitment cost. Combining these issues, we propose a Bi-Objective Task Planning (BOTP) problem in SC, where the server makes a task assignment and schedule for the workers to jointly optimize the workers’ familiarity levels on the locations of assigned tasks and the total cost of worker recruitment. The BOTP problem is proved to be NP-hard and thus intractable. To solve this challenging problem, we propose two algorithms: a divide-and-conquer algorithm based on the constraint method and a heuristic algorithm based on the multi-objective simulated annealing algorithm. The extensive evaluations on a real-world dataset demonstrate the effectiveness of the proposed algorithms.},
journal = {ACM Trans. Sen. Netw.},
month = mar,
articleno = {16},
numpages = {24},
keywords = {task planning, location-based service, location familiarity, Spatial crowdsourcing}
}

@article{10.1007/s00778-021-00685-2,
author = {Li, Yuanbing and Wu, Xian and Jin, Yifei and Li, Jian and Li, Guoliang and Feng, Jianhua},
title = {Adaptive algorithms for crowd-aided categorization},
year = {2021},
issue_date = {Nov 2022},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {31},
number = {6},
issn = {1066-8888},
url = {https://doi.org/10.1007/s00778-021-00685-2},
doi = {10.1007/s00778-021-00685-2},
abstract = {We study the problem of utilizing human intelligence to categorize a large number of objects. In this problem, given a category hierarchy and a set of objects, we can ask humans to check whether an object belongs to a category, and our goal is to find the most cost-effective strategy to locate the appropriate category in the hierarchy for each object, such that the cost (i.e., the number of questions to ask humans) is minimized. There are many important applications of this problem, including image classification and product categorization. We develop an online framework, in which category distribution is gradually learned and thus an effective order of questions are adaptively determined. We prove that even if the true category distribution is known in advance, the problem is computationally intractable. We develop an approximation algorithm, and prove that it achieves an approximation factor of 2. We also show that there is a fully polynomial time approximation scheme for the problem. Furthermore, we propose an online strategy which achieves nearly the same performance guarantee as the offline optimal strategy, even if there is no knowledge about category distribution beforehand. We develop effective techniques to tolerate crowd errors. Experiments on a real crowdsourcing platform demonstrate the effectiveness of our method.},
journal = {The VLDB Journal},
month = aug,
pages = {1311–1337},
numpages = {27},
keywords = {Approximation algorithm, Online algorithm, Categorization, Human-in-the-loop, Crowdsourcing}
}

@article{10.14778/2336664.2336676,
author = {Liu, Xuan and Lu, Meiyu and Ooi, Beng Chin and Shen, Yanyan and Wu, Sai and Zhang, Meihui},
title = {CDAS: a crowdsourcing data analytics system},
year = {2012},
issue_date = {June 2012},
publisher = {VLDB Endowment},
volume = {5},
number = {10},
issn = {2150-8097},
url = {https://doi.org/10.14778/2336664.2336676},
doi = {10.14778/2336664.2336676},
abstract = {Some complex problems, such as image tagging and natural language processing, are very challenging for computers, where even state-of-the-art technology is yet able to provide satisfactory accuracy. Therefore, rather than relying solely on developing new and better algorithms to handle such tasks, we look to the crowdsourcing solution -- employing human participation -- to make good the shortfall in current technology. Crowdsourcing is a good supplement to many computer tasks. A complex job may be divided into computer-oriented tasks and human-oriented tasks, which are then assigned to machines and humans respectively.To leverage the power of crowdsourcing, we design and implement a Crowdsourcing Data Analytics System, CDAS. CDAS is a framework designed to support the deployment of various crowdsourcing applications. The core part of CDAS is a quality-sensitive answering model, which guides the crowdsourcing engine to process and monitor the human tasks. In this paper, we introduce the principles of our quality-sensitive model. To satisfy user required accuracy, the model guides the crowdsourcing query engine for the design and processing of the corresponding crowdsourcing jobs. It provides an estimated accuracy for each generated result based on the human workers' historical performances. When verifying the quality of the result, the model employs an online strategy to reduce waiting time. To show the effectiveness of the model, we implement and deploy two analytics jobs on CDAS, a twitter sentiment analytics job and an image tagging job. We use real Twitter and Flickr data as our queries respectively. We compare our approaches with state-of-the-art classification and image annotation techniques. The results show that the human-assisted methods can indeed achieve a much higher accuracy. By embedding the quality-sensitive model into crowdsourcing query engine, we effectively reduce the processing cost while maintaining the required query answer quality.},
journal = {Proc. VLDB Endow.},
month = jun,
pages = {1040–1051},
numpages = {12}
}

@article{10.1109/TNET.2016.2533399,
author = {Wu, Di and Liu, Qiang and Li, Yong and McCann, Julie A. and Regan, Amelia C. and Venkatasubramanian, Nalini},
title = {Adaptive Lookup of Open WiFi Using Crowdsensing},
year = {2016},
issue_date = {December 2016},
publisher = {IEEE Press},
volume = {24},
number = {6},
issn = {1063-6692},
url = {https://doi.org/10.1109/TNET.2016.2533399},
doi = {10.1109/TNET.2016.2533399},
abstract = {Open WiFi access points APs are demonstrating that they can provide opportunistic data services to moving vehicles. We present CrowdWiFi, a novel system to look up roadside WiFi APs located outdoors or inside buildings. CrowdWiFi consists of two components: online compressive sensing CS and offline crowdsourcing. Online CS presents an efficient framework for the coarse-grained estimation of nearby APs along the driving route, where received signal strength RSS values are recorded at runtime, and the number and location of the APs are recovered immediately based on limited RSS readings and adaptive CS operations. Offline crowdsourcing assigns the online CS tasks to crowd-vehicles and aggregates answers on a bipartite graphical model. Crowd-server also iteratively infers the reliability of each crowd-vehicle from the aggregated sensing results, and then refines the estimation of the APs using weighted centroid processing. Extensive simulation results and real testbed experiments confirm that CrowdWiFi can successfully reduce the computation cost and energy consumption of roadside WiFi lookup, while maintaining satisfactory localization accuracy.},
journal = {IEEE/ACM Trans. Netw.},
month = dec,
pages = {3634–3647},
numpages = {14}
}

@article{10.1145/3408040,
author = {Talukder, Sajedul and Carbunar, Bogdan},
title = {A Study of Friend Abuse Perception in Facebook},
year = {2020},
issue_date = {December 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {3},
number = {4},
url = {https://doi.org/10.1145/3408040},
doi = {10.1145/3408040},
abstract = {Social networks like Facebook provide functionality that can expose users to abuse perpetrated by their contacts. For instance, Facebook users can often access sensitive profile information and timeline posts of their friends and also post abuse on the timeline and news feed of their friends. In this article, we introduce AbuSniff, a system to identify Facebook friends perceived to be abusive or strangers and protect the user by restricting the access to information for such friends. We develop a questionnaire to detect perceived strangers and friend abuse. We train supervised learning algorithms to predict questionnaire responses using features extracted from the mutual activities with Facebook friends. In our experiments, participants recruited from a crowdsourcing site agreed with 78\% of the defense actions suggested by AbuSniff, without having to answer any questions about their friends. When compared to a control app, AbuSniff significantly increased the willingness of participants to take a defensive action against friends. AbuSniff also increased the participant self-reported willingness to reject friend invitations from strangers and abusers, their awareness of friend abuse implications, and their perceived protection from friend abuse.},
journal = {Trans. Soc. Comput.},
month = sep,
articleno = {17},
numpages = {34},
keywords = {supervised detection, friend spam, Social network friend abuse}
}

@article{10.14778/2733004.2733024,
author = {Sun, Chong and Rampalli, Narasimhan and Yang, Frank and Doan, AnHai},
title = {Chimera: large-scale classification using machine learning, rules, and crowdsourcing},
year = {2014},
issue_date = {August 2014},
publisher = {VLDB Endowment},
volume = {7},
number = {13},
issn = {2150-8097},
url = {https://doi.org/10.14778/2733004.2733024},
doi = {10.14778/2733004.2733024},
abstract = {Large-scale classification is an increasingly critical Big Data problem. So far, however, very little has been published on how this is done in practice. In this paper we describe Chimera, our solution to classify tens of millions of products into 5000+ product types at WalmartLabs. We show that at this scale, many conventional assumptions regarding learning and crowdsourcing break down, and that existing solutions cease to work. We describe how Chimera employs a combination of learning, rules (created by in-house analysts), and crowdsourcing to achieve accurate, continuously improving, and cost-effective classification. We discuss a set of lessons learned for other similar Big Data systems. In particular, we argue that at large scales crowdsourcing is critical, but must be used in combination with learning, rules, and in-house analysts. We also argue that using rules (in conjunction with learning) is a must, and that more research attention should be paid to helping analysts create and manage (tens of thousands of) rules more effectively.},
journal = {Proc. VLDB Endow.},
month = aug,
pages = {1529–1540},
numpages = {12}
}

@article{10.1145/2717513,
author = {Hara, Kotaro and Azenkot, Shiri and Campbell, Megan and Bennett, Cynthia L. and Le, Vicki and Pannella, Sean and Moore, Robert and Minckler, Kelly and Ng, Rochelle H. and Froehlich, Jon E.},
title = {Improving Public Transit Accessibility for Blind Riders by Crowdsourcing Bus Stop Landmark Locations with Google Street View: An Extended Analysis},
year = {2015},
issue_date = {March 2015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {6},
number = {2},
issn = {1936-7228},
url = {https://doi.org/10.1145/2717513},
doi = {10.1145/2717513},
abstract = {Low-vision and blind bus riders often rely on known physical landmarks to help locate and verify bus stop locations (e.g., by searching for an expected shelter, bench, or newspaper bin). However, there are currently few, if any, methods to determine this information a priori via computational tools or services. In this article, we introduce and evaluate a new scalable method for collecting bus stop location and landmark descriptions by combining online crowdsourcing and Google Street View (GSV). We conduct and report on three studies: (i) a formative interview study of 18 people with visual impairments to inform the design of our crowdsourcing tool, (ii) a comparative study examining differences between physical bus stop audit data and audits conducted virtually with GSV, and (iii) an online study of 153 crowd workers on Amazon Mechanical Turk to examine the feasibility of crowdsourcing bus stop audits using our custom tool with GSV. Our findings reemphasize the importance of landmarks in nonvisual navigation, demonstrate that GSV is a viable bus stop audit dataset, and show that minimally trained crowd workers can find and identify bus stop landmarks with 82.5\% accuracy across 150 bus stop locations (87.3\% with simple quality control).},
journal = {ACM Trans. Access. Comput.},
month = mar,
articleno = {5},
numpages = {23},
keywords = {remote data collection, low-vision and blind users, bus stop auditing, accessible bus stops, Mechanical Turk, Google Street View, Crowdsourcing accessibility}
}

@article{10.1145/2733379,
author = {Mordacchini, Matteo and Passarella, Andrea and Conti, Marco and Allen, Stuart M. and Chorley, Martin J. and Colombo, Gualtiero B. and Tanasescu, Vlad and Whitaker, Roger M.},
title = {Crowdsourcing through Cognitive Opportunistic Networks},
year = {2015},
issue_date = {June 2015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {10},
number = {2},
issn = {1556-4665},
url = {https://doi.org/10.1145/2733379},
doi = {10.1145/2733379},
abstract = {Until recently crowdsourcing has been primarily conceived as an online activity to harness resources for problem solving. However, the emergence of Opportunistic Networking (ON) has opened up crowdsourcing to the spatial domain. In this article, we bring the ON model for potential crowdsourcing in the smart city environment. We introduce cognitive features of the ON that allow users’ mobile devices to become aware of the surrounding physical environment. Specifically, we exploit cognitive psychology studies on dynamic memory structures and cognitive heuristics—mental models that describe how the human brain handles decision making among complex and real-time stimuli. Combined with ON, these cognitive features allow devices to act as proxies in their users’ cyberworlds and exchange knowledge to deliver awareness of places in an urban environment. This is done through tags associated with locations. They represent features that are perceived by humans about a place. We consider the extent to which this knowledge becomes available to participants using interactions with locations and other nodes. This is assessed taking into account a wide range of cognitive parameters. Outcomes are important because this functionality could support a new type of recommendation system that is independent of the traditional forms of networking.},
journal = {ACM Trans. Auton. Adapt. Syst.},
month = jun,
articleno = {13},
numpages = {29},
keywords = {smart cities, crowdsourcing, cognitive heuristic, Opportunistic networks}
}

@article{10.1145/3555660,
author = {Gligoric, Kristina and \DH{}ordevic, Irena and West, Robert},
title = {Biased Bytes: On the Validity of Estimating Food Consumption from Digital Traces},
year = {2022},
issue_date = {November 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {6},
number = {CSCW2},
url = {https://doi.org/10.1145/3555660},
doi = {10.1145/3555660},
abstract = {Given that measuring food consumption at a population scale is a challenging task, researchers have begun to explore digital traces (e.g., from social media or from food-tracking applications) as potential proxies. However, it remains unclear to what extent digital traces reflect real food consumption. The present study aims to bridge this gap by quantifying the link between dietary behaviors as captured via social media (Twitter) v.s. a food-tracking application (MyFoodRepo). We focus on the case of Switzerland and contrast images of foods collected through the two platforms, by designing and deploying a novel crowdsourcing framework for estimating biases with respect to nutritional properties and appearance. We find that the food type distributions in social media v.s. food tracking diverge; e.g., bread is 2.5 times more frequent among consumed and tracked foods than on Twitter, whereas cake is 12 times more frequent on Twitter. Controlling for the different food type distributions, we contrast consumed and tracked foods of a given type with foods shared on Twitter. Across food types, food posted on Twitter is perceived as tastier, more caloric, less healthy, less likely to have been consumed at home, more complex, and larger-portioned, compared to consumed and tracked foods. The fact that there is a divergence between food consumption as measured via the two platforms implies that at least one of the two is not a faithful representation of the true food consumption in the general Swiss population. Thus, researchers should be attentive and aim to establish evidence of validity before using digital traces as a proxy for the true food consumption of a general population. We conclude by discussing the potential sources of these biases and their implications, outlining pitfalls and threats to validity, and proposing actionable ways for overcoming them.},
journal = {Proc. ACM Hum.-Comput. Interact.},
month = nov,
articleno = {497},
numpages = {27},
keywords = {validity, twitter, social media, images, food, crowdsourcing, biases}
}

@article{10.1145/3555634,
author = {Groh, Matthew and Harris, Caleb and Daneshjou, Roxana and Badri, Omar and Koochek, Arash},
title = {Towards Transparency in Dermatology Image Datasets with Skin Tone Annotations by Experts, Crowds, and an Algorithm},
year = {2022},
issue_date = {November 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {6},
number = {CSCW2},
url = {https://doi.org/10.1145/3555634},
doi = {10.1145/3555634},
abstract = {While artificial intelligence (AI) holds promise for supporting healthcare providers and improving the accuracy of medical diagnoses, a lack of transparency in the composition of datasets exposes AI models to the possibility of unintentional and avoidable mistakes. In particular, public and private image datasets of dermatological conditions rarely include information on skin color. As a start towards increasing transparency, AI researchers have appropriated the use of the Fitzpatrick skin type (FST) from a measure of patient photosensitivity to a measure for estimating skin tone in algorithmic audits of computer vision applications including facial recognition and dermatology diagnosis. In order to understand the variability of estimated FST annotations on images, we compare several FST annotation methods on a diverse set of 460 images of skin conditions from both textbooks and online dermatology atlases. These methods include expert annotation by board-certified dermatologists, algorithmic annotation via the Individual Typology Angle algorithm, which is then converted to estimated FST (ITA-FST), and two crowd-sourced, dynamic consensus protocols for annotating estimated FSTs. We find the inter-rater reliability between three board-certified dermatologists is comparable to the inter-rater reliability between the board-certified dermatologists and either of the crowdsourcing methods. In contrast, we find that the ITA-FST method produces annotations that are significantly less correlated with the experts' annotations than the experts' annotations are correlated with each other. These results demonstrate that algorithms based on ITA-FST are not reliable for annotating large-scale image datasets, but human-centered, crowd-based protocols can reliably add skin type transparency to dermatology datasets. Furthermore, we introduce the concept of dynamic consensus protocols with tunable parameters including expert review that increase the visibility of crowdwork and provide guidance for future crowdsourced annotations of large image datasets.},
journal = {Proc. ACM Hum.-Comput. Interact.},
month = nov,
articleno = {521},
numpages = {26},
keywords = {transparency, healthcare, fairness, crowdsourcing, artificial intelligence, accountability}
}

@article{10.1145/2873064,
author = {You, Linlin and Motta, Gianmario and Liu, Kaixu and Ma, Tianyi},
title = {CITY FEED: A Pilot System of Citizen-Sourcing for City Issue Management},
year = {2016},
issue_date = {July 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {4},
issn = {2157-6904},
url = {https://doi.org/10.1145/2873064},
doi = {10.1145/2873064},
abstract = {Crowdsourcing implies user collaboration and engagement, which fosters a renewal of city governance processes. In this article, we address a subset of crowdsourcing, named citizen-sourcing, where citizens interact with authorities collaboratively and actively. Many systems have experimented citizen-sourcing in city governance processes; however, their maturity levels are mixed. In order to focus on the service maturity, we introduce a city service maturity framework that contains five levels of service support and two levels of information integration. As an example, we introduce CITY FEED, which implements citizen-sourcing in city issue management process. In order to support such process, CITY FEED supports all levels of the maturity framework (publishing, transacting, interacting, collaborating, and evaluating) and integrates related information relationally and heterogeneously. In order to integrate heterogeneous information, it implements a threefold feed deduplication mechanism based on the geographic, text semantic, and image similarities of feeds. Currently, CITY FEED is in a pilot stage.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = mar,
articleno = {53},
numpages = {25},
keywords = {feed deduplication, city service maturity framework, citizen-sourcing, citizen-sourced city issue management, Crowdsourcing}
}

@article{10.5555/3546258.3546448,
author = {Song, Changyue and Liu, Kaibo and Zhang, Xi},
title = {Collusion detection and ground truth inference in crowdsourcing for labeling tasks},
year = {2021},
issue_date = {January 2021},
publisher = {JMLR.org},
volume = {22},
number = {1},
issn = {1532-4435},
abstract = {Crowdsourcing has been a prompt and cost-effective way of obtaining labels in many machine learning applications. In the literature, a number of algorithms have been developed to infer the ground truth based on the collected labels. However, most existing studies assume workers to be independent and are vulnerable to worker collusion. This paper aims at detecting the collusive behaviors of workers in labeling tasks. Specifically, we consider collusion in a pairwise manner and propose a penalized pairwise profile likelihood method based on the adaptive LASSO penalty for collusion detection. Many models that describe the behavior of independent workers can be incorporated into our proposed framework as the baseline model. We further investigate the theoretical properties of the proposed method that guarantee the asymptotic performance. An algorithm based on expectation-maximization algorithm and coordinate descent is proposed to numerically maximize the penalized pairwise profile likelihood function for parameter estimation. To the best of our knowledge, this is the first statistical model that simultaneously detects collusion, learns workers' capabilities, and infers the ground true labels. Numerical studies using synthetic and real data sets are also conducted to verify the performance of the method.},
journal = {J. Mach. Learn. Res.},
month = jan,
articleno = {190},
numpages = {45},
keywords = {pairwise profile likelihood, collusion, crowdsourcing, adaptive LASSO}
}

@article{10.14778/2824032.2824136,
author = {Gao, Jing and Li, Qi and Zhao, Bo and Fan, Wei and Han, Jiawei},
title = {Truth discovery and crowdsourcing aggregation: a unified perspective},
year = {2015},
issue_date = {August 2015},
publisher = {VLDB Endowment},
volume = {8},
number = {12},
issn = {2150-8097},
url = {https://doi.org/10.14778/2824032.2824136},
doi = {10.14778/2824032.2824136},
abstract = {In the era of Big Data, data entries, even describing the same objects or events, can come from a variety of sources, where a data source can be a web page, a database or a person. Consequently, conflicts among sources become inevitable. To resolve the conflicts and achieve high quality data, truth discovery and crowdsourcing aggregation have been studied intensively. However, although these two topics have a lot in common, they are studied separately and are applied to different domains. To answer the need of a systematic introduction and comparison of the two topics, we present an organized picture on truth discovery and crowdsourcing aggregation in this tutorial. They are compared on both theory and application levels, and their related areas as well as open questions are discussed.},
journal = {Proc. VLDB Endow.},
month = aug,
pages = {2048–2049},
numpages = {2}
}

@article{10.1145/3403931,
author = {Maddalena, Eddy and Ib\'{a}\~{n}ez, Luis-Daniel and Simperl, Elena},
title = {Mapping Points of Interest Through Street View Imagery and Paid Crowdsourcing},
year = {2020},
issue_date = {October 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {11},
number = {5},
issn = {2157-6904},
url = {https://doi.org/10.1145/3403931},
doi = {10.1145/3403931},
abstract = {We present the Virtual City Explorer (VCE), an online crowdsourcing platform for the collection of rich geotagged information in urban environments. Compared to other volunteered geographic information approaches, which are constrained by the number and availability of mapping enthusiasts on the ground, the VCE uses digital street imagery to allow people to virtually explore a city from anywhere in the world, using a browser or a mobile phone. In addition, contributions in VCE are designed as paid microtasks—small jobs that can be carried out without any specific knowledge of the local area or previous mapping expertise in exchange for a fee. We tested the VCE in two cities to map points of interest (PoIs) in transport and mobility, using FigureEight to recruit participants. We were able to show that our platform enables crowdworkers to submit PoI location seamlessly, cover almost all of the tested areas, and discover several PoIs not reported by other approaches. This allows the VCE to complement existing approaches that leverage experts or grassroot communities.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = aug,
articleno = {63},
numpages = {28},
keywords = {urban auditing, microtasks, mapping, geospatial information, geographic information, Crowdsourcing}
}

@article{10.1109/TNET.2018.2828415,
author = {Klos nee Muller, Sabrina and Tekin, Cem and van der Schaar, Mihaela and Klein, Anja},
title = {Context-Aware Hierarchical Online Learning for Performance Maximization in Mobile Crowdsourcing},
year = {2018},
issue_date = {June 2018},
publisher = {IEEE Press},
volume = {26},
number = {3},
issn = {1063-6692},
url = {https://doi.org/10.1109/TNET.2018.2828415},
doi = {10.1109/TNET.2018.2828415},
abstract = {In mobile crowdsourcing MCS, mobile users accomplish outsourced human intelligence tasks. MCS requires an appropriate task assignment strategy, since different workers may have different performance in terms of acceptance rate and quality. Task assignment is challenging, since a worker’s performance 1 may fluctuate, depending on both the worker’s current personal context and the task context and 2 is not known a priori, but has to be learned over time. Moreover, learning context-specific worker performance requires access to context information, which may not be available at a central entity due to communication overhead or privacy concerns. In addition, evaluating worker performance might require costly quality assessments. In this paper, we propose a context-aware hierarchical online learning algorithm addressing the problem of performance maximization in MCS. In our algorithm, a local controller LC in the mobile device of a worker regularly observes the worker’s context, her/his decisions to accept or decline tasks and the quality in completing tasks. Based on these observations, the LC regularly estimates the worker’s context-specific performance. The mobile crowdsourcing platform MCSP then selects workers based on performance estimates received from the LCs. This hierarchical approach enables the LCs to learn context-specific worker performance and it enables the MCSP to select suitable workers. In addition, our algorithm preserves worker context locally, and it keeps the number of required quality assessments low. We prove that our algorithm converges to the optimal task assignment strategy. Moreover, the algorithm outperforms simpler task assignment strategies in experiments based on synthetic and real data.},
journal = {IEEE/ACM Trans. Netw.},
month = jun,
pages = {1334–1347},
numpages = {14}
}

@article{10.5555/2946645.3007055,
author = {Zhang, Yuchen and Chen, Xi and Zhou, Dengyong and Jordan, Michael I.},
title = {Spectral methods meet EM: a provably optimal algorithm for crowdsourcing},
year = {2016},
issue_date = {January 2016},
publisher = {JMLR.org},
volume = {17},
number = {1},
issn = {1532-4435},
abstract = {Crowdsourcing is a popular paradigm for effectively collecting labels at low cost. The Dawid-Skene estimator has been widely used for inferring the true labels from the noisy labels provided by non-expert crowdsourcing workers. However, since the estimator maximizes a non-convex log-likelihood function, it is hard to theoretically justify its performance. In this paper, we propose a two-stage efficient algorithm for multi-class crowd labeling problems. The first stage uses the spectral method to obtain an initial estimate of parameters. Then the second stage refines the estimation by optimizing the objective function of the Dawid-Skene estimator via the EM algorithm. We show that our algorithm achieves the optimal convergence rate up to a logarithmic factor. We conduct extensive experiments on synthetic and real datasets. Experimental results demonstrate that the proposed algorithm is comparable to the most accurate empirical approach, while outperforming several other recently proposed methods.},
journal = {J. Mach. Learn. Res.},
month = jan,
pages = {3537–3580},
numpages = {44},
keywords = {spectral methods, non-convex optimization, minimax rate, crowdsourcing, EM, Dawid-Skene model}
}

@article{10.1145/2856102,
author = {Radanovic, Goran and Faltings, Boi and Jurca, Radu},
title = {Incentives for Effort in Crowdsourcing Using the Peer Truth Serum},
year = {2016},
issue_date = {July 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {4},
issn = {2157-6904},
url = {https://doi.org/10.1145/2856102},
doi = {10.1145/2856102},
abstract = {Crowdsourcing is widely proposed as a method to solve a large variety of judgment tasks, such as classifying website content, peer grading in online courses, or collecting real-world data. As the data reported by workers cannot be verified, there is a tendency to report random data without actually solving the task. This can be countered by making the reward for an answer depend on its consistency with answers given by other workers, an approach called peer consistency. However, it is obvious that the best strategy in such schemes is for all workers to report the same answer without solving the task.Dasgupta and Ghosh [2013] show that, in some cases, exerting high effort can be encouraged in the highest-paying equilibrium. In this article, we present a general mechanism that implements this idea and is applicable to most crowdsourcing settings. Furthermore, we experimentally test the novel mechanism, and validate its theoretical properties.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = mar,
articleno = {48},
numpages = {28},
keywords = {peer prediction, mechanism design, Crowdsourcing}
}

@article{10.1145/3359279,
author = {Reeves, Neal T. and Simperl, Elena},
title = {Efficient, but Effective? Volunteer Engagement in Short-term Virtual Citizen Science Projects},
year = {2019},
issue_date = {November 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {3},
number = {CSCW},
url = {https://doi.org/10.1145/3359279},
doi = {10.1145/3359279},
abstract = {Virtual citizen science (VCS) projects have proven to be a highly effective method to analyse large quantities of data for scientific research purposes. Yet if these projects are to achieve their goals, they must attract and maintain the interest of sufficient numbers of active, dedicated volunteers. Although CSCW and HCI research has typically focussed on designing platforms to support long-term engagement, in recent years a new project format has been trialled -- using short-term crowdsourcing activities lasting as little as 48 hours. In this paper, we explore two short-term projects to understand how they influence participant engagement in the task and discussion elements of VCS. We calculate descriptive statistics to characterise project participants. Additionally, using calculation of correlation coefficients and hypothesis testing, we identify factors influencing volunteer task engagement and the effect this has on project outcomes. Our findings contribute to the understanding of volunteer engagement in VCS.},
journal = {Proc. ACM Hum.-Comput. Interact.},
month = nov,
articleno = {177},
numpages = {35},
keywords = {social computing, online communities, crowdsourcing, citizen science}
}

@article{10.5555/3122009.3242026,
author = {Morstatter, Fred and Liu, Huan},
title = {In search of coherence and consensus: measuring the interpretability of statistical topics},
year = {2017},
issue_date = {January 2017},
publisher = {JMLR.org},
volume = {18},
number = {1},
issn = {1532-4435},
abstract = {Topic modeling is an important tool in natural language processing. Topic models provide two forms of output. The first is a predictive model. This type of model has the ability to predict unseen documents (e.g., their categories). When topic models are used in this way, there are ample measures to assess their performance. The second output of these models is the topics themselves. Topics are lists of keywords that describe the top words pertaining to each topic. Often, these lists of keywords are presented to a human subject who then assesses the meaning of the topic, which is ultimately subjective. One of the fundamental problems of topic models lies in assessing the quality of the topics from the perspective of human interpretability. Naturally, human subjects need to be employed to evaluate interpretability of a topic. Lately, crowdsourcing approaches are widely used to serve the role of human subjects in evaluation. In this work we study measures of interpretability and propose to measure topic interpretability from two perspectives: topic coherence and topic consensus. We start with an existing measure for topic coherence--model precision. It evaluates coherence of a topic by introducing an intruded word and measuring how well a human subject or a crowdsourcing approach could identify the intruded word: if it is easy to identify, the topic is coherent. We then investigate how we can measure coherence comprehensively by examining dimensions of topic coherence. For the second perspective of topic interpretability, we suggest topic consensus that measures how well the results of a crowdsourcing approach matches those given categories of topics. Good topics should lead to good categories, thus, high topic consensus. Therefore, if there is low topic consensus in terms of categories, topics could be of low interpretability. We then further discuss how topic coherence and topic consensus assess different aspects of topic interpretability and hope that this work can pave way for comprehensive measures of topic interpretability.},
journal = {J. Mach. Learn. Res.},
month = jan,
pages = {6177–6208},
numpages = {32}
}

@article{10.14778/2367502.2367555,
author = {Park, Hyunjung and Garcia-Molina, Hector and Pang, Richard and Polyzotis, Neoklis and Parameswaran, Aditya and Widom, Jennifer},
title = {Deco: a system for declarative crowdsourcing},
year = {2012},
issue_date = {August 2012},
publisher = {VLDB Endowment},
volume = {5},
number = {12},
issn = {2150-8097},
url = {https://doi.org/10.14778/2367502.2367555},
doi = {10.14778/2367502.2367555},
abstract = {Deco is a system that enables declarative crowdsourcing: answering SQL queries posed over data gathered from the crowd as well as existing relational data. Deco implements a novel push-pull hybrid execution model in order to support a flexible data model and a precise query semantics, while coping with the combination of latency, monetary cost, and uncertainty of crowdsourcing. We demonstrate Deco using two crowdsourcing platforms: Amazon Mechanical Turk and an in-house platform, to show how Deco provides a convenient means of collecting and querying crowdsourced data.},
journal = {Proc. VLDB Endow.},
month = aug,
pages = {1990–1993},
numpages = {4}
}

@article{10.1145/3484828,
author = {Amaral, Gabriel and Piscopo, Alessandro and Kaffee, Lucie-aim\'{e}e and Rodrigues, Odinaldo and Simperl, Elena},
title = {Assessing the Quality of Sources in Wikidata Across Languages: A Hybrid Approach},
year = {2021},
issue_date = {December 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {13},
number = {4},
issn = {1936-1955},
url = {https://doi.org/10.1145/3484828},
doi = {10.1145/3484828},
abstract = {Wikidata is one of the most important sources of structured data on the web, built by a worldwide community of volunteers. As a secondary source, its contents must be backed by credible references; this is particularly important, as Wikidata explicitly encourages editors to add claims for which there is no broad consensus, as long as they are corroborated by references. Nevertheless, despite this essential link between content and references, Wikidata's ability to systematically assess and assure the quality of its references remains limited. To this end, we carry out a mixed-methods study to determine the relevance, ease of access, and authoritativeness of Wikidata references, at scale and in different languages, using online crowdsourcing, descriptive statistics, and machine learning. Building on previous work of ours, we run a series of microtasks experiments to evaluate a large corpus of references, sampled from Wikidata triples with labels in several languages. We use a consolidated, curated version of the crowdsourced assessments to train several machine learning models to scale up the analysis to the whole of Wikidata. The findings help us ascertain the quality of references in Wikidata and identify common challenges in defining and capturing the quality of user-generated multilingual structured data on the web. We also discuss ongoing editorial practices, which could encourage the use of higher-quality references in a more immediate way. All data and code used in the study are available on GitHub for feedback and further improvement and deployment by the research community.},
journal = {J. Data and Information Quality},
month = oct,
articleno = {23},
numpages = {35},
keywords = {knowledge graphs, data quality, verifiability, crowdsourcing, Wikidata}
}

@article{10.5555/2789272.2789273,
author = {Chen, Xi and Lin, Qihang and Zhou, Dengyong},
title = {Statistical decision making for optimal budget allocation in crowd labeling},
year = {2015},
issue_date = {January 2015},
publisher = {JMLR.org},
volume = {16},
number = {1},
issn = {1532-4435},
abstract = {It has become increasingly popular to obtain machine learning labels through commercial crowdsourcing services. The crowdsourcing workers or annotators are paid for each label they provide, but the task requester usually has only a limited amount of the budget. Since the data instances have different levels of labeling difficulty and the workers have different reliability for the labeling task, it is desirable to wisely allocate the budget among all the instances and workers such that the overall labeling quality is maximized. In this paper, we formulate the budget allocation problem as a Bayesian Markov decision process (MDP), which simultaneously conducts learning and decision making. The optimal allocation policy can be obtained by using the dynamic programming (DP) recurrence. However, DP quickly becomes computationally intractable when the size of the problem increases. To solve this challenge, we propose a computationally eficient approximate policy which is called optimistic knowledge gradient. Our method applies to both pull crowdsourcing marketplaces with homogeneous workers and push marketplaces with heterogeneous workers. It can also incorporate the contextual information of instances when they are available. The experiments on both simulated and real data show that our policy achieves a higher labeling quality than other existing policies at the same budget level.},
journal = {J. Mach. Learn. Res.},
month = jan,
pages = {1–46},
numpages = {46},
keywords = {optimistic knowledge gradient, dynamic programming, crowdsourcing, budget allocation, Markov decision process}
}

@article{10.1145/3359238,
author = {Li, Tianyi and Manns, Chandler J. and North, Chris and Luther, Kurt},
title = {Dropping the Baton? Understanding Errors and Bottlenecks in a Crowdsourced Sensemaking Pipeline},
year = {2019},
issue_date = {November 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {3},
number = {CSCW},
url = {https://doi.org/10.1145/3359238},
doi = {10.1145/3359238},
abstract = {Crowdsourced sensemaking has shown great potential for enabling scalable analysis of complex data sets, from planning trips, to designing products, to solving crimes. Yet, most crowd sensemaking approaches still require expert intervention because of worker errors and bottlenecks that would otherwise harm the output quality. Mitigating these errors and bottlenecks would significantly reduce the burden on experts, yet little is known about the types of mistakes crowds make with sensemaking micro-tasks and how they propagate in the sensemaking loop. In this paper, we conduct a series of studies with 325 crowd workers using a crowd sensemaking pipeline to solve a fictional terrorist plot, focusing on understanding why errors and bottlenecks happen and how they propagate. We classify types of crowd errors and show how the amount and quality of input data influence worker performance. We conclude by suggesting design recommendations for integrated crowdsourcing systems and speculating how a complementary top-down path of the pipeline could refine crowd analyses.},
journal = {Proc. ACM Hum.-Comput. Interact.},
month = nov,
articleno = {136},
numpages = {26},
keywords = {text analytics, sensemaking, mysteries, investigations, intelligence analysis, crowdsourcing}
}

@article{10.14778/3275536.3275541,
author = {Yang, Jingru and Fan, Ju and Wei, Zhewei and Li, Guoliang and Liu, Tongyu and Du, Xiaoyong},
title = {Cost-effective data annotation using game-based crowdsourcing},
year = {2018},
issue_date = {September 2018},
publisher = {VLDB Endowment},
volume = {12},
number = {1},
issn = {2150-8097},
url = {https://doi.org/10.14778/3275536.3275541},
doi = {10.14778/3275536.3275541},
abstract = {Large-scale data annotation is indispensable for many applications, such as machine learning and data integration. However, existing annotation solutions either incur expensive cost for large datasets or produce noisy results. This paper introduces a cost-effective annotation approach, and focuses on the labeling rule generation problem that aims to generate high-quality rules to largely reduce the labeling cost while preserving quality. To address the problem, we first generate candidate rules, and then devise a game-based crowdsourcing approach CROWDGAME to select high-quality rules by considering coverage and precision. CROWDGAME employs two groups of crowd workers: one group answers rule validation tasks (whether a rule is valid) to play a role of rule generator, while the other group answers tuple checking tasks (whether the annotated label of a data tuple is correct) to play a role of rule refuter. We let the two groups play a two-player game: rule generator identifies high-quality rules with large coverage and precision, while rule refuter tries to refute its opponent rule generator by checking some tuples that provide enough evidence to reject rules covering the tuples. This paper studies the challenges in CROWDGAME. The first is to balance the trade-off between coverage and precision. We define the loss of a rule by considering the two factors. The second is rule precision estimation. We utilize Bayesian estimation to combine both rule validation and tuple checking tasks. The third is to select crowdsourcing tasks to fulfill the game-based framework for minimizing the loss. We introduce a minimax strategy and develop efficient task selection algorithms. We conduct experiments on entity matching and relation extraction, and the results show that our method outperforms state-of-the-art solutions.},
journal = {Proc. VLDB Endow.},
month = sep,
pages = {57–70},
numpages = {14}
}

@article{10.1145/3478106,
author = {Zhou, Tongqing and Cai, Zhiping and Liu, Fang},
title = {The Crowd Wisdom for Location Privacy of Crowdsensing Photos: Spear or Shield?},
year = {2021},
issue_date = {Sept 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {5},
number = {3},
url = {https://doi.org/10.1145/3478106},
doi = {10.1145/3478106},
abstract = {The incorporation of the mobile crowd in visual sensing provides a significant opportunity to explore and understand uncharted physical places. We investigate the gains and losses of the involvement of the crowd wisdom on users' location privacy in photo crowdsensing. For the negative effects, we design a novel crowdsensing photo location inference model, regardless of the robust location protection techniques, by jointly exploiting the visual representation, correlation, and geo-annotation capabilities extracted from the crowd. Compared with existing retrieval-based and model-based location inference techniques, our proposal poses more pernicious threats to location privacy by considering the no-reference-photos situations of crowdsensing. We conduct extensive analyses on the model with four photo datasets and crowdsourcing surveys for geo-annotation. The results indicate that being in a crowd of photos will, unfortunately, increase one's risk to be geo-identified, and highlights that the model can yield a considerable high inference accuracy (48\%~70\%) and serious privacy exposure (over 80\% of users get privacy disclosed) with a small portion of geo-annotated samples. In view of the threats, we further propose an adaptive grouping-based signing model that hides a user's track with the camouflage of a crowd of users. Wherein, ring signature is tailored for crowdsensing to provide indistinguishable while valid identities for every user's submission. We theoretically analyze its adjustable privacy protection capability and develop a prototype to evaluate the effectiveness and performance.},
journal = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
month = sep,
articleno = {142},
numpages = {23},
keywords = {mobile crowdsensing, location privacy, crowdsourcing, crowd wisdom}
}

@article{10.1145/3534574,
author = {Guo, Baoshen and Zuo, Weijian and Wang, Shuai and Lyu, Wenjun and Hong, Zhiqing and Ding, Yi and He, Tian and Zhang, Desheng},
title = {WePos: Weak-supervised Indoor Positioning with Unlabeled WiFi for On-demand Delivery},
year = {2022},
issue_date = {June 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {6},
number = {2},
url = {https://doi.org/10.1145/3534574},
doi = {10.1145/3534574},
abstract = {On-demand delivery is an emerging business in recent years where accurate indoor locations of Gig couriers play an important role in the order dispatch and delivery process. To cater to this need, WiFi-based indoor positioning methods have become an alternative method for on-demand delivery thanks to extensive WiFi deployment in the indoor environment. Existing WiFi-based indoor localization and positioning methods are not suitable for large-scale on-demand delivery scenarios due to high costs (e.g., high labor cost to collect fingerprints) and limited coverage due to limited labeled data. In this work, we explore (i) massive crowdsourced WiFi data collecting from wearable or mobile devices of couriers with little extra effort and (ii) natural manual reports data in the delivery process as two opportunities to perform merchant-level indoor positioning in a weak-supervised manner. Specifically, we proposed WePos, an end-to-end weak-supervised-based merchant-level positioning framework, which consists of the following three parts: (i) a Bidirectional Encoder Representations from Transformers (BERT) based pre-training module to learn latent embeddings of WiFi access points, (ii) a contrastive label self-generate module to produce pseudos for WiFi scanning lists by matching similarity embedding clustering results and couriers' reporting behaviors. (iii) a deep neural network-based classifier to fine-tune the whole training process and conduct online merchant-level position inference. To evaluate the performance of our system, we conduct extensive experiments in both a large-scale public crowdsourcing dataset with over 50 GB of WiFi signal records and a real-world WiFi crowdsourced dataset collected from Eleme, (i.e., one of the largest on-demand delivery platforms in China) in four multi-floor malls in Shanghai. Experimental results show that WePos outperforms state-of-the-art baselines in the merchant-level positioning performance, offer up to 91.4\% in positioning accuracy.},
journal = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
month = jul,
articleno = {54},
numpages = {25},
keywords = {Merchant-level Indoor Positioning, Weak-supervised Learning, WiFi}
}

@article{10.5555/3122009.3176837,
author = {Jagabathula, Srikanth and Subramanian, Lakshminarayanan and Venkataraman, Ashwin},
title = {Identifying unreliable and adversarial workers in crowdsourced labeling tasks},
year = {2017},
issue_date = {January 2017},
publisher = {JMLR.org},
volume = {18},
number = {1},
issn = {1532-4435},
abstract = {We study the problem of identifying unreliable and adversarial workers in crowdsourcing systems where workers (or users) provide labels for tasks (or items). Most existing studies assume that worker responses follow specific probabilistic models; however, recent evidence shows the presence of workers adopting non-random or even malicious strategies. To account for such workers, we suppose that workers comprise a mixture of honest and adversarial workers. Honest workers may be reliable or unreliable, and they provide labels according to an unknown but explicit probabilistic model. Adversaries adopt labeling strategies different from those of honest workers, whether probabilistic or not. We propose two reputation algorithms to identify unreliable honest workers and adversarial workers from only their responses. Our algorithms assume that honest workers are in the majority, and they classify workers with outlier label patterns as adversaries. Theoretically, we show that our algorithms successfully identify unreliable honest workers, workers adopting deterministic strategies, and worst-case sophisticated adversaries who can adopt arbitrary labeling strategies to degrade the accuracy of the inferred task labels. Empirically, we show that filtering out outliers using our algorithms can significantly improve the accuracy of several state-of-the-art label aggregation algorithms in real-world crowdsourcing datasets.},
journal = {J. Mach. Learn. Res.},
month = jan,
pages = {3233–3299},
numpages = {67},
keywords = {reputation, outliers, crowdsourcing, adversary}
}

@article{10.14778/3055540.3055547,
author = {Zheng, Yudian and Li, Guoliang and Li, Yuanbing and Shan, Caihua and Cheng, Reynold},
title = {Truth inference in crowdsourcing: is the problem solved?},
year = {2017},
issue_date = {January 2017},
publisher = {VLDB Endowment},
volume = {10},
number = {5},
issn = {2150-8097},
url = {https://doi.org/10.14778/3055540.3055547},
doi = {10.14778/3055540.3055547},
abstract = {Crowdsourcing has emerged as a novel problem-solving paradigm, which facilitates addressing problems that are hard for computers, e.g., entity resolution and sentiment analysis. However, due to the openness of crowdsourcing, workers may yield low-quality answers, and a redundancy-based method is widely employed, which first assigns each task to multiple workers and then infers the correct answer (called truth) for the task based on the answers of the assigned workers. A fundamental problem in this method is Truth Inference, which decides how to effectively infer the truth. Recently, the database community and data mining community independently study this problem and propose various algorithms. However, these algorithms are not compared extensively under the same framework and it is hard for practitioners to select appropriate algorithms. To alleviate this problem, we provide a detailed survey on 17 existing algorithms and perform a comprehensive evaluation using 5 real datasets. We make all codes and datasets public for future research. Through experiments we find that existing algorithms are not stable across different datasets and there is no algorithm that outperforms others consistently. We believe that the truth inference problem is not fully solved, and identify the limitations of existing algorithms and point out promising research directions.},
journal = {Proc. VLDB Endow.},
month = jan,
pages = {541–552},
numpages = {12}
}

@article{10.1109/TNET.2018.2812785,
author = {Ma, Qian and Gao, Lin and Liu, Ya-Feng and Huang, Jianwei},
title = {Incentivizing Wi-Fi Network Crowdsourcing: A Contract Theoretic Approach},
year = {2018},
issue_date = {June 2018},
publisher = {IEEE Press},
volume = {26},
number = {3},
issn = {1063-6692},
url = {https://doi.org/10.1109/TNET.2018.2812785},
doi = {10.1109/TNET.2018.2812785},
abstract = {Crowdsourced wireless community network enables the individual users to share their private Wi-Fi access points APs with each other, hence it can achieve a large Wi-Fi coverage with a small deployment cost via crowdsourcing. This paper presents a novel contract-based incentive framework to incentivize such a Wi-Fi network crowdsourcing under incomplete information where each user has certain private information such as mobility pattern and Wi-Fi access quality. In the proposed framework, the network operator designs and offers a set of contract items to users, each consisting of a Wi-Fi access price that a user can charge others for accessing his AP and a subscription fee that a user needs to pay the operator for joining the community. Different from the existing contracts in the literature, in our contract model, each user’s best choice depends not only on his private information but also on other user’s choices. This greatly complicates the contract design, as the operator needs to analyze the equilibrium choices of all users, rather than the best choice of each single user. We first derive the feasible contract that guarantees the user’s truthful information disclosure based on the equilibrium analysis of the user choice, and then derive the optimal and feasible contract that yields a maximal profit for the operator. Our analysis shows that a user who provides a higher Wi-Fi access quality is more likely to choose a higher Wi-Fi access price and subscription fee, regardless of the user mobility pattern. Simulation results further show that when increasing the average Wi-Fi access quality of users, the operator can gain more profit, but counter-intuitively offer lower Wi-Fi access prices and subscription fees for users.},
journal = {IEEE/ACM Trans. Netw.},
month = jun,
pages = {1035–1048},
numpages = {14}
}

@article{10.14778/3137765.3137833,
author = {Li, Guoliang},
title = {Human-in-the-loop data integration},
year = {2017},
issue_date = {August 2017},
publisher = {VLDB Endowment},
volume = {10},
number = {12},
issn = {2150-8097},
url = {https://doi.org/10.14778/3137765.3137833},
doi = {10.14778/3137765.3137833},
abstract = {Data integration aims to integrate data in different sources and provide users with a unified view. However, data integration cannot be completely addressed by purely automated methods. We propose a hybrid human-machine data integration framework that harnesses human ability to address this problem, and apply it initially to the problem of entity matching. The framework first uses rule-based algorithms to identify possible matching pairs and then utilizes the crowd to refine these candidate pairs in order to compute actual matching pairs. In the first step, we propose similarity-based rules and knowledge-based rules to obtain some candidate matching pairs, and develop effective algorithms to learn these rules based on some given positive and negative examples. We build a distributed in-memory system DIMA to efficiently apply these rules. In the second step, we propose a selection-inference-refine framework that uses the crowd to verify the candidate pairs. We first select some "beneficial" tasks to ask the crowd and then use transitivity and partial order to infer the answers of unasked tasks based on the crowdsourcing results of the asked tasks. Next we refine the inferred answers with high uncertainty due to the disagreement from the crowd. We develop a crowd-powered database system CDB and deploy it on real crowdsourcing platforms. CDB allows users to utilize a SQL-like language for processing crowd-based queries. Lastly, we provide emerging challenges in human-in-the-loop data integration.},
journal = {Proc. VLDB Endow.},
month = aug,
pages = {2006–2017},
numpages = {12}
}

@article{10.14778/2536360.2536374,
author = {Zhang, Chen Jason and Chen, Lei and Jagadish, H. V. and Cao, Chen Caleb},
title = {Reducing uncertainty of schema matching via crowdsourcing},
year = {2013},
issue_date = {July 2013},
publisher = {VLDB Endowment},
volume = {6},
number = {9},
issn = {2150-8097},
url = {https://doi.org/10.14778/2536360.2536374},
doi = {10.14778/2536360.2536374},
abstract = {Schema matching is a central challenge for data integration systems. Automated tools are often uncertain about schema matchings they suggest, and this uncertainty is inherent since it arises from the inability of the schema to fully capture the semantics of the represented data. Human common sense can often help. Inspired by the popularity and the success of easily accessible crowdsourcing platforms, we explore the use of crowdsourcing to reduce the uncertainty of schema matching.Since it is typical to ask simple questions on crowdsourcing platforms, we assume that each question, namely Correspondence Correctness Question (CCQ), is to ask the crowd to decide whether a given correspondence should exist in the correct matching. We propose frameworks and efficient algorithms to dynamically manage the CCQs, in order to maximize the uncertainty reduction within a limited budget of questions. We develop two novel approaches, namely "Single CCQ" and "Multiple CCQ", which adaptively select, publish and manage the questions. We verified the value of our solutions with simulation and real implementation.},
journal = {Proc. VLDB Endow.},
month = jul,
pages = {757–768},
numpages = {12}
}

@article{10.1145/2897370,
author = {Chen, Chen and Wo\'{z}niak, Pawe\l{} W. and Romanowski, Andrzej and Obaid, Mohammad and Jaworski, Tomasz and Kucharski, Jacek and Grudzie\'{n}, Krzysztof and Zhao, Shengdong and Fjeld, Morten},
title = {Using Crowdsourcing for Scientific Analysis of Industrial Tomographic Images},
year = {2016},
issue_date = {July 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {4},
issn = {2157-6904},
url = {https://doi.org/10.1145/2897370},
doi = {10.1145/2897370},
abstract = {In this article, we present a novel application domain for human computation, specifically for crowdsourcing, which can help in understanding particle-tracking problems. Through an interdisciplinary inquiry, we built a crowdsourcing system designed to detect tracer particles in industrial tomographic images, and applied it to the problem of bulk solid flow in silos. As images from silo-sensing systems cannot be adequately analyzed using the currently available computational methods, human intelligence is required. However, limited availability of experts, as well as their high cost, motivates employing additional nonexperts. We report on the results of a study that assesses the task completion time and accuracy of employing nonexpert workers to process large datasets of images in order to generate data for bulk flow research. We prove the feasibility of this approach by comparing results from a user study with data generated from a computational algorithm. The study shows that the crowd is more scalable and more economical than an automatic solution. The system can help analyze and understand the physics of flow phenomena to better inform the future design of silos, and is generalized enough to be applicable to other domains.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = jul,
articleno = {52},
numpages = {25},
keywords = {tomography, particle tracking, crowdsourcing, Silo}
}

@article{10.1145/3344720,
author = {Crescenzi, Valter and Merialdo, Paolo and Qiu, Disheng},
title = {Hybrid Crowd-Machine Wrapper Inference},
year = {2019},
issue_date = {October 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {13},
number = {5},
issn = {1556-4681},
url = {https://doi.org/10.1145/3344720},
doi = {10.1145/3344720},
abstract = {Wrapper inference deals in generating programs to extract data from Web pages. Several supervised and unsupervised wrapper inference approaches have been proposed in the literature. On one hand, unsupervised approaches produce erratic wrappers: whenever the sources do not satisfy underlying assumptions of the inference algorithm, their accuracy is compromised. On the other hand, supervised approaches produce accurate wrappers, but since they need training data, their scalability is limited. The recent advent of crowdsourcing platforms has opened new opportunities for supervised approaches, as they make possible the production of large amounts of training data with the support of workers recruited online. Nevertheless, involving human workers has monetary costs. We present an original hybrid crowd-machine wrapper inference system that offers the benefits of both approaches exploiting the cooperation of crowd workers and unsupervised algorithms. Based on a principled probabilistic model that estimates the quality of wrappers, humans workers are recruited only when unsupervised wrapper induction algorithms are not able to produce sufficiently accurate solutions.},
journal = {ACM Trans. Knowl. Discov. Data},
month = sep,
articleno = {51},
numpages = {43},
keywords = {wrapper inference, data extraction, Crowdsourcing}
}

@article{10.5555/3455716.3455849,
author = {Ma, Yao and Olshevsky, Alex and Saligrama, Venkatesh and Szepesvari, Csaba},
title = {Gradient descent for sparse rank-one matrix completion for crowd-sourced aggregation of sparsely interacting workers},
year = {2020},
issue_date = {January 2020},
publisher = {JMLR.org},
volume = {21},
number = {1},
issn = {1532-4435},
abstract = {We consider worker skill estimation for the single-coin Dawid-Skene crowdsourcing model. In practice, skill-estimation is challenging because worker assignments are sparse and irregular due to the arbitrary and uncontrolled availability of workers. We formulate skill estimation as a rank-one correlation-matrix completion problem, where the observed components correspond to observed label correlation between workers. We show that the correlation matrix can be successfully recovered and skills are identifiable if and only if the sampling matrix (observed components) does not have a bipartite connected component. We then propose a projected gradient descent scheme and show that skill estimates converge to the desired global optima for such sampling matrices. Our proof is original and the results are surprising in light of the fact that even the weighted rank-one matrix factorization problem is NP-hard in general. Next, we derive sample complexity bounds in terms of spectral properties of the signless Laplacian of the sampling matrix. Our proposed scheme achieves state-of-art performance on a number of real-world datasets.},
journal = {J. Mach. Learn. Res.},
month = jan,
articleno = {133},
numpages = {36},
keywords = {stochastic gradient descent, distributed optimization}
}

@article{10.1145/3476053,
author = {Wang, Liang and Yu, Zhiwen and Yang, Dingqi and Wang, Tian and Wang, En and Guo, Bin and Zhang, Daqing},
title = {Task Execution Quality Maximization for Mobile Crowdsourcing in Geo-Social Networks},
year = {2021},
issue_date = {October 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {5},
number = {CSCW2},
url = {https://doi.org/10.1145/3476053},
doi = {10.1145/3476053},
abstract = {With the rapid development of smart devices and high-quality wireless technologies, mobile crowdsourcing (MCS) has been drawing increasing attention with its great potential in collaboratively completing complicated tasks on a large scale. A key issue toward successful MCS is participant recruitment, where a MCS platform directly recruits suitable crowd participants to execute outsourced tasks by physically traveling to specified locations. Recently, a novel recruitment strategy, namely Word-of-Mouth(WoM)-based MCS, has emerged to effectively improve recruitment effectiveness, by fully exploring users' mobility traces and social relationships on geo-social networks. Against this background, we study in this paper a novel problem, namely Expected Task Execution Quality Maximization (ETEQM) for MCS in geo-social networks, which strives to search a subset of seed users to maximize the expected task execution quality of all recruited participants, under a given incentive budget. To characterize the MCS task propagation process over geo-social networks, we first adopt a propagation tree structure to model the autonomous recruitment between the referrers and the referrals. Based on the model, we then formalize the task execution quality and devise a novel incentive mechanism by harnessing the business strategy of multi-level marketing. We formulate our ETEQM problem as a combinatorial optimization problem, and analyze its NP hardness and high-dimensional characteristics. Based on a cooperative co-evolution framework, we proposed a divide-and-conquer problem-solving approach named ETEQM-CC. We conduct extensive simulation experiments and a case study, verifying the effectiveness of our proposed approach.},
journal = {Proc. ACM Hum.-Comput. Interact.},
month = oct,
articleno = {312},
numpages = {29},
keywords = {cooperative co-evolution, geo-social networks, mobile crowdsourcing, task propagation model}
}

@article{10.1145/3274390,
author = {McInnis, Brian and Leshed, Gilly and Cosley, Dan},
title = {Crafting Policy Discussion Prompts as a Task for Newcomers},
year = {2018},
issue_date = {November 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {2},
number = {CSCW},
url = {https://doi.org/10.1145/3274390},
doi = {10.1145/3274390},
abstract = {Inspired by policy deliberation methods and iterative writing in crowdsourcing, we developed and evaluated a task in which newcomers to an online policy discussion, before entering the discussion, generate prompts that encourage existing commenters to engage with each other. In an experiment with 453 Amazon Mechanical Turk (AMT) crowd workers, we found that newcomers can often craft acceptable prompts, especially when given guidance on prompt-writing and balanced opinions between the comments they synthesize. However, crafting these prompts had little effect on the quality of comments they posted to a simulated discussion forum following the prompt task, as measured by the reasoning and topic coherence of comments. Our results inform best practices and pose questions for the design of discussion systems, both in general and for online policy discussion in particular.},
journal = {Proc. ACM Hum.-Comput. Interact.},
month = nov,
articleno = {121},
numpages = {23},
keywords = {online discussion, newcomer, meta talk, deliberation, crowdsourcing}
}

@article{10.1145/3002172,
author = {Maddalena, Eddy and Mizzaro, Stefano and Scholer, Falk and Turpin, Andrew},
title = {On Crowdsourcing Relevance Magnitudes for Information Retrieval Evaluation},
year = {2017},
issue_date = {July 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {35},
number = {3},
issn = {1046-8188},
url = {https://doi.org/10.1145/3002172},
doi = {10.1145/3002172},
abstract = {Magnitude estimation is a psychophysical scaling technique for the measurement of sensation, where observers assign numbers to stimuli in response to their perceived intensity. We investigate the use of magnitude estimation for judging the relevance of documents for information retrieval evaluation, carrying out a large-scale user study across 18 TREC topics and collecting over 50,000 magnitude estimation judgments using crowdsourcing. Our analysis shows that magnitude estimation judgments can be reliably collected using crowdsourcing, are competitive in terms of assessor cost, and are, on average, rank-aligned with ordinal judgments made by expert relevance assessors.We explore the application of magnitude estimation for IR evaluation, calibrating two gain-based effectiveness metrics, nDCG and ERR, directly from user-reported perceptions of relevance. A comparison of TREC system effectiveness rankings based on binary, ordinal, and magnitude estimation relevance shows substantial variation; in particular, the top systems ranked using magnitude estimation and ordinal judgments differ substantially. Analysis of the magnitude estimation scores shows that this effect is due in part to varying perceptions of relevance: different users have different perceptions of the impact of relative differences in document relevance. These results have direct implications for IR evaluation, suggesting that current assumptions about a single view of relevance being sufficient to represent a population of users are unlikely to hold.},
journal = {ACM Trans. Inf. Syst.},
month = jan,
articleno = {19},
numpages = {32},
keywords = {relevance assessments, relevance, evaluation, Magnitude estimation}
}

@article{10.1145/3369819,
author = {Wan, Zhiyuan and Bao, Lingfeng and Gao, Debin and Toch, Eran and Xia, Xin and Mendel, Tamir and Lo, David},
title = {AppMoD: Helping Older Adults Manage Mobile Security with Online Social Help},
year = {2020},
issue_date = {December 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {3},
number = {4},
url = {https://doi.org/10.1145/3369819},
doi = {10.1145/3369819},
abstract = {The rapid adoption of Smartphone devices has caused increasing security and privacy risks and breaches. Catching up with ever-evolving contemporary smartphone technology challenges leads older adults (aged 50+) to reduce or to abandon their use of mobile technology. To tackle this problem, we present AppMoD, a community-based approach that allows delegation of security and privacy decisions a trusted social connection, such as a family member or a close friend. The trusted social connection can assist in the appropriate decision or make it on behalf of the user. We implement the approach as an Android app and describe the results of three user studies (n=50 altogether), in which pairs of older adults and family members used the app in a controlled experiment. Using app anomalies as an ongoing case study, we show how delegation improves the accuracy of decisions made by older adults. Also, we show how combining decision-delegation with crowdsourcing can enhance the advice given and improve the decision-making process. Our results suggest that a community-based approach can improve the state of mobile security and privacy.},
journal = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
month = sep,
articleno = {154},
numpages = {22},
keywords = {security, older adults, decision delegation, Mobile smartphones}
}

@article{10.1007/s00778-021-00671-8,
author = {Balayn, Agathe and Lofi, Christoph and Houben, Geert-Jan},
title = {Managing bias and unfairness in data for decision support: a survey of machine learning and data engineering approaches to identify and mitigate bias and unfairness within data management and analytics systems},
year = {2021},
issue_date = {Sep 2021},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {30},
number = {5},
issn = {1066-8888},
url = {https://doi.org/10.1007/s00778-021-00671-8},
doi = {10.1007/s00778-021-00671-8},
abstract = {The increasing use of data-driven decision support systems in industry and governments is accompanied by the discovery of a plethora of bias and unfairness issues in the outputs of these systems. Multiple computer science communities, and especially machine learning, have started to tackle this problem, often developing algorithmic solutions to mitigate biases to obtain fairer outputs. However, one of the core underlying causes for unfairness is bias in training data which is not fully covered by such approaches. Especially, bias in data is not yet a central topic in data engineering and management research. We survey research on bias and unfairness in several computer science domains, distinguishing between data management publications and other domains. This covers the creation of fairness metrics, fairness identification, and mitigation methods, software engineering approaches and biases in crowdsourcing activities. We identify relevant research gaps and show which data management activities could be repurposed to handle biases and which ones might reinforce such biases. In the second part, we argue for a novel data-centered approach overcoming the limitations of current algorithmic-centered methods. This approach focuses on eliciting and enforcing fairness requirements and constraints on data that systems are trained, validated, and used on. We argue for the need to extend database management systems to handle such constraints and mitigation methods. We discuss the associated future research directions regarding algorithms, formalization, modelling, users, and systems.},
journal = {The VLDB Journal},
month = may,
pages = {739–768},
numpages = {30},
keywords = {Bias constraints for DBMS, Bias mitigation, Data curation, Decision support systems, Bias and unfairness}
}

@article{10.14778/3137765.3137821,
author = {Jonathan, Christopher and Mokbel, Mohamed F.},
title = {A demonstration of stella: a crowdsourcing-based geotagging framework},
year = {2017},
issue_date = {August 2017},
publisher = {VLDB Endowment},
volume = {10},
number = {12},
issn = {2150-8097},
url = {https://doi.org/10.14778/3137765.3137821},
doi = {10.14778/3137765.3137821},
abstract = {This paper demonstrates Stella; an efficient crowdsourcing-based geotagging framework for any types of objects. In this demonstration, we showcase the effectiveness of Stella in geotagging images via two different scenarios: (1) we provide a graphical interface to show the process of a geotagging process that have been done by using Amazon Mechanical Turk, (2) we seek help from the conference attendees to propose an image to be geotagged or to help us geotag an image by using our application during the demonstration period. At the end of the demonstration period, we will show the geotagging result.},
journal = {Proc. VLDB Endow.},
month = aug,
pages = {1969–1972},
numpages = {4}
}

@article{10.1145/2903138,
author = {Rodr\'{\i}guez, Carlos and Daniel, Florian and Casati, Fabio},
title = {Mining and Quality Assessment of Mashup Model Patterns with the Crowd: A Feasibility Study},
year = {2016},
issue_date = {August 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {16},
number = {3},
issn = {1533-5399},
url = {https://doi.org/10.1145/2903138},
doi = {10.1145/2903138},
abstract = {Pattern mining, that is, the automated discovery of patterns from data, is a mathematically complex and computationally demanding problem that is generally not manageable by humans. In this article, we focus on small datasets and study whether it is possible to mine patterns with the help of the crowd by means of a set of controlled experiments on a common crowdsourcing platform. We specifically concentrate on mining model patterns from a dataset of real mashup models taken from Yahoo! Pipes and cover the entire pattern mining process, including pattern identification and quality assessment. The results of our experiments show that a sensible design of crowdsourcing tasks indeed may enable the crowd to identify patterns from small datasets (40 models). The results, however, also show that the design of tasks for the assessment of the quality of patterns to decide which patterns to retain for further processing and use is much harder (our experiments fail to elicit assessments from the crowd that are similar to those by an expert). The problem is relevant in general to model-driven development (e.g., UML, business processes, scientific workflows), in that reusable model patterns encode valuable modeling and domain knowledge, such as best practices, organizational conventions, or technical choices, that modelers can benefit from when designing their own models.},
journal = {ACM Trans. Internet Technol.},
month = jun,
articleno = {17},
numpages = {27},
keywords = {pattern mining, mashups, crowdsourcing, Model patterns}
}

@article{10.14778/3402755.3402809,
author = {Doan, AnHai and Franklin, Michael J. and Kossmann, Donald and Kraska, Tim},
title = {Crowdsourcing applications and platforms: a data management perspective},
year = {2011},
issue_date = {August 2011},
publisher = {VLDB Endowment},
volume = {4},
number = {12},
issn = {2150-8097},
url = {https://doi.org/10.14778/3402755.3402809},
doi = {10.14778/3402755.3402809},
abstract = {Over the past decade, crowdsourcing has emerged as a major problem-solving and data-gathering paradigm on the World-Wide Web. Well-known examples of crowdsourcing include Wikipedia, Linux, Yahoo! Answers, YouTube, Mechanical Turk-based applications, and much effort is being directed toward developing many more.},
journal = {Proc. VLDB Endow.},
month = aug,
pages = {1508–1509},
numpages = {2}
}

@article{10.14778/2367502.2367537,
author = {Morishima, Atsuyuki and Shinagawa, Norihide and Mitsuishi, Tomomi and Aoki, Hideto and Fukusumi, Shun},
title = {CyLog/Crowd4U: a declarative platform for complex data-centric crowdsourcing},
year = {2012},
issue_date = {August 2012},
publisher = {VLDB Endowment},
volume = {5},
number = {12},
issn = {2150-8097},
url = {https://doi.org/10.14778/2367502.2367537},
doi = {10.14778/2367502.2367537},
abstract = {This demo presents a principled approach to the problems of data-centric human/machine computations with Crowd4U, a crowdsourcing platform equipped with a suite of tools for rapid development of crowdsourcing applications. Using the demo, we show that declarative database abstraction can be used as a powerful tool to design, implement, and analyze data-centric crowdsourcing applications. The power of Crowd4U comes from CyLog, a database abstraction that handles complex data-centric human/machine computations. CyLog is a Datalog-like language that incorporates a principled feedback system for humans at the language level so that the semantics of the computation not closed in machines can be defined based on the game theory. We believe that the demo clearly shows that database abstraction can be a promising basis for designing complex data-centric applications requiring human/machine computations.},
journal = {Proc. VLDB Endow.},
month = aug,
pages = {1918–1921},
numpages = {4}
}

@article{10.5555/2600623.2600646,
author = {Riley, Derek and Nellen, Gwen and Barrera, Raul and Quevedo, Jesus},
title = {Crowdsourcing traffic simulation to improve signal timing},
year = {2014},
issue_date = {May 2014},
publisher = {Consortium for Computing Sciences in Colleges},
address = {Evansville, IN, USA},
volume = {29},
number = {5},
issn = {1937-4771},
abstract = {Inefficiency in automobile traffic light timing contributes to wasted time of travelers and wasted energy, but improving the timing of traffic lights is a difficult problem due to the inherent complexity of the problem. In this work, we present our crowdsourcing tool that allows Android mobile phone users to play a game challenging them to find optimal light timing for a simulated traffic intersection. To succeed at this game, players must configure the traffic light signals to optimize the throughput of vehicles. Scores and configurations of players are stored in a remote database, which is used to populate a high score list. The game uses crowdsourcing by allowing players to load configurations from the high score list to try to optimize them. We also present a case study of volunteers who tested our prototype.},
journal = {J. Comput. Sci. Coll.},
month = may,
pages = {112–118},
numpages = {7}
}

@article{10.14778/2921558.2921559,
author = {Zhang, Xiaohang and Li, Guoliang and Feng, Jianhua},
title = {Crowdsourced top-k algorithms: an experimental evaluation},
year = {2016},
issue_date = {April 2016},
publisher = {VLDB Endowment},
volume = {9},
number = {8},
issn = {2150-8097},
url = {https://doi.org/10.14778/2921558.2921559},
doi = {10.14778/2921558.2921559},
abstract = {Crowdsourced top-k computation has attracted significant attention recently, thanks to emerging crowdsourcing platforms, e.g., Amazon Mechanical Turk and CrowdFlower. Crowdsourced top-k algorithms ask the crowd to compare the objects and infer the top-k objects based on the crowdsourced comparison results. The crowd may return incorrect answers, but traditional top-k algorithms cannot tolerate the errors from the crowd. To address this problem, the database and machine-learning communities have independently studied the crowdsourced top-k problem. The database community proposes the heuristic-based solutions while the machine-learning community proposes the learningbased methods (e.g., maximum likelihood estimation). However, these two types of techniques have not been compared systematically under the same experimental framework. Thus it is rather difficult for a practitioner to decide which algorithm should be adopted. Furthermore, the experimental evaluation of existing studies has several weaknesses. Some methods assume the crowd returns high-quality results and some algorithms are only tested on simulated experiments. To alleviate these limitations, in this paper we present a comprehensive comparison of crowdsourced top-k algorithms. Using various synthetic and real datasets, we evaluate each algorithm in terms of result quality and efficiency on real crowdsourcing platforms. We reveal the characteristics of different techniques and provide guidelines on selecting appropriate algorithms for various scenarios.},
journal = {Proc. VLDB Endow.},
month = apr,
pages = {612–623},
numpages = {12}
}

@article{10.14778/2824032.2824122,
author = {Haas, Daniel and Krishnan, Sanjay and Wang, Jiannan and Franklin, Michael J. and Wu, Eugene},
title = {Wisteria: nurturing scalable data cleaning infrastructure},
year = {2015},
issue_date = {August 2015},
publisher = {VLDB Endowment},
volume = {8},
number = {12},
issn = {2150-8097},
url = {https://doi.org/10.14778/2824032.2824122},
doi = {10.14778/2824032.2824122},
abstract = {Analysts report spending upwards of 80\% of their time on problems in data cleaning. The data cleaning process is inherently iterative, with evolving cleaning workflows that start with basic exploratory data analysis on small samples of dirty data, then refine analysis with more sophisticated/expensive cleaning operators (e.g., crowdsourcing), and finally apply the insights to a full dataset. While an analyst often knows at a logical level what operations need to be done, they often have to manage a large search space of physical operators and parameters. We present Wisteria, a system designed to support the iterative development and optimization of data cleaning workflows, especially ones that utilize the crowd. Wisteria separates logical operations from physical implementations, and driven by analyst feedback, suggests optimizations and/or replacements to the analyst's choice of physical implementation. We highlight research challenges in sampling, in-flight operator replacement, and crowdsourcing. We overview the system architecture and these techniques, then provide a demonstration designed to showcase how Wisteria can improve iterative data analysis and cleaning. The code is available at: http://www.sampleclean.org.},
journal = {Proc. VLDB Endow.},
month = aug,
pages = {2004–2007},
numpages = {4}
}

@article{10.1145/3078852,
author = {Feyisetan, Oluwaseyi and Simperl, Elena},
title = {Social Incentives in Paid Collaborative Crowdsourcing},
year = {2017},
issue_date = {November 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {8},
number = {6},
issn = {2157-6904},
url = {https://doi.org/10.1145/3078852},
doi = {10.1145/3078852},
abstract = {Paid microtask crowdsourcing has traditionally been approached as an individual activity, with units of work created and completed independently by the members of the crowd. Other forms of crowdsourcing have, however, embraced more varied models, which allow for a greater level of participant interaction and collaboration. This article studies the feasibility and uptake of such an approach in the context of paid microtasks. Specifically, we compare engagement, task output, and task accuracy in a paired-worker model with the traditional, single-worker version. Our experiments indicate that collaboration leads to better accuracy and more output, which, in turn, translates into lower costs. We then explore the role of the social flow and social pressure generated by collaborating partners as sources of incentives for improved performance. We utilise a Bayesian method in conjunction with interface interaction behaviours to detect when one of the workers in a pair tries to exit the task. Upon this realisation, the other worker is presented with the opportunity to contact the exiting partner to stay: either for personal financial reasons (i.e., they have not completed enough tasks to qualify for a payment) or for fun (i.e., they are enjoying the task). The findings reveal that: (1) these socially motivated incentives can act as furtherance mechanisms to help workers attain and exceed their task requirements and produce better results than baseline collaborations; (2) microtask crowd workers are empathic (as opposed to selfish) agents, willing to go the extra mile to help their partners get paid; and, (3) social furtherance incentives create a win-win scenario for the requester and for the workers by helping more workers get paid by re-engaging them before they drop out.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = jul,
articleno = {73},
numpages = {31},
keywords = {social pressure, social flow, incentives, Crowdsourcing}
}

@article{10.1145/2661229.2661287,
author = {Zhu, Jun-Yan and Agarwala, Aseem and Efros, Alexei A. and Shechtman, Eli and Wang, Jue},
title = {Mirror mirror: crowdsourcing better portraits},
year = {2014},
issue_date = {November 2014},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {33},
number = {6},
issn = {0730-0301},
url = {https://doi.org/10.1145/2661229.2661287},
doi = {10.1145/2661229.2661287},
abstract = {We describe a method for providing feedback on portrait expressions, and for selecting the most attractive expressions from large video/photo collections. We capture a video of a subject's face while they are engaged in a task designed to elicit a range of positive emotions. We then use crowdsourcing to score the captured expressions for their attractiveness. We use these scores to train a model that can automatically predict attractiveness of different expressions of a given person. We also train a cross-subject model that evaluates portrait attractiveness of novel subjects and show how it can be used to automatically mine attractive photos from personal photo collections. Furthermore, we show how, with a little bit ($5-worth) of extra crowdsourcing, we can substantially improve the cross-subject model by "fine-tuning" it to a new individual using active learning. Finally, we demonstrate a training app that helps people learn how to mimic their best expressions.},
journal = {ACM Trans. Graph.},
month = nov,
articleno = {234},
numpages = {12},
keywords = {portraits, crowdsourcing, aesthetic visual quality assessment}
}

@article{10.14778/2732939.2732942,
author = {Parameswaran, Aditya and Boyd, Stephen and Garcia-Molina, Hector and Gupta, Ashish and Polyzotis, Neoklis and Widom, Jennifer},
title = {Optimal crowd-powered rating and filtering algorithms},
year = {2014},
issue_date = {May 2014},
publisher = {VLDB Endowment},
volume = {7},
number = {9},
issn = {2150-8097},
url = {https://doi.org/10.14778/2732939.2732942},
doi = {10.14778/2732939.2732942},
abstract = {We focus on crowd-powered filtering, i.e., filtering a large set of items using humans. Filtering is one of the most commonly used building blocks in crowdsourcing applications and systems. While solutions for crowd-powered filtering exist, they make a range of implicit assumptions and restrictions, ultimately rendering them not powerful enough for real-world applications. We describe two approaches to discard these implicit assumptions and restrictions: one, that carefully generalizes prior work, leading to an optimal, but often-times intractable solution, and another, that provides a novel way of reasoning about filtering strategies, leading to a sometimes suboptimal, but efficiently computable solution (that is asymptotically close to optimal). We demonstrate that our techniques lead to significant reductions in error of up to 30\% for fixed cost over prior work in a novel crowdsourcing application: peer evaluation in online courses.},
journal = {Proc. VLDB Endow.},
month = may,
pages = {685–696},
numpages = {12}
}

@article{10.14778/3007263.3007323,
author = {Amer-Yahia, Sihem and Roy, Senjuti Basu},
title = {Human factors in crowdsourcing},
year = {2016},
issue_date = {September 2016},
publisher = {VLDB Endowment},
volume = {9},
number = {13},
issn = {2150-8097},
url = {https://doi.org/10.14778/3007263.3007323},
doi = {10.14778/3007263.3007323},
abstract = {Today, crowdsourcing is used to "taskify" any job ranging from simple receipt transcription to collaborative editing, fan-subbing, citizen science, and citizen journalism. The crowd is typically volatile, its arrival and departure asynchronous, and its levels of attention and accuracy diverse. Tasks vary in complexity and may necessitate the participation of workers with varying degrees of expertise. Sometimes, workers need to collaborate explicitly and build on each other's contributions to complete a single task. For example, in disaster reporting, CrowdMap allows geographically closed people with diverse and complementary skills, to work together to report details about the course of a typhoon or the aftermath of an earthquake.This uber-ization of human labor requires the understanding of workers motivation in completing a task, their ability to work together in collaborative tasks, as well as, helping workers find relevant tasks. For over 40 years, organization studies have thoroughly examined human factors that affect workers in physical workplaces. More recently, computer scientists have developed algorithms that verify and leverage those findings in a virtual marketplace, in this case, a crowdsourcing platform.The goal of this tutorial is to review those two areas and discuss how their combination may improve workers' experience, task throughput and outcome quality for both micro-tasks and collaborative tasks. We will start with a coverage of motivation theory, team formation, and learning worker profiles. We will then address open research questions that result from this review.},
journal = {Proc. VLDB Endow.},
month = sep,
pages = {1615–1618},
numpages = {4}
}

@article{10.1145/3134729,
author = {Sheppard, S. Andrew and Turner, Julian and Thebault-Spieker, Jacob and Zhu, Haiyi and Terveen, Loren},
title = {Never Too Old, Cold or Dry to Watch the Sky: A Survival Analysis of Citizen Science Volunteerism},
year = {2017},
issue_date = {November 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {1},
number = {CSCW},
url = {https://doi.org/10.1145/3134729},
doi = {10.1145/3134729},
abstract = {CoCoRaHS is a multinational citizen science project for observing precipitation. Like many citizen science projects, volunteer retention is a key measure of engagement and data quality. Through survival analysis, we found that participant age (self-reported at account creation) is a significant predictor of retention. Compared to all other age groups, participants aged 60-70 are much more likely to sign up for CoCoRaHS, and to remain active for several years. We also measured the influence of task difficulty and the relative frequency of rain, finding small but statistically significant and counterintuitive effects. Finally, we confirmed previous work showing that participation levels within the first month are highly predictive of eventual retention. We conclude with implications for observational citizen science projects and crowdsourcing research in general.},
journal = {Proc. ACM Hum.-Comput. Interact.},
month = dec,
articleno = {94},
numpages = {21},
keywords = {volunteer retention, crowdsourcing, citizen science}
}

@article{10.14778/3389133.3389139,
author = {Li, Yuanbing and Wu, Xian and Jin, Yifei and Li, Jian and Li, Guoliang},
title = {Efficient algorithms for crowd-aided categorization},
year = {2020},
issue_date = {April 2020},
publisher = {VLDB Endowment},
volume = {13},
number = {8},
issn = {2150-8097},
url = {https://doi.org/10.14778/3389133.3389139},
doi = {10.14778/3389133.3389139},
abstract = {We study the problem of utilizing human intelligence to categorize a large number of objects. In this problem, given a category hierarchy and a set of objects, we can ask humans to check whether an object belongs to a category, and our goal is to find the most cost-effective strategy to locate the appropriate category in the hierarchy for each object, such that the cost (i.e., the number of questions to ask humans) is minimized. There are many important applications of this problem, including image classification and product categorization. We develop an online framework, in which category distribution is gradually learned and thus an effective order of questions are adaptively determined. We prove that even if the true category distribution is known in advance, the problem is computationally intractable. We develop an approximation algorithm, and prove that it achieves an approximation factor of 2. We also show that there is a fully polynomial time approximation scheme for the problem. Furthermore, we propose an online strategy which achieves nearly the same performance guarantee as the offline optimal strategy, even if there is no knowledge about category distribution beforehand. Experiments on a real crowdsourcing platform demonstrate the effectiveness of our method.},
journal = {Proc. VLDB Endow.},
month = apr,
pages = {1221–1233},
numpages = {13}
}

@article{10.1145/3432726,
author = {Ren, Pengjie and Chen, Zhumin and Ren, Zhaochun and Kanoulas, Evangelos and Monz, Christof and De Rijke, Maarten},
title = {Conversations with Search Engines: SERP-based Conversational Response Generation},
year = {2021},
issue_date = {October 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {39},
number = {4},
issn = {1046-8188},
url = {https://doi.org/10.1145/3432726},
doi = {10.1145/3432726},
abstract = {In this article, we address the problem of answering complex information needs by conducting conversations with search engines, in the sense that users can express their queries in natural language and directly receive the information they need from a short system response in a conversational manner. Recently, there have been some attempts towards a similar goal, e.g., studies on Conversational Agents (CAs) and Conversational Search (CS). However, they either do not address complex information needs in search scenarios or they are limited to the development of conceptual frameworks and/or laboratory-based user studies.We pursue two goals in this article: (1)the creation of a suitable dataset, the Search as a Conversation (SaaC) dataset, for the development of pipelines for conversations with search engines, and(2)the development of a state-of-the-art pipeline for conversations with search engines, Conversations with Search Engines (CaSE), using this dataset.
 SaaC is built based on a multi-turn conversational search dataset, where we further employ workers from a crowdsourcing platform to summarize each relevant passage into a short, conversational response. CaSE enhances the state-of-the-art by introducing a supporting token identification module and a prior-aware pointer generator, which enables us to generate more accurate responses.We carry out experiments to show that CaSE is able to outperform strong baselines. We also conduct extensive analyses on the SaaC dataset to show where there is room for further improvement beyond CaSE. Finally, we release the SaaC dataset and the code for CaSE and all models used for comparison to facilitate future research on this topic.},
journal = {ACM Trans. Inf. Syst.},
month = aug,
articleno = {47},
numpages = {29},
keywords = {neural model, dataset, search engine, Conversational modeling}
}

@article{10.1145/3274448,
author = {Wang, Qianru and Guo, Bin and Liu, Yan and Han, Qi and Xin, Tong and Yu, Zhiwen},
title = {CrowdNavi: Last-mile Outdoor Navigation for Pedestrians Using Mobile Crowdsensing},
year = {2018},
issue_date = {November 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {2},
number = {CSCW},
url = {https://doi.org/10.1145/3274448},
doi = {10.1145/3274448},
abstract = {Navigation services using digital maps make people's travel much easier. However, these services often fail to provide specific routes to those destinations that lack micro data in digital maps, such as a small laundry store in a shopping area. In this paper, we propose CrowdNavi, a last mile navigation service in outdoor environments using crowdsourcing based on the guider-follower model. First, we collect trajectories of guiders and images of reference objects along trajectories. To guide followers by reference objects along the route, we design a Semantic Crowd Navigation model to generate fine-grained maps by integrating guiders' data. Second, we design two score functions to fulfill two main requirements and plan hints. Last, we provide context-aware navigation for followers based on the fine-grained map and detect deviation in real-time. Real world experiments conducted in three different areas show that our proposed system in combination with images of reference objects is efficient.},
journal = {Proc. ACM Hum.-Comput. Interact.},
month = nov,
articleno = {179},
numpages = {23},
keywords = {mobile crowdsensing, last mile navigation, fine-grained map generation, context-aware navigation}
}

@article{10.1145/3274432,
author = {Song, Shuyi and Bu, Jiajun and Artmeier, Andreas and Shi, Keyue and Wang, Ye and Yu, Zhi and Wang, Can},
title = {Crowdsourcing-Based Web Accessibility Evaluation with Golden Maximum Likelihood Inference},
year = {2018},
issue_date = {November 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {2},
number = {CSCW},
url = {https://doi.org/10.1145/3274432},
doi = {10.1145/3274432},
abstract = {Web accessibility evaluation examines how well websites comply with accessibility guidelines which help people with disabilities to perceive, navigate and contribute to the Web. This demanding task usually requires manual assessment by experts with many years of training and experience. However, not enough experts are available to carry out the increasing number of evaluation projects while non-experts often have different opinions about the presence of accessibility barriers. Addressing these issues, we introduce a crowdsourcing system with a novel truth inference algorithm to derive reliable and accurate assessments from conflicting opinions of evaluators. Extensive evaluation on 23,901 complex tasks assessed by 50 people with and without disabilities shows that our approach outperforms state of the art approaches. In addition, we conducted surveys to identify frequent barriers that people with disabilities are facing in their daily lives and the difficulty to access Web pages when they encounter these barriers. The frequencies and severities of barriers correlate with their derived importance in our evaluation project.},
journal = {Proc. ACM Hum.-Comput. Interact.},
month = nov,
articleno = {163},
numpages = {21},
keywords = {web accessibility, user experience, evaluation system, disability, crowdsourcing, collaborative work}
}

@article{10.1145/3274447,
author = {Wang, Nai-Ching and Hicks, David and Luther, Kurt},
title = {Exploring Trade-Offs Between Learning and Productivity in Crowdsourced History},
year = {2018},
issue_date = {November 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {2},
number = {CSCW},
url = {https://doi.org/10.1145/3274447},
doi = {10.1145/3274447},
abstract = {Crowdsourcing more complex and creative tasks is seen as a desirable goal for both employers and workers, but these tasks traditionally require domain expertise. Employers can recruit only expert workers, but this approach does not scale well. Alternatively, employers can decompose complex tasks into simpler micro-tasks, but some domains, such as historical analysis, cannot be easily modularized in this way. A third approach is to train workers to learn the domain expertise. This approach offers clear benefits to workers, but is perceived as costly or infeasible for employers. In this paper, we explore the trade-offs between learning and productivity in training crowd workers to analyze historical documents. We compare CrowdSCIM, a novel approach that teaches historical thinking skills to crowd workers, with two crowd learning techniques from prior work and a baseline. Our evaluation (n=360) shows that CrowdSCIM allows workers to learn domain expertise while producing work of equal or higher quality versus other conditions, but efficiency is slightly lower.},
journal = {Proc. ACM Hum.-Comput. Interact.},
month = nov,
articleno = {178},
numpages = {24},
keywords = {scim-c, productivity, learning, history, historical thinking, domain expertise, crowdsourcing, analytical thinking}
}

@article{10.1145/3274374,
author = {Li, Tianyi and Luther, Kurt and North, Chris},
title = {CrowdIA: Solving Mysteries with Crowdsourced Sensemaking},
year = {2018},
issue_date = {November 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {2},
number = {CSCW},
url = {https://doi.org/10.1145/3274374},
doi = {10.1145/3274374},
abstract = {The increasing volume of text data is challenging the cognitive capabilities of expert analysts. Machine learning and crowdsourcing present new opportunities for large-scale sensemaking, but we must overcome the challenge of modeling the overall process so that many distributed agents can contribute to suitable components asynchronously and meaningfully. In this paper, we explore how to crowdsource the sensemaking process via a pipeline of modularized steps connected by clearly defined inputs and outputs. Our pipeline restructures and partitions information into "context slices" for individual workers. We implemented CrowdIA, a software platform to enable unsupervised crowd sensemaking using our pipeline. With CrowdIA, crowds successfully solved two mysteries, and were one step away from solving the third. The crowd's intermediate results revealed their reasoning process and provided evidence that justifies their conclusions. We suggest broader possibilities to optimize each component, as well as to evaluate and refine previous intermediate analyses to improve the final result.},
journal = {Proc. ACM Hum.-Comput. Interact.},
month = nov,
articleno = {105},
numpages = {29},
keywords = {text analytics, sensemaking, mysteries, investigation, intelligence analysis, crowdsourcing}
}

@article{10.1145/2939376,
author = {Do, Ngoc and Zhao, Ye and Hsu, Cheng-Hsin and Venkatasubramanian, Nalini},
title = {Crowdsourced Mobile Data Transfer with Delay Bound},
year = {2016},
issue_date = {December 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {16},
number = {4},
issn = {1533-5399},
url = {https://doi.org/10.1145/2939376},
doi = {10.1145/2939376},
abstract = {In this article, we design a crowdsourcing system, CrowdMAC, where mobile devices form a local community or marketplace to share network access and transfer data for each other. CrowdMAC enables (i) mobile clients to select and exploit multiple mobile hotspots in its vicinity for data transfer and (ii) mobile hotspots to open their cellular connectivity to admit/serve delay-bounded requests from mobile users for a fee. The evaluations of CrowdMAC indicate that (i) mobile clients can tune preferred trade-offs between cost and delay through a control knob, (ii) mobile hotspots comply with all delay bounds, and (iii) the system ensures stable and efficient transfer.},
journal = {ACM Trans. Internet Technol.},
month = dec,
articleno = {28},
numpages = {29},
keywords = {network optimization, crowdsourcing, cellular networks, P2P wireless networks, Internet access sharing}
}

@article{10.1145/3359282,
author = {Chen, Chunyang and Feng, Sidong and Xing, Zhenchang and Liu, Linda and Zhao, Shengdong and Wang, Jinshui},
title = {Gallery D.C.: Design Search and Knowledge Discovery through Auto-created GUI Component Gallery},
year = {2019},
issue_date = {November 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {3},
number = {CSCW},
url = {https://doi.org/10.1145/3359282},
doi = {10.1145/3359282},
abstract = {Online communities like Dribbble and GraphicBurger allow GUI designers to share their design artwork and learn from each other. These design sharing platforms are important sources for design inspiration, but our survey with GUI designers suggests additional information needs unmet by existing design sharing platforms. First, designers need to see the practical use of certain GUI designs in real applications, rather than just artworks. Second, designers want to see not only the overall designs but also the detailed design of the GUI components. Third, designers need advanced GUI design search abilities (e.g., multi-facets search) and knowledge discovery support (e.g., demographic investigation, cross-company design comparison). This paper presents Gallery D.C. http://mui-collection.herokuapp.com/, a gallery of GUI design components that harness GUI designs crawled from millions of real-world applications using reverse-engineering and computer vision techniques. Through a process of invisible crowdsourcing, Gallery D.C. supports novel ways for designers to collect, analyze, search, summarize and compare GUI designs on a massive scale. We quantitatively evaluate the quality of Gallery D.C. and demonstrate that Gallery D.C. offers additional support for design sharing and knowledge discovery beyond existing platforms.},
journal = {Proc. ACM Hum.-Comput. Interact.},
month = nov,
articleno = {180},
numpages = {22},
keywords = {object detection, multi-faceted design search, mobile application, gui design, design demographics, design comparison}
}

@article{10.1145/3157736,
author = {Nordio, Alessandro and Tarable, Alberto and Leonardi, Emilio and Marsan, Marco Ajmone},
title = {Selecting the Top-Quality Item Through Crowd Scoring},
year = {2018},
issue_date = {March 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {3},
number = {1},
issn = {2376-3639},
url = {https://doi.org/10.1145/3157736},
doi = {10.1145/3157736},
abstract = {We investigate crowdsourcing algorithms for finding the top-quality item within a large collection of objects with unknown intrinsic quality values. This is an important problem with many relevant applications, such as in networked recommendation systems. The core of the algorithms is that objects are distributed to crowd workers, who return a noisy and biased evaluation. All received evaluations are then combined to identify the top-quality object. We first present a simple probabilistic model for the system under investigation. Then we devise and study a class of efficient adaptive algorithms to assign in an effective way objects to workers. We compare the performance of several algorithms, which correspond to different choices of the design parameters/metrics. In the simulations, we show that some of the algorithms achieve near optimal performance for a suitable setting of the system parameters.},
journal = {ACM Trans. Model. Perform. Eval. Comput. Syst.},
month = feb,
articleno = {2},
numpages = {26},
keywords = {resource allocation, recommendation systems, Crowd scoring}
}

@article{10.1145/3415164,
author = {Bhuiyan, Md Momen and Zhang, Amy X. and Sehat, Connie Moon and Mitra, Tanushree},
title = {Investigating Differences in Crowdsourced News Credibility Assessment: Raters, Tasks, and Expert Criteria},
year = {2020},
issue_date = {October 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {4},
number = {CSCW2},
url = {https://doi.org/10.1145/3415164},
doi = {10.1145/3415164},
abstract = {Misinformation about critical issues such as climate change and vaccine safety is oftentimes amplified on online social and search platforms. The crowdsourcing of content credibility assessment by laypeople has been proposed as one strategy to combat misinformation by attempting to replicate the assessments of experts at scale. In this work, we investigate news credibility assessments by crowds versus experts to understand when and how ratings between them differ. We gather a dataset of over 4,000 credibility assessments taken from 2 crowd groups---journalism students and Upwork workers---as well as 2 expert groups---journalists and scientists---on a varied set of 50 news articles related to climate science, a topic with widespread disconnect between public opinion and expert consensus. Examining the ratings, we find differences in performance due to the makeup of the crowd, such as rater demographics and political leaning, as well as the scope of the tasks that the crowd is assigned to rate, such as the genre of the article and partisanship of the publication. Finally, we find differences between expert assessments due to differing expert criteria that journalism versus science experts use---differences that may contribute to crowd discrepancies, but that also suggest a way to reduce the gap by designing crowd tasks tailored to specific expert criteria. From these findings, we outline future research directions to better design crowd processes that are tailored to specific crowds and types of content.},
journal = {Proc. ACM Hum.-Comput. Interact.},
month = oct,
articleno = {93},
numpages = {26},
keywords = {news, misinformation, expert, crowdsourcing, credibility}
}

@article{10.1145/3046790,
author = {Ahmetovic, Dragan and Manduchi, Roberto and Coughlan, James M. and Mascetti, Sergio},
title = {Mind Your Crossings: Mining GIS Imagery for Crosswalk Localization},
year = {2017},
issue_date = {December 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {9},
number = {4},
issn = {1936-7228},
url = {https://doi.org/10.1145/3046790},
doi = {10.1145/3046790},
abstract = {For blind travelers, finding crosswalks and remaining within their borders while traversing them is a crucial part of any trip involving street crossings. While standard Orientation \&amp; Mobility (O&amp;M) techniques allow blind travelers to safely negotiate street crossings, additional information about crosswalks and other important features at intersections would be helpful in many situations, resulting in greater safety and/or comfort during independent travel. For instance, in planning a trip a blind pedestrian may wish to be informed of the presence of all marked crossings near a desired route.We have conducted a survey of several O&amp;M experts from the United States and Italy to determine the role that crosswalks play in travel by blind pedestrians. The results show stark differences between survey respondents from the U.S. compared with Italy: the former group emphasized the importance of following standard O&amp;M techniques at all legal crossings (marked or unmarked), while the latter group strongly recommended crossing at marked crossings whenever possible. These contrasting opinions reflect differences in the traffic regulations of the two countries and highlight the diversity of needs that travelers in different regions may have.To address the challenges faced by blind pedestrians in negotiating street crossings, we devised a computer vision--based technique that mines existing spatial image databases for discovery of zebra crosswalks in urban settings. Our algorithm first searches for zebra crosswalks in satellite images; all candidates thus found are validated against spatially registered Google Street View images. This cascaded approach enables fast and reliable discovery and localization of zebra crosswalks in large image datasets. While fully automatic, our algorithm can be improved by a final crowdsourcing validation. To this end, we developed a Pedestrian Crossing Human Validation web service, which supports crowdsourcing, to rule out false positives and identify false negatives.},
journal = {ACM Trans. Access. Comput.},
month = apr,
articleno = {11},
numpages = {25},
keywords = {visual impairments and blindness, satellite and street-level imagery, crowdsourcing, autonomous navigation, Orientation and mobility}
}

@article{10.1145/3301414,
author = {Spicker, Marc and G\"{o}tz-Hahn, Franz and Lindemeier, Thomas and Saupe, Dietmar and Deussen, Oliver},
title = {Quantifying Visual Abstraction Quality for Computer-Generated Illustrations},
year = {2019},
issue_date = {January 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {16},
number = {1},
issn = {1544-3558},
url = {https://doi.org/10.1145/3301414},
doi = {10.1145/3301414},
abstract = {We investigate how the perceived abstraction quality of computer-generated illustrations is related to the number of primitives (points and small lines) used to create them. Since it is difficult to find objective functions that quantify the visual quality of such illustrations, we propose an approach to derive perceptual models from a user study. By gathering comparative data in a crowdsourcing user study and employing a paired comparison model, we can reconstruct absolute quality values. Based on an exemplary study for stippling, we show that it is possible to model the perceived quality of stippled representations based on the properties of an input image. The generalizability of our approach is demonstrated by comparing models for different stippling methods. By showing that our proposed approach also works for small lines, we demonstrate its applicability toward quantifying different representational drawing elements. Our results can be related to Weber--Fechner’s law from psychophysics and indicate a logarithmic relationship between number of rendering primitives in an illustration and the perceived abstraction quality thereof.},
journal = {ACM Trans. Appl. Percept.},
month = feb,
articleno = {5},
numpages = {20},
keywords = {user study, stippling, perception, non-photorealistic rendering, Visual abstraction}
}

@article{10.1145/3359158,
author = {Zhang, Zijian and Singh, Jaspreet and Gadiraju, Ujwal and Anand, Avishek},
title = {Dissonance Between Human and Machine Understanding},
year = {2019},
issue_date = {November 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {3},
number = {CSCW},
url = {https://doi.org/10.1145/3359158},
doi = {10.1145/3359158},
abstract = {Complex machine learning models are deployed in several critical domains including healthcare and autonomous vehicles nowadays, albeit as functional blackboxes. Consequently, there has been a recent surge in interpreting decisions of such complex models in order to explain their actions to humans. Models which correspond to human interpretation of a task are more desirable in certain contexts and can help attribute liability, build trust, expose biases and in turn build better models. It is therefore crucial to understand how and which models conform to human understanding of tasks. In this paper we present a large-scale crowdsourcing study that reveals and quantifies the dissonance between human and machine understanding, through the lens of an image classification task. In particular, we seek to answer the following questions: Which (well performing) complex ML models are closer to humans in their use of features to make accurate predictions? How does task difficulty affect the feature selection capability of machines in comparison to humans? Are humans consistently better at selecting features that make image recognition more accurate? Our findings have important implications on human-machine collaboration, considering that a long term goal in the field of artificial intelligence is to make machines capable of learning and reasoning like humans.},
journal = {Proc. ACM Hum.-Comput. Interact.},
month = nov,
articleno = {56},
numpages = {23},
keywords = {object recognition, neural networks, machines, machine learning models, interpretability, image understanding, humans, human intelligence, dissonance, crowdsourcing}
}

@article{10.1145/3134653,
author = {Aitamurto, Tanja and Saldivar, Jorge},
title = {Motivating Participation in Crowdsourced Policymaking: The Interplay of Epistemic and Interactive Aspects},
year = {2017},
issue_date = {November 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {1},
number = {CSCW},
url = {https://doi.org/10.1145/3134653},
doi = {10.1145/3134653},
abstract = {In this paper, we examine the changes in motivation factors in crowdsourced policymaking. By drawing on longitudinal data from a crowdsourced law reform, we show that people participated because they wanted to improve the law, learn, and solve problems. When crowdsourcing reached a saturation point, the motivation factors weakened and the crowd disengaged. Learning was the only factor that did not weaken. The participants learned while interacting with others, and the more actively the participants commented, the more likely they stayed engaged. Crowdsourced policymaking should thus be designed to support both epistemic and interactive aspects. While the crowd's motives were rooted in self-interest, their knowledge perspective showed common-good orientation, implying that rather than being dichotomous, motivation factors move on a continuum. The design of crowdsourced policymaking should support the dynamic nature of the process and the motivation factors driving it.},
journal = {Proc. ACM Hum.-Comput. Interact.},
month = dec,
articleno = {18},
numpages = {22},
keywords = {policymaking, participatory democracy, motivation factors, democratic innovations, crowdsourcing}
}

@article{10.5555/3013558.3013568,
author = {Ho, Chien-Ju and Slivkins, Aleksandrs and Vaughan, Jennifer Wortman},
title = {Adaptive contract design for crowdsourcing markets: bandit algorithms for repeated principal-agent problems},
year = {2016},
issue_date = {January 2016},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {55},
number = {1},
issn = {1076-9757},
abstract = {Crowdsourcing markets have emerged as a popular platform for matching available workers with tasks to complete. The payment for a particular task is typically set by the task's requester, and may be adjusted based on the quality of the completed work, for example, through the use of "bonus" payments. In this paper, we study the requester's problem of dynamically adjusting quality-contingent payments for tasks. We consider a multi-round version of the well-known principal-agent model, whereby in each round a worker makes a strategic choice of the effort level which is not directly observable by the requester. In particular, our formulation significantly generalizes the budget-free online task pricing problems studied in prior work. We treat this problem as a multi-armed bandit problem, with each "arm" representing a potential contract. To cope with the large (and in fact, infinite) number of arms, we propose a new algorithm, AgnosticZooming, which discretizes the contract space into a finite number of regions, effectively treating each region as a single arm. This discretization is adaptively refined, so that more promising regions of the contract space are eventually discretized more finely. We analyze this algorithm, showing that it achieves regret sublinear in the time horizon and substantially improves over non-adaptive discretization (which is the only competing approach in the literature). Our results advance the state of art on several different topics: the theory of crowdsourcing markets, principal-agent problems, multi-armed bandits, and dynamic pricing.},
journal = {J. Artif. Int. Res.},
month = jan,
pages = {317–359},
numpages = {43}
}

@article{10.1145/3352590,
author = {Zhao, Zhenjie and Ma, Xiaojuan},
title = {ShadowPlay2.5D: A 360-Degree Video Authoring Tool for Immersive Appreciation of Classical Chinese Poetry},
year = {2020},
issue_date = {February 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {13},
number = {1},
issn = {1556-4673},
url = {https://doi.org/10.1145/3352590},
doi = {10.1145/3352590},
abstract = {An immersive experience brought about by virtual reality can potentially enhance the appreciation of classical Chinese poetry, which is difficult to describe clearly in everyday language or ordinary media. However, making 3-dimensional illustrations for a 360-degree display in virtual reality is usually a labor-intensive and time-consuming procedure and hard to master for non-professional media creators, such as teachers. Motivated by the homology theory of classical Chinese poetry and painting, we propose an image-based approach of building 2.5-dimensional immersive stories to visualize classical Chinese poetry. Specifically, using Chinese shadow play as a metaphor, we have designed and implemented ShadowPlay2.5D, a sketch-based authoring tool to help novices create 360-degree videos of classical Chinese poetry easily. To ensure coverage of the diverse themes in Chinese poetry and preserve the sense of culture, we build a Chinese ink-painting style image repository of essential poetic elements identified via crowdsourcing. To facilitate construction of 2.5-dimensional scenes, we design features that support puppet-like animation, instancing, and camera organization in a 3-dimensional environment. Through two user studies, we show that ShadowPlay2.5D can help novices make a short 360-degree video in about 10--15 minutes, and the 2.5D stylized illustrations created can bring about a better immersive experience for poetry appreciation.},
journal = {J. Comput. Cult. Herit.},
month = feb,
articleno = {5},
numpages = {20},
keywords = {virtual reality, storytelling, pen-based interface, image-based modeling, education, authoring tool, Classical Chinese poetry, 360-degree video, 2.5D animation}
}

@article{10.1145/3375752,
author = {Wu, Fang-Jing and Luo, Tie},
title = {CrowdPrivacy: Publish More Useful Data with Less Privacy Exposure in Crowdsourced Location-Based Services},
year = {2020},
issue_date = {February 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {23},
number = {1},
issn = {2471-2566},
url = {https://doi.org/10.1145/3375752},
doi = {10.1145/3375752},
abstract = {Location-based services (LBSs) typically crowdsource geo-tagged data from mobile users. Collecting more data will generally improve the utility for LBS providers; however, it also leads to more privacy exposure of users’ mobility patterns. Although the tension between data utility and user privacy has been recognized, there lacks a solution that determines how much data to collect—in both spatial and temporal domains—is the “best” for both mobile users and the service provider. This article proposes a strategy toward making an optimal tradeoff such that a user submits data only if her mobility privacy will not be compromised and the data utility of the LBS provider will be sufficiently improved. To this end, we first define and formulate a concept called privacy exposure, which incorporates both the spatial distribution and the temporal transition of a user’s activity points. Second, we define and quantify data utility in terms of spatial repetitions and temporal closeness among data based on an economic principle. Then, we propose a PRivacy-preserving and UTility-Enhancing Crowdsourcing (PRUTEC) algorithm to determine, on behalf of each mobile user, whether a newly sensed piece of data should be submitted to the LBS provider. Our simulation demonstrates that PRUTEC improves the data utility of the service provider with a much less amount of data to collect and reduces privacy exposure for mobile users while collecting useful data continuously.},
journal = {ACM Trans. Priv. Secur.},
month = feb,
articleno = {6},
numpages = {25},
keywords = {smart cities, privacy preservation, participatory sensing, location-based services, crowdsourcing, Cyber-physical systems}
}

@article{10.5555/2750423.2750445,
author = {Zilli, Davide and Parson, Oliver and Merrett, Geoff V. and Rogers, Alex},
title = {A hidden Markov model-based acoustic cicada detector for crowdsourced smartphone biodiversity monitoring},
year = {2014},
issue_date = {September 2014},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {51},
number = {1},
issn = {1076-9757},
abstract = {In recent years, the field of computational sustainability has striven to apply artificial intelligence techniques to solve ecological and environmental problems. In ecology, a key issue for the safeguarding of our planet is the monitoring of biodiversity. Automated acoustic recognition of species aims to provide a cost-effective method for biodiversity monitoring. This is particularly appealing for detecting endangered animals with a distinctive call, such as the New Forest cicada. To this end, we pursue a crowdsourcing approach, whereby the millions of visitors to the New Forest, where this insect was historically found, will help to monitor its presence by means of a smartphone app that can detect its mating call. Existing research in the field of acoustic insect detection has typically focused upon the classification of recordings collected from fixed field microphones. Such approaches segment a lengthy audio recording into individual segments of insect activity, which are independently classified using cepstral coefficients extracted from the recording as features. This paper reports on a contrasting approach, whereby we use crowdsourcing to collect recordings via a smartphone app, and present an immediate feedback to the users as to whether an insect has been found. Our classification approach does not remove silent parts of the recording via segmentation, but instead uses the temporal patterns throughout each recording to classify the insects present. We show that our approach can successfully discriminate between the call of the New Forest cicada and similar insects found in the New Forest, and is robust to common types of environment noise. A large scale trial deployment of our smartphone app collected over 6000 reports of insect activity from over 1000 users. Despite the cicada not having been rediscovered in the New Forest, the effectiveness of this approach was confirmed for both the detection algorithm, which successfully identified the same cicada through the app in countries where the same species is still present, and of the crowdsourcing methodology, which collected a vast number of recordings and involved thousands of contributors.},
journal = {J. Artif. Int. Res.},
month = sep,
pages = {805–827},
numpages = {23}
}

@article{10.5555/2789272.2886801,
author = {Moreno, Pablo G. and Art\'{e}s-Rodr\'{\i}guez, Antonio and Teh, Yee Whye and Perez-Cruz, Fernando},
title = {Bayesian nonparametric crowdsourcing},
year = {2015},
issue_date = {January 2015},
publisher = {JMLR.org},
volume = {16},
number = {1},
issn = {1532-4435},
abstract = {Crowdsourcing has been proven to be an effective and efficient tool to annotate large data-sets. User annotations are often noisy, so methods to combine the annotations to produce reliable estimates of the ground truth are necessary. We claim that considering the existence of clusters of users in this combination step can improve the performance. This is especially important in early stages of crowdsourcing implementations, where the number of annotations is low. At this stage there is not enough information to accurately estimate the bias introduced by each annotator separately, so we have to resort to models that consider the statistical links among them. In addition, finding these clusters is interesting in itself as knowing the behavior of the pool of annotators allows implementing efficient active learning strategies. Based on this, we propose in this paper two new fully unsupervised models based on a Chinese restaurant process (CRP) prior and a hierarchical structure that allows inferring these groups jointly with the ground truth and the properties of the users. Efficient inference algorithms based on Gibbs sampling with auxiliary variables are proposed. Finally, we perform experiments, both on synthetic and real databases, to show the advantages of our models over state-of-the-art algorithms.},
journal = {J. Mach. Learn. Res.},
month = jan,
pages = {1607–1627},
numpages = {21},
keywords = {multiple annotators, hierarchical clustering, Gibbs sampling, Dirichlet process, Bayesian nonparametrics}
}

@article{10.1145/2483669.2483676,
author = {Burrows, Steven and Potthast, Martin and Stein, Benno},
title = {Paraphrase acquisition via crowdsourcing and machine learning},
year = {2013},
issue_date = {June 2013},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {4},
number = {3},
issn = {2157-6904},
url = {https://doi.org/10.1145/2483669.2483676},
doi = {10.1145/2483669.2483676},
abstract = {To paraphrase means to rewrite content while preserving the original meaning. Paraphrasing is important in fields such as text reuse in journalism, anonymizing work, and improving the quality of customer-written reviews. This article contributes to paraphrase acquisition and focuses on two aspects that are not addressed by current research: (1) acquisition via crowdsourcing, and (2) acquisition of passage-level samples. The challenge of the first aspect is automatic quality assurance; without such a means the crowdsourcing paradigm is not effective, and without crowdsourcing the creation of test corpora is unacceptably expensive for realistic order of magnitudes. The second aspect addresses the deficit that most of the previous work in generating and evaluating paraphrases has been conducted using sentence-level paraphrases or shorter; these short-sample analyses are limited in terms of application to plagiarism detection, for example. We present the Webis Crowd Paraphrase Corpus 2011 (Webis-CPC-11), which recently formed part of the PAN 2010 international plagiarism detection competition. This corpus comprises passage-level paraphrases with 4067 positive samples and 3792 negative samples that failed our criteria, using Amazon's Mechanical Turk for crowdsourcing. In this article, we review the lessons learned at PAN 2010, and explain in detail the method used to construct the corpus. The empirical contributions include machine learning experiments to explore if passage-level paraphrases can be identified in a two-class classification problem using paraphrase similarity features, and we find that a k-nearest-neighbor classifier can correctly distinguish between paraphrased and nonparaphrased samples with 0.980 precision at 0.523 recall. This result implies that just under half of our samples must be discarded (remaining 0.477 fraction), but our cost analysis shows that the automation we introduce results in a 18\% financial saving and over 100 hours of time returned to the researchers when repeating a similar corpus design. On the other hand, when building an unrelated corpus requiring, say, 25\% training data for the automated component, we show that the financial outcome is cost neutral, while still returning over 70 hours of time to the researchers. The work presented here is the first to join the paraphrasing and plagiarism communities.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = jul,
articleno = {43},
numpages = {21},
keywords = {plagiarism, cost analysis, corpus, Paraphrase generation, Mechanical Turk}
}

@article{10.14778/3421424.3421429,
author = {Heo, Geon and Roh, Yuji and Hwang, Seonghyeon and Lee, Dayun and Whang, Steven Euijong},
title = {Inspector gadget: a data programming-based labeling system for industrial images},
year = {2020},
issue_date = {September 2020},
publisher = {VLDB Endowment},
volume = {14},
number = {1},
issn = {2150-8097},
url = {https://doi.org/10.14778/3421424.3421429},
doi = {10.14778/3421424.3421429},
abstract = {As machine learning for images becomes democratized in the Software 2.0 era, one of the serious bottlenecks is securing enough labeled data for training. This problem is especially critical in a manufacturing setting where smart factories rely on machine learning for product quality control by analyzing industrial images. Such images are typically large and may only need to be partially analyzed where only a small portion is problematic (e.g., identifying defects on a surface). Since manual labeling these images is expensive, weak supervision is an attractive alternative where the idea is to generate weak labels that are not perfect, but can be produced at scale. Data programming is a recent paradigm in this category where it uses human knowledge in the form of labeling functions and combines them into a generative model. Data programming has been successful in applications based on text or structured data and can also be applied to images usually if one can find a way to convert them into structured data. In this work, we expand the horizon of data programming by directly applying it to images without this conversion, which is a common scenario for industrial applications. We propose Inspector Gadget, an image labeling system that combines crowdsourcing, data augmentation, and data programming to produce weak labels at scale for image classification. We perform experiments on real industrial image datasets and show that Inspector Gadget obtains better performance than other weak-labeling techniques: Snuba, GOGGLES, and self-learning baselines using convolutional neural networks (CNNs) without pre-training.},
journal = {Proc. VLDB Endow.},
month = sep,
pages = {28–36},
numpages = {9}
}

@article{10.1109/TNET.2020.2992939,
author = {Hu, Yidan and Zhang, Rui},
title = {A Spatiotemporal Approach for Secure Crowdsourced Radio Environment Map Construction},
year = {2020},
issue_date = {Aug. 2020},
publisher = {IEEE Press},
volume = {28},
number = {4},
issn = {1063-6692},
url = {https://doi.org/10.1109/TNET.2020.2992939},
doi = {10.1109/TNET.2020.2992939},
abstract = {Database-driven Dynamic Spectrum Sharing (DSS) is the de-facto technical paradigm adopted by Federal Communications Commission for increasing spectrum efficiency, which allows licensed spectrum to be opportunistically used by secondary users. In database-driven DSS, a geo-location database administrator (DBA) maintains spectrum availability information over its service region in the form of a Radio Environment Map (REM), where the received signal strength from the primary user at every location is either directly measured via spectrum sensing or estimated via statistical spatial interpolation. Crowdsourcing-based spectrum sensing is a promising approach for periodically collecting spectrum measurements over a large geographic area but is unfortunately vulnerable to false spectrum measurements. Despite a large body of prior work on secure cooperative spectrum sensing, how to construct an accurate REM in the presence of false measurements remains an open challenge. In this paper, we introduce ST-REM, a novel spatiotemporal approach for securely constructing an REM in the presence of false spectrum measurements. Inspired by the self-label techniques developed for semi-supervised learning, ST-REM iteratively constructs an REM from a small number of spectrum measurements from trusted anchor sensors and many more measurements from mobile users. During each iteration, the DBA evaluates the trustworthiness of each measurement by jointly considering its spatial fitness with other trusted measurements and the mobile user's long-term behavior. By gradually incorporating the most trustworthy spectrum measurements, the DBA is able to construct a REM with high accuracy. Extensive simulation studies using a real spectrum measurement dataset confirm the efficacy and efficiency of ST-REM.},
journal = {IEEE/ACM Trans. Netw.},
month = aug,
pages = {1790–1803},
numpages = {14}
}

@article{10.1145/3364181,
author = {Hatzivasilis, George and Soultatos, Othonas and Ioannidis, Sotiris and Spanoudakis, George and Katos, Vasilios and Demetriou, Giorgos},
title = {MobileTrust: Secure Knowledge Integration in VANETs},
year = {2020},
issue_date = {July 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {4},
number = {3},
issn = {2378-962X},
url = {https://doi.org/10.1145/3364181},
doi = {10.1145/3364181},
abstract = {Vehicular Ad hoc NETworks (VANET) are becoming popular due to the emergence of the Internet of Things and ambient intelligence applications. In such networks, secure resource sharing functionality is accomplished by incorporating trust schemes. Current solutions adopt peer-to-peer technologies that can cover the large operational area. However, these systems fail to capture some inherent properties of VANETs, such as fast and ephemeral interaction, making robust trust evaluation of crowdsourcing challenging. In this article, we propose MobileTrust—a hybrid trust-based system for secure resource sharing in VANETs. The proposal is a breakthrough in centralized trust computing that utilizes cloud and upcoming 5G technologies to provide robust trust establishment with global scalability. The ad hoc communication is energy-efficient and protects the system against threats that are not countered by the current settings. To evaluate its performance and effectiveness, MobileTrust is modelled in the SUMO simulator and tested on the traffic features of the small-size German city of Eichstatt. Similar schemes are implemented in the same platform to provide a fair comparison. Moreover, MobileTrust is deployed on a typical embedded system platform and applied on a real smart car installation for monitoring traffic and road-state parameters of an urban application. The proposed system is developed under the EU-founded THREAT-ARREST project, to provide security, privacy, and trust in an intelligent and energy-aware transportation scenario, bringing closer the vision of sustainable circular economy.},
journal = {ACM Trans. Cyber-Phys. Syst.},
month = mar,
articleno = {33},
numpages = {25},
keywords = {trust, reputation, privacy, parallel computing, mobility, cloud, circular economy, VANET, MEC, IoT, GPU, CPS}
}

@article{10.5555/3322706.3362021,
author = {Li, Ruilin and Ye, Xiaojing and Zhou, Haomin and Zha, Hongyuan},
title = {Learning to match via inverse optimal transport},
year = {2019},
issue_date = {January 2019},
publisher = {JMLR.org},
volume = {20},
number = {1},
issn = {1532-4435},
abstract = {We propose a unified data-driven framework based on inverse optimal transport that can learn adaptive, nonlinear interaction cost function from noisy and incomplete empirical matching matrix and predict new matching in various matching contexts. We emphasize that the discrete optimal transport plays the role of a variational principle which gives rise to an optimization based framework for modeling the observed empirical matching data. Our formulation leads to a non-convex optimization problem which can be solved efficiently by an alternating optimization method. A key novel aspect of our formulation is the incorporation of marginal relaxation via regularized Wasserstein distance, significantly improving the robustness of the method in the face of noisy or missing empirical matching data. Our model falls into the category of prescriptive models, which not only predict potential future matching, but is also able to explain what leads to empirical matching and quantifies the impact of changes in matching factors. The proposed approach has wide applicability including predicting matching in online dating, labor market, college application and crowdsourcing. We back up our claims with numerical experiments on both synthetic data and real world data sets.},
journal = {J. Mach. Learn. Res.},
month = jan,
pages = {2954–2990},
numpages = {37},
keywords = {variational inference, robustification, optimal transport, matching, inverse problem}
}

@article{10.1145/3152889,
author = {Dumitrache, Anca and Aroyo, Lora and Welty, Chris},
title = {Crowdsourcing Ground Truth for Medical Relation Extraction},
year = {2018},
issue_date = {June 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {8},
number = {2},
issn = {2160-6455},
url = {https://doi.org/10.1145/3152889},
doi = {10.1145/3152889},
abstract = {Cognitive computing systems require human labeled data for evaluation and often for training. The standard practice used in gathering this data minimizes disagreement between annotators, and we have found this results in data that fails to account for the ambiguity inherent in language. We have proposed the CrowdTruth method for collecting ground truth through crowdsourcing, which reconsiders the role of people in machine learning based on the observation that disagreement between annotators provides a useful signal for phenomena such as ambiguity in the text. We report on using this method to build an annotated data set for medical relation extraction for the cause and treat relations, and how this data performed in a supervised training experiment. We demonstrate that by modeling ambiguity, labeled data gathered from crowd workers can (1) reach the level of quality of domain experts for this task while reducing the cost, and (2) provide better training data at scale than distant supervision. We further propose and validate new weighted measures for precision, recall, and F-measure, which account for ambiguity in both human and machine performance on this task.},
journal = {ACM Trans. Interact. Intell. Syst.},
month = jul,
articleno = {11},
numpages = {20},
keywords = {relation extraction, natural language ambiguity, inter-annotator disagreement, crowdtruth, crowd truth, clinical natural language processing, Ground truth}
}

@article{10.1145/3287047,
author = {Iwamoto, Eiichi and Matsubara, Masaki and Ota, Chihiro and Nakamura, Satoshi and Terada, Tsutomu and Kitagawa, Hiroyuki and Morishima, Atsuyuki},
title = {Passerby Crowdsourcing: Workers' Behavior and Data Quality Management},
year = {2018},
issue_date = {December 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {2},
number = {4},
url = {https://doi.org/10.1145/3287047},
doi = {10.1145/3287047},
abstract = {Worker recruitment is one of the important problems in crowdsourcing, and many proposals have been presented for placing equipment in physical spaces for recruiting workers. One of the essential challenges of the approach is how to keep people attracted because those who perform tasks at first gradually lose interest and do not access the equipment. This study uses a different approach to the worker recruitment problem. In our approach, we dive into people's personal spaces by projecting task images on the floor, thereby allowing the passersby to effortlessly access tasks while walking. The problem then changes from how to keep people engaged to how to manage data quality because many passersby unconsciously or intentionally walk through the task screen on the floor without doing the task, which produces unintended results. We explore a machine-learning approach to select only the intended results and manage the data quality. The system assesses the workers' intention from their behavior. We identify the features for classifiers based on our observations of the passersby. We then conduct extensive evaluations with real data. The results show that the features are effective in practice, and the classifiers improve the data quality.},
journal = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
month = dec,
articleno = {169},
numpages = {20},
keywords = {Worker recruitment, Long-term practical use, Crowdsourcing}
}

@article{10.1145/3186195,
author = {Goldberg, David and Trotman, Andrew and Wang, Xiao and Min, Wei and Wan, Zongru},
title = {Further Insights on Drawing Sound Conclusions from Noisy Judgments},
year = {2018},
issue_date = {October 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {36},
number = {4},
issn = {1046-8188},
url = {https://doi.org/10.1145/3186195},
doi = {10.1145/3186195},
abstract = {The effectiveness of a search engine is typically evaluated using hand-labeled datasets, where the labels indicate the relevance of documents to queries. Often the number of labels needed is too large to be created by the best annotators, and so less expensive labels (e.g., from crowdsourcing) are used. This introduces errors in the labels, and thus errors in standard effectiveness metrics (such as P@k and DCG). These errors must be taken into consideration when using the metrics. Previous work has approached assessor error by taking aggregates over multiple inexpensive assessors. We take a different approach and introduce equations and algorithms that can adjust the metrics to the values they would have had if there were no annotation errors.This is especially important when two search engines are compared on their metrics. We give examples where one engine appeared to be statistically significantly better than the other, but the effect disappeared after the metrics were corrected for annotation error. In other words, the evidence supporting a statistical difference was illusory and caused by a failure to account for annotation error.},
journal = {ACM Trans. Inf. Syst.},
month = apr,
articleno = {36},
numpages = {31},
keywords = {statistical significance, standard error, Precision}
}

@article{10.1145/3372407,
author = {Dong, Jialin and Yang, Kai and Shi, Yuanming},
title = {Ranking from Crowdsourced Pairwise Comparisons via Smoothed Riemannian Optimization},
year = {2020},
issue_date = {April 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {14},
number = {2},
issn = {1556-4681},
url = {https://doi.org/10.1145/3372407},
doi = {10.1145/3372407},
abstract = {Social Internet of Things has recently become a promising paradigm for augmenting the capability of humans and devices connected in the networks to provide services. In social Internet of Things network, crowdsourcing that collects the intelligence of the human crowd has served as a powerful tool for data acquisition and distributed computing. To support critical applications (e.g., a recommendation system and assessing the inequality of urban perception), in this article, we shall focus on the collaborative ranking problems for user preference prediction from crowdsourced pairwise comparisons. Based on the Bradley--Terry--Luce (BTL) model, a maximum likelihood estimation (MLE) is proposed via low-rank approach in order to estimate the underlying weight/score matrix, thereby predicting the ranking list for each user. A novel regularized formulation with the smoothed surrogate of elementwise infinity norm is proposed in order to address the unique challenge of the coupled the non-smooth elementwise infinity norm constraint and non-convex low-rank constraint in the MLE problem. We solve the resulting smoothed rank-constrained optimization problem via developing the Riemannian trust-region algorithm on quotient manifolds of fixed-rank matrices, which enjoys the superlinear convergence rate. The admirable performance and algorithmic advantages of the proposed method over the state-of-the-art algorithms are demonstrated via numerical results. Moreover, the proposed method outperforms state-of-the-art algorithms on large collaborative filtering datasets in both success rate of inferring preference and normalized discounted cumulative gain.},
journal = {ACM Trans. Knowl. Discov. Data},
month = feb,
articleno = {19},
numpages = {26},
keywords = {social Internet of Things, smoothed matrix manifold optimization, pairwise comparison, low-rank optimization, crowdsourced data, Ranking}
}

@article{10.1145/2627751,
author = {Hu, Chang and Resnik, Philip and Bederson, Benjamin B.},
title = {Crowdsourced Monolingual Translation},
year = {2014},
issue_date = {August 2014},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {21},
number = {4},
issn = {1073-0516},
url = {https://doi.org/10.1145/2627751},
doi = {10.1145/2627751},
abstract = {An enormous potential exists for solving certain classes of computational problems through rich collaboration among crowds of humans supported by computers. Solutions to these problems used to involve human professionals, who are expensive to hire or difficult to find. Despite significant advances, fully automatic systems still have much room for improvement. Recent research has involved recruiting large crowds of skilled humans (“crowdsourcing”), but crowdsourcing solutions are still restricted by the availability of those skilled human participants. With translation, for example, professional translators incur a high cost and are not always available; machine translation systems have been greatly improved recently but still can only provide passable translation; and crowdsourced translation is limited by the availability of bilingual humans.This article describes crowdsourced monolingual translation, where monolingual translation is translation performed by monolingual people. Crowdsourced monolingual translation is a collaborative form of translation performed by two crowds of people who speak the source or the target language, respectively, with machine translation as the mediating device.This article describes a general protocol to handle crowdsourced monolingual translation and analyzes three systems that implemented the protocol. These systems were studied in various settings and were found to supply significant improvement in quality over both machine translation and monolingual editing of machine translation output (“postediting”).},
journal = {ACM Trans. Comput.-Hum. Interact.},
month = aug,
articleno = {22},
numpages = {35},
keywords = {translation, human computation, Crowdsourcing}
}

@article{10.1145/3392858,
author = {Song, Jean Y. and Chung, John Joon Young and Fouhey, David F. and Lasecki, Walter S.},
title = {C-Reference: Improving 2D to 3D Object Pose Estimation Accuracy via Crowdsourced Joint Object Estimation},
year = {2020},
issue_date = {May 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {4},
number = {CSCW1},
url = {https://doi.org/10.1145/3392858},
doi = {10.1145/3392858},
abstract = {Converting widely-available 2D images and videos, captured using an RGB camera, to 3D can help accelerate the training of machine learning systems in spatial reasoning domains ranging from in-home assistive robots to augmented reality to autonomous vehicles. However, automating this task is challenging because it requires not only accurately estimating object location and orientation, but also requires knowing currently unknown camera properties (e.g., focal length). A scalable way to combat this problem is to leverage people's spatial understanding of scenes by crowdsourcing visual annotations of 3D object properties. Unfortunately, getting people to directly estimate 3D properties reliably is difficult due to the limitations of image resolution, human motor accuracy, and people's 3D perception (i.e., humans do not "see" depth like a laser range finder). In this paper, we propose a crowd-machine hybrid approach that jointly uses crowds' approximate measurements of multiple in-scene objects to estimate the 3D state of a single target object. Our approach can generate accurate estimates of the target object by combining heterogeneous knowledge from multiple contributors regarding various different objects that share a spatial relationship with the target object. We evaluate our joint object estimation approach with 363 crowd workers and show that our method can reduce errors in the target object's 3D location estimation by over 40\%, while requiring only $35$\% as much human time. Our work introduces a novel way to enable groups of people with different perspectives and knowledge to achieve more accurate collective performance on challenging visual annotation tasks.},
journal = {Proc. ACM Hum.-Comput. Interact.},
month = may,
articleno = {51},
numpages = {28},
keywords = {soft constraints, optimization, human computation, crowdsourcing, computer vision, answer aggregation, 3D pose estimation}
}

@article{10.1109/TASLP.2019.2900910,
author = {Lubis, Nurul and Sakti, Sakriani and Yoshino, Koichiro and Nakamura, Satoshi},
title = {Positive Emotion Elicitation in Chat-Based Dialogue Systems},
year = {2019},
issue_date = {April 2019},
publisher = {IEEE Press},
volume = {27},
number = {4},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2019.2900910},
doi = {10.1109/TASLP.2019.2900910},
abstract = {We aim to draw on an important overlooked potential of affective dialogue systems—their application to promote positive emotional states, similar to that of emotional support between humans. This can be achieved by eliciting a more positive emotional valence throughout a dialogue system interaction, i.e., positive emotion elicitation. Existing works on emotion elicitation have not yet paid attention to the emotional benefit for the users. Moreover, a positive emotion elicitation corpus does not yet exist despite the growing number of emotion-rich corpora. Towards this goal, first, we propose a response retrieval approach for positive emotion elicitation by utilizing examples of emotion appraisal from a dialogue corpus. Second, we efficiently construct a corpus using the proposed retrieval method, by replacing responses in a dialogue with those that elicit a more positive emotion. We validate the corpus through crowdsourcing to ensure its quality. Finally, we propose a novel neural network architecture for an emotion-sensitive neural chat-based dialogue system, optimized on the constructed corpus to elicit positive emotion. Objective and subjective evaluations show that the proposed methods result in dialogue responses that are more natural and elicit a more positive emotional response. Further analyses of the results are discussed in this paper.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = apr,
pages = {866–877},
numpages = {12}
}

@article{10.14778/3137765.3137806,
author = {Li, Yan and Kou, Ngai Meng and Wang, Hao and U, Leong Hou and Gong, Zhiguo},
title = {A confidence-aware top-k query processing toolkit on crowdsourcing},
year = {2017},
issue_date = {August 2017},
publisher = {VLDB Endowment},
volume = {10},
number = {12},
issn = {2150-8097},
url = {https://doi.org/10.14778/3137765.3137806},
doi = {10.14778/3137765.3137806},
abstract = {Ranking techniques have been widely used in ubiquitous applications like recommendation, information retrieval, etc. For ranking computation hostile but human friendly items, crowdsourcing is considered as an emerging technique to process the ranking by human power. However, there is a lack of an easy-to-use toolkit for answering crowdsourced top-k query with minimal effort.In this work, we demonstrate an interactive programming toolkit that is a unified solution for answering the crowd-sourced top-k queries. The toolkit employs a new confidence-aware crowdsourced top-k algorithm, SPR. The whole progress of the algorithm is monitored and visualized to end users in a timely manner. Besides the visualized result and the statistics, the system also reports the estimation of the monetary cost and the breakdown of each phase. Based on the estimation, end users can strike a balance between the budget and the quality through the interface of this toolkit.},
journal = {Proc. VLDB Endow.},
month = aug,
pages = {1909–1912},
numpages = {4}
}

@article{10.1145/2684066,
author = {Davidson, Susan and Khanna, Sanjeev and Milo, Tova and Roy, Sudeepa},
title = {Top-k and Clustering with Noisy Comparisons},
year = {2015},
issue_date = {December 2014},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {39},
number = {4},
issn = {0362-5915},
url = {https://doi.org/10.1145/2684066},
doi = {10.1145/2684066},
abstract = {We study the problems of max/top-k and clustering when the comparison operations may be performed by oracles whose answer may be erroneous. Comparisons may either be of type or of value: given two data elements, the answer to a type comparison is “yes” if the elements have the same type and therefore belong to the same group (cluster); the answer to a value comparison orders the two data elements. We give efficient algorithms that are guaranteed to achieve correct results with high probability, analyze the cost of these algorithms in terms of the total number of comparisons (i.e., using a fixed-cost model), and show that they are essentially the best possible. We also show that fewer comparisons are needed when values and types are correlated, or when the error model is one in which the error decreases as the distance between the two elements in the sorted order increases. Finally, we examine another important class of cost functions, concave functions, which balances the number of rounds of interaction with the oracle with the number of questions asked of the oracle. Results of this article form an important first step in providing a formal basis for max/top-k and clustering queries in crowdsourcing applications, that is, when the oracle is implemented using the crowd. We explain what simplifying assumptions are made in the analysis, what results carry to a generalized crowdsourcing setting, and what extensions are required to support a full-fledged model.},
journal = {ACM Trans. Database Syst.},
month = dec,
articleno = {35},
numpages = {39},
keywords = {crowdsourcing, clustering, approximation, algorithm, Top-k}
}

@article{10.1145/3195727,
author = {Harris, Martyn and Levene, Mark and Zhang, Dell and Levene, Dan},
title = {Finding Parallel Passages in Cultural Heritage Archives},
year = {2018},
issue_date = {September 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {11},
number = {3},
issn = {1556-4673},
url = {https://doi.org/10.1145/3195727},
doi = {10.1145/3195727},
abstract = {It is of great interest to researchers and scholars in many disciplines (particularly those working on cultural heritage projects) to study parallel passages (i.e., identical or similar pieces of text describing the same thing) in digital text archives. Although there exist a few software tools for this purpose, they are restricted to a specific domain (e.g., the Bible) or a specific language (e.g., Hebrew). In this article, we present in detail how we build a digital infrastructure that can facilitate the search and discovery of parallel passages for any domain in any language. It is at the core of our Samtla (Search And Mining Tools with Linguistic Analysis) system designed in collaboration with historians and linguists. The system has already been used to support research on five large text corpora that span a number of different domains and languages. The key to such a domain-independent and language-independent digital infrastructure is a novel combination of a character-based n-gram language model, space-optimized suffix tree, and generalized edit distance. A comprehensive evaluation through crowdsourcing shows that the effectiveness of our system’s search functionality is on par with the human-level performance.},
journal = {J. Comput. Cult. Herit.},
month = aug,
articleno = {15},
numpages = {24},
keywords = {suffix trees, statistical language models, information retrieval, Digital archives}
}

@article{10.1145/3143560,
author = {Wasik, Szymon and Antczak, Maciej and Badura, Jan and Laskowski, Artur and Sternal, Tomasz},
title = {A Survey on Online Judge Systems and Their Applications},
year = {2018},
issue_date = {January 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {51},
number = {1},
issn = {0360-0300},
url = {https://doi.org/10.1145/3143560},
doi = {10.1145/3143560},
abstract = {Online judges are systems designed for the reliable evaluation of algorithm source code submitted by users, which is next compiled and tested in a homogeneous environment. Online judges are becoming popular in various applications. Thus, we would like to review the state of the art for these systems. We classify them according to their principal objectives into systems supporting organization of competitive programming contests, enhancing education and recruitment processes, facilitating the solving of data mining challenges, online compilers and development platforms integrated as components of other custom systems. Moreover, we introduce a formal definition of an online judge system and summarize the common evaluation methodology supported by such systems. Finally, we briefly discuss an Optil.io platform as an example of an online judge system, which has been proposed for the solving of complex optimization problems. We also analyze the competition results conducted using this platform. The competition proved that online judge systems, strengthened by crowdsourcing concepts, can be successfully applied to accurately and efficiently solve complex industrial- and science-driven challenges.},
journal = {ACM Comput. Surv.},
month = jan,
articleno = {3},
numpages = {34},
keywords = {evaluation as a service, crowdsourcing, contest, challenge, Online judge}
}

@article{10.14778/2536336.2536337,
author = {Whang, Steven Euijong and Lofgren, Peter and Garcia-Molina, Hector},
title = {Question selection for crowd entity resolution},
year = {2013},
issue_date = {April 2013},
publisher = {VLDB Endowment},
volume = {6},
number = {6},
issn = {2150-8097},
url = {https://doi.org/10.14778/2536336.2536337},
doi = {10.14778/2536336.2536337},
abstract = {We study the problem of enhancing Entity Resolution (ER) with the help of crowdsourcing. ER is the problem of clustering records that refer to the same real-world entity and can be an extremely difficult process for computer algorithms alone. For example, figuring out which images refer to the same person can be a hard task for computers, but an easy one for humans. We study the problem of resolving records with crowdsourcing where we ask questions to humans in order to guide ER into producing accurate results. Since human work is costly, our goal is to ask as few questions as possible. We propose a probabilistic framework for ER that can be used to estimate how much ER accuracy we obtain by asking each question and select the best question with the highest expected accuracy. Computing the expected accuracy is #P-hard, so we propose approximation techniques for efficient computation. We evaluate our best question algorithms on real and synthetic datasets and demonstrate how we can obtain high ER accuracy while significantly reducing the number of questions asked to humans.},
journal = {Proc. VLDB Endow.},
month = apr,
pages = {349–360},
numpages = {12}
}

@article{10.1145/3324926,
author = {Wang, Tian and Luo, Hao and Zheng, Xi and Xie, Mande},
title = {Crowdsourcing Mechanism for Trust Evaluation in CPCS Based on Intelligent Mobile Edge Computing},
year = {2019},
issue_date = {November 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {10},
number = {6},
issn = {2157-6904},
url = {https://doi.org/10.1145/3324926},
doi = {10.1145/3324926},
abstract = {Both academia and industry have directed tremendous interest toward the combination of Cyber Physical Systems and Cloud Computing, which enables a new breed of applications and services. However, due to the relative long distance between remote cloud and end nodes, Cloud Computing cannot provide effective and direct management for end nodes, which leads to security vulnerabilities. In this article, we first propose a novel trust evaluation mechanism using crowdsourcing and Intelligent Mobile Edge Computing. The mobile edge users with relatively strong computation and storage ability are exploited to provide direct management for end nodes. Through close access to end nodes, mobile edge users can obtain various information of the end nodes and determine whether the node is trustworthy. Then, two incentive mechanisms, i.e., Trustworthy Incentive and Quality-Aware Trustworthy Incentive Mechanisms, are proposed for motivating mobile edge users to conduct trust evaluation. The first one aims to motivate edge users to upload their real information about their capability and costs. The purpose of the second one is to motivate edge users to make trustworthy effort to conduct tasks and report results. Detailed theoretical analysis demonstrates the validity of Quality-Aware Trustworthy Incentive Mechanism from data trustfulness, effort trustfulness, and quality trustfulness, respectively. Extensive experiments are carried out to validate the proposed trust evaluation and incentive mechanisms. The results corroborate that the proposed mechanisms can efficiently stimulate mobile edge users to perform evaluation task and improve the accuracy of trust evaluation.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = oct,
articleno = {62},
numpages = {19},
keywords = {trust evaluation, mobile edge computing, artificial intelligence, Crowdsourcing}
}

@article{10.1145/3230665,
author = {Wilson, Shomir and Schaub, Florian and Liu, Frederick and Sathyendra, Kanthashree Mysore and Smullen, Daniel and Zimmeck, Sebastian and Ramanath, Rohan and Story, Peter and Liu, Fei and Sadeh, Norman and Smith, Noah A.},
title = {Analyzing Privacy Policies at Scale: From Crowdsourcing to Automated Annotations},
year = {2018},
issue_date = {February 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {13},
number = {1},
issn = {1559-1131},
url = {https://doi.org/10.1145/3230665},
doi = {10.1145/3230665},
abstract = {Website privacy policies are often long and difficult to understand. While research shows that Internet users care about their privacy, they do not have the time to understand the policies of every website they visit, and most users hardly ever read privacy policies. Some recent efforts have aimed to use a combination of crowdsourcing, machine learning, and natural language processing to interpret privacy policies at scale, thus producing annotations for use in interfaces that inform Internet users of salient policy details. However, little attention has been devoted to studying the accuracy of crowdsourced privacy policy annotations, how crowdworker productivity can be enhanced for such a task, and the levels of granularity that are feasible for automatic analysis of privacy policies. In this article, we present a trajectory of work addressing each of these topics. We include analyses of crowdworker performance, evaluation of a method to make a privacy-policy oriented task easier for crowdworkers, a coarse-grained approach to labeling segments of policy text with descriptive themes, and a fine-grained approach to identifying user choices described in policy text. Together, the results from these efforts show the effectiveness of using automated and semi-automated methods for extracting from privacy policies the data practice details that are salient to Internet users’ interests.},
journal = {ACM Trans. Web},
month = dec,
articleno = {1},
numpages = {29},
keywords = {privacy policies, natural language processing, machine learning, human computer interaction (HCI), crowdsourcing, Privacy}
}

@article{10.14778/3137628.3137662,
author = {Garcia-Ulloa, Daniel A. and Xiong, Li and Sunderam, Vaidy},
title = {Truth discovery for spatio-temporal events from crowdsourced data},
year = {2017},
issue_date = {August 2017},
publisher = {VLDB Endowment},
volume = {10},
number = {11},
issn = {2150-8097},
url = {https://doi.org/10.14778/3137628.3137662},
doi = {10.14778/3137628.3137662},
abstract = {One of the greatest challenges in spatial crowdsourcing is determining the veracity of reports from multiple users about a particular event or phenomenon. In this paper, we address the difficulties of truth discovery in spatio-temporal tasks and present a new method based on recursive Bayesian estimation (BE) from multiple reports of users. Our method incorporates a reliability model for users, which improves as more reports arrive while increasing the accuracy of the model in labeling the state of the event. The model is further improved by Kalman estimation (BE+KE) that models the spatio-temporal correlations of the events and predicts the next state of an event and is corrected when new reports arrive. The methods are tested in a simulated environment, as well as using real-world data. Experimental results show that our methods are adaptable to the available data, can incorporate previous beliefs, and outperform existing truth discovery methods of spatio-temporal events.},
journal = {Proc. VLDB Endow.},
month = aug,
pages = {1562–1573},
numpages = {12}
}

@article{10.14778/3137628.3137642,
author = {Marchant, Neil G. and Rubinstein, Benjamin I. P.},
title = {In search of an entity resolution OASIS: optimal asymptotic sequential importance sampling},
year = {2017},
issue_date = {August 2017},
publisher = {VLDB Endowment},
volume = {10},
number = {11},
issn = {2150-8097},
url = {https://doi.org/10.14778/3137628.3137642},
doi = {10.14778/3137628.3137642},
abstract = {Entity resolution (ER) presents unique challenges for evaluation methodology. While crowdsourcing platforms acquire ground truth, sound approaches to sampling must drive labelling efforts. In ER, extreme class imbalance between matching and non-matching records can lead to enormous labelling requirements when seeking statistically consistent estimates for rigorous evaluation. This paper addresses this important challenge with the OASIS algorithm: a sampler and F-measure estimator for ER evaluation. OASIS draws samples from a (biased) instrumental distribution, chosen to ensure estimators with optimal asymptotic variance. As new labels are collected OASIS updates this instrumental distribution via a Bayesian latent variable model of the annotator oracle, to quickly focus on unlabelled items providing more information. We prove that resulting estimates of F-measure, precision, recall converge to the true population values. Thorough comparisons of sampling methods on a variety of ER datasets demonstrate significant labelling reductions of up to 83\% without loss to estimate accuracy.},
journal = {Proc. VLDB Endow.},
month = aug,
pages = {1322–1333},
numpages = {12}
}

@article{10.1145/3402440,
author = {Goy, Annamaria and Colla, Davide and Magro, Diego and Accornero, Cristina and Loreto, Fabrizio and Radicioni, Daniele Paolo},
title = {Building Semantic Metadata for Historical Archives through an Ontology-driven User Interface},
year = {2020},
issue_date = {October 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {13},
number = {3},
issn = {1556-4673},
url = {https://doi.org/10.1145/3402440},
doi = {10.1145/3402440},
abstract = {Historical archives represent an immense wealth, the potential of which is endangered by the lack of effective management and access tools. We believe that this issue can be faced by providing archive catalogs with a semantic layer, containing rich semantic metadata, representing the content of documents in a full-fledged formal machine-readable format. In this article, we present the contribution offered in this direction by the PRiSMHA project, in which the conceptual vocabulary of the semantic layer is represented by computational ontologies. However, acquiring semantic knowledge represents a well-known bottleneck for knowledge-based systems; to solve this problem, PRiSMHA relies on a crowdsourcing collaborative model, i.e., an online community of users who collaborate in building semantic representations of the content of archival documents. In this perspective, this article aims at answering the following research question: Starting from the axioms characterizing concepts in the computational ontology underlying the system, how can we derive a user interface enabling users to formally represent the content of archival documents by exploiting the conceptual vocabulary provided by the ontology?Our solution includes the following steps: (a) a manually defined configuration, acting as a pre-filter, to hide “unsuited” classes, properties, and relations; (b) an algorithm, combining heuristics and reasoning, which extracts from the ontology all and only the “compatible” properties and relations, given an entity (event) type; and (c) a set of strategies to rank, group, and present the entity (event) properties and relations, based on the results of a study with users. This integrated solution enabled us to design an ontology-driven user interface enabling users to characterize entities, and in particular (historical) events, on the basis of the vocabulary provided by the ontology.},
journal = {J. Comput. Cult. Herit.},
month = aug,
articleno = {25},
numpages = {36},
keywords = {historical archives, crowdsourcing platform, computational ontologies, Ontology-driven user interfaces}
}

@article{10.1145/3072959.3073598,
author = {Koyama, Yuki and Sato, Issei and Sakamoto, Daisuke and Igarashi, Takeo},
title = {Sequential line search for efficient visual design optimization by crowds},
year = {2017},
issue_date = {August 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {36},
number = {4},
issn = {0730-0301},
url = {https://doi.org/10.1145/3072959.3073598},
doi = {10.1145/3072959.3073598},
abstract = {Parameter tweaking is a common task in various design scenarios. For example, in color enhancement of photographs, designers tweak multiple parameters such as "brightness" and "contrast" to obtain the best visual impression. Adjusting one parameter is easy; however, if there are multiple correlated parameters, the task becomes much more complex, requiring many trials and a large cognitive load. To address this problem, we present a novel extension of Bayesian optimization techniques, where the system decomposes the entire parameter tweaking task into a sequence of one-dimensional line search queries that are easy for human to perform by manipulating a single slider. In addition, we present a novel concept called crowd-powered visual design optimizer, which queries crowd workers, and provide a working implementation of this concept. Our single-slider manipulation microtask design for crowdsourcing accelerates the convergence of the optimization relative to existing comparison-based microtask designs. We applied our framework to two different design domains: photo color enhancement and material BRDF design, and thereby showed its applicability to various design domains.},
journal = {ACM Trans. Graph.},
month = jul,
articleno = {48},
numpages = {11},
keywords = {human computation, crowdsourcing, computational design, bayesian optimization}
}

@article{10.1145/3066166,
author = {Zhang, Chao and Lei, Dongming and Yuan, Quan and Zhuang, Honglei and Kaplan, Lance and Wang, Shaowen and Han, Jiawei},
title = {GeoBurst+: Effective and Real-Time Local Event Detection in Geo-Tagged Tweet Streams},
year = {2018},
issue_date = {May 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {9},
number = {3},
issn = {2157-6904},
url = {https://doi.org/10.1145/3066166},
doi = {10.1145/3066166},
abstract = {The real-time discovery of local events (e.g., protests, disasters) has been widely recognized as a fundamental socioeconomic task. Recent studies have demonstrated that the geo-tagged tweet stream serves as an unprecedentedly valuable source for local event detection. Nevertheless, how to effectively extract local events from massive geo-tagged tweet streams in real time remains challenging. To bridge the gap, we propose a method for effective and real-time local event detection from geo-tagged tweet streams. Our method, named GeoBurst+, first leverages a novel cross-modal authority measure to identify several pivots in the query window. Such pivots reveal different geo-topical activities and naturally attract similar tweets to form candidate events. GeoBurst+ further summarizes the continuous stream and compares the candidates against the historical summaries to pinpoint truly interesting local events. Better still, as the query window shifts, GeoBurst+ is capable of updating the event list with little time cost, thus achieving continuous monitoring of the stream. We used crowdsourcing to evaluate GeoBurst+ on two million-scale datasets and found it significantly more effective than existing methods while being orders of magnitude faster.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = jan,
articleno = {34},
numpages = {24},
keywords = {spatiotemporal data mining, social media, location-based service, local event, data stream, Event detection}
}

@article{10.1145/3012003,
author = {Goldberg, Sean and Wang, Daisy Zhe and Grant, Christan},
title = {A Probabilistically Integrated System for Crowd-Assisted Text Labeling and Extraction},
year = {2017},
issue_date = {February 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {8},
number = {2},
issn = {1936-1955},
url = {https://doi.org/10.1145/3012003},
doi = {10.1145/3012003},
abstract = {The amount of text data has been growing exponentially in recent years, giving rise to automatic information extraction methods that store text annotations in a database. The current state-of-the-art structured prediction methods, however, are likely to contain errors and it is important to be able to manage the overall uncertainty of the database. On the other hand, the advent of crowdsourcing has enabled humans to aid machine algorithms at scale. In this article, we introduce pi-CASTLE, a system that optimizes and integrates human and machine computing as applied to a complex structured prediction problem involving Conditional Random Fields (CRFs). We propose strategies grounded in information theory to select a token subset, formulate questions for the crowd to label, and integrate these labelings back into the database using a method of constrained inference. On both a text segmentation task over academic citations and a named entity recognition task over tweets we show an order of magnitude improvement in accuracy gain over baseline methods.},
journal = {J. Data and Information Quality},
month = feb,
articleno = {10},
numpages = {23},
keywords = {probabilistic models, probabilistic database systems, information extraction, Crowdsourcing}
}

@article{10.14778/3229863.3236255,
author = {Govind, Yash and Paulson, Erik and Nagarajan, Palaniappan and C., Paul Suganthan G. and Doan, AnHai and Park, Youngchoon and Fung, Glenn M. and Conathan, Devin and Carter, Marshall and Sun, Mingju},
title = {Cloudmatcher: a hands-off cloud/crowd service for entity matching},
year = {2018},
issue_date = {August 2018},
publisher = {VLDB Endowment},
volume = {11},
number = {12},
issn = {2150-8097},
url = {https://doi.org/10.14778/3229863.3236255},
doi = {10.14778/3229863.3236255},
abstract = {As data science applications proliferate, more and more lay users must perform data integration (DI) tasks, which used to be done by sophisticated CS developers. Thus, it is increasingly critical that we develop hands-off DI services, which lay users can use to perform such tasks without asking for help from developers. We propose to demonstrate such a service. Specifically, we will demonstrate CloudMatcher, a hands-off cloud/crowd service for entity matching (EM). To use CloudMatcher to match two tables, a lay user only needs to upload them to the CloudMatcher's Web page then iteratively label a set of tuple pairs as match/no-match. Alternatively, the user can enlist a crowd of workers to label the pairs. In either case, the lay user can easily perform EM end-to-end without having to involve any developers. Cloud-Matcher has been used in several domain science projects at UW-Madison and at several organizations, and is scheduled to be deployed in a large company in Summer 2018. In the demonstration we will show how easy it is for lay users to perform EM (either via interactive labeling or crowdsourcing), how users can easily create and experiment with a range of EM workflows, and how CloudMatcher can scale to many concurrent users and large datasets.},
journal = {Proc. VLDB Endow.},
month = aug,
pages = {2042–2045},
numpages = {4}
}

@article{10.1145/2451236.2451244,
author = {Bousseau, Adrien and O'shea, James P. and Durand, Fr\'{e}do and Ramamoorthi, Ravi and Agrawala, Maneesh},
title = {Gloss perception in painterly and cartoon rendering},
year = {2013},
issue_date = {April 2013},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {32},
number = {2},
issn = {0730-0301},
url = {https://doi.org/10.1145/2451236.2451244},
doi = {10.1145/2451236.2451244},
abstract = {Depictions with traditional media such as painting and drawing represent scene content in a stylized manner. It is unclear, however, how well stylized images depict scene properties like shape, material, and lighting. In this article, we describe the first study of material perception in stylized images (specifically painting and cartoon) and use nonphotorealistic rendering algorithms to evaluate how such stylization alters the perception of gloss. Our study reveals a compression of the range of representable gloss in stylized images so that shiny materials appear more diffuse in painterly rendering, while diffuse materials appear shinier in cartoon images. From our measurements we estimate the function that maps realistic gloss parameters to their perception in a stylized rendering. This mapping allows users of NPR algorithms to predict the perception of gloss in their images. The inverse of this function exaggerates gloss properties to make the contrast between materials in a stylized image more faithful. We have conducted our experiment both in a lab and on a crowdsourcing Web site. While crowdsourcing allows us to quickly design our pilot study, a lab experiment provides more control on how subjects perform the task. We provide a detailed comparison of the results obtained with the two approaches and discuss their advantages and drawbacks for studies like ours.},
journal = {ACM Trans. Graph.},
month = apr,
articleno = {18},
numpages = {13},
keywords = {painterly rendering, material perception, crowdsourcing, cartoon rendering, Nonphotorealistic rendering}
}

@article{10.1109/TNET.2018.2823272,
author = {Jin, Xiaocong and Zhang, Yanchao},
title = {Privacy-Preserving Crowdsourced Spectrum Sensing},
year = {2018},
issue_date = {June 2018},
publisher = {IEEE Press},
volume = {26},
number = {3},
issn = {1063-6692},
url = {https://doi.org/10.1109/TNET.2018.2823272},
doi = {10.1109/TNET.2018.2823272},
abstract = {Dynamic spectrum access is promising for mitigating worldwide wireless spectrum shortage. Crowdsourced spectrum sensing CSS refers to recruiting ubiquitous mobile users to perform real-time spectrum sensing at specified locations and has great potential in mitigating the drawbacks of current spectrum database operations. Without strong incentives and location privacy protection in place, however, mobile users will be reluctant to act as mobile crowdsourcing workers for spectrum-sensing tasks. In this paper, we first formulate participant selection in CSS systems as a reverse auction problem, in which each participant’s true cost for spectrum sensing is closely tied to his current location. Then, we demonstrate how the location privacy of CSS participants can be easily breached under the framework. Finally, we present PriCSS, a novel framework for a CSS service provider to select CSS participants in a differentially privacy-preserving manner. In this framework, we propose PriCSS− and PriCSS+, two different schemes under distinct design objectives and assumptions. PriCSS− is an approximately truthful scheme that achieves differential location privacy and an approximate minimum payment, while PriCSS+ is a truthful scheme that achieves differential location privacy and an approximate minimum social cost. The detailed theoretical analysis and simulation studies are performed to demonstrate the efficacy of both schemes.},
journal = {IEEE/ACM Trans. Netw.},
month = jun,
pages = {1236–1249},
numpages = {14}
}

@article{10.1007/s00778-015-0385-2,
author = {Basu Roy, Senjuti and Lykourentzou, Ioanna and Thirumuruganathan, Saravanan and Amer-Yahia, Sihem and Das, Gautam},
title = {Task assignment optimization in knowledge-intensive crowdsourcing},
year = {2015},
issue_date = {August    2015},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {24},
number = {4},
issn = {1066-8888},
url = {https://doi.org/10.1007/s00778-015-0385-2},
doi = {10.1007/s00778-015-0385-2},
abstract = {We present SmartCrowd, a framework for optimizing task assignment in knowledge-intensive crowdsourcing (KI-C). SmartCrowd distinguishes itself by formulating, for the first time, the problem of worker-to-task assignment in KI-C as an optimization problem, by proposing efficient adaptive algorithms to solve it and by accounting for human factors, such as worker expertise, wage requirements, and availability inside the optimization process. We present rigorous theoretical analyses of the task assignment optimization problem and propose optimal and approximation algorithms with guarantees, which rely on index pre-computation and adaptive maintenance. We perform extensive performance and quality experiments using real and synthetic data to demonstrate that the SmartCrowd approach is necessary to achieve efficient task assignments of high-quality under guaranteed cost budget.},
journal = {The VLDB Journal},
month = aug,
pages = {467–491},
numpages = {25},
keywords = {Optimization, Knowledge-intensive crowdsourcing, Human factors, Collaborative crowdsourcing}
}

@article{10.1162/COLI_a_00301,
author = {Vuli\'{c}, Ivan and Gerz, Daniela and Kiela, Douwe and Hill, Felix and Korhonen, Anna},
title = {Hyperlex: A large-scale evaluation of graded lexical entailment},
year = {2017},
issue_date = {December 2017},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
volume = {43},
number = {4},
issn = {0891-2017},
url = {https://doi.org/10.1162/COLI_a_00301},
doi = {10.1162/COLI_a_00301},
abstract = {We introduce HyperLex-a data set and evaluation resource that quantifies the extent of the semantic category membership, that is, type-of relation, also known as hyponymy-hypernymy or lexical entailment LE relation between 2,616 concept pairs. Cognitive psychology research has established that typicality and category/class membership are computed in human semantic memory as a gradual rather than binary relation. Nevertheless, most NLP research and existing large-scale inventories of concept category membership WordNet, DBPedia, etc. treat category membership and LE as binary. To address this, we asked hundreds of native English speakers to indicate typicality and strength of category membership between a diverse range of concept pairs on a crowdsourcing platform. Our results confirm that category membership and LE are indeed more gradual than binary. We then compare these human judgments with the predictions of automatic systems, which reveals a huge gap between human performance and state-of-the-art LE, distributional and representation learning models, and substantial differences between the models themselves. We discuss a pathway for improving semantic models to overcome this discrepancy, and indicate future application areas for improved graded LE systems.},
journal = {Comput. Linguist.},
month = dec,
pages = {781–835},
numpages = {55}
}

@article{10.14778/2733085.2733105,
author = {Zhang, Chen Jason and Tong, Yongxin and Chen, Lei},
title = {Where to: crowd-aided path selection},
year = {2014},
issue_date = {October 2014},
publisher = {VLDB Endowment},
volume = {7},
number = {14},
issn = {2150-8097},
url = {https://doi.org/10.14778/2733085.2733105},
doi = {10.14778/2733085.2733105},
abstract = {With the widespread use of geo-positioning services (GPS), GPS-based navigation systems have become ever more of an integral part of our daily lives. GPS-based navigation systems usually suggest multiple paths for any given pair of source and target, leaving users perplexed when trying to select the best one among them, namely the problem of best path selection. Too many suggested paths may jeopardize the usability of the recommendation data, and decrease user satisfaction. Although existing studies have already partially relieved this problem through integrating historical traffic logs or updating traffic conditions periodically, their solutions neglect the potential contribution of human experience.In this paper, we resort to crowdsourcing to ease the pain of the best path selection. The first step of appropriately using the crowd is to ask proper questions. For the best path selection problem, simple questions (e.g. binary voting) over compete paths cannot be directly applied to road networks due to their being too complex for crowd workers. Thus, this paper makes the first contribution by designing two types of questions, namely Routing Query (RQ) and Binary Routing Query (BRQ), to ask the crowd to decide which direction to take at each road intersection. Furthermore, we propose a series of efficient algorithms to dynamically manage the questions in order to reduce the selection hardness within a limited budget. Finally, we compare the proposed methods against two baselines, and the effectiveness and efficiency of our proposals are verified by the results from simulations and experiments on a real-world crowdsourcing platform.},
journal = {Proc. VLDB Endow.},
month = oct,
pages = {2005–2016},
numpages = {12}
}

@article{10.1145/3402883,
author = {Elbassuoni, Shady and Amer-Yahia, Sihem and Ghizzawi, Ahmad},
title = {Fairness of Scoring in Online Job Marketplaces},
year = {2020},
issue_date = {November 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {1},
number = {4},
issn = {2691-1922},
url = {https://doi.org/10.1145/3402883},
doi = {10.1145/3402883},
abstract = {We study fairness of scoring in online job marketplaces. We focus on group fairness and aim to algorithmically explore how a scoring function, through which individuals are ranked for jobs, treats different demographic groups. Previous work on group-level fairness has focused on the case where groups are pre-defined or where they are defined using a single protected attribute (e.g., whites vs. blacks or males vs. females). In this article, we argue for the need to examine fairness for groups of people defined with any combination of protected attributes (the-so called subgroup fairness). Existing work also assumes the availability of worker’s data (i.e., data transparency) and the scoring function (i.e., process transparency). We relax that assumption in this work and run user studies to assess the effect of different data and process transparency settings on the ability to assess fairness.To quantify the fairness of a scoring of a group of individuals, we formulate an optimization problem to find a partitioning of those individuals on their protected attributes that exhibits the highest unfairness with respect to the scoring function. The scoring function yields one histogram of score distributions per partition and we rely on Earth Mover’s Distance, a measure that is commonly used to compare histograms, to quantify unfairness. Since the number of ways to partition individuals is exponential in the number of their protected attributes, we propose a heuristic algorithm to navigate the space of all possible partitionings to identify the one with the highest unfairness. We evaluate our algorithm using a simulation of a crowdsourcing platform and show that it can effectively quantify unfairness of various scoring functions. We additionally run experiments to assess the applicability of our approach in other less-transparent data and process settings. Finally, we demonstrate the effectiveness of our approach in assessing fairness of scoring in a real dataset crawled from the online job marketplace TaskRabbit.},
journal = {ACM/IMS Trans. Data Sci.},
month = nov,
articleno = {29},
numpages = {30},
keywords = {virtual marketplaces, transparency, scoring, group fairness, discrimination, demographic disparity, Algorithmic fairness}
}

@article{10.1145/3047408,
author = {Rafian, Paymon and Legge, Gordon E.},
title = {Remote Sighted Assistants for Indoor Location Sensing of Visually Impaired Pedestrians},
year = {2017},
issue_date = {July 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {14},
number = {3},
issn = {1544-3558},
url = {https://doi.org/10.1145/3047408},
doi = {10.1145/3047408},
abstract = {Because indoor navigation is difficult for people with visual impairment, there is a need for the development of assistive technology. Indoor location sensing, the ability to identify a pedestrian's location and orientation, is a key component of such technology. We tested the accuracy of a potential crowdsourcing-based indoor location sensing method. Normally sighted subjects were asked to identify the location and facing direction of photos taken by a pedestrian in a building. The subjects had available a floor plan and a small number of representative photos from key locations within the floor plan. Subjects were able to provide accurate location estimates (median location accuracy 3.87ft). This finding indicates that normally sighted subjects, with minimal training, using a simple graphical representation of a floor plan, can provide accurate location estimates based on a single, suitable photo taken by a pedestrian. We conclude that indoor localization is possible using remote, crowdsourced, human assistance. This method has the potential to be used for the location-sensing component of an indoor navigation aid for people with visual impairment.},
journal = {ACM Trans. Appl. Percept.},
month = jul,
articleno = {19},
numpages = {14},
keywords = {visual impairment, low vision, location sensing, indoor navigation, crowdsourcing, blind, Wayfinding}
}

@article{10.14778/3352063.3352117,
author = {Lakshmanan, Laks V. S. and Simpson, Michael and Thirumuruganathan, Saravanan},
title = {Combating fake news: a data management and mining perspective},
year = {2019},
issue_date = {August 2019},
publisher = {VLDB Endowment},
volume = {12},
number = {12},
issn = {2150-8097},
url = {https://doi.org/10.14778/3352063.3352117},
doi = {10.14778/3352063.3352117},
abstract = {Fake news is a major threat to global democracy resulting in diminished trust in government, journalism and civil society. The public popularity of social media and social networks has caused a contagion of fake news where conspiracy theories, disinformation and extreme views flourish. Detection and mitigation of fake news is one of the fundamental problems of our times and has attracted widespread attention. While fact checking websites such as snopes, politifact and major companies such as Google, Facebook, and Twitter have taken preliminary steps towards addressing fake news, much more remains to be done. As an interdisciplinary topic, various facets of fake news have been studied by communities as diverse as machine learning, databases, journalism, political science and many more.The objective of this tutorial is two-fold. First, we wish to familiarize the database community with the efforts by other communities on combating fake news. We provide a panoramic view of the state-of-the-art of research on various aspects including detection, propagation, mitigation, and intervention of fake news. Next, we provide a concise and intuitive summary of prior research by the database community and discuss how it could be used to counteract fake news. The tutorial covers research from areas such as data integration, truth discovery and fusion, probabilistic databases, knowledge graphs and crowdsourcing from the lens of fake news. Effective tools for addressing fake news could only be built by leveraging the synergistic relationship between database and other research communities. We hope that our tutorial provides an impetus towards such synthesis of ideas and the creation of new ones.},
journal = {Proc. VLDB Endow.},
month = aug,
pages = {1990–1993},
numpages = {4}
}

@article{10.1145/3133327,
author = {Prandi, Catia and Mirri, Silvia and Ferretti, Stefano and Salomoni, Paola},
title = {On the Need of Trustworthy Sensing and Crowdsourcing for Urban Accessibility in Smart City},
year = {2017},
issue_date = {February 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {18},
number = {1},
issn = {1533-5399},
url = {https://doi.org/10.1145/3133327},
doi = {10.1145/3133327},
abstract = {Mobility in urban environments is an undisputed key factor that can affect citizens’ well-being and quality of life. This is particularly relevant for those people with disabilities or with reduced mobility who have to face the presence of barriers in urban areas. In this scenario, the availability of information about such architectural elements (together with facilities) can greatly support citizens’ mobility by enhancing their independence and their abilities in conducting daily outdoor activities. With this in mind, we have designed and developed mobile Pervasive Accessibility Social Sensing (mPASS), a system that provides users with personalized paths, computed on the basis of their own preferences and needs, with a customizable and accessible interface. The system collects data from crowdsourcing and crowdsensing to map urban and architectural accessibility by providing reliable information coming from different data sources with different levels of trustworthiness. In this context, reliability can be ensured by properly managing crowdsourced and crowdsensed data, combined when possible with authoritative datasets, provided by disability rights organizations and local authorities. To demonstrate this claim, in this article we present our trustworthiness model and discuss results we have obtained by simulations.},
journal = {ACM Trans. Internet Technol.},
month = oct,
articleno = {4},
numpages = {21},
keywords = {Urban Accessibility, Trustworthiness, Crowdsourcing, Crowdsensing, Credibility}
}

@article{10.14778/3402755.3402777,
author = {Feng, Amber and Franklin, Michael and Kossmann, Donald and Kraska, Tim and Madden, Samuel and Ramesh, Sukriti and Wang, Andrew and Xin, Reynold},
title = {CrowdDB: query processing with the VLDB crowd},
year = {2011},
issue_date = {August 2011},
publisher = {VLDB Endowment},
volume = {4},
number = {12},
issn = {2150-8097},
url = {https://doi.org/10.14778/3402755.3402777},
doi = {10.14778/3402755.3402777},
abstract = {Databases often give incorrect answers when data are missing or semantic understanding of the data is required. Processing such queries requires human input for providing the missing information, for performing computationally difficult functions, and for matching, ranking, or aggregating results based on fuzzy criteria. In this demo we present CrowdDB, a hybrid database system that automatically uses crowdsourcing to integrate human input for processing queries that a normal database system cannot answer.CrowdDB uses SQL both as a language to ask complex queries and as a way to model data stored electronically and provided by human input. Furthermore, queries are automatically compiled and optimized. Special operators provide user interfaces in order to integrate and cleanse human input. Currently CrowdDB supports two crowdsourcing platforms: Amazon Mechanical Turk and our own mobile phone platform. During the demo, the mobile platform will allow the VLDB crowd to participate as workers and help answer otherwise impossible queries.},
journal = {Proc. VLDB Endow.},
month = aug,
pages = {1387–1390},
numpages = {4}
}

@article{10.1145/3284934,
author = {R\'{\i}os, Julio C\'{e}sar Cort\'{e}s and Paton, Norman W. and Fernandes, Alvaro A. A. and Abel, Edward and Keane, John A.},
title = {Crowdsourced Targeted Feedback Collection for Multicriteria Data Source Selection},
year = {2019},
issue_date = {March 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {11},
number = {1},
issn = {1936-1955},
url = {https://doi.org/10.1145/3284934},
doi = {10.1145/3284934},
abstract = {A multicriteria data source selection (MCSS) scenario identifies, from a set of candidate data sources, the subset that best meets users’ needs. These needs are expressed using several criteria, which are used to evaluate the candidate data sources. An MCSS problem can be solved using multidimensional optimization techniques that trade off the different objectives. Sometimes one may have uncertain knowledge regarding how well the candidate data sources meet the criteria. In order to overcome this uncertainty, one may rely on end-users or crowds to annotate the data items produced by the sources in relation to the selection criteria. In this article, a proposed Targeted Feedback Collection (TFC) approach is introduced that aims to identify those data items on which feedback should be collected, thereby providing evidence on how the sources satisfy the required criteria. The proposed TFC targets feedback by considering the confidence intervals around the estimated criteria values, with a view to increasing the confidence in the estimates that are most relevant to the multidimensional optimization. Variants of the proposed TFC approach have been developed for use where feedback is expected to be reliable (e.g., where it is provided by trusted experts) and where feedback is expected to be unreliable (e.g., from crowd workers). Both variants have been evaluated, and positive results are reported against other approaches to feedback collection, including active learning, in experiments that involve real-world datasets and crowdsourcing.},
journal = {J. Data and Information Quality},
month = jan,
articleno = {2},
numpages = {27},
keywords = {uncertainty handling, single-objective optimization, pay as you go, multiobjective optimization, feedback collection, crowdsourcing, Source selection}
}

@article{10.1145/2897367,
author = {Boer, Patrick M. De and Bernstein, Abraham},
title = {PPLib: Toward the Automated Generation of Crowd Computing Programs Using Process Recombination and Auto-Experimentation},
year = {2016},
issue_date = {July 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {4},
issn = {2157-6904},
url = {https://doi.org/10.1145/2897367},
doi = {10.1145/2897367},
abstract = {Crowdsourcing is increasingly being adopted to solve simple tasks such as image labeling and object tagging, as well as more complex tasks, where crowd workers collaborate in processes with interdependent steps. For the whole range of complexity, research has yielded numerous patterns for coordinating crowd workers in order to optimize crowd accuracy, efficiency, and cost. Process designers, however, often don't know which pattern to apply to a problem at hand when designing new applications for crowdsourcing.In this article, we propose to solve this problem by systematically exploring the design space of complex crowdsourced tasks via automated recombination and auto-experimentation for an issue at hand. Specifically, we propose an approach to finding the optimal process for a given problem by defining the deep structure of the problem in terms of its abstract operators, generating all possible alternatives via the (re)combination of the abstract deep structure with concrete implementations from a Process Repository, and then establishing the best alternative via auto-experimentation.To evaluate our approach, we implemented PPLib (pronounced “People Lib”), a program library that allows for the automated recombination of known processes stored in an easily extensible Process Repository. We evaluated our work by generating and running a plethora of process candidates in two scenarios on Amazon's Mechanical Turk followed by a meta-evaluation, where we looked at the differences between the two evaluations. Our first scenario addressed the problem of text translation, where our automatic recombination produced multiple processes whose performance almost matched the benchmark established by an expert translation. In our second evaluation, we focused on text shortening; we automatically generated 41 crowd process candidates, among them variations of the well-established Find-Fix-Verify process. While Find-Fix-Verify performed well in this setting, our recombination engine produced five processes that repeatedly yielded better results. We close the article by comparing the two settings where the Recombinator was used, and empirically show that the individual processes performed differently in the two settings, which led us to contend that there is no unifying formula, hence emphasizing the necessity for recombination.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = apr,
articleno = {49},
numpages = {20},
keywords = {Human computation algorithms}
}

@article{10.1145/3324300,
author = {Hafizo\u{g}lu, Feyza Merve and Sen, Sandip},
title = {Understanding the Influences of Past Experience on Trust in Human-agent Teamwork},
year = {2019},
issue_date = {November 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {19},
number = {4},
issn = {1533-5399},
url = {https://doi.org/10.1145/3324300},
doi = {10.1145/3324300},
abstract = {People use the knowledge acquired from past experiences in assessing the trustworthiness of a trustee. In a time where the agents are being increasingly accepted as partners in collaborative efforts and activities, it is critical to understand all aspects of human trust development in agent partners. For human-agent virtual ad hoc teams to be effective, humans must be able to trust their agent counterparts. To earn the humans’ trust, agents need to quickly develop an understanding of the expectation of human team members and adapt accordingly. This study empirically investigates the impact of past experience on human trust in and reliance on agent teammates. To do so, we developed a team coordination game, the Game of Trust (GoT), in which two players repeatedly cooperate to complete team tasks without prior assignment of subtasks. The effects of past experience on human trust are evaluated by performing an extensive set of controlled experiments with participants recruited from Amazon Mechanical Turk, a crowdsourcing marketplace. We collect both teamwork performance data as well as surveys to gauge participants’ trust in their agent teammates. The results show that positive (negative) past experience increases (decreases) human trust in agent teammates; lack of past experience leads to higher trust levels compared to positive past experience; positive (negative) past experience facilitates (hinders) reliance on agent teammates; the relationship between trust in and reliance on agent teammates is not always correlated. These findings provide clear and significant evidence of the influence of key factors on human trust in virtual agent teammates and enhance our understanding of the changes in human trust in peer-level agent teammates with respect to past experience.},
journal = {ACM Trans. Internet Technol.},
month = sep,
articleno = {45},
numpages = {22},
keywords = {trust, reliance, past experience, Human-agent teamwork}
}

@article{10.1145/3193107,
author = {Lau, Manfred and Dev, Kapil and Dorsey, Julie and Rushmeier, Holly},
title = {A Human-Perceived Softness Measure of Virtual 3D Objects},
year = {2018},
issue_date = {July 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {15},
number = {3},
issn = {1544-3558},
url = {https://doi.org/10.1145/3193107},
doi = {10.1145/3193107},
abstract = {We introduce the problem of computing a human-perceived softness measure for virtual 3D objects. As the virtual objects do not exist in the real world, we do not directly consider their physical properties but instead compute the human-perceived softness of the geometric shapes. In an initial experiment, we find that humans are highly consistent in their responses when given a pair of vertices on a 3D model and asked to select the vertex that they perceive to be more soft. This motivates us to take a crowdsourcing and machine learning framework. We collect crowdsourced data for such pairs of vertices. We then combine a learning-to-rank approach and a multi-layer neural network to learn a non-linear softness measure mapping any vertex to a softness value. For a new 3D shape, we can use the learned measure to compute the relative softness of every vertex on its surface. We demonstrate the robustness of our framework with a variety of 3D shapes and compare our non-linear learning approach with a linear method from previous work. Finally, we demonstrate the accuracy of our learned measure with user studies comparing our measure with the human-perceived softness of both virtual and real objects, and we show the usefulness of our measure with some applications.},
journal = {ACM Trans. Appl. Percept.},
month = jun,
articleno = {19},
numpages = {18},
keywords = {learning, fabrication, crowdsourcing, 3D modeling}
}

@article{10.14778/2350229.2350263,
author = {Wang, Jiannan and Kraska, Tim and Franklin, Michael J. and Feng, Jianhua},
title = {CrowdER: crowdsourcing entity resolution},
year = {2012},
issue_date = {July 2012},
publisher = {VLDB Endowment},
volume = {5},
number = {11},
issn = {2150-8097},
url = {https://doi.org/10.14778/2350229.2350263},
doi = {10.14778/2350229.2350263},
abstract = {Entity resolution is central to data integration and data cleaning. Algorithmic approaches have been improving in quality, but remain far from perfect. Crowdsourcing platforms offer a more accurate but expensive (and slow) way to bring human insight into the process. Previous work has proposed batching verification tasks for presentation to human workers but even with batching, a human-only approach is infeasible for data sets of even moderate size, due to the large numbers of matches to be tested. Instead, we propose a hybrid human-machine approach in which machines are used to do an initial, coarse pass over all the data, and people are used to verify only the most likely matching pairs. We show that for such a hybrid system, generating the minimum number of verification tasks of a given size is NP-Hard, but we develop a novel two-tiered heuristic approach for creating batched tasks. We describe this method, and present the results of extensive experiments on real data sets using a popular crowdsourcing platform. The experiments show that our hybrid approach achieves both good efficiency and high accuracy compared to machine-only or human-only alternatives.},
journal = {Proc. VLDB Endow.},
month = jul,
pages = {1483–1494},
numpages = {12}
}

@article{10.1145/3177882,
author = {Salehi, Niloufar and Bernstein, Michael S.},
title = {Ink: Increasing Worker Agency to Reduce Friction in Hiring Crowd Workers},
year = {2018},
issue_date = {April 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {25},
number = {2},
issn = {1073-0516},
url = {https://doi.org/10.1145/3177882},
doi = {10.1145/3177882},
abstract = {The web affords connections by which end-users can receive paid, expert help—such as programming, design, and writing—to reach their goals. While a number of online marketplaces have emerged to facilitate such connections, most end-users do not approach a market to hire an expert when faced with a challenge. To reduce friction in hiring from peer-to-peer expert crowd work markets, we propose Ink, a system that crowd workers can use to showcase their services by embedding tasks inside web tutorials—a common destination for users with information needs. Workers have agency to define and manage tasks, through which users can request their help to review or execute each step of the tutorial, for example, to give feedback on a paper outline, perform a statistical analysis, or host a practice programming interview. In a public deployment, over 25,000 pageviews led 168 tutorial readers to pay crowd workers for their services, most of whom had not previously hired from crowdsourcing marketplaces. A field experiment showed that users were more likely to hire crowd experts when the task was embedded inside the tutorial rather than when they were redirected to the same worker’s Upwork profile to hire them. Qualitative analysis of interviews showed that Ink framed hiring expert crowd workers within users’ well-established information seeking habits and gave workers more control over their work.},
journal = {ACM Trans. Comput.-Hum. Interact.},
month = apr,
articleno = {10},
numpages = {17},
keywords = {crowdsourcing, Social computing}
}

@article{10.5555/3381569.3381601,
author = {Talukder, Sajedul},
title = {AbuSniff: an automated social network abuse detection system},
year = {2019},
issue_date = {October 2019},
publisher = {Consortium for Computing Sciences in Colleges},
address = {Evansville, IN, USA},
volume = {35},
number = {3},
issn = {1937-4771},
abstract = {In our research, we sought to develop an automated social network abuse detection system which is able to reduce the attack surface of its users, by reducing the number of, or isolating friends predicted to be perceived as potential attack vectors. This presents a substantial challenge as adversaries leverage social network friend relationships to collect sensitive data from users and target them with abuse that includes profile cloning, stalking, identity theft, fake news, cyberbullying, malware, and propaganda. We leverage these findings to develop AbuSniff (Abuse from Social Network Friends), a system that evaluates, predicts and protects users against perceived friend abuse by suggesting several personalized defensive actions for such friends. We began by developing the first ever mobile app questionnaire, that can detect perceived strangers and friend abusers. To replace the questionnaire, we then introduced mutual Facebook activity features that have statistically significant overall association with the AbuSniff decision and showed that they can train supervised learning algorithms to predict questionnaire responses using 10- fold cross validation. We trained our system with several supervised learning algorithms, including Random Forest (RF), Decision Trees (DT), SVM, PART, SimpleLogistic, MultiClassClassifier, K-Nearest Neighbors (KNN) and Naive Bayes and chose the best performing algorithm for predicting each of the questionnaire questions. Our approach provides a method to evaluate AbuSniff system through online experiments with participants recruited from the crowdsourcing site from 25 countries across 6 continents. Results showed that the predictive version of AbuSniff was highly accurate (F-Measure up-to 97.3\%) in predicting strangers or abusive friends and participants agreed to take the AbuSniff suggested actions in 78\% of the cases. When compared to a control app, AbuSniff significantly increased the participant self-reported willingness to reject invitations from strangers and abusers, their awareness of friend abuse implications and their perceived protection from friend abuse.},
journal = {J. Comput. Sci. Coll.},
month = oct,
pages = {209–210},
numpages = {2}
}

@article{10.1145/2897824.2925981,
author = {Streuber, Stephan and Quiros-Ramirez, M. Alejandra and Hill, Matthew Q. and Hahn, Carina A. and Zuffi, Silvia and O'Toole, Alice and Black, Michael J.},
title = {Body talk: crowdshaping realistic 3D avatars with words},
year = {2016},
issue_date = {July 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {35},
number = {4},
issn = {0730-0301},
url = {https://doi.org/10.1145/2897824.2925981},
doi = {10.1145/2897824.2925981},
abstract = {Realistic, metrically accurate, 3D human avatars are useful for games, shopping, virtual reality, and health applications. Such avatars are not in wide use because solutions for creating them from high-end scanners, low-cost range cameras, and tailoring measurements all have limitations. Here we propose a simple solution and show that it is surprisingly accurate. We use crowdsourcing to generate attribute ratings of 3D body shapes corresponding to standard linguistic descriptions of 3D shape. We then learn a linear function relating these ratings to 3D human shape parameters. Given an image of a new body, we again turn to the crowd for ratings of the body shape. The collection of linguistic ratings of a photograph provides remarkably strong constraints on the metric 3D shape. We call the process crowdshaping and show that our Body Talk system produces shapes that are perceptually indistinguishable from bodies created from high-resolution scans and that the metric accuracy is sufficient for many tasks. This makes body "scanning" practical without a scanner, opening up new applications including database search, visualization, and extracting avatars from books.},
journal = {ACM Trans. Graph.},
month = jul,
articleno = {54},
numpages = {14},
keywords = {perception, human body modeling, crowdsourcing, body shape, avatars, anthropometry, 3D shape}
}

@article{10.5555/3128489.3128560,
author = {Nguyen, Quoc Viet and Duong, Chi Thang and Nguyen, Thanh Tam and Weidlich, Matthias and Aberer, Karl and Yin, Hongzhi and Zhou, Xiaofang},
title = {Argument discovery via crowdsourcing},
year = {2017},
issue_date = {August    2017},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {26},
number = {4},
issn = {1066-8888},
abstract = {The amount of controversial issues being discussed on the Web has been growing dramatically. In articles, blogs, and wikis, people express their points of view in the form of arguments, i.e., claims that are supported by evidence. Discovery of arguments has a large potential for informing decision-making. However, argument discovery is hindered by the sheer amount of available Web data and its unstructured, free-text representation. The former calls for automatic text-mining approaches, whereas the latter implies a need for manual processing to extract the structure of arguments. In this paper, we propose a crowdsourcing-based approach to build a corpus of arguments, an argumentation base, thereby mediating the trade-off of automatic text-mining and manual processing in argument discovery. We develop an end-to-end process that minimizes the crowd cost while maximizing the quality of crowd answers by: (1) ranking argumentative texts, (2) pro-actively eliciting user input to extract arguments from these texts, and (3) aggregating heterogeneous crowd answers. Our experiments with real-world datasets highlight that our method discovers virtually all arguments in documents when processing only 25\% of the text with more than 80\% precision, using only 50\% of the budget consumed by a baseline algorithm.},
journal = {The VLDB Journal},
month = aug,
pages = {511–535},
numpages = {25},
keywords = {Web mining, Graphical models, Crowdsourcing}
}

@article{10.14778/3297753.3297758,
author = {Dolatshah, Mohamad and Teoh, Mathew and Wang, Jiannan and Pei, Jian},
title = {Cleaning crowdsourced labels using oracles for statistical classification},
year = {2018},
issue_date = {December 2018},
publisher = {VLDB Endowment},
volume = {12},
number = {4},
issn = {2150-8097},
url = {https://doi.org/10.14778/3297753.3297758},
doi = {10.14778/3297753.3297758},
abstract = {Nowadays, crowdsourcing is being widely used to collect training data for solving classification problems. However, crowdsourced labels are often noisy, and there is a performance gap between classification with noisy labels and classification with ground-truth labels. In this paper, we consider how to apply oracle-based label cleaning to reduce the gap. We propose TARS, a label-cleaning advisor that can provide two pieces of valuable advice for data scientists when they need to train or test a model using noisy labels. Firstly, in the model testing stage, given a test dataset with noisy labels, and a classification model, TARS can use the test data to estimate how well the model will perform w.r.t. ground-truth labels. Secondly, in the model training stage, given a training dataset with noisy labels, and a classification algorithm, TARS can determine which label should be sent to an oracle to clean such that the model can be improved the most. For the first advice, we propose an effective estimation technique, and study how to compute confidence intervals to bound its estimation error. For the second advice, we propose a novel cleaning strategy along with two optimization techniques, and illustrate that it is superior to the existing cleaning strategies. We evaluate TARS on both simulated and real-world datasets. The results show that (1) TARS can use noisy test data to accurately estimate a model's true performance for various evaluation metrics; and (2) TARS can improve the model accuracy by a larger margin than the existing cleaning strategies, for the same cleaning budget.},
journal = {Proc. VLDB Endow.},
month = dec,
pages = {376–389},
numpages = {14}
}

@article{10.1145/2897824.2925927,
author = {Lau, Manfred and Dev, Kapil and Shi, Weiqi and Dorsey, Julie and Rushmeier, Holly},
title = {Tactile mesh saliency},
year = {2016},
issue_date = {July 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {35},
number = {4},
issn = {0730-0301},
url = {https://doi.org/10.1145/2897824.2925927},
doi = {10.1145/2897824.2925927},
abstract = {While the concept of visual saliency has been previously explored in the areas of mesh and image processing, saliency detection also applies to other sensory stimuli. In this paper, we explore the problem of tactile mesh saliency, where we define salient points on a virtual mesh as those that a human is more likely to grasp, press, or touch if the mesh were a real-world object. We solve the problem of taking as input a 3D mesh and computing the relative tactile saliency of every mesh vertex. Since it is difficult to manually define a tactile saliency measure, we introduce a crowdsourcing and learning framework. It is typically easy for humans to provide relative rankings of saliency between vertices rather than absolute values. We thereby collect crowdsourced data of such relative rankings and take a learning-to-rank approach. We develop a new formulation to combine deep learning and learning-to-rank methods to compute a tactile saliency measure. We demonstrate our framework with a variety of 3D meshes and various applications including material suggestion for rendering and fabrication.},
journal = {ACM Trans. Graph.},
month = jul,
articleno = {52},
numpages = {11},
keywords = {saliency, perception, fabrication material suggestion, deep learning, crowdsourcing}
}

@article{10.1145/3239574,
author = {Kiesel, Johannes and Kneist, Florian and Alshomary, Milad and Stein, Benno and Hagen, Matthias and Potthast, Martin},
title = {Reproducible Web Corpora: Interactive Archiving with Automatic Quality Assessment},
year = {2018},
issue_date = {December 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {10},
number = {4},
issn = {1936-1955},
url = {https://doi.org/10.1145/3239574},
doi = {10.1145/3239574},
abstract = {The evolution of web pages from static HTML pages toward dynamic pieces of software has rendered archiving them increasingly difficult. Nevertheless, an accurate, reproducible web archive is a necessity to ensure the reproducibility of web-based research. Archiving web pages reproducibly, however, is currently not part of best practices for web corpus construction. As a result, and despite the ongoing efforts of other stakeholders to archive the web, tools for the construction of reproducible web corpora are insufficient or ill-fitted. This article presents a new tool tailored to this purpose. It relies on emulating user interactions with a web page while recording all network traffic. The customizable user interactions can be replayed on demand, while requests sent by the archived page are served with the recorded responses. The tool facilitates reproducible user studies, user simulations, and evaluations of algorithms that rely on extracting data from web pages. To evaluate our tool, we conduct the first systematic assessment of reproduction quality for rendered web pages. Using our tool, we create a corpus of 10,000&nbsp;web pages carefully sampled from the Common Crawl and manually annotated with regard to reproduction quality via crowdsourcing. Based on this data, we test three approaches to automatic reproduction-quality assessment. An off-the-shelf neural network, trained on visual differences between the web page during archiving and reproduction, matches the manual assessments best. This automatic assessment of reproduction quality allows for immediate bugfixing during archiving and continuous development of our tool as the web continues to evolve.},
journal = {J. Data and Information Quality},
month = oct,
articleno = {17},
numpages = {25},
keywords = {web collection, reproduction quality prediction, reproduction quality, Web archive}
}

@article{10.5555/3176764.3176778,
author = {Pappas, Nikolaos and Popescu-Belis, Andrei},
title = {Explicit document modeling through weighted multiple-instance learning},
year = {2017},
issue_date = {January 2017},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {58},
number = {1},
issn = {1076-9757},
abstract = {Representing documents is a crucial component in many NLP tasks, for instance predicting aspect ratings in reviews. Previous methods for this task treat documents globally, and do not acknowledge that target categories are often assigned by their authors with generally no indication of the specific sentences that motivate them. To address this issue, we adopt a weakly supervised learning model, which jointly learns to focus on relevant parts of a document according to the context along with a classifier for the target categories. Derived from the weighted multiple-instance regression (MIR) framework, the model learns decomposable document vectors for each individual category and thus overcomes the representational bottleneck in previous methods due to a fixed-length document vector. During prediction, the estimated relevance or saliency weights explicitly capture the contribution of each sentence to the predicted rating, thus offering an explanation of the rating. Our model achieves state-of-the-art performance on multi-aspect sentiment analysis, improving over several baselines. Moreover, the predicted saliency weights are close to human estimates obtained by crowdsourcing, and increase the performance of lexical and topical features for review segmentation and summarization.},
journal = {J. Artif. Int. Res.},
month = jan,
pages = {591–626},
numpages = {36}
}

@article{10.1145/3134754,
author = {Zhang, Ark Fangzhou and Livneh, Danielle and Budak, Ceren and Robert, Lionel P. and Romero, Daniel M.},
title = {Crowd Development: The Interplay between Crowd Evaluation and Collaborative Dynamics in Wikipedia},
year = {2017},
issue_date = {November 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {1},
number = {CSCW},
url = {https://doi.org/10.1145/3134754},
doi = {10.1145/3134754},
abstract = {Collaborative crowdsourcing is an increasingly common way of accomplishing work in our economy. Yet, we know very little about how the behavior of these crowds changes over time and how these dynamics impact their performance. In this paper, we take a group development approach that considers how the behavior of crowds change over time in anticipation and as a result of their evaluation and recognition. Towards this goal, this paper studies the collaborative behavior of groups comprised of editors of articles that have been recognized for their outstanding quality and given the Good Articles (GA) status and those that eventually become Featured Articles (FA) on Wikipedia. The results show that the collaborative behavior of GA groups radically changes just prior to their nomination. In particular, the GA groups experience increases in the level of activity, centralization of workload, and level of GA experience and decreases in conflict (i.e., reverts) among editors. After being promoted to GA, they converge back to their typical behavior and composition. This indicates that crowd behavior prior to their evaluation period is dramatically different than behavior before or after. In addition, the collaborative behaviors of crowds during their promotion to GA are predictive of whether they are eventually promoted to FA. Our findings shed new light on the importance of time in understanding the relationship between crowd performance and collaborative measures such as centralization, conflict and experience.},
journal = {Proc. ACM Hum.-Comput. Interact.},
month = dec,
articleno = {119},
numpages = {21}
}

@article{10.1145/3043948,
author = {Teng, Xiaoqiang and Guo, Deke and Guo, Yulan and Zhou, Xiaolei and Ding, Zeliu and Liu, Zhong},
title = {IONavi: An Indoor-Outdoor Navigation Service via Mobile Crowdsensing},
year = {2017},
issue_date = {May 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {13},
number = {2},
issn = {1550-4859},
url = {https://doi.org/10.1145/3043948},
doi = {10.1145/3043948},
abstract = {The proliferation of mobile computing has prompted navigation to be one of the most attractive and promising applications. Conventional designs of navigation systems mainly focus on either indoor or outdoor navigation. However, people have a strong need for navigation from a large open indoor environment to an outdoor destination in real life. This article presents IONavi, a joint navigation solution, which can enable passengers to easily deploy indoor-outdoor navigation service for subway transportation systems in a crowdsourcing way. Any self-motivated passenger records and shares individual walking traces from a location inside a subway station to an uncertain outdoor destination within a given range, such as one kilometer. IONavi further extracts navigation traces from shared individual traces, each of which is not necessary to be accurate. A subsequent following user achieves indoor-outdoor navigation services by tracking a recommended navigation trace. Extensive experiments are conducted on a subway transportation system. The experimental results indicate that IONavi exhibits outstanding navigation performance from an uncertain location inside a subway station to an outdoor destination. Although IONavi is to enable indoor-outdoor navigation for subway transportation systems, the basic idea can naturally be extended to joint navigation from other open indoor environments to outdoor environments.},
journal = {ACM Trans. Sen. Netw.},
month = apr,
articleno = {12},
numpages = {28},
keywords = {trace clustering, subway station navigation, mobile crowdsensing, indoor localization, Indoor-outdoor navigation}
}

@article{10.1145/3039868,
author = {Tsvetkova, Milena and Yasseri, Taha and Meyer, Eric T. and Pickering, J. Brian and Engen, Vegard and Walland, Paul and L\"{u}ders, Marika and F\o{}lstad, Asbj\o{}rn and Bravos, George},
title = {Understanding Human-Machine Networks: A Cross-Disciplinary Survey},
year = {2017},
issue_date = {January 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {50},
number = {1},
issn = {0360-0300},
url = {https://doi.org/10.1145/3039868},
doi = {10.1145/3039868},
abstract = {In the current hyperconnected era, modern Information and Communication Technology (ICT) systems form sophisticated networks where not only do people interact with other people, but also machines take an increasingly visible and participatory role. Such Human-Machine Networks (HMNs) are embedded in the daily lives of people, both for personal and professional use. They can have a significant impact by producing synergy and innovations. The challenge in designing successful HMNs is that they cannot be developed and implemented in the same manner as networks of machines nodes alone, or following a wholly human-centric view of the network. The problem requires an interdisciplinary approach. Here, we review current research of relevance to HMNs across many disciplines. Extending the previous theoretical concepts of socio-technical systems, actor-network theory, cyber-physical-social systems, and social machines, we concentrate on the interactions among humans and between humans and machines. We identify eight types of HMNs: public-resource computing, crowdsourcing, web search engines, crowdsensing, online markets, social media, multiplayer online games and virtual worlds, and mass collaboration. We systematically select literature on each of these types and review it with a focus on implications for designing HMNs. Moreover, we discuss risks associated with HMNs and identify emerging design and development trends.},
journal = {ACM Comput. Surv.},
month = apr,
articleno = {12},
numpages = {35},
keywords = {social media, peer-to-peer, mass collaboration, human-machine networks, crowdsensing, complex networks, Crowdsourcing}
}

@article{10.1145/3359164,
author = {Chung, John Joon Young and Song, Jean Y. and Kutty, Sindhu and Hong, Sungsoo (Ray) and Kim, Juho and Lasecki, Walter S.},
title = {Efficient Elicitation Approaches to Estimate Collective Crowd Answers},
year = {2019},
issue_date = {November 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {3},
number = {CSCW},
url = {https://doi.org/10.1145/3359164},
doi = {10.1145/3359164},
abstract = {When crowdsourcing the creation of machine learning datasets, statistical distributions that capture diverse answers can represent ambiguous data better than a single best answer. Unfortunately, collecting distributions is expensive because a large number of responses need to be collected to form a stable distribution. Despite this, the efficient collection of answer distributions-that is, ways to use less human effort to collect estimates of the eventual distribution that would be formed by a large group of responses-is an under-studied topic. In this paper, we demonstrate that this type of estimation is possible and characterize different elicitation approaches to guide the development of future systems. We investigate eight elicitation approaches along two dimensions: annotation granularity and estimation perspective. Annotation granularity is varied by annotating i) a single "best" label, ii) all relevant labels, iii) a ranking of all relevant labels, or iv) real-valued weights for all relevant labels. Estimation perspective is varied by prompting workers to either respond with their own answer or an estimate of the answer(s) that they expect other workers would provide. Our study collected ordinal annotations on the emotional valence of facial images from 1,960 crowd workers and found that, surprisingly, the most fine-grained elicitation methods were not the most accurate, despite workers spending more time to provide answers. Instead, the most efficient approach was to ask workers to choose all relevant classes that others would have selected. This resulted in a 21.4\% reduction in the human time required to reach the same performance as the baseline (i.e., selecting a single answer with their own perspective). By analyzing cases in which finer-grained annotations degraded performance, we contribute to a better understanding of the trade-offs between answer elicitation approaches. Our work makes it more tractable to use answer distributions in large-scale tasks such as ML training, and aims to spark future work on techniques that can efficiently estimate answer distributions.},
journal = {Proc. ACM Hum.-Comput. Interact.},
month = nov,
articleno = {62},
numpages = {25},
keywords = {crowdsourcing, answer distributions, annotation, ambiguity}
}

@article{10.1109/TNET.2017.2680448,
author = {Gu, Fei and Niu, Jianwei and Duan, Lingjie},
title = {WAIPO: A Fusion-Based Collaborative Indoor Localization System on Smartphones},
year = {2017},
issue_date = {August 2017},
publisher = {IEEE Press},
volume = {25},
number = {4},
issn = {1063-6692},
url = {https://doi.org/10.1109/TNET.2017.2680448},
doi = {10.1109/TNET.2017.2680448},
abstract = {Indoor localization based on smartphone can enhance user’s experiences in indoor environments. Although some innovative solutions have been proposed in the past two decades, how to accurately and efficiently localize users in indoor environments is still a challenging problem. Traditional indoor positioning systems based on Wi-Fi fingerprints or dead reckoning suffer from the variation of Wi-Fi signals and the drift of dead reckoning problems, respectively. Crowdsourcing and ambient sensing stimulate new ways to improve existing localization systems’ accuracy. Using human social factors to calibrate the accuracy of localization is practical and awarding. In this paper, we propose WAIPO, a collaborative indoor localization system with the fusion of Wi-Fi and magnetic fingerprints, image-matching, and people co-occurrence. Specifically, we could obtain the most likely top-$n$ locations based on Wi-Fi fingerprints. We utilize the statistics of users’ historical locations known by image-matching, for which we propose a photo-room matching algorithm, to reduce estimating areas. In order to further improve the accuracy of localization, we propose a co-occurrence and non-co-occurrence detection algorithm to detect users’ spatial-temporal co-occurrence and determine users’ locations with magnetic calibration. We have fully implemented WAIPO on the Android platform and perform testbed experiments. The experimental results demonstrate that WAIPO achieves an accuracy of 87.3\% on average, which outperforms the state-of-the-art indoor localization systems.},
journal = {IEEE/ACM Trans. Netw.},
month = aug,
pages = {2267–2280},
numpages = {14}
}

@article{10.1162/COLI_a_00182,
author = {Riezler, Stefan},
title = {On the problem of theoretical terms in empirical computational linguistics},
year = {2014},
issue_date = {March 2014},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
volume = {40},
number = {1},
issn = {0891-2017},
url = {https://doi.org/10.1162/COLI_a_00182},
doi = {10.1162/COLI_a_00182},
abstract = {Philosophy of science has pointed out a problem of theoretical terms in empirical sciences. This problem arises if all known measuring procedures for a quantity of a theory presuppose the validity of this very theory, because then statements containing theoretical terms are circular. We argue that a similar circularity can happen in empirical computational linguistics, especially in cases where data are manually annotated by experts. We define a criterion of T-non-theoretical grounding as guidance to avoid such circularities, and exemplify how this criterion can be met by crowdsourcing, by task-related data annotation, or by data in the wild. We argue that this criterion should be considered as a necessary condition for an empirical science, in addition to measures for reliability of data annotation.},
journal = {Comput. Linguist.},
month = mar,
pages = {235–245},
numpages = {11}
}

@article{10.14778/3007263.3007290,
author = {Amsterdamer, Yael and Milo, Tova and Somech, Amit and Youngmann, Brit},
title = {December: a declarative tool for crowd member selection},
year = {2016},
issue_date = {September 2016},
publisher = {VLDB Endowment},
volume = {9},
number = {13},
issn = {2150-8097},
url = {https://doi.org/10.14778/3007263.3007290},
doi = {10.14778/3007263.3007290},
abstract = {Adequate crowd selection is an important factor in the success of crowdsourcing platforms, increasing the quality and relevance of crowd answers and their performance in different tasks. The optimal crowd selection can greatly vary depending on properties of the crowd and of the task. To this end, we present December, a declarative platform with novel capabilities for flexible crowd selection. December supports the personalized selection of crowd members via a dedicated query language Member-QL. This language enables specifying and combining common crowd selection criteria such as properties of a crowd member's profile and history, similarity between profiles in specific aspects and relevance of the member to a given task. This holistic, customizable approach differs from previous work that has mostly focused on dedicated algorithms for crowd selection in specific settings. To allow efficient query execution, we implement novel algorithms in December based on our generic, semantically-aware definitions of crowd member similarity and expertise.We demonstrate the effectiveness of December and Member-QL by using the VLDB community as crowd members and allowing conference participants to choose from among these members for different purposes and in different contexts.},
journal = {Proc. VLDB Endow.},
month = sep,
pages = {1485–1488},
numpages = {4}
}

@article{10.14778/2350229.2350264,
author = {Cao, Caleb Chen and She, Jieying and Tong, Yongxin and Chen, Lei},
title = {Whom to ask? jury selection for decision making tasks on micro-blog services},
year = {2012},
issue_date = {July 2012},
publisher = {VLDB Endowment},
volume = {5},
number = {11},
issn = {2150-8097},
url = {https://doi.org/10.14778/2350229.2350264},
doi = {10.14778/2350229.2350264},
abstract = {It is universal to see people obtain knowledge on micro-blog services by asking others decision making questions. In this paper, we study the Jury Selection Problem(JSP) by utilizing crowdsourcing for decision making tasks on micro-blog services. Specifically, the problem is to enroll a subset of crowd under a limited budget, whose aggregated wisdom via Majority Voting scheme has the lowest probability of drawing a wrong answer(Jury Error Rate-JER).Due to various individual error-rates of the crowd, the calculation of JER is non-trivial. Firstly, we explicitly state that JER is the probability when the number of wrong jurors is larger than half of the size of a jury. To avoid the exponentially increasing calculation of JER, we propose two efficient algorithms and an effective bounding technique. Furthermore, we study the Jury Selection Problem on two crowdsourcing models, one is for altruistic users(AltrM) and the other is for incentive-requiring users(PayM) who require extra payment when enrolled into a task. For the AltrM model, we prove the monotonicity of JER on individual error rate and propose an efficient exact algorithm for JSP. For the PayM model, we prove the NP-hardness of JSP on PayM and propose an efficient greedy-based heuristic algorithm. Finally, we conduct a series of experiments to investigate the traits of JSP, and validate the efficiency and effectiveness of our proposed algorithms on both synthetic and real micro-blog data.},
journal = {Proc. VLDB Endow.},
month = jul,
pages = {1495–1506},
numpages = {12}
}

@article{10.1145/3230670,
author = {Can, G\"{u}lcan and Odobez, Jean-Marc and Gatica-Perez, Daniel},
title = {How to Tell Ancient Signs Apart? Recognizing and Visualizing Maya Glyphs with CNNs},
year = {2018},
issue_date = {December 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {11},
number = {4},
issn = {1556-4673},
url = {https://doi.org/10.1145/3230670},
doi = {10.1145/3230670},
abstract = {Thanks to the digital preservation of cultural heritage materials, multimedia tools (e.g., based on automatic visual processing) considerably ease the work of scholars in the humanities and help them to perform quantitative analysis of their data. In this context, this article assesses three different Convolutional Neural Network (CNN) architectures along with three learning approaches to train them for hieroglyph classification, which is a very challenging task due to the limited availability of segmented ancient Maya glyphs. More precisely, the first approach, the baseline, relies on pretrained networks as feature extractor. The second one investigates a transfer learning method by fine-tuning a pretrained network for our glyph classification task. The third approach considers directly training networks from scratch with our glyph data. The merits of three different network architectures are compared: a generic sequential model (i.e., LeNet), a sketch-specific sequential network (i.e., Sketch-a-Net), and the recent Residual Networks. The sketch-specific model trained from scratch outperforms other models and training strategies. Even for a challenging 150-class classification task, this model achieves 70.3\% average accuracy and proves itself promising in case of a small amount of cultural heritage shape data. Furthermore, we visualize the discriminative parts of glyphs with the recent Grad-CAM method, and demonstrate that the discriminative parts learned by the model agree, in general, with the expert annotation of the glyph specificity (diagnostic features). Finally, as a step toward systematic evaluation of these visualizations, we conduct a perceptual crowdsourcing study. Specifically, we analyze the interpretability of the representations from Sketch-a-Net and ResNet-50. Overall, our article takes two important steps toward providing tools to scholars in the digital humanities: increased performance for automation and improved interpretability of algorithms.},
journal = {J. Comput. Cult. Herit.},
month = dec,
articleno = {20},
numpages = {25},
keywords = {transfer learning, shape recognition, crowdsourcing, convolutional neural networks, Maya glyphs}
}

@article{10.1145/3130942,
author = {Liu, Ruilin and Yang, Yu and Kwak, Daehan and Zhang, Desheng and Iftode, Liviu and Nath, Badri},
title = {Your Search Path Tells Others Where to Park: Towards Fine-Grained Parking Availability Crowdsourcing Using Parking Decision Models},
year = {2017},
issue_date = {September 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {1},
number = {3},
url = {https://doi.org/10.1145/3130942},
doi = {10.1145/3130942},
abstract = {A main challenge faced by the state-of-the-art parking sensing systems is to infer the state of the spots not covered by participants’ parking/unparking events (called background availability) when the system penetration rate is limited. In this paper, we tackle this problem by exploring an empirical phenomenon that ignoring a spot along a driver’s parking search trajectory is likely due to the unavailability. However, complications caused by drivers’ preferences, e.g. ignoring the spots too far from the driver’s destination, have to be addressed based on human parking decisions. We build a model based on a dataset of more than 55,000 real parking decisions to predict the probability that a driver would take a spot, assuming the spot is available. Then, we present a crowdsourcing system, called ParkScan, which leverages the learned parking decision model in collaboration with the hidden Markov model to estimate background parking spot availability. We evaluated ParkScan with real-world data from both off-street scenarios (i.e., two public parking lots) and an on-street parking scenario (i.e., 35 urban blocks in Seattle). Both of the experiments showed that with a 5\% penetration rate, ParkScan reduces over 12.9\% of availability estimation errors for all the spots during parking peak hours, compared to the baseline using only the historical data. Also, even with a single participant driver, ParkScan cuts off at least 15\% of the estimation errors for the spots along the driver’s parking search trajectory.},
journal = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
month = sep,
articleno = {78},
numpages = {27},
keywords = {Parking, Mobile Sensing, Human Decision Modeling, Crowdsourcing}
}

@article{10.1145/3170430,
author = {Chu, Cing-Yu and Chen, Shannon and Yen, Yu-Chuan and Yeh, Su-Ling and Chu, Hao-Hua and Huang, Polly},
title = {EQ: A QoE-Centric Rate Control Mechanism for VoIP Calls},
year = {2018},
issue_date = {March 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {3},
number = {1},
issn = {2376-3639},
url = {https://doi.org/10.1145/3170430},
doi = {10.1145/3170430},
abstract = {The rising popularity of data calls and the slowed global economy have posed a challenge to voice data networking—how to satisfy the growing user demand for VoIP calls under limited network resources. In a bandwidth-constrained network in particular, raising the bitrate for one call implies a lowered bitrate for another. Therefore, knowing whether it is worthwhile to raise one call's bitrate while other users might complain is crucial to the design of a user-centric rate control mechanism. To this end, previous work (Chen et al. 2012) has reported a log-like relationship between bitrate and user experience (i.e., QoE) in Skype calls. To show that the relationship extends to more general VoIP calls, we conduct a 60-participant user study via the Amazon Mechanical Turk crowdsourcing platform and reaffirm the log-like relationship between the call bitrate and user experience in widely used AMR-WB. The relationship gives rise to a simple and practical rate control scheme that exponentially quantizes the steps of rate change, therefore the name—exponential quantization (EQ). To support that EQ is effective in addressing the challenge, we show through a formal analysis that the resulting bandwidth allocation is optimal in both the overall QoE and the number of calls served. To relate EQ to existing rate control mechanisms, we show in a simulation study that the bitrates of calls administered by EQ converge over time and outperform those controlled by a (na\"{\i}ve) greedy mechanism and the mechanism implemented in Skype.},
journal = {ACM Trans. Model. Perform. Eval. Comput. Syst.},
month = feb,
articleno = {4},
numpages = {20},
keywords = {VoIP, Skype, Rate Control, QoE, Proportional Fairness}
}

@article{10.1145/3264914,
author = {Elbakly, Rizanne and Elhamshary, Moustafa and Youssef, Moustafa},
title = {HyRise: A Robust and Ubiquitous Multi-Sensor Fusion-based Floor Localization System},
year = {2018},
issue_date = {September 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {2},
number = {3},
url = {https://doi.org/10.1145/3264914},
doi = {10.1145/3264914},
abstract = {Floor localization is an integral part of indoor localization systems that are deployed in any typical high-rise building. Nevertheless, while many efforts have been made to detect floor change events leveraging phone-embedded sensors, there are still a number of pitfalls that need to be overcome to provide robust and accurate localization in the 3D space.In this paper, we present HyRise: a robust and ubiquitous probabilistic crowdsourcing-based floor determination system. HyRise is a hybrid system that combines the barometer sensor and the ubiquitous Wi-Fi access points installed in the building into a probabilistic framework to identify the user's floor. In particular, HyRise incorporates a discrete Markov localization algorithm where the motion model is based on the vertical transitions detected from the sampled pressure readings and the observation model is based on the overheard Wi-Fi access points (APs) to find the most probable floor of the user. HyRise also has provisions to handle practical deployment issues including handling the inherent drift in the barometer readings, the noisy wireless environment, heterogeneous devices, among others.HyRise is implemented on Android phones and evaluated using three different testbeds: a campus building, a shopping mall, and a residential building with different floorplan layouts and APs densities. The results show that HyRise can identify the exact user's floor correctly in 93\%, 92\% and 77\% of the cases for the campus building, the shopping mall, and the more challenging residential building; respectively. In addition, it can identify the floor with at most 1-floor error in 100\% of the cases for all three testbeds. Moreover, the floor localization accuracy outperforms that achieved by other state-of-the-art techniques by at least 79\% and up to 278\%. This accuracy is achieved with no training overhead, is robust to the different user devices, and is consistent in buildings with different structures and APs densities.},
journal = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
month = sep,
articleno = {104},
numpages = {23},
keywords = {crowdsourcing, Sensor-based floor estimation, 3D indoor localization}
}

@article{10.14778/2733004.2733025,
author = {Bonifati, Angela and Ciucanu, Radu and Staworko, S\l{}awek},
title = {Interactive join query inference with JIM},
year = {2014},
issue_date = {August 2014},
publisher = {VLDB Endowment},
volume = {7},
number = {13},
issn = {2150-8097},
url = {https://doi.org/10.14778/2733004.2733025},
doi = {10.14778/2733004.2733025},
abstract = {Specifying join predicates may become a cumbersome task in many situations e.g., when the relations to be joined come from disparate data sources, when the values of the attributes carry little or no knowledge of metadata, or simply when the user is unfamiliar with querying formalisms. Such task is recurrent in many traditional data management applications, such as data integration, constraint inference, and database denormalization, but it is also becoming pivotal in novel crowdsourcing applications. We present Jim (Join Inference Machine), a system for interactive join specification tasks, where the user infers an n-ary join predicate by selecting tuples that are part of the join result via Boolean membership queries. The user can label tuples as positive or negative, while the system allows to identify and gray out the uninformative tuples i.e., those that do not add any information to the final learning goal. The tool also guides the user to reach her join inference goal with a minimal number of interactions.},
journal = {Proc. VLDB Endow.},
month = aug,
pages = {1541–1544},
numpages = {4}
}

@article{10.1145/2978655,
author = {Zhou, Yipeng and Chen, Liang and Jing, Mi and Zou, Shenglong and Ma, Richard Tianbai},
title = {Design, Implementation, and Measurement of a Crowdsourcing-Based Content Distribution Platform},
year = {2016},
issue_date = {December 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {12},
number = {5s},
issn = {1551-6857},
url = {https://doi.org/10.1145/2978655},
doi = {10.1145/2978655},
abstract = {Content distribution, especially the distribution of video content, unavoidably consumes bandwidth resources heavily. Internet content providers invest heavily in purchasing content distribution network (CDN) services. By deploying tens of thousands of edge servers close to end users, CDN companies are able to distribute content efficiently and effectively, but at considerable cost. Thus, it is of great importance to develop a new system that distributes content at a lower cost but comparable service quality. In lieu of expensive CDN systems, we implement a crowdsourcing-based content distribution system, Thunder Crystal, by renting bandwidth for content upload/download and storage for content cache from agents. This is a large-scale system with tens of thousands of agents, whose resources significantly amplify Thunder Crystal’s content distribution capacity. The involved agents are either from ordinary Internet users or enterprises. Monetary rewards are paid to agents based on their upload traffic so as to motivate them to keep contributing resources. As far as we know, this is a novel system that has not been studied or implemented before. This article introduces the design principles and implementation details before presenting the measurement study. In summary, with the help of agent devices, Thunder Crystal is able to reduce the content distribution cost by one half and amplify the content distribution capacity by 11 to 15 times.},
journal = {ACM Trans. Multimedia Comput. Commun. Appl.},
month = nov,
articleno = {80},
numpages = {23},
keywords = {video distribution, agent, Crowdsourcing, CDN}
}

@article{10.1145/3364697,
author = {Dimri, Anuj and Singh, Harsimran and Aggarwal, Naveen and Raman, Bhaskaran and Ramakrishnan, K. K. and Bansal, Divya},
title = {BaroSense: Using Barometer for Road Traffic Congestion Detection and Path Estimation with Crowdsourcing},
year = {2019},
issue_date = {February 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {16},
number = {1},
issn = {1550-4859},
url = {https://doi.org/10.1145/3364697},
doi = {10.1145/3364697},
abstract = {Traffic congestion on urban roadways is a serious problem requiring novel ways to detect and mitigate it. Determining the routes that lead to the traffic congestion segment is also vital in devising mitigation strategies. Further, crowdsourcing this information allows for use of these strategies quickly and in places where infrastructure is not available. In this work, we present an unconventional method, using the barometer sensor of mobile phones to (a) detect road traffic congestion and (b) estimate the paths that lead to the congested road segment. We make the observation that roads are not completely flat and very often, altitude varies along the road. The barometer sensor chips are sensitive enough to measure these variations and consume very little energy of the mobile phone, compared to other sensors such as the GPS or accelerometer. We devise a feature set to map the rate of change of this altitude as the user moves into activities characterized as “still” and “motion,” which are further used by the traffic congestion detection algorithm (RoadSphygmo) to classify the group of users as being in “moving,” “congestion,” &nbsp;or “stuck” states. To estimate the paths that lead to the congested road segment, we compare the user’s barometer sensor readings with a pre-stored road signature of barometer values using Dynamic Time Warping (DTW). We show that by using correlation of barometer sensor values, we can determine if users are in the same vehicle. We crowdsource this information from multiple mobile phones and use majority voting technique to improve the accuracy of traffic congestion detection and path estimation. We find a significant increase in the accuracies using crowdsourced information as compared to individual mobile phones. Further, we show that we can use barometer sensor for other applications such as bus occupancy, boarding/deboarding of a vehicle, and so on. The validation of the state determined by RoadSphygmo is done by comparing it with average GPS speed calculated during the same time period. The path estimation is validated over different intersections and considering various cases of commuter travel. The results obtained are promising and show that the traffic state determination and the estimation of the path taken by the commuter can achieve high accuracy.},
journal = {ACM Trans. Sen. Netw.},
month = nov,
articleno = {4},
numpages = {24},
keywords = {traffic congestion detection, smartphones, path estimation, crowdsourcing, barometer sensor, Activity recognition}
}

@article{10.14778/3025111.3025118,
author = {Zheng, Yudian and Li, Guoliang and Cheng, Reynold},
title = {DOCS: a domain-aware crowdsourcing system using knowledge bases},
year = {2016},
issue_date = {November 2016},
publisher = {VLDB Endowment},
volume = {10},
number = {4},
issn = {2150-8097},
url = {https://doi.org/10.14778/3025111.3025118},
doi = {10.14778/3025111.3025118},
abstract = {Crowdsourcing is a new computing paradigm that harnesses human effort to solve computer-hard problems, such as entity resolution and photo tagging. The crowd (or workers) have diverse qualities and it is important to effectively model a worker's quality. Most of existing worker models assume that workers have the same quality on different tasks. In practice, however, tasks belong to a variety of diverse domains, and workers have different qualities on different domains. For example, a worker who is a basketball fan should have better quality for the task of labeling a photo related to 'Stephen Curry' than the one related to 'Leonardo DiCaprio'. In this paper, we study how to leverage domain knowledge to accurately model a worker's quality. We examine using knowledge base (KB), e.g., Wikipedia and Freebase, to detect the domains of tasks and workers. We develop Domain Vector Estimation, which analyzes the domains of a task with respect to the KB. We also study Truth Inference, which utilizes the domain-sensitive worker model to accurately infer the true answer of a task. We design an Online Task Assignment algorithm, which judiciously and efficiently assigns tasks to appropriate workers. To implement these solutions, we have built DOCS, a system deployed on the Amazon Mechanical Turk. Experiments show that DOCS performs much better than the state-of-the-art approaches.},
journal = {Proc. VLDB Endow.},
month = nov,
pages = {361–372},
numpages = {12}
}

@article{10.1162/COLI_a_00255,
author = {De Clercq, Orph\'{e}e and Hoste, V\'{e}ronique},
title = {All mixed up? finding the optimal feature set for general readability prediction and its application to english and dutch},
year = {2016},
issue_date = {September 2016},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
volume = {42},
number = {3},
issn = {0891-2017},
url = {https://doi.org/10.1162/COLI_a_00255},
doi = {10.1162/COLI_a_00255},
abstract = {Readability research has a long and rich tradition, but there has been too little focus on general readability prediction without targeting a specific audience or text genre. Moreover, although NLP-inspired research has focused on adding more complex readability features, there is still no consensus on which features contribute most to the prediction. In this article, we investigate in close detail the feasibility of constructing a readability prediction system for English and Dutch generic text using supervised machine learning. Based on readability assessments by both experts and crowdsourcing, we implement different types of text characteristics ranging from easy-to-compute superficial text characteristics to features requiring deep linguistic processing, resulting in ten different feature groups. Both a regression and classification set-up are investigated reflecting the two possible readability prediction tasks: scoring individual texts or comparing two texts. We show that going beyond correlation calculations for readability optimization using a wrapper-based genetic algorithm optimization approach is a promising task that provides considerable insights in which feature combinations contribute to the overall readability prediction. Because we also have gold standard information available for those features requiring deep processing, we are able to investigate the true upper bound of our Dutch system. Interestingly, we will observe that the performance of our fully automatic readability prediction pipeline is on par with the pipeline using gold-standard deep syntactic and semantic information.},
journal = {Comput. Linguist.},
month = sep,
pages = {457–490},
numpages = {34}
}

@article{10.1145/3365523,
author = {Kobs, Konstantin and Zehe, Albin and Bernstetter, Armin and Chibane, Julian and Pfister, Jan and Tritscher, Julian and Hotho, Andreas},
title = {Emote-Controlled: Obtaining Implicit Viewer Feedback Through Emote-Based Sentiment Analysis on Comments of Popular Twitch.tv Channels},
year = {2020},
issue_date = {June 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {3},
number = {2},
url = {https://doi.org/10.1145/3365523},
doi = {10.1145/3365523},
abstract = {In recent years, streaming platforms for video games have seen increasingly large interest, as so-called esports have developed into a lucrative branch of business. Like for other sports, watching esports has become a new kind of entertainment medium, which is possible due to platforms that allow gamers to live stream their gameplay, the most popular platform being Twitch.tv. On these platforms, users can comment on streams in real time and thereby express their opinion about the events in the stream. Due to the popularity of Twitch.tv, this can be a valuable source of feedback for streamers aiming to improve their reception in a gaming-oriented audience. In this work, we explore the possibility of deriving feedback for video streams on Twitch.tv by analyzing the sentiment of live text comments made by stream viewers in highly active channels. Automatic sentiment analysis on these comments is a challenging task, as one can compare the language used in Twitch.tv with that used by an audience in a stadium, shouting as loud as possible in sometimes nonorganized ways. This language is very different from common English, mixing Internet slang and gaming-related language with abbreviations, intentional and unintentional grammatical and orthographic mistakes, and emoji-like images called emotes. Classic lexicon-based sentiment analysis techniques therefore fail when applied to Twitch comments.To overcome the challenge posed by the nonstandard language, we propose two unsupervised lexicon-based approaches that make heavy use of the information encoded in emotes, as well as a weakly supervised neural network–based classifier trained on the lexicon-based outputs, which is supposed to help generalization to unknown words by use of domain-specific word embeddings. To enable better understanding of Twitch.tv comments, we analyze a large dataset of comments, uncovering specific properties of their language, and provide a smaller set of comments labeled with sentiment information by crowdsourcing.We present two case studies showing the effectiveness of our methods in generating sentiment trajectories for events live streamed on Twitch.tv that correlate well with specific topics in the given stream. This allows for a new kind of implicit real-time feedback gathering for Twitch streamers and companies producing games or streaming content on Twitch.We make our datasets and code publicly available for further research.1},
journal = {Trans. Soc. Comput.},
month = apr,
articleno = {7},
numpages = {34},
keywords = {sentiment analysis, feedback, emotes, Twitch}
}

@article{10.1145/2601097.2601101,
author = {Laffont, Pierre-Yves and Ren, Zhile and Tao, Xiaofeng and Qian, Chao and Hays, James},
title = {Transient attributes for high-level understanding and editing of outdoor scenes},
year = {2014},
issue_date = {July 2014},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {33},
number = {4},
issn = {0730-0301},
url = {https://doi.org/10.1145/2601097.2601101},
doi = {10.1145/2601097.2601101},
abstract = {We live in a dynamic visual world where the appearance of scenes changes dramatically from hour to hour or season to season. In this work we study "transient scene attributes" -- high level properties which affect scene appearance, such as "snow", "autumn", "dusk", "fog". We define 40 transient attributes and use crowdsourcing to annotate thousands of images from 101 webcams. We use this "transient attribute database" to train regressors that can predict the presence of attributes in novel images. We demonstrate a photo organization method based on predicted attributes. Finally we propose a high-level image editing method which allows a user to adjust the attributes of a scene, e.g. change a scene to be "snowy" or "sunset". To support attribute manipulation we introduce a novel appearance transfer technique which is simple and fast yet competitive with the state-of-the-art. We show that we can convincingly modify many transient attributes in outdoor scenes.},
journal = {ACM Trans. Graph.},
month = jul,
articleno = {149},
numpages = {11},
keywords = {image database, attribute-based image editing}
}

@article{10.1145/2766898,
author = {Liu, Tianqiang and Hertzmann, Aaron and Li, Wilmot and Funkhouser, Thomas},
title = {Style compatibility for 3D furniture models},
year = {2015},
issue_date = {August 2015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {34},
number = {4},
issn = {0730-0301},
url = {https://doi.org/10.1145/2766898},
doi = {10.1145/2766898},
abstract = {This paper presents a method for learning to predict the stylistic compatibility between 3D furniture models from different object classes: e.g., how well does this chair go with that table? To do this, we collect relative assessments of style compatibility using crowdsourcing. We then compute geometric features for each 3D model and learn a mapping of them into a space where Euclidean distances represent style incompatibility. Motivated by the geometric subtleties of style, we introduce part-aware geometric feature vectors that characterize the shapes of different parts of an object separately. Motivated by the need to compute style compatibility between different object classes, we introduce a method to learn object class-specific mappings from geometric features to a shared feature space. During experiments with these methods, we find that they are effective at predicting style compatibility agreed upon by people. We find in user studies that the learned compatibility metric is useful for novel interactive tools that: 1) retrieve stylistically compatible models for a query, 2) suggest a piece of furniture for an existing scene, and 3) help guide an interactive 3D modeler towards scenes with compatible furniture.},
journal = {ACM Trans. Graph.},
month = jul,
articleno = {85},
numpages = {9},
keywords = {style, scene synthesis, crowdsourcing, compatibility}
}

@article{10.1007/s00778-013-0328-8,
author = {Lee, Jongwuk and Cho, Hyunsouk and Park, Jin-Woo and Cha, Young-Rok and Hwang, Seung-Won and Nie, Zaiqing and Wen, Ji-Rong},
title = {Hybrid entity clustering using crowds and data},
year = {2013},
issue_date = {October   2013},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {22},
number = {5},
issn = {1066-8888},
url = {https://doi.org/10.1007/s00778-013-0328-8},
doi = {10.1007/s00778-013-0328-8},
abstract = {Query result clustering has attracted considerable attention as a means of providing users with a concise overview of results. However, little research effort has been devoted to organizing the query results for  entities  which refer to real-world concepts, e.g., people, products, and locations. Entity-level result clustering is more challenging because diverse similarity notions between entities need to be supported in heterogeneous domains, e.g.,  image resolution  is an important feature for cameras, but not for fruits. To address this challenge, we propose a hybrid relationship clustering algorithm, called Hydra, using  co-occurrence  and  numeric features . Algorithm Hydra captures diverse user perceptions from co-occurrence and disambiguates different senses using feature-based similarity. In addition, we extend Hydra into  $${mathsf{Hydra }_mathsf{gData }}$$  Hydra gData   with different sources, i.e.,  entity types  and  crowdsourcing . Experimental results show that the proposed algorithms achieve effectiveness and efficiency in real-life and synthetic datasets.},
journal = {The VLDB Journal},
month = oct,
pages = {711–726},
numpages = {16},
keywords = {Subspace clustering, Hybrid entity clustering, Entity-level search, Crowdsourcing}
}

@article{10.5555/2946645.3053498,
author = {Chen, Xi and Jiao, Kevin and Lin, Qihang},
title = {Bayesian decision process for cost-efficient dynamic ranking via crowdsourcing},
year = {2016},
issue_date = {January 2016},
publisher = {JMLR.org},
volume = {17},
number = {1},
issn = {1532-4435},
abstract = {Rank aggregation based on pairwise comparisons over a set of items has a wide range of applications. Although considerable research has been devoted to the development of rank aggregation algorithms, one basic question is how to efficiently collect a large amount of high-quality pairwise comparisons for the ranking purpose. Because of the advent of many crowdsourcing services, a crowd of workers are often hired to conduct pairwise comparisons with a small monetary reward for each pair they compare. Since different workers have different levels of reliability and different pairs have different levels of ambiguity, it is desirable to wisely allocate the limited budget for comparisons among the pairs of items and workers so that the global ranking can be accurately inferred from the comparison results. To this end, we model the active sampling problem in crowdsourced ranking as a Bayesian Markov decision process, which dynamically selects item pairs and workers to improve the ranking accuracy under a budget constraint. We further develop a computationally efficient sampling policy based on knowledge gradient as well as a moment matching technique for posterior approximation. Experimental evaluations on both synthetic and real data show that the proposed policy achieves high ranking accuracy with a lower labeling cost.},
journal = {J. Mach. Learn. Res.},
month = jan,
pages = {7617–7656},
numpages = {40},
keywords = {moment matching, knowledge gradient, dynamic programming, crowdsourced ranking, Markov decision process, Bayesian}
}

@article{10.1145/2897369,
author = {Katsimerou, Christina and Albeda, Joris and Huldtgren, Alina and Heynderickx, Ingrid and Redi, Judith A.},
title = {Crowdsourcing Empathetic Intelligence: The Case of the Annotation of EMMA Database for Emotion and Mood Recognition},
year = {2016},
issue_date = {July 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {4},
issn = {2157-6904},
url = {https://doi.org/10.1145/2897369},
doi = {10.1145/2897369},
abstract = {Unobtrusive recognition of the user's mood is an essential capability for affect-adaptive systems. Mood is a subtle, long-term affective state, often misrecognized even by humans. The challenge to train a machine to recognize it from, for example, a video of the user, is significant, and already begins with the lack of ground truth for supervised learning. Existing affective databases consist mainly of short videos, annotated in terms of expressed emotions rather than mood. In very few cases, we encounter perceived mood annotations, of questionable reliability, however, due to the subjectivity of mood estimation and the small number of coders involved. In this work, we introduce a new database for mood recognition from video. Our database contains 180 long, acted videos, depicting typical daily scenarios, and subtle facial and bodily expressions. The videos cover three visual modalities (face, body, Kinect data), and are annotated in terms of emotions (via G-trace) and mood (via the Self-Assessment Manikin and the AffectButton). To annotate the database exhaustively, we exploit crowdsourcing to reach out to an extensive number of nonexpert coders. We validate the reliability of our crowdsourced annotations by (1) adopting a number of criteria to filter out unreliable coders, and (2) comparing the annotations of a subset of our videos with those collected in a controlled lab setting.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = may,
articleno = {51},
numpages = {27},
keywords = {mood recognition, emotion recognition, crowdsourcing, affective annotation, Multimodal database}
}

@article{10.14778/2824032.2824109,
author = {Chu, Xu and Morcos, John and Ilyas, Ihab F. and Ouzzani, Mourad and Papotti, Paolo and Tang, Nan and Ye, Yin},
title = {KATARA: reliable data cleaning with knowledge bases and crowdsourcing},
year = {2015},
issue_date = {August 2015},
publisher = {VLDB Endowment},
volume = {8},
number = {12},
issn = {2150-8097},
url = {https://doi.org/10.14778/2824032.2824109},
doi = {10.14778/2824032.2824109},
abstract = {Data cleaning with guaranteed reliability is hard to achieve without accessing external sources, since the truth is not necessarily discoverable from the data at hand. Furthermore, even in the presence of external sources, mainly knowledge bases and humans, effectively leveraging them still faces many challenges, such as aligning heterogeneous data sources and decomposing a complex task into simpler units that can be consumed by humans. We present Katara, a novel end-to-end data cleaning system powered by knowledge bases and crowdsourcing. Given a table, a kb, and a crowd, Katara (i) interprets the table semantics w.r.t. the given kb; (ii) identifies correct and wrong data; and (iii) generates top-k possible repairs for the wrong data. Users will have the opportunity to experience the following features of Katara: (1) Easy specification: Users can define a Katara job with a browser-based specification; (2) Pattern validation: Users can help the system to resolve the ambiguity of different table patterns (i.e., table semantics) discovered by Katara; (3) Data annotation: Users can play the role of internal crowd workers, helping Katara annotate data. Moreover, Katara will visualize the annotated data as correct data validated by the kb, correct data jointly validated by the kb and the crowd, or erroneous tuples along with their possible repairs.},
journal = {Proc. VLDB Endow.},
month = aug,
pages = {1952–1955},
numpages = {4}
}

@article{10.1145/2490823,
author = {Zhang, Lei and Rui, Yong},
title = {Image search—from thousands to billions in 20 years},
year = {2013},
issue_date = {October 2013},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {9},
number = {1s},
issn = {1551-6857},
url = {https://doi.org/10.1145/2490823},
doi = {10.1145/2490823},
abstract = {This article presents a comprehensive review and analysis on image search in the past 20 years, emphasizing the challenges and opportunities brought by the astonishing increase of dataset scales from thousands to billions in the same time period, which was witnessed first-hand by the authors as active participants in this research area. Starting with a retrospective review of three stages of image search in the history, the article highlights major breakthroughs around the year 2000 in image search features, indexing methods, and commercial systems, which marked the transition from stage two to stage three. Subsequent sections describe the image search research from four important aspects: system framework, feature extraction and image representation, indexing, and big data's potential. Based on the review, the concluding section discusses open research challenges and suggests future research directions in effective visual representation, image knowledge base construction, implicit user feedback and crowdsourcing, mobile image search, and creative multimedia interfaces.},
journal = {ACM Trans. Multimedia Comput. Commun. Appl.},
month = oct,
articleno = {36},
numpages = {20},
keywords = {Review, Web image search, big data, content-based, global feature, image feature, image knowledge base, image retrieval, indexing, local feature, visual representation}
}

@article{10.5555/3241691.3241698,
author = {Kara, Yunus Emre and Genc, Gaye and Aran, Oya and Akarun, Lale},
title = {Actively estimating crowd annotation consensus},
year = {2018},
issue_date = {January 2018},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {61},
number = {1},
issn = {1076-9757},
abstract = {The rapid growth of storage capacity and processing power has caused machine learning applications to increasingly rely on using immense amounts of labeled data. It has become more important than ever to have fast and inexpensive ways to annotate vast amounts of data. With the emergence of crowdsourcing services, the research direction has gravitated toward putting the wisdom of crowds to better use. Unfortunately, spammers and inattentive annotators pose a threat to the quality and trustworthiness of the consensus. Thus, high quality consensus estimation from crowd annotated data requires a meticulous choice of the candidate annotator and the sample in need of a new annotation. Due to time and budget limitations, it is of utmost importance that this choice is carried out while the annotation collection is in progress. We call this process active crowd-labeling. To this end, we propose an active crowd-labeling approach for actively estimating consensus from continuous-valued crowd annotations. Our method is based on annotator models with unknown parameters, and Bayesian inference is employed to reach a consensus in the form of ordinal, binary, or continuous values. We introduce ranking functions for choosing the candidate annotator and sample pair for requesting an annotation. In addition, we propose a penalizing method for preventing annotator domination, investigate the explore-exploit trade-off for incorporating new annotators into the system, and study the effects of inducing a stopping criterion based on consensus quality. We also introduce the crowd-labeled Head Pose Annotations datasets. Experimental results on the benchmark datasets used in the literature and the Head Pose Annotations datasets suggest that our method provides high-quality consensus by using as few as one _fth of the annotations (~ 80\% cost reduction), thereby providing a budget and time-sensitive solution to the crowd-labeling problem.},
journal = {J. Artif. Int. Res.},
month = jan,
pages = {363–405},
numpages = {43}
}

@article{10.1145/2601097.2601110,
author = {O'Donovan, Peter and Lundefinedbeks, Jundefinednis and Agarwala, Aseem and Hertzmann, Aaron},
title = {Exploratory font selection using crowdsourced attributes},
year = {2014},
issue_date = {July 2014},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {33},
number = {4},
issn = {0730-0301},
url = {https://doi.org/10.1145/2601097.2601110},
doi = {10.1145/2601097.2601110},
abstract = {This paper presents interfaces for exploring large collections of fonts for design tasks. Existing interfaces typically list fonts in a long, alphabetically-sorted menu that can be challenging and frustrating to explore. We instead propose three interfaces for font selection. First, we organize fonts using high-level descriptive attributes, such as "dramatic" or "legible." Second, we organize fonts in a tree-based hierarchical menu based on perceptual similarity. Third, we display fonts that are most similar to a user's currently-selected font. These tools are complementary; a user may search for "graceful" fonts, select a reasonable one, and then refine the results from a list of fonts similar to the selection. To enable these tools, we use crowdsourcing to gather font attribute data, and then train models to predict attribute values for new fonts. We use attributes to help learn a font similarity metric using crowdsourced comparisons. We evaluate the interfaces against a conventional list interface and find that our interfaces are preferred to the baseline. Our interfaces also produce better results in two real-world tasks: finding the nearest match to a target font, and font selection for graphic designs.},
journal = {ACM Trans. Graph.},
month = jul,
articleno = {92},
numpages = {9},
keywords = {typography, font, design, crowdsourcing, attribute}
}

@article{10.1145/2873063,
author = {Moshfeghi, Yashar and Rosero, Alvaro Francisco Huertas and Jose, Joemon M.},
title = {A Game-Theory Approach for Effective Crowdsource-Based Relevance Assessment},
year = {2016},
issue_date = {July 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {4},
issn = {2157-6904},
url = {https://doi.org/10.1145/2873063},
doi = {10.1145/2873063},
abstract = {Despite the ever-increasing popularity of crowdsourcing (CS) in both industry and academia, procedures that ensure quality in its results are still elusive. We hypothesise that a CS design based on game theory can persuade workers to perform their tasks as quickly as possible with the highest quality. In order to do so, in this article we propose a CS framework inspired by the n-person Chicken game. Our aim is to address the problem of CS quality without compromising on CS benefits such as low monetary cost and high task completion speed. With that goal in mind, we study the effects of knowledge updates as well as incentives for good workers to continue playing. We define a general task with the characteristics of relevance assessment as a case study, because it has been widely explored in the past with CS due to its potential cost and complexity. In order to investigate our hypotheses, we conduct a simulation where we study the effect of the proposed framework on data accuracy, task completion time, and total monetary rewards. Based on a game-theoretical analysis, we study how different types of individuals would behave under a particular game scenario. In particular, we simulate a population comprised of different types of workers with varying ability to formulate optimal strategies and learn from their experiences. A simulation of the proposed framework produced results that support our hypothesis.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = mar,
articleno = {55},
numpages = {25},
keywords = {relevance assessment, crowdsourcing, Game theory}
}

@article{10.1109/TASLP.2016.2634123,
author = {Granell, Emilio and Martinez-Hinarejos, Carlos-D. and Granell, Emilio and Martinez-Hinarejos, Carlos-D},
title = {Multimodal Crowdsourcing for Transcribing Handwritten Documents},
year = {2017},
issue_date = {February 2017},
publisher = {IEEE Press},
volume = {25},
number = {2},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2016.2634123},
doi = {10.1109/TASLP.2016.2634123},
abstract = {Transcription of handwritten documents is an important research topic for multiple applications, such as document classification or information extraction. In the case of historical documents, their transcription allows to preserve cultural heritage because of the amount of historical data contained in those documents. The transcription process can employ state-of-the-art handwritten text recognition systems in order to obtain an initial transcription. This transcription is usually not good enough for the quality standards, but that may speed up the final transcription of the expert. In this framework, the use of collaborative transcription applications crowdsourcing has risen in the recent years, but these platforms are mainly limited by the use of non-mobile devices. Thus, the recruiting initiatives get reduced to a smaller set of potential volunteers. In this paper, an alternative that allows the use of mobile devices is presented. The proposal consists of using speech dictation of handwritten text lines. Then, by using multimodal combination of speech and handwritten text images, a draft transcription can be obtained, presenting more quality than that obtained by only using handwritten text recognition. The speech dictation platform is implemented as a mobile device application, which allows for a wider range of population for recruiting volunteers. A real acquisition on the contents of a Spanish historical handwritten book was obtained with the platform. This data was used to perform experiments on the behaviour of the proposed framework. Some experiments were performed to study how to optimise the collaborators effort in terms of number of collaborations, including how many lines and which lines should be selected for the speech dictation.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = feb,
pages = {409–419},
numpages = {11}
}

@article{10.14778/2732951.2732966,
author = {To, Hien and Ghinita, Gabriel and Shahabi, Cyrus},
title = {A framework for protecting worker location privacy in spatial crowdsourcing},
year = {2014},
issue_date = {June 2014},
publisher = {VLDB Endowment},
volume = {7},
number = {10},
issn = {2150-8097},
url = {https://doi.org/10.14778/2732951.2732966},
doi = {10.14778/2732951.2732966},
abstract = {Spatial Crowdsourcing (SC) is a transformative platform that engages individuals, groups and communities in the act of collecting, analyzing, and disseminating environmental, social and other spatio-temporal information. The objective of SC is to outsource a set of spatio-temporal tasks to a set of workers, i.e., individuals with mobile devices that perform the tasks by physically traveling to specified locations of interest. However, current solutions require the workers, who in many cases are simply volunteering for a cause, to disclose their locations to untrustworthy entities. In this paper, we introduce a framework for protecting location privacy of workers participating in SC tasks. We argue that existing location privacy techniques are not sufficient for SC, and we propose a mechanism based on differential privacy and geocasting that achieves effective SC services while offering privacy guarantees to workers. We investigate analytical models and task assignment strategies that balance multiple crucial aspects of SC functionality, such as task completion rate, worker travel distance and system overhead. Extensive experimental results on real-world datasets show that the proposed technique protects workers' location privacy without incurring significant performance metrics penalties.},
journal = {Proc. VLDB Endow.},
month = jun,
pages = {919–930},
numpages = {12}
}

@article{10.5555/3013589.3013603,
author = {Venanzi, Matteo and Guiver, John and Kohli, Pushmeet and Jennings, Nicholas R.},
title = {Time-sensitive Bayesian information aggregation for crowdsourcing systems},
year = {2016},
issue_date = {May 2016},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {56},
number = {1},
issn = {1076-9757},
abstract = {Many aspects of the design of efficient crowdsourcing processes, such as defining worker's bonuses, fair prices and time limits of the tasks, involve knowledge of the likely duration of the task at hand. In this work we introduce a new time-sensitive Bayesian aggregation method that simultaneously estimates a task's duration and obtains reliable aggregations of crowdsourced judgments. Our method, called BCCTime, uses latent variables to represent the uncertainty about the workers' completion time, the tasks' duration and the workers' accuracy. To relate the quality of a judgment to the time a worker spends on a task, our model assumes that each task is completed within a latent time window within which all workers with a propensity to genuinely attempt the labelling task (i.e., no spammers) are expected to submit their judgments. In contrast, workers with a lower propensity to valid labelling, such as spammers, bots or lazy labellers, are assumed to perform tasks considerably faster or slower than the time required by normal workers. Specifically, we use efficient message-passing Bayesian inference to learn approximate posterior probabilities of (i) the confusion matrix of each worker, (ii) the propensity to valid labelling of each worker, (iii) the unbiased duration of each task and (iv) the true label of each task. Using two real-world public datasets for entity linking tasks, we show that BCCTime produces up to 11\% more accurate classifications and up to 100\% more informative estimates of a task's duration compared to state-of-the-art methods.},
journal = {J. Artif. Int. Res.},
month = may,
pages = {517–545},
numpages = {29}
}

@article{10.1145/2746353,
author = {Tranquillini, Stefano and Daniel, Florian and Kucherbaev, Pavel and Casati, Fabio},
title = {Modeling, Enacting, and Integrating Custom Crowdsourcing Processes},
year = {2015},
issue_date = {May 2015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {9},
number = {2},
issn = {1559-1131},
url = {https://doi.org/10.1145/2746353},
doi = {10.1145/2746353},
abstract = {Crowdsourcing (CS) is the outsourcing of a unit of work to a crowd of people via an open call for contributions. Thanks to the availability of online CS platforms, such as Amazon Mechanical Turk or CrowdFlower, the practice has experienced a tremendous growth over the past few years and demonstrated its viability in a variety of fields, such as data collection and analysis or human computation. Yet it is also increasingly struggling with the inherent limitations of these platforms: each platform has its own logic of how to crowdsource work (e.g., marketplace or contest), there is only very little support for structured work (work that requires the coordination of multiple tasks), and it is hard to integrate crowdsourced tasks into state-of-the-art business process management (BPM) or information systems.We attack these three shortcomings by (1) developing a flexible CS platform (we call it Crowd Computer, or CC) that allows one to program custom CS logics for individual and structured tasks, (2) devising a BPMN--based modeling language that allows one to program CC intuitively, (3) equipping the language with a dedicated visual editor, and (4) implementing CC on top of standard BPM technology that can easily be integrated into existing software and processes. We demonstrate the effectiveness of the approach with a case study on the crowd-based mining of mashup model patterns.},
journal = {ACM Trans. Web},
month = may,
articleno = {7},
numpages = {43},
keywords = {tactics, processes, Crowdsourcing, Crowd Computer, BPMN4Crowd}
}

@article{10.1145/3239570,
author = {Hopfgartner, Frank and Hanbury, Allan and M\"{u}ller, Henning and Eggel, Ivan and Balog, Krisztian and Brodt, Torben and Cormack, Gordon V. and Lin, Jimmy and Kalpathy-Cramer, Jayashree and Kando, Noriko and Kato, Makoto P. and Krithara, Anastasia and Gollub, Tim and Potthast, Martin and Viegas, Evelyne and Mercer, Simon},
title = {Evaluation-as-a-Service for the Computational Sciences: Overview and Outlook},
year = {2018},
issue_date = {December 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {10},
number = {4},
issn = {1936-1955},
url = {https://doi.org/10.1145/3239570},
doi = {10.1145/3239570},
abstract = {Evaluation in empirical computer science is essential to show progress and assess technologies developed. Several research domains such as information retrieval have long relied on systematic evaluation to measure progress: here, the Cranfield paradigm of creating shared test collections, defining search tasks, and collecting ground truth for these tasks has persisted up until now. In recent years, however, several new challenges have emerged that do not fit this paradigm very well: extremely large data sets, confidential data sets as found in the medical domain, and rapidly changing data sets as often encountered in industry. Crowdsourcing has also changed the way in which industry approaches problem-solving with companies now organizing challenges and handing out monetary awards to incentivize people to work on their challenges, particularly in the field of machine learning.This article is based on discussions at a workshop on Evaluation-as-a-Service (EaaS). EaaS is the paradigm of not providing data sets to participants and have them work on the data locally, but keeping the data central and allowing access via Application Programming Interfaces (API), Virtual Machines (VM), or other possibilities to ship executables. The objectives of this article are to summarize and compare the current approaches and consolidate the experiences of these approaches to outline the next steps of EaaS, particularly toward sustainable research infrastructures.The article summarizes several existing approaches to EaaS and analyzes their usage scenarios and also the advantages and disadvantages. The many factors influencing EaaS are summarized, and the environment in terms of motivations for the various stakeholders, from funding agencies to challenge organizers, researchers and participants, to industry interested in supplying real-world problems for which they require solutions.EaaS solves many problems of the current research environment, where data sets are often not accessible to many researchers. Executables of published tools are equally often not available making the reproducibility of results impossible. EaaS, however, creates reusable/citable data sets as well as available executables. Many challenges remain, but such a framework for research can also foster more collaboration between researchers, potentially increasing the speed of obtaining research results.},
journal = {J. Data and Information Quality},
month = oct,
articleno = {15},
numpages = {32},
keywords = {information access systems, benchmarking, Evaluation-as-a-service}
}

@article{10.1145/2776896,
author = {Siddharthan, Advaith and Lambin, Christopher and Robinson, Anne-Marie and Sharma, Nirwan and Comont, Richard and O'mahony, Elaine and Mellish, Chris and Wal, Ren\'{e} Van Der},
title = {Crowdsourcing Without a Crowd: Reliable Online Species Identification Using Bayesian Models to Minimize Crowd Size},
year = {2016},
issue_date = {July 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {4},
issn = {2157-6904},
url = {https://doi.org/10.1145/2776896},
doi = {10.1145/2776896},
abstract = {We present an incremental Bayesian model that resolves key issues of crowd size and data quality for consensus labeling. We evaluate our method using data collected from a real-world citizen science program, BeeWatch, which invites members of the public in the United Kingdom to classify (label) photographs of bumblebees as one of 22 possible species. The biological recording domain poses two key and hitherto unaddressed challenges for consensus models of crowdsourcing: (1) the large number of potential species makes classification difficult, and (2) this is compounded by limited crowd availability, stemming from both the inherent difficulty of the task and the lack of relevant skills among the general public. We demonstrate that consensus labels can be reliably found in such circumstances with very small crowd sizes of around three to five users (i.e., through group sourcing). Our incremental Bayesian model, which minimizes crowd size by re-evaluating the quality of the consensus label following each species identification solicited from the crowd, is competitive with a Bayesian approach that uses a larger but fixed crowd size and outperforms majority voting. These results have important ecological applicability: biological recording programs such as BeeWatch can sustain themselves when resources such as taxonomic experts to confirm identifications by photo submitters are scarce (as is typically the case), and feedback can be provided to submitters in a timely fashion. More generally, our model provides benefits to any crowdsourced consensus labeling task where there is a cost (financial or otherwise) associated with soliciting a label.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = may,
articleno = {45},
numpages = {20},
keywords = {consensus model, citizen science, bumblebee identification, biological recording, Crowdsourcing, Bayesian reasoning}
}

@article{10.1145/2641577,
author = {Sanchez-Cortes, Dairazalia and Kumano, Shiro and Otsuka, Kazuhiro and Gatica-Perez, Daniel},
title = {In the Mood for Vlog: Multimodal Inference in Conversational Social Video},
year = {2015},
issue_date = {July 2015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {5},
number = {2},
issn = {2160-6455},
url = {https://doi.org/10.1145/2641577},
doi = {10.1145/2641577},
abstract = {The prevalent “share what's on your mind” paradigm of social media can be examined from the perspective of mood: short-term affective states revealed by the shared data. This view takes on new relevance given the emergence of conversational social video as a popular genre among viewers looking for entertainment and among video contributors as a channel for debate, expertise sharing, and artistic expression. From the perspective of human behavior understanding, in conversational social video both verbal and nonverbal information is conveyed by speakers and decoded by viewers. We present a systematic study of classification and ranking of mood impressions in social video, using vlogs from YouTube. Our approach considers eleven natural mood categories labeled through crowdsourcing by external observers on a diverse set of conversational vlogs. We extract a comprehensive number of nonverbal and verbal behavioral cues from the audio and video channels to characterize the mood of vloggers. Then we implement and validate vlog classification and vlog ranking tasks using supervised learning methods. Following a reliability and correlation analysis of the mood impression data, our study demonstrates that, while the problem is challenging, several mood categories can be inferred with promising performance. Furthermore, multimodal features perform consistently better than single-channel features. Finally, we show that addressing mood as a ranking problem is a promising practical direction for several of the mood categories studied.},
journal = {ACM Trans. Interact. Intell. Syst.},
month = jun,
articleno = {9},
numpages = {24},
keywords = {vlogs, video blogs, verbal content, nonverbal behavior, mood, Social video}
}

@article{10.1145/3134651,
author = {Karahalios, Karrie and Fitzpatrick, Geraldine and Monroy-Hern\'{a}ndez, Andr\'{e}s},
title = {Editor's Note/Chairs' Welcome},
year = {2017},
issue_date = {November 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {1},
number = {CSCW},
url = {https://doi.org/10.1145/3134651},
doi = {10.1145/3134651},
abstract = {Welcome to this issue of the Proceedings of the ACM on Human-Computer Interaction, which will focus on contributions from the research community Computer-Supported Cooperative Work and Social Computing (CSCW). This diverse research community explores how different types of social groups affect, and are affected by, information and communication technology. The topics explored by this community can include social media use, crowdsourcing and micro-work, societal effects of computing, and much more. Like many other HCI communities, CSCW approaches these topics with a broad range of scientific techniques, theoretical perspectives and technology platforms.The call for papers for this issue on CSCW attracted 385 submissions, from Asia, Canada, Australia, Europe, Africa, and the United States. After the first round of reviewing, 207 (54\%) papers were invited to the Revise and Resubmit phase. The editorial committee worked hard over August 2017 to arrive at final decisions, with a Virtual Committee meeting held to discuss those papers that needed collective deliberation. In the end, 105 papers (27\%) were accepted.This issue exists because of the dedicated volunteer effort of 101 senior editors who served as Associate Chairs (ACs), and 885 expert reviewers to ensure high quality and insightful reviews for all papers in both rounds. Reviewers and committee members were kept constant for papers that submitted to both rounds. Senior members of the editorial group also helped shepherd some papers, reflecting the deep commitment of this research community.We are excited by the compelling and thought-provoking work that resulted in this PACMHCI CSCW issue and look forward to equally high quality submissions for the next submission cycle from this research community in the Spring of 2018. For those interested in this area, this group holds their next annual conference November 3-7, 2018 in New York City's Hudson River (Jersey City). That conference will provide many opportunities to share ideas with other researchers and practitioners from institutions around the world.},
journal = {Proc. ACM Hum.-Comput. Interact.},
month = dec,
articleno = {16},
numpages = {1}
}

@article{10.1145/2601097.2601206,
author = {Bell, Sean and Bala, Kavita and Snavely, Noah},
title = {Intrinsic images in the wild},
year = {2014},
issue_date = {July 2014},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {33},
number = {4},
issn = {0730-0301},
url = {https://doi.org/10.1145/2601097.2601206},
doi = {10.1145/2601097.2601206},
abstract = {Intrinsic image decomposition separates an image into a reflectance layer and a shading layer. Automatic intrinsic image decomposition remains a significant challenge, particularly for real-world scenes. Advances on this longstanding problem have been spurred by public datasets of ground truth data, such as the MIT Intrinsic Images dataset. However, the difficulty of acquiring ground truth data has meant that such datasets cover a small range of materials and objects. In contrast, real-world scenes contain a rich range of shapes and materials, lit by complex illumination.In this paper we introduce Intrinsic Images in the Wild, a large-scale, public dataset for evaluating intrinsic image decompositions of indoor scenes. We create this benchmark through millions of crowdsourced annotations of relative comparisons of material properties at pairs of points in each scene. Crowdsourcing enables a scalable approach to acquiring a large database, and uses the ability of humans to judge material comparisons, despite variations in illumination. Given our database, we develop a dense CRF-based intrinsic image algorithm for images in the wild that outperforms a range of state-of-the-art intrinsic image algorithms. Intrinsic image decomposition remains a challenging problem; we release our code and database publicly to support future research on this problem, available online at http://intrinsic.cs.cornell.edu/.},
journal = {ACM Trans. Graph.},
month = jul,
articleno = {159},
numpages = {12},
keywords = {shading, reflectance, intrinsic images, crowdsourcing}
}

@article{10.1145/1746259.1746260,
author = {Bernstein, Michael S. and Tan, Desney and Smith, Greg and Czerwinski, Mary and Horvitz, Eric},
title = {Personalization via friendsourcing},
year = {2008},
issue_date = {May 2010},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {17},
number = {2},
issn = {1073-0516},
url = {https://doi.org/10.1145/1746259.1746260},
doi = {10.1145/1746259.1746260},
abstract = {When information is known only to friends in a social network, traditional crowdsourcing mechanisms struggle to motivate a large enough user population and to ensure accuracy of the collected information. We thus introduce friendsourcing, a form of crowdsourcing aimed at collecting accurate information available only to a small, socially-connected group of individuals. Our approach to friendsourcing is to design socially enjoyable interactions that produce the desired information as a side effect.We focus our analysis around Collabio, a novel social tagging game that we developed to encourage friends to tag one another within an online social network. Collabio encourages friends, family, and colleagues to generate useful information about each other. We describe the design space of incentives in social tagging games and evaluate our choices by a combination of usage log analysis and survey data. Data acquired via Collabio is typically accurate and augments tags that could have been found on Facebook or the Web. To complete the arc from data collection to application, we produce a trio of prototype applications to demonstrate how Collabio tags could be utilized: an aggregate tag cloud visualization, a personalized RSS feed, and a question and answer system. The social data powering these applications enables them to address needs previously difficult to support, such as question answering for topics comprehensible only to a few of a user's friends.},
journal = {ACM Trans. Comput.-Hum. Interact.},
month = may,
articleno = {6},
numpages = {28},
keywords = {social tagging, human computation, friendsourcing, Social computing}
}

@article{10.1145/2461912.2462002,
author = {Bell, Sean and Upchurch, Paul and Snavely, Noah and Bala, Kavita},
title = {OpenSurfaces: a richly annotated catalog of surface appearance},
year = {2013},
issue_date = {July 2013},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {32},
number = {4},
issn = {0730-0301},
url = {https://doi.org/10.1145/2461912.2462002},
doi = {10.1145/2461912.2462002},
abstract = {The appearance of surfaces in real-world scenes is determined by the materials, textures, and context in which the surfaces appear. However, the datasets we have for visualizing and modeling rich surface appearance in context, in applications such as home remodeling, are quite limited. To help address this need, we present OpenSurfaces, a rich, labeled database consisting of thousands of examples of surfaces segmented from consumer photographs of interiors, and annotated with material parameters (reflectance, material names), texture information (surface normals, rectified textures), and contextual information (scene category, and object names).Retrieving usable surface information from uncalibrated Internet photo collections is challenging. We use human annotations and present a new methodology for segmenting and annotating materials in Internet photo collections suitable for crowdsourcing (e.g., through Amazon's Mechanical Turk). Because of the noise and variability inherent in Internet photos and novice annotators, designing this annotation engine was a key challenge; we present a multi-stage set of annotation tasks with quality checks and validation. We demonstrate the use of this database in proof-of-concept applications including surface retexturing and material and image browsing, and discuss future uses. OpenSurfaces is a public resource available at http://opensurfaces.cs.cornell.edu/.},
journal = {ACM Trans. Graph.},
month = jul,
articleno = {111},
numpages = {17},
keywords = {textures, reflectance, materials, crowdsourcing}
}

@article{10.1145/2983646,
author = {Xu, Haitao and Liu, Daiping and Wang, Haining and Stavrou, Angelos},
title = {An Empirical Investigation of Ecommerce-Reputation-Escalation-as-a-Service},
year = {2017},
issue_date = {May 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {11},
number = {2},
issn = {1559-1131},
url = {https://doi.org/10.1145/2983646},
doi = {10.1145/2983646},
abstract = {In online markets, a store’s reputation is closely tied to its profitability. Sellers’ desire to quickly achieve a high reputation has fueled a profitable underground business that operates as a specialized crowdsourcing marketplace and accumulates wealth by allowing online sellers to harness human laborers to conduct fake transactions to improve their stores’ reputations. We term such an underground market a seller-reputation-escalation (SRE) market. In this article, we investigate the impact of the SRE service on reputation escalation by performing in-depth measurements of the prevalence of the SRE service, the business model and market size of SRE markets, and the characteristics of sellers and offered laborers. To this end, we have infiltrated five SRE markets and studied their operations using daily data collection over a continuous period of 2 months. We identified more than 11,000 online sellers posting at least 219,165 fake-purchase tasks on the five SRE markets. These transactions earned at least $46,438 in revenue for the five SRE markets, and the total value of merchandise involved exceeded $3,452,530. Our study demonstrates that online sellers using the SRE service can increase their stores’ reputations at least 10 times faster than legitimate ones while about 25\% of them were visibly penalized. Even worse, we found a much stealthier and more hazardous service that can, within a single day, boost a seller’s reputation by such a degree that would require a legitimate seller at least a year to accomplish. Armed with our analysis of the operational characteristics of the underground economy, we offer some insights into potential mitigation strategies. Finally, we revisit the SRE ecosystem 1 year later to evaluate the latest dynamism of the SRE markets, especially the statuses of the online stores once identified to launch fake-transaction campaigns on the SRE markets. We observe that the SRE markets are not as active as they were 1 year ago and about 17\% of the involved online stores become inaccessible likely because they have been forcibly shut down by the corresponding E-commerce marketplace for conducting fake transactions.},
journal = {ACM Trans. Web},
month = may,
articleno = {13},
numpages = {35},
keywords = {reputation manipulation, fake transaction, E-commerce}
}

@article{10.1145/2766929,
author = {Lun, Zhaoliang and Kalogerakis, Evangelos and Sheffer, Alla},
title = {Elements of style: learning perceptual shape style similarity},
year = {2015},
issue_date = {August 2015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {34},
number = {4},
issn = {0730-0301},
url = {https://doi.org/10.1145/2766929},
doi = {10.1145/2766929},
abstract = {The human perception of stylistic similarity transcends structure and function: for instance, a bed and a dresser may share a common style. An algorithmically computed style similarity measure that mimics human perception can benefit a range of computer graphics applications. Previous work in style analysis focused on shapes within the same class, and leveraged structural similarity between these shapes to facilitate analysis. In contrast, we introduce the first structure-transcending style similarity measure and validate it to be well aligned with human perception of stylistic similarity. Our measure is inspired by observations about style similarity in art history literature, which point to the presence of similarly shaped, salient, geometric elements as one of the key indicators of stylistic similarity. We translate these observations into an algorithmic measure by first quantifying the geometric properties that make humans perceive geometric elements as similarly shaped and salient in the context of style, then employing this quantification to detect pairs of matching style related elements on the analyzed models, and finally collating the element-level geometric similarity measurements into an object-level style measure consistent with human perception. To achieve this consistency we employ crowdsourcing to quantify the different components of our measure; we learn the relative perceptual importance of a range of elementary shape distances and other parameters used in our measurement from 50K responses to cross-structure style similarity queries provided by over 2500 participants.We train and validate our method on this dataset, showing it to successfully predict relative style similarity with near 90\% accuracy based on 10-fold cross-validation.},
journal = {ACM Trans. Graph.},
month = jul,
articleno = {84},
numpages = {14},
keywords = {style similarity, machine learning, crowdsourcing}
}

@article{10.14778/3402755.3402800,
author = {Triantafillou, Peter},
title = {Anthropocentric data systems},
year = {2011},
issue_date = {August 2011},
publisher = {VLDB Endowment},
volume = {4},
number = {12},
issn = {2150-8097},
url = {https://doi.org/10.14778/3402755.3402800},
doi = {10.14778/3402755.3402800},
abstract = {Arguably, it all started with Mike Dertouzos' vision on the Information Marketplace [2]. Then, an explosion occurred. Social networks. Social computing. Social software. Groupware. Shareware. Open-source software. Personalized query answering and personalized information systems. Tagging. Folksonomies. Log and clickstream mining. Recommender systems. Crowdsourcing. Human-in-the loop and human-centered systems. Provably, these buzzwords have dominated the academic landscape within the data systems (and not only) community. There is a fundamental paradigm shift going on here. The old world, where the human was simply a passive user, has given way to a new world where humans contribute data, (storage, communication, and compute) resources, and software. Further, recently, humans take on tasks that actually alleviate and improve the jobs performed by machines and recent research from different domains have started looking into this realm where humans and computers share tasks, collaborating to achieve goals [4, 12, 11, 6, 1]!},
journal = {Proc. VLDB Endow.},
month = aug,
pages = {1478–1480},
numpages = {3}
}

@article{10.5555/2503308.2188401,
author = {Raykar, Vikas C. and Yu, Shipeng},
title = {Eliminating spammers and ranking annotators for crowdsourced labeling tasks},
year = {2012},
issue_date = {January 2012},
publisher = {JMLR.org},
volume = {13},
number = {1},
issn = {1532-4435},
abstract = {With the advent of crowdsourcing services it has become quite cheap and reasonably effective to get a data set labeled by multiple annotators in a short amount of time. Various methods have been proposed to estimate the consensus labels by correcting for the bias of annotators with different kinds of expertise. Since we do not have control over the quality of the annotators, very often the annotations can be dominated by spammers, defined as annotators who assign labels randomly without actually looking at the instance. Spammers can make the cost of acquiring labels very expensive and can potentially degrade the quality of the final consensus labels. In this paper we propose an empirical Bayesian algorithm called SpEMthat iteratively eliminates the spammers and estimates the consensus labels based only on the good annotators. The algorithm is motivated by defining a spammer score that can be used to rank the annotators. Experiments on simulated and real data show that the proposed approach is better than (or as good as) the earlier approaches in terms of the accuracy and uses a significantly smaller number of annotators.},
journal = {J. Mach. Learn. Res.},
month = feb,
pages = {491–518},
numpages = {28},
keywords = {spammers, ranking annotators, multiple annotators, crowdsourcing}
}

@article{10.5555/2188385.2188401,
author = {Raykar, Vikas C. and Yu, Shipeng},
title = {Eliminating spammers and ranking annotators for crowdsourced labeling tasks},
year = {2012},
issue_date = {3/1/2012},
publisher = {JMLR.org},
volume = {13},
number = {null},
issn = {1532-4435},
abstract = {With the advent of crowdsourcing services it has become quite cheap and reasonably effective to get a data set labeled by multiple annotators in a short amount of time. Various methods have been proposed to estimate the consensus labels by correcting for the bias of annotators with different kinds of expertise. Since we do not have control over the quality of the annotators, very often the annotations can be dominated by spammers, defined as annotators who assign labels randomly without actually looking at the instance. Spammers can make the cost of acquiring labels very expensive and can potentially degrade the quality of the final consensus labels. In this paper we propose an empirical Bayesian algorithm called SpEMthat iteratively eliminates the spammers and estimates the consensus labels based only on the good annotators. The algorithm is motivated by defining a spammer score that can be used to rank the annotators. Experiments on simulated and real data show that the proposed approach is better than (or as good as) the earlier approaches in terms of the accuracy and uses a significantly smaller number of annotators.},
journal = {J. Mach. Learn. Res.},
month = feb,
pages = {491–518},
numpages = {28},
keywords = {spammers, ranking annotators, multiple annotators, crowdsourcing}
}

@article{10.1145/2897510,
author = {Xie, Hong and Lui, John C. S. and Towsley, Don},
title = {Design and Analysis of Incentive and Reputation Mechanisms for Online Crowdsourcing Systems},
year = {2016},
issue_date = {May 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {1},
number = {3},
issn = {2376-3639},
url = {https://doi.org/10.1145/2897510},
doi = {10.1145/2897510},
abstract = {Today, online crowdsourcing services like Amazon Mechanical Turk, UpWork, and Yahoo! Answers are gaining in popularity. For such online services, it is important to attract “workers” to provide high-quality solutions to the “tasks” outsourced by “requesters.” The challenge is that workers have different skill sets and can provide different amounts of effort. In this article, we design a class of incentive and reputation mechanisms to solicit high-quality solutions from workers. Our incentive mechanism allows multiple workers to solve a task, splits the reward among workers based on requester evaluations of the solution quality, and guarantees that high-skilled workers provide high-quality solutions. However, our incentive mechanism suffers the potential risk that a requester will eventually collects low-quality solutions due to fundamental limitations in task assigning accuracy. Our reputation mechanism ensures that low-skilled workers do not provide low-quality solutions by tracking workers’ historical contributions and penalizing those workers having poor reputations. We show that by coupling our reputation mechanism with our incentive mechanism, a requester can collect at least one high-quality solution. We present an optimization framework to select parameters for our reputation mechanism. We show that there is a trade-off between system efficiency (i.e., the number of tasks that can be solved for a given reward) and revenue (i.e., the amount of transaction fees), and we present the optimal trade-off curve between system efficiency and revenue. We demonstrate the applicability and effectiveness of our mechanisms through experiments using a real-world dataset from UpWork. We infer model parameters from this data, use them to determine proper rewards, and select the parameters of our incentive and reputation mechanisms for UpWork. Experimental results show that our incentive and reputation mechanisms achieve 98.82\% of the maximum system efficiency while only sacrificing 4\% of revenue.},
journal = {ACM Trans. Model. Perform. Eval. Comput. Syst.},
month = may,
articleno = {13},
numpages = {27},
keywords = {repeated game, equilibrium, Bayesian game}
}

@article{10.5555/2627435.2697067,
author = {Nguyen-Dinh, Long-Van and Calatroni, Alberto and Tr\"{o}ster, Gerhard},
title = {Robust online gesture recognition with crowdsourced annotations},
year = {2014},
issue_date = {January 2014},
publisher = {JMLR.org},
volume = {15},
number = {1},
issn = {1532-4435},
abstract = {Crowdsourcing is a promising way to reduce the effort of collecting annotations for training gesture recognition systems. Crowdsourced annotations suffer from "noise" such as mislabeling, or inaccurate identification of start and end time of gesture instances. In this paper we present SegmentedLCSS and WarpingLCSS, two template-matching methods offering robustness when trained with noisy crowdsourced annotations to spot gestures from wearable motion sensors. The methods quantize signals into strings of characters and then apply variations of the longest common subsequence algorithm (LCSS) to spot gestures. We compare the noise robustness of our methods against baselines which use dynamic time warping (DTW) and support vector machines (SVM). The experiments are performed on data sets with various gesture classes (10-17 classes) recorded from accelerometers on arms, with both real and synthetic crowdsourced annotations. WarpingLCSS has similar or better performance than baselines in absence of noisy annotations. In presence of 60\% mislabeled instances, WarpingLCSS outperformed SVM by 22\% F1-score and outperformed DTW-based methods by 36\% F1-score on average. SegmentedLCSS yields similar performance as WarpingLCSS, however it performs one order of magnitude slower. Additionally, we show to use our methods to filter out the noise in the crowdsourced annotation before training a traditional classifier. The filtering increases the performance of SVM by 20\% F1-score and of DTW-based methods by 8\% F1-score on average in the noisy real crowdsourced annotations.},
journal = {J. Mach. Learn. Res.},
month = jan,
pages = {3187–3220},
numpages = {34},
keywords = {template matching methods, longest common subsequence, gesture spotting, crowdsourced annotation, accelerometer sensors}
}

@article{10.1162/COLI_a_00169,
author = {Zaidan, Omar F. and Callison-Burch, Chris},
title = {Arabic dialect identification},
year = {2014},
issue_date = {March 2014},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
volume = {40},
number = {1},
issn = {0891-2017},
url = {https://doi.org/10.1162/COLI_a_00169},
doi = {10.1162/COLI_a_00169},
abstract = {The written form of the Arabic language, Modern Standard Arabic MSA, differs in a non-trivial manner from the various spoken regional dialects of Arabic "the true native language" of Arabic speakers. Those dialects, in turn, differ quite a bit from each other. However, due to MSA's prevalence in written form, almost all Arabic data sets have predominantly MSA content. In this article, we describe the creation of a novel Arabic resource with dialect annotations. We have created a large monolingual data set rich in dialectal Arabic content called the Arabic On-line Commentary Data set Zaidan and Callison-Burch 2011. We describe our annotation effort to identify the dialect level and dialect itself in each of more than 100,000 sentences from the data set by crowdsourcing the annotation task, and delve into interesting annotator behaviors like over-identification of one's own dialect. Using this new annotated data set, we consider the task of Arabic dialect identification: Given the word sequence forming an Arabic sentence, determine the variety of Arabic in which it is written. We use the data to train and evaluate automatic classifiers for dialect identification, and establish that classifiers using dialectal data significantly and dramatically outperform baselines that use MSA-only data, achieving near-human classification accuracy. Finally, we apply our classifiers to discover dialectical data from a large Web crawl consisting of 3.5 million pages mined from on-line Arabic newspapers.},
journal = {Comput. Linguist.},
month = mar,
pages = {171–202},
numpages = {32}
}

@article{10.14778/2824032.2824062,
author = {Haas, Daniel and Ansel, Jason and Gu, Lydia and Marcus, Adam},
title = {Argonaut: macrotask crowdsourcing for complex data processing},
year = {2015},
issue_date = {August 2015},
publisher = {VLDB Endowment},
volume = {8},
number = {12},
issn = {2150-8097},
url = {https://doi.org/10.14778/2824032.2824062},
doi = {10.14778/2824032.2824062},
abstract = {Crowdsourced workflows are used in research and industry to solve a variety of tasks. The databases community has used crowd workers in query operators/optimization and for tasks such as entity resolution. Such research utilizes microtasks where crowd workers are asked to answer simple yes/no or multiple choice questions with little training. Typically, microtasks are used with voting algorithms to combine redundant responses from multiple crowd workers to achieve result quality. Microtasks are powerful, but fail in cases where larger context (e.g., domain knowledge) or significant time investment is needed to solve a problem, for example in large-document structured data extraction.In this paper, we consider context-heavy data processing tasks that may require many hours of work, and refer to such tasks as macrotasks. Leveraging the infrastructure and worker pools of existing crowdsourcing platforms, we automate macrotask scheduling, evaluation, and pay scales. A key challenge in macrotask-powered work, however, is evaluating the quality of a worker's output, since ground truth is seldom available and redundancy-based quality control schemes are impractical. We present Argonaut, a framework that improves macrotask powered work quality using a hierarchical review. Argonaut uses a predictive model of worker quality to select trusted workers to perform review, and a separate predictive model of task quality to decide which tasks to review. Finally, Argonaut can identify the ideal trade-off between a single phase of review and multiple phases of review given a constrained review budget in order to maximize overall output quality. We evaluate an industrial use of Argonaut to power a structured data extraction pipeline that has utilized over half a million hours of crowd worker input to complete millions of macrotasks. We show that Argonaut can capture up to 118\% more errors than random spot-check reviews in review budget-constrained environments with up to two review layers.},
journal = {Proc. VLDB Endow.},
month = aug,
pages = {1642–1653},
numpages = {12}
}

@article{10.14778/2047485.2047487,
author = {Marcus, Adam and Wu, Eugene and Karger, David and Madden, Samuel and Miller, Robert},
title = {Human-powered sorts and joins},
year = {2011},
issue_date = {September 2011},
publisher = {VLDB Endowment},
volume = {5},
number = {1},
issn = {2150-8097},
url = {https://doi.org/10.14778/2047485.2047487},
doi = {10.14778/2047485.2047487},
abstract = {Crowdsourcing markets like Amazon's Mechanical Turk (MTurk) make it possible to task people with small jobs, such as labeling images or looking up phone numbers, via a programmatic interface. MTurk tasks for processing datasets with humans are currently designed with significant reimplementation of common workflows and ad-hoc selection of parameters such as price to pay per task. We describe how we have integrated crowds into a declarative workflow engine called Qurk to reduce the burden on workflow designers. In this paper, we focus on how to use humans to compare items for sorting and joining data, two of the most common operations in DBMSs. We describe our basic query interface and the user interface of the tasks we post to MTurk. We also propose a number of optimizations, including task batching, replacing pairwise comparisons with numerical ratings, and pre-filtering tables before joining them, which dramatically reduce the overall cost of running sorts and joins on the crowd. In an experiment joining two sets of images, we reduce the overall cost from $67 in a naive implementation to about $3, without substantially affecting accuracy or latency. In an end-to-end experiment, we reduced cost by a factor of 14.5.},
journal = {Proc. VLDB Endow.},
month = sep,
pages = {13–24},
numpages = {12}
}

@article{10.14778/2535568.2448944,
author = {Marcus, Adam and Karger, David and Madden, Samuel and Miller, Robert and Oh, Sewoong},
title = {Counting with the crowd},
year = {2012},
issue_date = {December 2012},
publisher = {VLDB Endowment},
volume = {6},
number = {2},
issn = {2150-8097},
url = {https://doi.org/10.14778/2535568.2448944},
doi = {10.14778/2535568.2448944},
abstract = {In this paper, we address the problem of selectivity estimation in a crowdsourced database. Specifically, we develop several techniques for using workers on a crowdsourcing platform like Amazon's Mechanical Turk to estimate the fraction of items in a dataset (e.g., a collection of photos) that satisfy some property or predicate (e.g., photos of trees). We do this without explicitly iterating through every item in the dataset. This is important in crowd-sourced query optimization to support predicate ordering and in query evaluation, when performing a GROUP BY operation with a COUNT or AVG aggregate. We compare sampling item labels, a traditional approach, to showing workers a collection of items and asking them to estimate how many satisfy some predicate. Additionally, we develop techniques to eliminate spammers and colluding attackers trying to skew selectivity estimates when using this count estimation approach. We find that for images, counting can be much more effective than sampled labeling, reducing the amount of work necessary to arrive at an estimate that is within 1\% of the true fraction by up to an order of magnitude, with lower worker latency. We also find that sampled labeling outperforms count estimation on a text processing task, presumably because people are better at quickly processing large batches of images than they are at reading strings of text. Our spammer detection technique, which is applicable to both the label- and count-based approaches, can improve accuracy by up to two orders of magnitude.},
journal = {Proc. VLDB Endow.},
month = dec,
pages = {109–120},
numpages = {12}
}

@article{10.14778/2536258.2536270,
author = {Rekatsinas, Theodoros and Deshpande, Amol and Machanavajjhala, Ashwin},
title = {SPARSI: partitioning sensitive data amongst multiple adversaries},
year = {2013},
issue_date = {August 2013},
publisher = {VLDB Endowment},
volume = {6},
number = {13},
issn = {2150-8097},
url = {https://doi.org/10.14778/2536258.2536270},
doi = {10.14778/2536258.2536270},
abstract = {We present SPARSI, a novel theoretical framework for partitioning sensitive data across multiple non-colluding adversaries. Most work in privacy-aware data sharing has considered disclosing summaries where the aggregate information about the data is preserved, but sensitive user information is protected. Nonetheless, there are applications, including online advertising, cloud computing and crowdsourcing markets, where detailed and fine-grained user data must be disclosed. We consider a new data sharing paradigm and introduce the problem of privacy-aware data partitioning, where a sensitive dataset must be partitioned among k untrusted parties (adversaries). The goal is to maximize the utility derived by partitioning and distributing the dataset, while minimizing the total amount of sensitive information disclosed. The data should be distributed so that an adversary, without colluding with other adversaries, cannot draw additional inferences about the private information, by linking together multiple pieces of information released to her. The assumption of no collusion is both reasonable and necessary in the above application domains that require release of private user information. SPARSI enables us to formally define privacy-aware data partitioning using the notion of sensitive properties for modeling private information and a hypergraph representation for describing the interdependencies between data entries and private information. We show that solving privacy-aware partitioning is, in general, NP-hard, but for specific information disclosure functions, good approximate solutions can be found using relaxation techniques. Finally, we present a local search algorithm applicable to generic information disclosure functions. We conduct a rigorous performance evaluation with real-world and synthetic datasets that illustrates the effectiveness of SPARSI at partitioning sensitive data while minimizing disclosure.},
journal = {Proc. VLDB Endow.},
month = aug,
pages = {1594–1605},
numpages = {12}
}

@article{10.1007/s00778-013-0324-z,
author = {Demartini, Gianluca and Difallah, Djellel Eddine and Cudr\'{e}-Mauroux, Philippe},
title = {Large-scale linked data integration using probabilistic reasoning and crowdsourcing},
year = {2013},
issue_date = {October   2013},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {22},
number = {5},
issn = {1066-8888},
url = {https://doi.org/10.1007/s00778-013-0324-z},
doi = {10.1007/s00778-013-0324-z},
abstract = {We tackle the problems of semiautomatically matching linked data sets and of linking large collections of Web pages to linked data. Our system, ZenCrowd, (1) uses a three-stage blocking technique in order to obtain the best possible instance matches while minimizing both computational complexity and latency, and (2) identifies entities from natural language text using state-of-the-art techniques and automatically connects them to the linked open data cloud. First, we use structured inverted indices to quickly find potential candidate results from entities that have been indexed in our system. Our system then analyzes the candidate matches and refines them whenever deemed necessary using computationally more expensive queries on a graph database. Finally, we resort to human computation by dynamically generating crowdsourcing tasks in case the algorithmic components fail to come up with convincing results. We integrate all results from the inverted indices, from the graph database and from the crowd using a probabilistic framework in order to make sensible decisions about candidate matches and to identify unreliable human workers. In the following, we give an overview of the architecture of our system and describe in detail our novel three-stage blocking technique and our probabilistic decision framework. We also report on a series of experimental results on a standard data set, showing that our system can achieve a 95 \% average accuracy on instance matching (as compared to the initial 88 \% average accuracy of the purely automatic baseline) while drastically limiting the amount of work performed by the crowd. The experimental evaluation of our system on the entity linking task shows an average relative improvement of 14 \% over our best automatic approach.},
journal = {The VLDB Journal},
month = oct,
pages = {665–687},
numpages = {23},
keywords = {Probabilistic reasoning, Instance matching, Entity linking, Data integration, Crowdsourcing}
}

@article{10.1162/COLI_a_00199,
author = {Mairesse, Fran\c{c}ois and Young, Steve},
title = {Stochastic language generation in dialogue using factored language models},
year = {2014},
issue_date = {December 2014},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
volume = {40},
number = {4},
issn = {0891-2017},
url = {https://doi.org/10.1162/COLI_a_00199},
doi = {10.1162/COLI_a_00199},
abstract = {Most previous work on trainable language generation has focused on two paradigms: (a) using a generation decisions of an existing generator. Both approaches rely on the existence of a handcrafted generation component, which is likely to limit their scalability to new domains. The first contribution of this article is to present Bagel, a fully data-driven generation method that treats the language generation task as a search for the most likely sequence of semantic concepts and realization phrases, according to Factored Language Models (FLMs). As domain utterances are not readily available for most natural language generation tasks, a large creative effort is required to produce the data necessary to represent human linguistic variation for nontrivial domains. This article is based on the assumption that learning to produce paraphrases can be facilitated by collecting data from a large sample of untrained annotators using crowdsourcing—rather than a few domain experts—by relying on a coarse meaning representation. A second contribution of this article is to use crowdsourced data to show how dialogue naturalness can be improved by learning to vary the output utterances generated for a given semantic input. Two data-driven methods for generating paraphrases in dialogue are presented: (a) by sampling from the n-best list of realizations produced by Bagel's FLM reranker; and (b) by learning a structured perceptron predicting whether candidate realizations are valid paraphrases. We train Bagel on a set of 1,956 utterances produced by 137 annotators, which covers 10 types of dialogue acts and 128 semantic concepts in a tourist information system for Cambridge. An automated evaluation shows that Bagel outperforms utterance class LM baselines on this domain. A human evaluation of 600 resynthesized dialogue extracts shows that Bagel's FLM output produces utterances comparable to a handcrafted baseline, whereas the perceptron classifier performs worse. Interestingly, human judges find the system sampling from the n-best list to be more natural than a system always returning the first-best utterance. The judges are also more willing to interact with the n-best system in the future. These results suggest that capturing the large variation found in human language using data-driven methods is beneficial for dialogue interaction.},
journal = {Comput. Linguist.},
month = dec,
pages = {763–799},
numpages = {37}
}

