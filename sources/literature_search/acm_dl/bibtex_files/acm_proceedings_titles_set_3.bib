@inproceedings{10.1145/3274895.3274981,
author = {Costa, Camila F. and Nascimento, Mario A.},
title = {In-route task selection in crowdsourcing},
year = {2018},
isbn = {9781450358897},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3274895.3274981},
doi = {10.1145/3274895.3274981},
abstract = {We consider a spatial crowdsourcing scenario where (1) a worker is traveling on a preferred/typical path within a road network where (2) there is a set of tasks, each associated with a positive reward, available to be performed and (3) that the worker is willing to possibly deviate from his/her preferred path to perform tasks as long as (4) he/she travels at most a total given distance/time. We name this the In-Route Task Selection (IRTS) problem and investigate it using the skyline paradigm in order to obtain a set of diverse solutions yielding good combinations of detour and reward. Given the NP-hardness of the IRTS problem we present a heuristic approach that produces solutions with good values of precision and recall for problems of realistic sizes within practical query processing time.},
booktitle = {Proceedings of the 26th ACM SIGSPATIAL International Conference on Advances in Geographic Information Systems},
pages = {524–527},
numpages = {4},
keywords = {in-route queries, road networks, skyline, spatial crowdsourcing},
location = {Seattle, Washington},
series = {SIGSPATIAL '18}
}

@inproceedings{10.1145/3491102.3517653,
author = {Jung, Ji-Youn and Qiu, Sihang and Bozzon, Alessandro and Gadiraju, Ujwal},
title = {Great Chain of Agents: The Role of Metaphorical Representation of Agents in Conversational Crowdsourcing},
year = {2022},
isbn = {9781450391573},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3491102.3517653},
doi = {10.1145/3491102.3517653},
abstract = {Conversational agents are being widely adopted across several domains to serve a variety of purposes ranging from providing intelligent assistance to companionship. Recent literature has shown that users develop intuitive folk theories and a metaphorical understanding of conversational agents (CAs) due to the lack of a mental model of the agents. However, investigation of metaphorical agent representation in the HCI community has mainly focused on the human level, despite non-human metaphors for agents being prevalent in the real world. We adopted Lakoff and Turner’s ‘Great Chain of Being’ framework to systematically investigate the impact of using non-human metaphors to represent conversational agents on worker engagement in crowdsourcing marketplaces. We designed a text-based conversational agent that assists crowd workers in task execution. Through a between-subjects experimental study (N = 341), we explored how different human and non-human metaphors affect worker engagement, the perceived cognitive load of workers, intrinsic motivation, and their trust in the agents. Our findings bridge the gap of how users experience CAs with non-human metaphors in the context of conversational crowdsourcing.},
booktitle = {Proceedings of the 2022 CHI Conference on Human Factors in Computing Systems},
articleno = {57},
numpages = {22},
keywords = {Conceptual metaphors, Conversational agent, Crowdsourcing, Engagement, Great chain of being, Human-AI interaction, Human-agent interaction, Trust},
location = {New Orleans, LA, USA},
series = {CHI '22}
}

@inproceedings{10.1145/3269206.3269292,
author = {Mizusawa, Ken and Tajima, Keishi and Matsubara, Masaki and Amagasa, Toshiyuki and Morishima, Atsuyuki},
title = {Efficient Pipeline Processing of Crowdsourcing Workflows},
year = {2018},
isbn = {9781450360142},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3269206.3269292},
doi = {10.1145/3269206.3269292},
abstract = {This paper addresses the pipeline processing of sequential workflows in crowdsourcing. Sequential workflows consisting of several subtasks are ubiquitous in crowdsourcing. Our approach is to control the budget distribution to subtasks in order to balance the execution speed of the subtasks and to improve throughput of overall sequential workflows. As we cannot control the price for earlier steps retrospectively in the stepwise batch execution, we explore pipeline processing schemes. Our experimental results show that our pipeline processing scheme with price control achieves significantly higher throughput of sequential workflows.},
booktitle = {Proceedings of the 27th ACM International Conference on Information and Knowledge Management},
pages = {1559–1562},
numpages = {4},
keywords = {crowdsourcing, improved throughput, price control},
location = {Torino, Italy},
series = {CIKM '18}
}

@inproceedings{10.1007/978-3-030-47436-2_20,
author = {Kim, Joonyoung and Lee, Donghyeon and Jung, Kyomin},
title = {Reliable Aggregation Method for Vector Regression Tasks in Crowdsourcing},
year = {2020},
isbn = {978-3-030-47435-5},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-47436-2_20},
doi = {10.1007/978-3-030-47436-2_20},
abstract = {Crowdsourcing platforms are widely used for collecting large amount of labeled data. Due to low-paid workers and inherent noise, the quality of acquired data could be easily degraded. To solve this, most previous studies have sought to infer the true answer from noisy labels in discrete multiple-choice tasks that ask workers to select one of several answer candidates. However, recent crowdsourcing tasks have become more complicated and usually consist of real-valued vectors. In this paper, we propose a novel inference algorithm for vector regression tasks which ask workers to provide accurate vectors such as image object localization and human posture estimation. Our algorithm can estimate the true answer of each task and a reliability of each worker by updating two types of messages iteratively. We also prove its performance bound which depends on the number of queries per task and the average quality of workers. Under a certain condition, we prove that its average performance becomes close to an oracle estimator which knows the reliability of every worker. Through extensive experiments with both real-world and synthetic datasets, we verify that our algorithm are superior to other state-of-the-art algorithms.},
booktitle = {Advances in Knowledge Discovery and Data Mining: 24th Pacific-Asia Conference, PAKDD 2020, Singapore, May 11–14, 2020, Proceedings, Part II},
pages = {261–273},
numpages = {13},
keywords = {Crowdsourcing, Vector regression, Algorithm},
location = {Singapore, Singapore}
}

@inproceedings{10.1145/3379336.3381499,
author = {Tauchmann, Christopher and Daxenberger, Johannes and Mieskes, Margot},
title = {The Influence of Input Data Complexity on Crowdsourcing Quality},
year = {2020},
isbn = {9781450375139},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3379336.3381499},
doi = {10.1145/3379336.3381499},
abstract = {Crowdsourcing has a huge impact on data gathering for NLP tasks. However, most quality control measures rely on data aggregation methods which are only employed after the crowdsourcing process and thus cannot deal with different worker qualifications during data gathering. This is time-consuming and cost-ineffective because some datapoints might have to be re-labeled or discarded. Training workers and distributing work according to worker qualifications beforehand helps to overcome this limitation. We propose a setup that accounts for input data complexity and allows only a set of workers that successfully completed tasks of rising complexity to continue work on more difficult subsets. Like this, we are able to train workers and at the same time exclude unqualified workers. In initial experiments, our method achieves higher agreement with four annotations by qualified crowd workers compared to five annotations from random crowd workers on the same dataset.},
booktitle = {Companion Proceedings of the 25th International Conference on Intelligent User Interfaces},
pages = {71–72},
numpages = {2},
keywords = {Crowdsourcing, Natural Language Processing, Task distribution},
location = {Cagliari, Italy},
series = {IUI '20 Companion}
}

@inproceedings{10.1109/DySPAN.2017.7920766,
author = {Van den Bergh, Bertold and Giustiniano, Domenico and Cordob\'{e}s, H\'{e}ctor and Fuchs, Markus and Calvo-Palomino, Roberto and Pollin, Sofie and Rajendran, Sreeraj and Lenders, Vincent},
title = {Electrosense: Crowdsourcing spectrum monitoring},
year = {2017},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/DySPAN.2017.7920766},
doi = {10.1109/DySPAN.2017.7920766},
abstract = {We present Electrosense: a distributed, collaborative and low-cost wireless spectrum monitoring solution which is deployed on a large scale. The proposed framework provides tools to enable and promote a crowdsourced open spectrum monitoring platform for wide area deployments. The collected spectrum data is stored and processed in the backend which can be easily retrieved by the users through an open API. The framework also allows using various signal processing algorithms deployed on the sensors as well as in the backend. These algorithms provide statistics on spectrum usage, collaborative spectrum data decoding, help in applications like anomaly detection and localization. The goal of the demo is to introduce the framework, show the infrastructure already deployed, how to join the network and demo a few built-in applications.},
booktitle = {2017 IEEE International Symposium on Dynamic Spectrum Access Networks (DySPAN)},
pages = {1–2},
numpages = {2},
location = {Baltimore, MD, USA}
}

@inproceedings{10.1007/978-3-030-62005-9_32,
author = {Qian, Jing and Liu, Shushu and Liu, An},
title = {Spatial and Temporal Pricing Approach for Tasks in Spatial Crowdsourcing},
year = {2020},
isbn = {978-3-030-62004-2},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-62005-9_32},
doi = {10.1007/978-3-030-62005-9_32},
abstract = {Pricing is an important issue in spatial crowdsourcing (SC). Current pricing mechanisms are usually built on online learning algorithms, so they fail to capture the dynamics of users’ price preference timely. In this paper, we focus on the pricing for task requesters with the goal of maximizing the total revenue gained by the SC platform. By considering the relationship between the price and the task, space, and time, a spatial and temporal pricing framework based task-transaction history is proposed. We model the price of a task as a three-dimensional tensor (task-space-time) and complete the missing entries with the assistant of historical data and other three context matrices. We conduct extensive experiments on a real taxi-hailing dataset. The experimental results show the effectiveness of the proposed pricing framework.},
booktitle = {Web Information Systems Engineering – WISE 2020: 21st International Conference, Amsterdam, The Netherlands, October 20–24, 2020, Proceedings, Part I},
pages = {445–457},
numpages = {13},
keywords = {Spatial crowdsourcing, Pricing, Task assignment},
location = {Amsterdam, The Netherlands}
}

@inproceedings{10.1145/3336191.3371875,
author = {Drutsa, Alexey and Fedorova, Valentina and Ustalov, Dmitry and Megorskaya, Olga and Zerminova, Evfrosiniya and Baidakova, Daria},
title = {Practice of Efficient Data Collection via Crowdsourcing: Aggregation, Incremental Relabelling, and Pricing},
year = {2020},
isbn = {9781450368223},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3336191.3371875},
doi = {10.1145/3336191.3371875},
abstract = {In this tutorial, we present a portion of unique industry experience in efficient data labelling via crowdsourcing shared by both leading researchers and engineers from Yandex. We will make an introduction to data labelling via public crowdsourcing marketplaces and will present key components of efficient label collection. This will be followed by a practice session, where participants will choose one of the real label collection tasks, experiment with selecting settings for the labelling process, and launch their label collection project on Yandex.Toloka, one of the largest crowdsourcing marketplaces. The projects will be run on real crowds within the tutorial session. Finally, participants will receive a feedback about their projects and practical advice to make them more efficient. We expect that our tutorial will address an audience with a wide range of background and interests. We do not require specific prerequisite knowledge or skills. We invite beginners, advanced specialists, and researchers to learn how to efficiently collect labelled data.},
booktitle = {Proceedings of the 13th International Conference on Web Search and Data Mining},
pages = {873–876},
numpages = {4},
keywords = {aggregation, crowdsourcing, data collection, efficient crowdsourcing pipeline, incremental relabelling, practice, pricing},
location = {Houston, TX, USA},
series = {WSDM '20}
}

@inproceedings{10.1609/aaai.v33i01.33012629,
author = {Zhao, Yan and Xia, Jinfu and Liu, Guanfeng and Su, Han and Lian, Defu and Shang, Shuo and Zheng, Kai},
title = {Preference-aware task assignment in spatial crowdsourcing},
year = {2019},
isbn = {978-1-57735-809-1},
publisher = {AAAI Press},
url = {https://doi.org/10.1609/aaai.v33i01.33012629},
doi = {10.1609/aaai.v33i01.33012629},
abstract = {With the ubiquity of smart devices, Spatial Crowdsourcing (SC) has emerged as a new transformative platform that engages mobile users to perform spatio-temporal tasks by physically traveling to specified locations. Thus, various SC techniques have been studied for performance optimization, among which one of the major challenges is how to assign workers the tasks that they are really interested in and willing to perform. In this paper, we propose a novel preference-aware spatial task assignment system based on workers' temporal preferences, which consists of two components: History-based Context-aware Tensor Decomposition (HCTD) for workers' temporal preferences modeling and preference-aware task assignment. We model worker preferences with a three-dimension tensor (worker-task-time). Supplementing the missing entries of the tensor through HCTD with the assistant of historical data and other two context matrices, we recover worker preferences for different categories of tasks in different time slots. Several preference-aware task assignment algorithms are then devised, aiming to maximize the total number of task assignments at every time instance, in which we give higher priorities to the workers who are more interested in the tasks. We conduct extensive experiments using a real dataset, verifying the practicability of our proposed methods.},
booktitle = {Proceedings of the Thirty-Third AAAI Conference on Artificial Intelligence and Thirty-First Innovative Applications of Artificial Intelligence Conference and Ninth AAAI Symposium on Educational Advances in Artificial Intelligence},
articleno = {324},
numpages = {8},
location = {Honolulu, Hawaii, USA},
series = {AAAI'19/IAAI'19/EAAI'19}
}

@inproceedings{10.1109/GLOCOM.2017.8254454,
author = {Chen, Yanjiao and Yin, Xiaoyan},
title = {Stable Job Assignment for Crowdsourcing},
year = {2017},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/GLOCOM.2017.8254454},
doi = {10.1109/GLOCOM.2017.8254454},
abstract = {In crowdsourcing systems, job assignment is one of the fundamental concerns. Existing works address the job assignment problem solely from the perspective of the crowdsourcer, aiming at maximizing the utility or minimizing the cost for the crowdsourcer. In this paper, we take into consideration users' preferences towards different jobs, and propose a novel matching framework for job assignment in crowdsourcing systems. Assigning multiple users to the same job will improve its quality, however, the crowdsourcer has to guarantee that the total payment to these users is less than the budget of this job. As users ask for different payments due to heterogeneity in their quality levels, classic deferred acceptance algorithm cannot reach a stable job assignment. In this paper, we formulate the job assignment problem in crowdsourcing systems as a many-to-one matching with budget constraints. Then, we design an algorithm that produces a stable job assignment in spite of user heterogeneity. Simulation results show that the proposed job assignment algorithm yields a high average job quality with a low computational complexity.},
booktitle = {GLOBECOM 2017 - 2017 IEEE Global Communications Conference},
pages = {1–6},
numpages = {6},
location = {Singapore}
}

@inproceedings{10.5555/3304652.3304754,
author = {Kohler, Rachel and Purviance, John and Luther, Kurt},
title = {Geolocating images with crowdsourcing and diagramming},
year = {2018},
isbn = {9780999241127},
publisher = {AAAI Press},
abstract = {Many types of investigative work involve verifying the legitimacy of visual evidence by identifying the precise geographic location where a photo or video was taken. Professional geolocation is often a manual, time-consuming process that can involve searching large areas of satellite imagery for potential matches. In this paper, we explore how crowdsourcing can be used to support expert image geolocation. We adapt an expert diagramming technique to overcome spatial reasoning limitations of novice crowds so that they can support an expert's search. In an experiment (n=540), we found that diagrams work significantly better than ground-level photos and allow crowds to reduce a search area by half before any expert intervention. We also discuss hybrid approaches to complex image analysis combining crowds, experts, and computer vision.},
booktitle = {Proceedings of the 27th International Joint Conference on Artificial Intelligence},
pages = {5299–5303},
numpages = {5},
location = {Stockholm, Sweden},
series = {IJCAI'18}
}

@inproceedings{10.5555/3237383.3238085,
author = {Truong, Nhat V.Q. and Stein, Sebastian and Tran-Thanh, Long and Jennings, Nicholas R.},
title = {Adaptive Incentive Selection for Crowdsourcing Contests},
year = {2018},
publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
address = {Richland, SC},
booktitle = {Proceedings of the 17th International Conference on Autonomous Agents and MultiAgent Systems},
pages = {2100–2102},
numpages = {3},
keywords = {budgeted mab, crowdsourcing contests, incentive},
location = {Stockholm, Sweden},
series = {AAMAS '18}
}

@inproceedings{10.1145/3313831.3376288,
author = {Davis, Keith M. and Kangassalo, Lauri and Spap\'{e}, Michiel and Ruotsalo, Tuukka},
title = {Brainsourcing: Crowdsourcing Recognition Tasks via Collaborative Brain-Computer Interfacing},
year = {2020},
isbn = {9781450367080},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3313831.3376288},
doi = {10.1145/3313831.3376288},
abstract = {This paper introduces brainsourcing: utilizing brain responses of a group of human contributors each performing a recognition task to determine classes of stimuli. We investigate to what extent it is possible to infer reliable class labels using data collected utilizing electroencephalography (EEG) from participants given a set of common stimuli. An experiment (N=30) measuring EEG responses to visual features of faces (gender, hair color, age, smile) revealed an improved F1 score of 0.94 for a crowd of twelve participants compared to an F1 score of 0.67 derived from individual participants and a random chance of 0.50. Our results demonstrate the methodological and pragmatic feasibility of brainsourcing in labeling tasks and opens avenues for more general applications using brain-computer interfacing in a crowdsourced setting.},
booktitle = {Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems},
pages = {1–14},
numpages = {14},
keywords = {brain-computer interfaces, brainsourcing, crowdsourcing},
location = {Honolulu, HI, USA},
series = {CHI '20}
}

@inproceedings{10.1007/978-3-030-29381-9_28,
author = {Hettiachchi, Danula and van Berkel, Niels and Hosio, Simo and Kostakos, Vassilis and Goncalves, Jorge},
title = {Effect of Cognitive Abilities on Crowdsourcing Task Performance},
year = {2019},
isbn = {978-3-030-29380-2},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-29381-9_28},
doi = {10.1007/978-3-030-29381-9_28},
abstract = {Matching crowd workers to suitable tasks is highly desirable as it can enhance task performance, reduce the cost for requesters, and increase worker satisfaction. In this paper, we propose a method that considers workers’ cognitive ability to predict their suitability for a wide range of crowdsourcing tasks. We measure cognitive ability via fast-paced online cognitive tests with a combined average duration of 6.2&nbsp;min. We then demonstrate that our proposed method can effectively assign or recommend workers to five different popular crowd tasks: Classification, Counting, Proofreading, Sentiment Analysis, and Transcription. Using our approach we demonstrate a significant improvement in the expected overall task accuracy. While previous methods require access to worker history or demographics, our work offers a quick and accurate way to determine which workers are more suitable for which tasks.},
booktitle = {Human-Computer Interaction – INTERACT 2019: 17th IFIP TC 13 International Conference, Paphos, Cyprus, September 2–6, 2019, Proceedings, Part I},
pages = {442–464},
numpages = {23},
keywords = {Crowdsourcing, Cognitive ability, Task performance},
location = {Paphos, Cyprus}
}

@inproceedings{10.1145/3132847.3133039,
author = {Yuan, Dong and Li, Guoliang and Li, Qi and Zheng, Yudian},
title = {Sybil Defense in Crowdsourcing Platforms},
year = {2017},
isbn = {9781450349185},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3132847.3133039},
doi = {10.1145/3132847.3133039},
abstract = {Crowdsourcing platforms have been widely deployed to solve many computer-hard problems, e.g., image recognition and entity resolution. Quality control is an important issue in crowdsourcing, which has been extensively addressed by existing quality-control algorithms, e.g., voting-based algorithms and probabilistic graphical models. However, these algorithms cannot ensure quality under sybil attacks, which leverages a large number of sybil accounts to generate results for dominating answers of normal workers. To address this problem, we propose a sybil defense framework for crowdsourcing, which can help crowdsourcing platforms to identify sybil workers and defense the sybil attack. We develop a similarity function to quantify worker similarity. Based on worker similarity, we cluster workers into different groups such that we can utilize a small number of golden questions to accurately identify the sybil groups. We also devise online algorithms to instantly detect sybil workers to throttle the attacks. Our method also has ability to detect multi-attackers in one task. To the best of our knowledge, this is the first framework for sybil defense in crowdsourcing. Experimental results on real-world datasets demonstrate that our method can effectively identify and throttle sybil workers.},
booktitle = {Proceedings of the 2017 ACM on Conference on Information and Knowledge Management},
pages = {1529–1538},
numpages = {10},
keywords = {crowdsourcing, sybil defense},
location = {Singapore, Singapore},
series = {CIKM '17}
}

@inproceedings{10.1145/3460319.3464805,
author = {Pan, Yicheng and Ma, Meng and Jiang, Xinrui and Wang, Ping},
title = {Faster, deeper, easier: crowdsourcing diagnosis of microservice kernel failure from user space},
year = {2021},
isbn = {9781450384599},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3460319.3464805},
doi = {10.1145/3460319.3464805},
abstract = {With the widespread use of cloud-native architecture, increasing web applications (apps) choose to build on microservices. Simultaneously, troubleshooting becomes full of challenges owing to the high dynamics and complexity of anomaly propagation. Existing diagnostic methods rely heavily on monitoring metrics collected from the kernel side of microservice systems. Without a comprehensive monitoring infrastructure, application owners and even cloud operators cannot resort to these kernel-space solutions. This paper summarizes several insights on operating a top commercial cloud platform. Then, for the first time, we put forward the idea of user-space diagnosis for microservice kernel failures. To this end, we develop a crowdsourcing solution - DyCause, to resolve the asymmetric diagnostic information problem. DyCause deploys on the application side in a distributed manner. Through lightweight API log sharing, apps collect the operational status of kernel services collaboratively and initiate diagnosis on demand. Deploying DyCause is fast and lightweight as we do not have any architectural and functional requirements for the kernel. To reveal more accurate correlations from asymmetric diagnostic information, we design a novel statistical algorithm that can efficiently discover the time-varying causalities between services. This algorithm also helps us build the temporal order of the anomaly propagation. Therefore, by using DyCause, we can obtain more in-depth and interpretable diagnostic clues with limited indicators. We apply and evaluate DyCause on both a simulated test-bed and a real-world cloud system. Experimental results verify that DyCause running in the user-space outperforms several state-of-the-art algorithms running in the kernel on accuracy. Besides, DyCause shows superior advantages in terms of algorithmic efficiency and data sensitivity. Simply put, DyCause produces a significantly better result than other baselines when analyzing much fewer or sparser metrics. To conclude, DyCause is faster to act, deeper in analysis, and easier to deploy.},
booktitle = {Proceedings of the 30th ACM SIGSOFT International Symposium on Software Testing and Analysis},
pages = {646–657},
numpages = {12},
keywords = {dynamic service dependency, granger causal intervals, microservice system, root cause analysis},
location = {Virtual, Denmark},
series = {ISSTA 2021}
}

@inproceedings{10.1007/978-3-030-22338-0_36,
author = {Tavanapour, Navid and Bittner, Eva A. C.},
title = {Human Collaboration on Crowdsourcing Platforms – a Content Analysis},
year = {2019},
isbn = {978-3-030-22337-3},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-22338-0_36},
doi = {10.1007/978-3-030-22338-0_36},
abstract = {The crowdsourcing phenomenon offers the opportunity to address an open call to the crowd. Crowd workers may work together to find a solution that satisfies the open call. One of the major benefits for a crowdsourcer is the pool of crowd workers that can be accessed over crowdsourcing platforms. However, the produced outcomes of crowd workers are often on a low level with weak elaboration and quality. The key to high quality work is the collaboration of crowd workers. This has already been addressed in the collaboration process design framework for crowdsourcing (CPDF). At this point we position this work and widen our view by conducting a content analysis on crowdsourcing platforms in order to understand the collaboration of crowd workers on real world crowdsourcing platforms better as well as investigate the weaknesses of the collaboration process design framework for crowdsourcing to improve work practices. By doing so, we redesign the CPDF based on the results of the content analysis and present an improved collaboration process of crowd workers within the CPDF.},
booktitle = {HCI in Business, Government and Organizations. Information Systems and Analytics: 6th International Conference, HCIBGO 2019, Held as Part of the 21st HCI International Conference, HCII 2019, Orlando, FL, USA, July 26-31, 2019, Proceedings, Part II},
pages = {443–458},
numpages = {16},
keywords = {Human collaboration, Crowdsourcing, Content analysis, Design science research, Crowd work, CPDF},
location = {Orlando, FL, USA}
}

@inproceedings{10.5555/3304415.3304633,
author = {Fang, Yili and Sun, Hailong and Chen, Pengpeng and Huai, Jinpeng},
title = {On the cost complexity of crowdsourcing},
year = {2018},
isbn = {9780999241127},
publisher = {AAAI Press},
abstract = {Existing efforts mainly use empirical analysis to evaluate the effectiveness of crowdsourcing methods, which is often unreliable across experimental settings. Consequently, it is of great importance to study theoretical methods. This work, for the first time, defines the cost complexity of crowdsourcing, and presents two theorems to compute the cost complexity. Our theorems provide a general theoretical method to model the trade-off between costs and quality, which can be used to evaluate and design crowdsourcing algorithms, and characterize the complexity of crowdsourcing problems. Moreover, following our theorems, we prove a set of corollaries that can obtain existing theoretical results for special cases. We have verified our work theoretically and empirically.},
booktitle = {Proceedings of the 27th International Joint Conference on Artificial Intelligence},
pages = {1531–1537},
numpages = {7},
location = {Stockholm, Sweden},
series = {IJCAI'18}
}

@inproceedings{10.1109/SECON52354.2021.9491623,
author = {Aly, Heba and Youssef, Moustafa and Agrawala, Ashok},
title = {Better off This Way!: Ubiquitous Accessibility Digital Maps via Smartphone-based Crowdsourcing},
year = {2021},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/SECON52354.2021.9491623},
doi = {10.1109/SECON52354.2021.9491623},
abstract = {Accessibility maps are key to support individuals with disabilities to actively participate in the society. The Americans with Disabilities Act (ADA) defines minimum requirements for roads and other public accommodation spaces to be accessible. Yet, it is sufficient to have one accessible route in a place, and available digital-maps lack accessibility information to help finding that accessible route.In this paper, we present the AccessMap system to automatically extend road-maps with accessibility semantics. It enables indoor and outdoor spaces to be automatically marked as visually-impaired and/or wheel-chaired accessible/inaccessible. AccessMap passively crowdsources measurements from sensors available in the users' smartphones to detect accessibility semantics. It employs a probabilistic framework to build and update the map with the semantics. Evaluation of AccessMap in different countries shows that it can passively detect a wide-range of accessibility semantics with high precision and recall (on average around 89.8\% and 86.3\% respectively). Furthermore, its probabilistic crowdsourcing framework increases the generated map’s average precision and recall to 98.7\% and 99\% with as few as seven encounters per semantic.},
booktitle = {2021 18th Annual IEEE International Conference on Sensing, Communication, and Networking (SECON)},
pages = {1–9},
numpages = {9},
location = {Rome, Italy}
}

@inproceedings{10.1145/3474624.3476008,
author = {Santos, Vinicius and Iwazaki, Anderson and Souza, \'{E}rica and Felizardo, Katia and Vijaykumar, Nandamudi},
title = {CrowdSLR: a tool to support the use of crowdsourcing in systematic literature reviews},
year = {2021},
isbn = {9781450390613},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3474624.3476008},
doi = {10.1145/3474624.3476008},
abstract = {Systematic Literature Reviews (SLR) have been used by Software Engineering (SE) community to produce reliable scientific evidence. An SLR process can be exhaustive and time-consuming, therefore, many approaches have been proposed to reduce time and efforts during the SLR conduction process. Although the SLR process is amenable to automation, nowadays full automation is not yet possible. An alternative to reduce time and efforts in SLR conduction is the use of crowdsourcing. However, there is no crowdsourcing tool to support a crowd-based SLR process. In this context, we present CrowdSLR, a tool to support the application of crowdsourcing in SLR during the selection of primary studies. Furthermore, we present its main features, potential users, and the architecture that was implemented to allow researchers to adopt this tool. The results of the CrowdSLR application indicate that the tool is able to provide the use crowdsourcing during the SLR selection process. The results that the proposed tool, indeed, show a significant improvement in the crowdsourcing approach in terms of time and effort to facilitate the SLR selection activity. Demo Video: https://youtu.be/UoQTC-R-Mv0},
booktitle = {Proceedings of the XXXV Brazilian Symposium on Software Engineering},
pages = {341–346},
numpages = {6},
keywords = {Crowdsourcing, SLR, Systematic Literature Review, Tool},
location = {Joinville, Brazil},
series = {SBES '21}
}

@inproceedings{10.1109/INFOCOM42981.2021.9488748,
author = {Shi, Zhuan and Jiang, Shanyang and Zhang, Lan and Du, Yang and Li, Xiang-Yang},
title = {Crowdsourcing System for Numerical Tasks based on Latent Topic Aware Worker Reliability},
year = {2021},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/INFOCOM42981.2021.9488748},
doi = {10.1109/INFOCOM42981.2021.9488748},
abstract = {Crowdsourcing is a widely adopted way for various labor-intensive tasks. One of the core problems in crowdsourcing systems is how to assign tasks to most suitable workers for better results, which heavily relies on the accurate profiling of each worker’s reliability for different topics of tasks. Many previous work have studied worker reliability for either explicit topics represented by task descriptions or latent topics for categorical tasks. In this work, we aim to accurately estimate more fine-grained worker reliability for latent topics in numerical tasks, so as to further improve the result quality. We propose a bayesian probabilistic model named Gaussian Latent Topic Model(GLTM) to mine the latent topics of numerical tasks based on workers’ behaviors and to estimate workers’ topic-level reliability. By utilizing the GLTM, we propose a truth inference algorithm named TI-GLTM to accurately infer the tasks’ truth and topics simultaneously and dynamically update workers’ topic-level reliability. We also design an online task assignment mechanism called MRA-GLTM, which assigns appropriate tasks to workers with the Maximum Reduced Ambiguity principle. The experiment results show our algorithms can achieve significantly lower MAE and MSE than that of the state-of-the-art approaches.},
booktitle = {IEEE INFOCOM 2021 - IEEE Conference on Computer Communications},
pages = {1–10},
numpages = {10},
location = {Vancouver, BC, Canada}
}

@inproceedings{10.1145/2994310.2994347,
author = {Erti\"{o}, Titiana and Maunula, Gail L. and Blomqvist, Kirsimarja},
title = {Crowdsourcing: the state-of-the-art and the way forward},
year = {2016},
isbn = {9781450343671},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2994310.2994347},
doi = {10.1145/2994310.2994347},
abstract = {This one-day workshop, in the context of Academic Mindtrek 2016, brings academics and practitioners together for collaboration and co-creation of novel and relevant knowledge. In one setting, participants have the opportunity to receive feedback from fellow academics and practitioners in the workshop's morning session. The afternoon session goes one step further by allowing the reciprocal benefits of bringing these two groups together to foster a robust examination of current issues related to crowdsourcing through real-world topics raised by practitioners.},
booktitle = {Proceedings of the 20th International Academic Mindtrek Conference},
pages = {460–461},
numpages = {2},
keywords = {crowdsourcing, evidence-based workshop, knowledge management, technological disruption},
location = {Tampere, Finland},
series = {AcademicMindtrek '16}
}

@inproceedings{10.1145/3411764.3445493,
author = {Dudley, John J. and Jacques, Jason T. and Kristensson, Per Ola},
title = {Crowdsourcing Design Guidance for Contextual Adaptation of Text Content in Augmented Reality},
year = {2021},
isbn = {9781450380966},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3411764.3445493},
doi = {10.1145/3411764.3445493},
abstract = {Augmented Reality (AR) can deliver engaging user experiences that seamlessly meld virtual content with the physical environment. However, building such experiences is challenging due to the developer’s inability to assess how uncontrolled deployment contexts may influence the user experience. To address this issue, we demonstrate a method for rapidly conducting AR experiments and real-world data collection in the user’s own physical environment using a privacy-conscious mobile web application. The approach leverages the large number of distinct user contexts accessible through crowdsourcing to efficiently source diverse context and perceptual preference data. The insights gathered through this method complement emerging design guidance and sample-limited lab-based studies. The utility of the method is illustrated by re-examining the design challenge of adapting AR text content to the user’s environment. Finally, we demonstrate how gathered design insight can be operationalized to provide adaptive text content functionality in an AR headset.},
booktitle = {Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems},
articleno = {731},
numpages = {14},
keywords = {Augmented Reality, Crowdsourcing, Privacy},
location = {Yokohama, Japan},
series = {CHI '21}
}

@inproceedings{10.1145/3411764.3445181,
author = {Lemmer, Stephan J. and Song, Jean Y. and Corso, Jason J.},
title = {Crowdsourcing More Effective Initializations for Single-Target Trackers Through Automatic Re-querying},
year = {2021},
isbn = {9781450380966},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3411764.3445181},
doi = {10.1145/3411764.3445181},
abstract = {In single-target video object tracking, an initial bounding box is drawn around a target object and propagated through a video. When this bounding box is provided by a careful human expert, it is expected to yield strong overall tracking performance that can be mimicked at scale by novice crowd workers with the help of advanced quality control methods. However, we show through an investigation of 900 crowdsourced initializations that such quality control strategies are inadequate for this task in two major ways: first, the high level of redundancy in these methods (e.g., averaging multiple responses to reduce error) is unnecessary, as 23\% of crowdsourced initializations perform just as well as the gold-standard initialization. Second, even nearly perfect initializations can lead to degraded long-term performance due to the complexity of object tracking. Considering these findings, we evaluate novel approaches for automatically selecting bounding boxes to re-query, and introduce Smart Replacement, an efficient method that decides whether to use the crowdsourced replacement initialization.},
booktitle = {Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems},
articleno = {391},
numpages = {13},
keywords = {crowd-AI collaboration, crowdsourcing, seed rejection, single-target video object tracking, smart replacement},
location = {Yokohama, Japan},
series = {CHI '21}
}

@inproceedings{10.1145/3139367.3139405,
author = {Kostopoulos, Alexandros and Chochliouros, Ioannis and Papafili, Ioanna and Drakos, Andreas},
title = {Enhancing Privacy Using Crowdsourcing Mechanisms},
year = {2017},
isbn = {9781450353557},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3139367.3139405},
doi = {10.1145/3139367.3139405},
abstract = {Personal data have become a merchandisable asset encouraging various stakeholders to collect data and trade them without the end-user's awareness and acceptance. Privacy Flag combines crowd sourcing, ICT technology and legal expertise in order to enable citizens to monitor and control their privacy with a user friendly solution provided as a smart phone application and a web browser add-on. In this paper, firstly, we focus on collective protection frameworks and tools that intend to address the arising challenges with respect to citizen awareness over data protection. Then, we present the Universal Privacy Risk Area Assessment Methodology, as well as the main functionalities of two Privacy Flag tools we developed; the browser add-on and the smartphone application.1},
booktitle = {Proceedings of the 21st Pan-Hellenic Conference on Informatics},
articleno = {28},
numpages = {5},
keywords = {Privacy, browser add-on, data protection, smartphone application},
location = {Larissa, Greece},
series = {PCI '17}
}

@inproceedings{10.1145/3457339.3457979,
author = {P\"{o}pper, Christina},
title = {High we Fly: Wireless Witnessing and Crowdsourcing for Air-Traffic Communication Security},
year = {2021},
isbn = {9781450384025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3457339.3457979},
doi = {10.1145/3457339.3457979},
abstract = {In this talk I will address aviation security and secure surveillance/control that are crucial for the safety of air traffic. Two wireless systems widely deployed in this context are the Global Positioning System (GPS) and ADS-B (Automatic Dependent Surveillance Broadcast). I will discuss why wireless systems like GPS and ADS-B are hard to fully secure and I will present novel mechanisms to improve air-traffic-related security at the intersection of these two systems. In particular, I will talk about (1) Crowd-GPS-Sec, a system to detect and localize GPS spoofing attacks on moving airborne targets such as UAVs or commercial airliners, as well as (2) ADS-B-Trust, an approach to leverage machine learning for ADS-B and GPS attack detection. These systems demonstrate the potential of sensor-based crowdsourcing and wireless witnessing for attack detection purposes in a typically rather closed industry sector.},
booktitle = {Proceedings of the 7th ACM on Cyber-Physical System Security Workshop},
pages = {3–4},
numpages = {2},
keywords = {ADS-B, GPS, aviation security, wireless system security},
location = {Virtual Event, Hong Kong},
series = {CPSS '21}
}

@inproceedings{10.1007/978-3-031-25158-0_22,
author = {Gao, Yucen and Kong, Dejun and Dai, Haipeng and Gao, Xiaofeng and Zheng, Jiaqi and Wu, Fan and Chen, Guihai},
title = {DE-DQN: A Dual-Embedding Based Deep Q-Network for&nbsp;Task Assignment Problem in&nbsp;Spatial Crowdsourcing},
year = {2023},
isbn = {978-3-031-25157-3},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-031-25158-0_22},
doi = {10.1007/978-3-031-25158-0_22},
abstract = {Along with the rapid development of sharing economy, spatial crowdsourcing has become a hot topic of general interests in recent years. One of its core issues is the task assignment problem, in which tasks are released on the platforms and then assigned to available workers. Due to the various characteristics of tasks and workers, finding the optimal assignment scheme and routing plan are difficult.In this paper, we define the utility-driven destination-aware spatial task assignment problem (UDSTA), which is proved to be NP-complete. Our goal is to maximize the total utility of workers. We propose a dual-embedding based deep Q-Network (DE-DQN) to sequentially assign tasks to suitable workers. Specifically, we design a utility embedding to reflect the top-k utility tasks for workers and worker-task pairs, and propose a coverage embedding to represent the potential future utility of an assignment action. For the first time, we combine the dual embedding with DQN to realize the multi-task and multi-worker matching, and obtain route plans of workers. Experiments based on both synthetic and real-world datasets indicate that DE-DQN performs well and shows significant advantages over the baseline methods.},
booktitle = {Web and Big Data: 6th International Joint Conference, APWeb-WAIM 2022, Nanjing, China, November 25–27, 2022, Proceedings, Part I},
pages = {280–295},
numpages = {16},
keywords = {Dual embedding, Deep Q-Network, Task assignment, Spatial crowdsourcing},
location = {Nanjing, China}
}

@inproceedings{10.1609/aaai.v33i01.33019837,
author = {Sheng, Victor S. and Zhang, Jing},
title = {Machine learning with crowdsourcing: a brief summary of the past research and future directions},
year = {2019},
isbn = {978-1-57735-809-1},
publisher = {AAAI Press},
url = {https://doi.org/10.1609/aaai.v33i01.33019837},
doi = {10.1609/aaai.v33i01.33019837},
abstract = {With crowdsourcing systems, labels can be obtained with low cost, which facilitates the creation of training sets for prediction model learning. However, the labels obtained from crowdsourcing are often imperfect, which brings great challenges in model learning. Since 2008, the machine learning community has noticed the great opportunities brought by crowdsourcing and has developed a large number of techniques to deal with inaccuracy, randomness, and uncertainty issues when learning with crowdsourcing. This paper summarizes the technical progress in this field during past eleven years. We focus on two fundamental issues: the data (label) quality and the prediction model quality. For data quality, we summarize ground truth inference methods and some machine learning based methods to further improve data quality. For the prediction model quality, we summarize several learning paradigms developed under the crowdsourcing scenario. Finally, we further discuss several promising future research directions to attract researchers to make contributions in crowdsourcing.},
booktitle = {Proceedings of the Thirty-Third AAAI Conference on Artificial Intelligence and Thirty-First Innovative Applications of Artificial Intelligence Conference and Ninth AAAI Symposium on Educational Advances in Artificial Intelligence},
articleno = {1222},
numpages = {7},
location = {Honolulu, Hawaii, USA},
series = {AAAI'19/IAAI'19/EAAI'19}
}

@inproceedings{10.1145/3208031.3208036,
author = {de G\'{o}mez P\'{e}rez, David Gil and Suokas, Matti and Bednarik, Roman},
title = {Crowdsourcing pupil annotation datasets: boundary vs. center, what performs better?},
year = {2018},
isbn = {9781450357890},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3208031.3208036},
doi = {10.1145/3208031.3208036},
abstract = {Pupil-related feature detection is one of the most common approaches used in the eye-tracking literature and practice. Validation and benchmarking of the detection algorithms relies on accurate ground-truth datasets, but creating of these is costly. Many approaches have been used to obtain human based annotations. A recent proposal to obtain these work-intensive data is through a crowdsourced registration of the pupil center, in which a large number of users provide a single click to indicate the pupil center [Gil de G\'{o}mez P\'{e}rez and Bednarik 2018a]. In this paper we compare the existing approach to a method based on multiple clicks on the boundary of the pupil region, in order to determine which approach provides better results. To compare both methods, a new data collection was performed over the same image database. Several metrics were applied in order to evaluate the accuracy of the two methods.},
booktitle = {Proceedings of the 7th Workshop on Pervasive Eye Tracking and Mobile Eye-Based Interaction},
articleno = {3},
numpages = {7},
keywords = {RANSAC, annotation, border, ellipse, eye tracking, human-computer interaction, interaction design, pupil, tools},
location = {Warsaw, Poland},
series = {PETMEI '18}
}

@inproceedings{10.1145/3358961.3358970,
author = {Posadas, Brianna B. and Hanumappa, Mamatha and Gilbert, Juan E.},
title = {Opinions concerning crowdsourcing applications in agriculture in D.C.},
year = {2020},
isbn = {9781450376792},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3358961.3358970},
doi = {10.1145/3358961.3358970},
abstract = {As big data has become increasingly necessary in modern farming techniques, the dependence on high quality and quantity of ground truthing data has risen. Collecting ground truthing data is one of the most labor-intensive aspects of the research process. A crowdsourcing platform application to aid laypeople in completing ground truthing data can improve the quality and quantity of data for growers and agricultural researchers. Focus groups were conducted to gauge opinions on crowdsourcing initiatives in agriculture to inform the design of the platform. Preliminary results demonstrate that the greatest motivation for the participants was having opportunities to develop their skills and access to educational resources. They also discussed having a finite timeframe for collecting the data, feeling appreciated by the researchers, and being informed on the context and next steps of the research. The results of these focus groups will be used to develop design prototypes for the crowdsourcing platform.},
booktitle = {Proceedings of the IX Latin American Conference on Human Computer Interaction},
articleno = {3},
numpages = {4},
keywords = {big data, focus groups, precision agriculture, urban agriculture},
location = {Panama City, Panama},
series = {CLIHC '19}
}

@inproceedings{10.1109/ICRA48506.2021.9562114,
author = {Gorjup, Gal and Gerez, Lucas and Liarokapis, Minas},
title = {Enhancing Robot Perception in Grasping and Dexterous Manipulation through Crowdsourcing and Gamification},
year = {2021},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICRA48506.2021.9562114},
doi = {10.1109/ICRA48506.2021.9562114},
abstract = {Robot grasping and manipulation planning in unstructured and dynamic environments is heavily dependent on the attributes of manipulated objects. Although deep learning approaches have delivered exceptional performance in robot perception, human perception and reasoning are still superior in processing novel object classes. Moreover, training such models requires large datasets that are generally expensive to obtain. This work combines crowdsourcing and gamification to leverage human intelligence, enhancing the object recognition and attribute estimation aspects of robot perception. The framework employs an attribute matching system that encodes visual information into an online puzzle game, utilizing the collective intelligence of players to expand an initial attribute database and react to real-time perception conflicts. The framework is deployed and evaluated in a proof-of-concept application for enhancing object recognition in autonomous robot grasping and a model for estimating the response time is proposed. The obtained results demonstrate that given enough players, the framework can offer near real-time labeling of novel objects, based purely on visual information and human experience.},
booktitle = {2021 IEEE International Conference on Robotics and Automation (ICRA)},
pages = {2569–2575},
numpages = {7},
location = {Xi'an, China}
}

@inproceedings{10.1145/3022198.3023270,
author = {Gaikwad, Snehalkumar (Neil) S. and Whiting, Mark E. and Gamage, Dilrukshi and Mullings, Catherine A. and Majeti, Dinesh and Goyal, Shirish and Gilbee, Aaron and Chhibber, Nalin and Ginzberg, Adam and Richmond-Fuller, Angela and Matin, Sekandar and Sehgal, Vibhor and Sarma, Tejas Seshadri and Nasser, Ahmed and Ballav, Alipta and Regino, Jeff and Zhou, Sharon and Mananova, Kamila and Srinivas, Preethi and Ziulkoski, Karolina and Dhakal, Dinesh and Stolzoff, Alexander and Niranga, Senadhipathige S. and Salih, Mohamed Hashim and Sinha, Akshansh and Vaish, Rajan and Bernstein, Michael S.},
title = {The Daemo Crowdsourcing Marketplace},
year = {2017},
isbn = {9781450346887},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3022198.3023270},
doi = {10.1145/3022198.3023270},
abstract = {The success of crowdsourcing markets is dependent on a strong foundation of trust between workers and requesters. In current marketplaces, workers and requesters are often unable to trust each other's quality, and their mental models of tasks are misaligned due to ambiguous instructions or confusing edge cases. This breakdown of trust typically arises from (1) flawed reputation systems which do not accurately reflect worker and requester quality, and from (2) poorly designed tasks. In this demo, we present how Boomerang and Prototype Tasks, the fundamental building blocks of the Daemo crowdsourcing marketplace, help restore trust between workers and requesters. Daemo's Boomerang reputation system incentivizes alignment between opinion and ratings by determining the likelihood that workers and requesters will work together in the future based on how they rate each other. Daemo's Prototype tasks require that new tasks go through a feedback iteration phase with a small number of workers so that requesters can revise their instructions and task designs before launch.},
booktitle = {Companion of the 2017 ACM Conference on Computer Supported Cooperative Work and Social Computing},
pages = {1–4},
numpages = {4},
keywords = {Crowdsourcing Markets, Trust},
location = {Portland, Oregon, USA},
series = {CSCW '17 Companion}
}

@inproceedings{10.1007/978-3-030-03596-9_49,
author = {Binzagr, Faisal and Medjahed, Brahim},
title = {CrowdMashup: Recommending Crowdsourcing Teams for Mashup Development},
year = {2018},
isbn = {978-3-030-03595-2},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-03596-9_49},
doi = {10.1007/978-3-030-03596-9_49},
abstract = {Mashups involve the collaboration of multiple developers to build Web applications out of pre-existing APIs. A large body of research focused on recommending APIs for mashups. However, very few contributions looked at recommending developers. In this paper, we propose CrowdMashup, a crowdsourcing approach for mashup teams recommendation. We analyze online developer communities and API directories to infer developers’ interests in APIs through natural language processing. We predict missing interest values using the alternating least square method for collaborative filtering. We also model interactions (comments and replies) among developers as a weighted undirected graph and introduce a sociometric to identify socially related developers. We propose an algorithm, based on the concept of cliques in graph theory, that combines developers’ skills and sociometric to recommend efficient and balanced teams. We describe a prototype implementation and conduct extensive experiments on real-world data and APIs to evaluate our approach.},
booktitle = {Service-Oriented Computing: 16th International Conference, ICSOC 2018, Hangzhou, China, November 12-15, 2018, Proceedings},
pages = {679–693},
numpages = {15},
keywords = {Mashup, Crowdsourcing, Team recommendation, Sociometric, Skills},
location = {Hangzhou, China}
}

@inproceedings{10.1145/3428658.3431083,
author = {de Amorim, Marcello N. and Santos, Celso A. S. and de L. Tavares, Orivaldo},
title = {A Crowdsourcing Method for Sign Segmentation in Brazilian Sign Language Videos},
year = {2020},
isbn = {9781450381963},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3428658.3431083},
doi = {10.1145/3428658.3431083},
abstract = {Like the spoken languages, sign languages are not universal and vary in different countries. LIBRAS (Brazilian Sign Language) is the second official language of Brazil and it is the language adopted by Brazilian Deaf's community to communicate. The signs of LIBRAS are composed of hand configurations, facial expressions and are affected by space and intensity modifiers, which makes their recognition more complicated than the simple identification of hand signs. The signs are arranged, according to a grammar, respecting form phrases, clauses, and sentences like any other spoken or sign language. The automatic machine translation of a sign language typically includes an initial phase for detecting sign boundaries. In this paper, we apply a crowdsourcing method to identifying signs boundaries present in pre-recorded videos those features LIBRAS interpreters. The limits or boundaries of the signs in the videos were established from the processing of contributions from workers from different countries, who have supposedly never heard of LIBRAS nor any other sign languages. To evaluate the segmentation process, we compared the sign boundaries identified by the crowd with the ground truth provided by a team of LIBRAS experts, who also assessed the quality of the delimitation of the identified signs. Our analysis showed that our crowdsourcing method was able to get 93.75\% of the sign boundaries successfully.},
booktitle = {Proceedings of the Brazilian Symposium on Multimedia and the Web},
pages = {105–112},
numpages = {8},
keywords = {Crowdsourcing, LIBRAS, segmentation, translation video},
location = {S\~{a}o Lu\'{\i}s, Brazil},
series = {WebMedia '20}
}

@inproceedings{10.1145/3447548.3467070,
author = {Wang, Hao and Liu, Chi Harold and Dai, Zipeng and Tang, Jian and Wang, Guoren},
title = {Energy-Efficient 3D Vehicular Crowdsourcing for Disaster Response by Distributed Deep Reinforcement Learning},
year = {2021},
isbn = {9781450383325},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3447548.3467070},
doi = {10.1145/3447548.3467070},
abstract = {Fast and efficient access to environmental and life data is key to the successful disaster response. Vehicular crowdsourcing (VC) by a group of unmanned vehicles (UVs) like drones and unmanned ground vehicles to collect these data from Point-of-Interests (PoIs) e.g., possible survivor spots and fire site, provides an efficient way to assist disaster rescue. In this paper, we explicitly consider to navigate a group of UVs in a 3-dimensional (3D) disaster workzone to maximize the amount of collected data, geographical fairness, energy efficiency, while minimizing data dropout due to limited transmission rate. We propose DRL-DisasterVC(3D), a distributed deep reinforcement learning framework, with a repetitive experience replay (RER) to improve learning efficiency, and a clipped target network to increase learning stability. We also use a 3D convolutional neural network (3D CNN) with multi-head-relational attention (MHRA) for spatial modeling, and add auxiliary pixel control (PC) for spatial exploration. We designed a novel disaster response simulator, called "DisasterSim", and conduct extensive experiments to show that DRL-DisasterVC(3D) outperforms all five baselines in terms of energy efficiency when varying the numbers of UVs, PoIs and SNR threshold.},
booktitle = {Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery \&amp; Data Mining},
pages = {3679–3687},
numpages = {9},
keywords = {disaster response, distributed deep reinforcement learning, energy-efficiency, vehicular crowdsourcing},
location = {Virtual Event, Singapore},
series = {KDD '21}
}

@inproceedings{10.1145/3173574.3173850,
author = {Hosio, Simo Johannes and Karppinen, Jaro and Takala, Esa-Pekka and Takatalo, Jani and Goncalves, Jorge and van Berkel, Niels and Konomi, Shin'ichi and Kostakos, Vassilis},
title = {Crowdsourcing Treatments for Low Back Pain},
year = {2018},
isbn = {9781450356206},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3173574.3173850},
doi = {10.1145/3173574.3173850},
abstract = {Low back pain (LBP) is a globally common condition with no silver bullet solutions. Further, the lack of therapeutic consensus causes challenges in choosing suitable solutions to try. In this work, we crowdsourced knowledge bases on LBP treatments. The knowledge bases were used to rank and offer best-matching LBP treatments to end users. We collected two knowledge bases: one from clinical professionals and one from non-professionals. Our quantitative analysis revealed that non-professional end users perceived the best treatments by both groups as equally good. However, the worst treatments by non-professionals were clearly seen as inferior to the lowest ranking treatments by professionals. Certain treatments by professionals were also perceived significantly differently by non-professionals and professionals themselves. Professionals found our system handy for self-reflection and for educating new patients, while non-professionals appreciated the reliable decision support that also respected the non-professional opinion.},
booktitle = {Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems},
pages = {1–12},
numpages = {12},
keywords = {crowdsourcing, health information, low back pain},
location = {Montreal QC, Canada},
series = {CHI '18}
}

@inproceedings{10.1145/3306618.3314282,
author = {Goel, Naman and Faltings, Boi},
title = {Crowdsourcing with Fairness, Diversity and Budget Constraints},
year = {2019},
isbn = {9781450363242},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3306618.3314282},
doi = {10.1145/3306618.3314282},
abstract = {Recent studies have shown that the labels collected from crowdworkers can be discriminatory with respect to sensitive attributes such as gender and race. This raises questions about the suitability of using crowdsourced data for further use, such as for training machine learning algorithms. In this work, we address the problem of fair and diverse data collection from a crowd under budget constraints. We propose a novel algorithm which maximizes the expected accuracy of the collected data, while ensuring that the errors satisfy desired notions of fairness. We provide guarantees on the performance of our algorithm and show that the algorithm performs well in practice through experiments on a real dataset.},
booktitle = {Proceedings of the 2019 AAAI/ACM Conference on AI, Ethics, and Society},
pages = {297–304},
numpages = {8},
keywords = {bias, crowdsourcing, data quality, fairness},
location = {Honolulu, HI, USA},
series = {AIES '19}
}

@inproceedings{10.1145/3311957.3359469,
author = {Wang, Yihong and Papangelis, Konstantinos and Lykourentzou, Ioanna and Khan, Vassilis-Javed},
title = {The Changing Landscape of Crowdsourcing in China: From Individual Crowdworkers to Crowdfarms},
year = {2019},
isbn = {9781450366922},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3311957.3359469},
doi = {10.1145/3311957.3359469},
abstract = {We report of a new crowdsourcing work paradigm that we came across while interviewing crowdworkers in China mid-May 2019 - that of companies that solely focus in undertaking and doing crowdsourcing tasks en masse. In addition, we discuss why such companies emerged recently, and how it affects the crowdsourcing landscape in China. With this work we highlight an important change in the rapidly changing crowdsourcing landscape of China that merits more research in the future.},
booktitle = {Companion Publication of the 2019 Conference on Computer Supported Cooperative Work and Social Computing},
pages = {413–417},
numpages = {5},
keywords = {china, crowdfarms, crowdsourcing, crowdworkers},
location = {Austin, TX, USA},
series = {CSCW '19 Companion}
}

@inproceedings{10.1145/2808797.2809318,
author = {Arora, Piyush and Ganguly, Debasis and Jones, Gareth J. F.},
title = {The Good, the Bad and their Kins: Identifying Questions with Negative Scores in StackOverflow},
year = {2015},
isbn = {9781450338547},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2808797.2809318},
doi = {10.1145/2808797.2809318},
abstract = {A rapid increase in the number of questions posted on community question answering (CQA) forums is creating a need for automated methods of question quality moderation to improve the effectiveness of such forums in terms of response time and quality. Such automated approaches should aim to classify questions as good or bad for a particular forum as soon as they are posted based on the guidelines and quality standards defined/listed by the forum. Thus, if a question meets the standard of the forum then it is classified as good else we classify it as bad. In this paper, we propose a method to address this problem of question classification by retrieving similar questions previously asked in the same forum, and then using the text from these previously asked similar questions to predict the quality of the current question. We empirically validate our proposed approach on the set of StackOverflow data, a massive CQA forum for programmers, comprising of about 8M questions. With the use of these additional text retrieved from similar questions, we are able to improve the question quality prediction accuracy by about 2.8\% and improve the recall of negatively scored questions by about 4.2\%. This improvement of 4.2\% in recall would be helpful in automatically flagging questions as bad (unsuitable) for the forum and will speed up the moderation process thus saving time and human effort.},
booktitle = {Proceedings of the 2015 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining 2015},
pages = {1232–1239},
numpages = {8},
location = {Paris, France},
series = {ASONAM '15}
}

@inproceedings{10.1109/GLOBECOM46510.2021.9685885,
author = {Samanta, Riya and Ghosh, Soumya K. and Das, Sajal K.},
title = {SWill-TAC: Skill-oriented Dynamic Task Allocation with Willingness for Complex Job in Crowdsourcing},
year = {2021},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/GLOBECOM46510.2021.9685885},
doi = {10.1109/GLOBECOM46510.2021.9685885},
abstract = {Allocating tasks to the best-fit candidates is a classical problem in crowdsourcing (CS). Most of the existing approaches assume that the task and candidate knowledge is known in advance and ignore the effect of enrolled candidates' willingness on the CS system's selection decision. For instance, an unwilling candidate assigned to a task may quit without completing it, thus depreciating the utility of the CS platform. In practice, a task or candidate may arrive or leave the CS system dynamically. Moreover, a complex task may be broken into smaller sub-tasks, each requiring a variety of computations and expertise. To overcome these challenges, based on a greedy algorithm, we propose a novel approach for skill-oriented dynamic task allocation with willingness factor for complex assignments (SWill-TAC). This approach iteratively attempts to delegate candidates (workers) to tasks depending on the skills required for executing the tasks and the candidates' skill set. SWill-TAC also considers the willingness of eligible candidates and keeps track of the budget constraints of tasks. Finally, the feasibility and efficiency of our approach are demonstrated using the UpWork dataset. Experimental results show that SWill-TAC outperforms Online Greedy, TM-Uniform, Random selection-based, and Minimum payment-based task allocations in terms of the completed tasks count, the utility gained, and success ratio.},
booktitle = {2021 IEEE Global Communications Conference (GLOBECOM)},
pages = {1–6},
numpages = {6},
location = {Madrid, Spain}
}

@inproceedings{10.1109/ASONAM49781.2020.9381339,
author = {Osho, Abiola and Waters, Caden and Amariucai, George},
title = {An implicit crowdsourcing approach to rumor identification in online social networks},
year = {2021},
isbn = {9781728110561},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ASONAM49781.2020.9381339},
doi = {10.1109/ASONAM49781.2020.9381339},
abstract = {With the increasing use of online social networks as a source of news and information, the propensity for a rumor to disseminate widely and quickly poses a great concern, especially in disaster situations where users do not have enough time to fact-check posts before making the informed decision to react to a post that appears to be credible. At the same time, we know that misinformation is easily detectable by a certain few, very skeptical, or very informed users. In this study, we demonstrate how blending artificial intelligence and human skills can create a new paradigm for credibility prediction. The crowdsourcing part of the detection mechanism is implemented implicitly, by simply observing the natural interaction between users encountering the messages. Specifically, we explore the spread of information on Twitter at the microscopic (user-to-user propagation) level and propose a model that predicts if a message is True or False by observing the latent attributes of the message, along with those of the users interacting with it, and their reactions to the message. We demonstrate the application of this model to the detection of misinformation and rank the relevant message and user features that are most critical in influencing the spread of rumor over the network. Our experiments using real-world data show that the proposed model achieves over 90\% accuracy in predicting the credibility of posts on Twitter, a significant boost over state-of-the-art models.},
booktitle = {Proceedings of the 12th IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining},
pages = {174–182},
numpages = {9},
keywords = {bayesian learning, classification and regression, crowdsourcing, dimensionality reduction/feature selection, misinformation, social networks},
location = {Virtual Event, Netherlands},
series = {ASONAM '20}
}

@inproceedings{10.1145/3258692,
author = {Smucker, Mark},
title = {Session details: Session 7A: Crowdsourcing \&amp; Assessment},
year = {2018},
isbn = {9781450356572},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3258692},
doi = {10.1145/3258692},
booktitle = {The 41st International ACM SIGIR Conference on Research \&amp; Development in Information Retrieval},
location = {Ann Arbor, MI, USA},
series = {SIGIR '18}
}

@inproceedings{10.1145/3183713.3196929,
author = {Tong, Yongxin and Wang, Libin and Zhou, Zimu and Chen, Lei and Du, Bowen and Ye, Jieping},
title = {Dynamic Pricing in Spatial Crowdsourcing: A Matching-Based Approach},
year = {2018},
isbn = {9781450347037},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3183713.3196929},
doi = {10.1145/3183713.3196929},
abstract = {In spatial crowdsourcing, requesters submit their task-related locations and increase the demand of a local area. The platform prices these tasks and assigns spatial workers to serve if the prices are accepted by requesters. There exist mature pricing strategies which specialize in tackling the imbalance between supply and demand in a local market. However, in global optimization, the platform should consider the mobility of workers; that is, any single worker can be the potential supply for several areas, while it can only be the true supply of one area when assigned by the platform. The hardness lies in the uncertainty of the true supply of each area, hence the existing pricing strategies do not work. In the paper, we formally define this &lt;u&gt;G&lt;/u&gt;lobal &lt;u&gt;D&lt;/u&gt;ynamic &lt;u&gt;P&lt;/u&gt;ricing(GDP) problem in spatial crowdsourcing. And since the objective is concerned with how the platform matches the supply to areas, we let the matching algorithm guide us how to price. We propose a &lt;u&gt;MA&lt;/u&gt;tching-based &lt;u&gt;P&lt;/u&gt;ricing &lt;u&gt;S&lt;/u&gt;trategy (MAPS) with guaranteed bound. Extensive experiments conducted on the synthetic and real datasets demonstrate the effectiveness of MAPS.},
booktitle = {Proceedings of the 2018 International Conference on Management of Data},
pages = {773–788},
numpages = {16},
keywords = {pricing strategy, spatial crowdsourcing},
location = {Houston, TX, USA},
series = {SIGMOD '18}
}

@inproceedings{10.1109/GLOBECOM38437.2019.9014329,
author = {Pandey, Shashi Raj and Tran, Nguyen H. and Bennis, Mehdi and Tun, Yan Kyaw and Han, Zhu and Hong, Choong Seon},
title = {Incentivize to Build: A Crowdsourcing Framework for Federated Learning},
year = {2019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/GLOBECOM38437.2019.9014329},
doi = {10.1109/GLOBECOM38437.2019.9014329},
abstract = {Federated learning (FL) rests on the notion of training a global model in a decentralized manner. Under this setting, mobile devices perform computations on their local data before uploading the required updates to the central aggregator for improving the global model. However, a key challenge is to maintain communication efficiency (i.e., the number of communications per iteration) when participating clients implement uncoordinated computation strategy during aggregation of model parameters. We formulate a utility maximization problem to tackle this difficulty, and propose a novel crowdsourcing framework, involving a number of participating clients with local training data to leverage FL. We show the incentive-based interaction between the crowdsourcing platform and the participating client's independent strategies for training a global learning model, where each side maximizes its own benefit. We formulate a two-stage Stackelberg game to analyze such scenario and find the game's equilibria. Further, we illustrate the efficacy of our proposed framework with simulation results. Results show that the proposed mechanism outperforms the heuristic approach with up to 22\% gain in the offered reward to attain a level of target accuracy.},
booktitle = {2019 IEEE Global Communications Conference (GLOBECOM)},
pages = {1–6},
numpages = {6},
location = {Waikoloa, HI, USA}
}

@inproceedings{10.1145/3377049.3377106,
author = {Islam, Saima Sharleen and Bhuiyan, Md. Mahmudul Hasan and Ahmed, Zahiduddin and Al-Amin, Md.},
title = {An Empirical Survey on Crowdsourcing-Based Data Management Techniques},
year = {2020},
isbn = {9781450377782},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3377049.3377106},
doi = {10.1145/3377049.3377106},
abstract = {Crowdsourcing platforms are an efficacious approach to connect the abilities of people to relate with human estimation for distinctive tasks like sentiment analysis and image recognition etc. Accompanying the expansion of various crowdsourcing platforms like Toluna, Ushahidi, Microworkers, Google crowdsourcing platform or for other crowdsourcing platforms a large workforce and a large knowledge base needed. Automated processes cannot completely address any important analytical duties and data management tasks. These tasks, such as recognizing images and sentiment analysis can be enhanced by using human intellectual capability. Crowd-sourced management of data has therefore become an ever-greater research field and business attention. We study four distinctive platforms in this paper and conduct a broad variety of crowdsourcing data management studies. Based on this research, we then identify three primary considerations for improving data management in crowd sources.},
booktitle = {Proceedings of the International Conference on Computing Advancements},
articleno = {60},
numpages = {7},
keywords = {Cost Control, Crowdsourcing, Data Management, Human Computation, Latency Control, Machine Translation, Natural Language Processing, Quality Control, Supervised Machine Learning},
location = {Dhaka, Bangladesh},
series = {ICCA 2020}
}

@inproceedings{10.1109/DySPAN.2019.8935832,
author = {Lin, Yousi and Ye, Yuxian and Yang, Yaling},
title = {Crowdsourcing-based Spectrum Monitoring at A Large Geographical Scale},
year = {2019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/DySPAN.2019.8935832},
doi = {10.1109/DySPAN.2019.8935832},
abstract = {Spectrum monitoring is a powerful tool in dynamic spectrum access to help secondary users access the unused spectrum white space. The common approach for spectrum monitoring is to build infrastructures (e.g. spectrum observatories), which cost much money and manpower but have relatively low coverage. To aid in this, we propose a crowdsourcing based spectrum monitoring system for a large geographical area that leverages the power of masses of portable mobile devices. The system can accurately predict future spectrum utilization and intelligently schedule the spectrum monitoring tasks among mobile secondary users accordingly, so that the energy of mobile devices can be saved and more spectrum activities can be monitored. We also demonstrate our system’s ability to capture not only the existing spectrum access patterns but also the unknown patterns where no historical spectrum information exist. The experiment shows that our spectrum monitoring system can obtain a high spectrum monitoring coverage and low energy consumption.},
booktitle = {2019 IEEE International Symposium on Dynamic Spectrum Access Networks (DySPAN)},
pages = {1–10},
numpages = {10},
location = {Newark, NJ, USA}
}

@inproceedings{10.5555/3016387.3016540,
author = {Pan, Zhengxiang and Yu, Han and Miao, Chunyan and Leung, Cyril},
title = {Efficient collaborative crowdsourcing},
year = {2016},
publisher = {AAAI Press},
abstract = {We consider the problem of making efficient quality-time-cost trade-offs in collaborative crowdsourcing systems in which different skills from multiple workers need to be combined to complete a task. We propose CrowdAsm - an approach which helps collaborative crowdsourcing systems determine how to combine the expertise of available workers to maximize the expected quality of results while minimizing the expected delays. Analysis proves that CrowdAsm can achieve close to optimal profit for workers in a given crowdsourcing system if they follow the recommendations.},
booktitle = {Proceedings of the Thirtieth AAAI Conference on Artificial Intelligence},
pages = {4248–4249},
numpages = {2},
location = {Phoenix, Arizona},
series = {AAAI'16}
}

@inproceedings{10.1109/ISIT.2017.8006912,
author = {Raman, Ravi Kiran and Varshney, Lav R.},
title = {Budget-optimal clustering via crowdsourcing},
year = {2017},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ISIT.2017.8006912},
doi = {10.1109/ISIT.2017.8006912},
abstract = {This paper defines and studies the problem of universal clustering using responses of crowd workers, without knowledge of worker reliability or task difficulty. We model stochastic worker response distributions by incorporating traits of memory for similar objects and traits of distance among differing objects. We are particularly interested in two limiting worker types — temporary and long-term workers, without and with memory respectively. We first define clustering algorithms for these limiting cases and then integrate them into an algorithm for the unified worker model. We prove asymptotic consistency of the algorithms and establish sufficient conditions on the sample complexity of the algorithm. Converse arguments establish necessary conditions on sample complexity, proving that the defined algorithms are asymptotically order-optimal in cost.},
booktitle = {2017 IEEE International Symposium on Information Theory (ISIT)},
pages = {2163–2167},
numpages = {5},
location = {Aachen, Germany}
}

@inproceedings{10.1145/3391403.3399549,
author = {Stouras, Konstantinos I. and Erat, Sanjiv and Lichtendahl, Kenneth C.},
title = {Prizes on Crowdsourcing Platforms: An Equilibrium Analysis of Competing Contests},
year = {2020},
isbn = {9781450379755},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3391403.3399549},
doi = {10.1145/3391403.3399549},
abstract = {On a typical crowdsourcing platform solvers can self-select which (if any) of the concurrently running contests to participate in. Thus, firms which offer prizes and organize contests on these platforms are competing among themselves (for solver participation and effort). We formalize and model this competition among contests and examine the equilibrium outcomes. Our analysis reveals that, in general, there is a unique dominant strategy for each firm to offer multiple identical prizes. Moreover, when the quality of submitted solutions is sufficiently noise-driven (as opposed to effort-driven), we find that a single winner-take-all reward is the unique equilibrium allocation. Our analytical framework integrates and extends prior results of the monopolistic contest.},
booktitle = {Proceedings of the 21st ACM Conference on Economics and Computation},
pages = {875–876},
numpages = {2},
keywords = {crowdsourcing, dueling contests, endogenous participation, incentives, winner-takes-all},
location = {Virtual Event, Hungary},
series = {EC '20}
}

@inproceedings{10.1007/978-3-030-45442-5_26,
author = {La Barbera, David and Roitero, Kevin and Demartini, Gianluca and Mizzaro, Stefano and Spina, Damiano},
title = {Crowdsourcing Truthfulness: The Impact of Judgment Scale and Assessor Bias},
year = {2020},
isbn = {978-3-030-45441-8},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-45442-5_26},
doi = {10.1007/978-3-030-45442-5_26},
abstract = {News content can sometimes be misleading and influence users’ decision making processes (e.g., voting decisions). Quantitatively assessing the truthfulness of content becomes key, but it is often challenging and thus done by experts. In this work we look at how experts and non-expert assess truthfulness of content by focusing on the effect of the adopted judgment scale and of assessors’ own bias on the judgments they perform. Our results indicate a clear effect of the assessors’ political background on their judgments where they tend to trust content which is aligned to their own belief, even if experts have marked it as false. Crowd assessors also seem to have a preference towards coarse-grained scales, as they tend to use a few extreme values rather than the full breadth of fine-grained scales.},
booktitle = {Advances in Information Retrieval: 42nd European Conference on IR Research, ECIR 2020, Lisbon, Portugal, April 14–17, 2020, Proceedings, Part II},
pages = {207–214},
numpages = {8},
location = {Lisbon, Portugal}
}

@inproceedings{10.1109/SMC42975.2020.9283027,
author = {Ghaffaripour, Shadan and Miri, Ali},
title = {A Decentralized, Privacy-preserving and Crowdsourcing-based Approach to Medical Research},
year = {2020},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/SMC42975.2020.9283027},
doi = {10.1109/SMC42975.2020.9283027},
abstract = {Access to data at large scales expedites the progress of research in medical fields. Nevertheless, accessibility to patients’ data faces significant challenges on regulatory, organizational and technical levels. In light of this, we present a novel approach based on the crowdsourcing paradigm to solve this data scarcity problem. Utilizing the infrastructure that blockchain provides, our decentralized platform enables researchers to solicit contributions to their well-defined research study from a large crowd of volunteers. Furthermore, to overcome the challenge of breach of privacy and mutual trust, we employed the cryptographic primitive of Zero-knowledge Argument of Knowledge (zk-SNARK). This not only allows participants to make contributions without exposing their privacy-sensitive health data, but also provides a means for a distributed network of users to verify the validity of the contributions in an efficient manner. Finally, since without an incentive mechanism in place, the crowdsourcing platform would be rendered ineffective, we incorporated smart contracts to ensure a fair reciprocal exchange of data for reward between patients and researchers.},
booktitle = {2020 IEEE International Conference on Systems, Man, and Cybernetics (SMC)},
pages = {4510–4515},
numpages = {6},
location = {Toronto, ON}
}

@inproceedings{10.1007/978-3-030-95405-5_19,
author = {Wang, Yan and Tian, Yun and Zhang, Xuyun and He, Xiaonan and Li, Shu and Zhu, Jia},
title = {Deep Reinforcement Learning Based Iterative Participant Selection Method for Industrial IoT Big Data Mobile Crowdsourcing},
year = {2022},
isbn = {978-3-030-95404-8},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-95405-5_19},
doi = {10.1007/978-3-030-95405-5_19},
abstract = {With the massive deployment of mobile devices, crowdsourcing has become a new service paradigm in which a task requester can proactively recruit a batch of participants with a mobile IoT device from our system for quick and accurate results. In a mobile industrial crowdsourcing platform, a large amount of data is collected, extracted information, and distributed to requesters. In an entire task process, the system receives a task, allocates some suitable participants to complete it, and collects feedback from the requesters. We present a participant selection method, which adopts an end-to-end deep neural network to iteratively update the participant selection policy. The neural network consists of three main parts: (1) task and participant ability prediction part which adopts a bag of words method to extract the semantic information of a query, (2) feature transformation part which adopts a series of linear and nonlinear transformations and (3) evaluation part which uses requesters’ feedback to update the network. In addition, the policy gradient method which is proved effective in the deep reinforcement learning field is adopted to update our participant selection method with the help of requesters’ feedback. Finally, we conduct an extensive performance evaluation based on the combination of real traces and a real question and answer dataset and numerical results demonstrate that our method can achieve superior performance and improve more than 150\% performance gain over a baseline method.},
booktitle = {Advanced Data Mining and Applications: 17th International Conference, ADMA 2021, Sydney, NSW, Australia, February 2–4, 2022, Proceedings, Part I},
pages = {258–272},
numpages = {15},
keywords = {Reinforcement learning, Mobile crowdsourcing},
location = {Sydney, NSW, Australia}
}

@inproceedings{10.1109/DySPAN.2019.8935689,
author = {Jiang, Yonghang and Liu, Yang and Li, Zhenjiang},
title = {Data Fusion and Alignment for Location-Aware Crowdsourcing Applications},
year = {2019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/DySPAN.2019.8935689},
doi = {10.1109/DySPAN.2019.8935689},
abstract = {As an emerging technique, crowdsourcing has drawn people's great attention in recent years. The crowdsourced data, however, can hardly be fused easily to enable usable applications for the reason that the data are collected by different users, in different locations, at different time, with different noises and distortions. Although different crowdsourcing services have proposed different data fusing methods, we find that they may not fully leverage the data collected from multiple dimensions that can potentially lead to a better fusion result. In order to harness this opportunity, we propose a more general solution, which can fuse the multi-dimension crowdsourced data together and align them with the consistent time and location stamps by using the features of the sensory data only, and thus can provide a high-quality crowdsourcing service from the raw data samplings collected from the environment. We conduct evaluations and experiments using different commercial smart phones to verify the effectiveness of our proposed method.},
booktitle = {2019 IEEE International Symposium on Dynamic Spectrum Access Networks (DySPAN)},
pages = {1–8},
numpages = {8},
location = {Newark, NJ, USA}
}

@inproceedings{10.1145/3274192.3274206,
author = {Pestana, Maria Clara and Vieira, Vaninha},
title = {Context-Aware Task Distribution for Mobile Crowdsourcing},
year = {2018},
isbn = {9781450366014},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3274192.3274206},
doi = {10.1145/3274192.3274206},
abstract = {In Mobile Crowdsourcing, tasks are distributed to workers according to sensor data, aiming to solve problems using collective intelligence. However, some tasks are not completed mainly because task subjects are not compatible to user capabilities. This article presents a context-aware approach to improve the task distribution in crowdsourcing systems. In this approach, tasks are selected to each worker according to the contexts previously defined. We developed a conceptual task model which uses sensitive information to select participants accordingly to task's contextual requirements and an architecture that illustrates the distribution of tasks through a mobile application. We also presented Contask, a prototype based on the developed architecture, which was used in a study case for evaluating task distribution. The accuracy of the distribution had a result of 63\%, followed by 73\% of precision and 63\% of revocation. These results may be related to the ubiquity characteristic of context-aware computing since the system automatically adapts to user needs and preferences.},
booktitle = {Proceedings of the 17th Brazilian Symposium on Human Factors in Computing Systems},
articleno = {14},
numpages = {8},
keywords = {Context Awareness, Crowdsourcing, Task Distribution},
location = {Bel\'{e}m, Brazil},
series = {IHC '18}
}

@inproceedings{10.1007/978-3-030-44429-7_11,
author = {van Vliet, Martijn and Groen, Eduard C. and Dalpiaz, Fabiano and Brinkkemper, Sjaak},
title = {Identifying and Classifying User Requirements in Online Feedback via Crowdsourcing},
year = {2020},
isbn = {978-3-030-44428-0},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-44429-7_11},
doi = {10.1007/978-3-030-44429-7_11},
abstract = {[Context and motivation] App stores and social media channels such as Twitter enable users to share feedback regarding software. Due to its high volume, it is hard to effectively and systematically process such feedback to obtain a good understanding of users’ opinions about a software product. [Question/problem] Tools based on natural language processing and machine learning have been proposed as an inexpensive mechanism for classifying user feedback. Unfortunately, the accuracy of these tools is imperfect, which jeopardizes the reliability of the analysis results. We investigate whether assigning micro-tasks to crowd workers could be an alternative technique for identifying and classifying requirements in user feedback. [Principal ideas/results] We present a crowdsourcing method for filtering out irrelevant app store reviews and for identifying features and qualities. A validation study has shown positive results in terms of feasibility, accuracy, and cost. [Contribution] We provide evidence that crowd workers can be an inexpensive yet accurate resource for classifying user reviews. Our findings contribute to the debate on the roles of and synergies between humans and AI techniques.},
booktitle = {Requirements Engineering: Foundation for Software Quality: 26th International Working Conference, REFSQ 2020, Pisa, Italy, March 24–27, 2020, Proceedings},
pages = {143–159},
numpages = {17},
keywords = {Crowd-based requirements engineering, Crowdsourcing, Online user reviews, Quality requirements, User feedback analysis},
location = {Pisa, Italy}
}

@inproceedings{10.1145/3269206.3271779,
author = {Inel, Oana and Haralabopoulos, Giannis and Li, Dan and Van Gysel, Christophe and Szl\'{a}vik, Zolt\'{a}n and Simperl, Elena and Kanoulas, Evangelos and Aroyo, Lora},
title = {Studying Topical Relevance with Evidence-based Crowdsourcing},
year = {2018},
isbn = {9781450360142},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3269206.3271779},
doi = {10.1145/3269206.3271779},
abstract = {Information Retrieval systems rely on large test collections to measure their effectiveness in retrieving relevant documents. While the demand is high, the task of creating such test collections is laborious due to the large amounts of data that need to be annotated, and due to the intrinsic subjectivity of the task itself. In this paper we study the topical relevance from a user perspective by addressing the problems of subjectivity and ambiguity. We compare our approach and results with the established TREC annotation guidelines and results. The comparison is based on a series of crowdsourcing pilots experimenting with variables, such as relevance scale, document granularity, annotation template and the number of workers. Our results show correlation between relevance assessment accuracy and smaller document granularity, i.e., aggregation of relevance on paragraph level results in a better relevance accuracy, compared to assessment done at the level of the full document. As expected, our results also show that collecting binary relevance judgments results in a higher accuracy compared to the ternary scale used in the TREC annotation guidelines. Finally, the crowdsourced annotation tasks provided a more accurate document relevance ranking than a single assessor relevance label. This work resulted is a reliable test collection around the TREC Common Core track.},
booktitle = {Proceedings of the 27th ACM International Conference on Information and Knowledge Management},
pages = {1253–1262},
numpages = {10},
keywords = {crowdsourcing, ir evaluation, trec common core track},
location = {Torino, Italy},
series = {CIKM '18}
}

@inproceedings{10.1007/978-3-030-03596-9_22,
author = {Fan, Jiahao and Zhou, Xinbo and Gao, Xiaofeng and Chen, Guihai},
title = {Crowdsourcing Task Scheduling in Mobile Social Networks},
year = {2018},
isbn = {978-3-030-03595-2},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-03596-9_22},
doi = {10.1007/978-3-030-03596-9_22},
abstract = {With the growing popularity of mobile devices, a new paradigm called mobile crowdsourcing emerged in the recent years. Mobile users with restricted computational capability and sensing ability are now able to conduct complex tasks with the help of other users in the same mobile crowdsourcing system. In this paper, we consider the mobile crowdsourcing system model based on the spontaneously-formed mobile social networks (MSNs). We introduce two crowdsourcing task scheduling problems under this system model, with one problem aiming to minimize the operating cost of some crowdsourcing tasks and the other focusing on minimizing the overall completion time of tasks belonging to the same project. Correspondingly, under offline settings, we propose an optimal algorithm and an approximation algorithm for these two problems respectively. The optimality and the approximation ratio are analyzed accordingly. Based on these two algorithms, we further design two online algorithms to deal with the problems under online settings and their competitive ratios are computed. Finally, we verify the effectiveness and efficiency of the proposed methods through extensive numerical experiments on synthetic datasets.},
booktitle = {Service-Oriented Computing: 16th International Conference, ICSOC 2018, Hangzhou, China, November 12-15, 2018, Proceedings},
pages = {317–331},
numpages = {15},
keywords = {Crowdsourcing, Task scheduling, Mobile social network},
location = {Hangzhou, China}
}

@inproceedings{10.1109/SMAP.2015.7370084,
author = {Simko, Jakub and Bielikova, Maria},
title = {Gaze-tracked crowdsourcing},
year = {2015},
isbn = {9781467383950},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/SMAP.2015.7370084},
doi = {10.1109/SMAP.2015.7370084},
booktitle = {Proceedings of the 2015 10th International Workshop on Semantic and Social Media Adaptation and Personalization (SMAP)},
pages = {1–5},
numpages = {5},
series = {SMAP '15}
}

@inproceedings{10.1145/3394486.3403369,
author = {Lioznova, Anna and Drutsa, Alexey and Kukushkin, Vladimir and Bezzubtseva, Anastasia},
title = {Prediction of Hourly Earnings and Completion Time on a Crowdsourcing Platform},
year = {2020},
isbn = {9781450379984},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3394486.3403369},
doi = {10.1145/3394486.3403369},
abstract = {We study the problem of predicting future hourly earnings and task completion time for a crowdsourcing platform user who sees the list of available tasks and wants to select one of them to execute. Namely, for each task shown in the list, one needs to have an estimated value of the user's performance (i.e., hourly earnings and completion time) that will be if she selects this task. We address this problem on real crowd tasks completed on one of the global crowdsourcing marketplaces by (1) conducting a survey and an A/B test on real users; the results confirm the dominance of monetary incentives and importance of knowledge on hourly earnings for users; (2) an in-depth analysis of user behavior that shows that the prediction problem is challenging: (a) users and projects are highly heterogeneous, (b) there exists the so-called "learning effect" of a user selected a new task; and (3) the solution to the problem of predicting user performance that demonstrates improvement of prediction quality by up to 25\% for hourly earnings and up to $32\%$ completion time w.r.t. a naive baseline which is based solely on historical performance of users on tasks. In our experimentation, we use data about 18 million real crowdsourcing tasks performed by $161$ thousand users on the crowd platform; we publish this dataset. The hourly earning prediction has been deployed in Yandex.Toloka.},
booktitle = {Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery \&amp; Data Mining},
pages = {3172–3182},
numpages = {11},
keywords = {crowdsourcing marketplace, hourly earnings, task completion time},
location = {Virtual Event, CA, USA},
series = {KDD '20}
}

@inproceedings{10.1145/3371238.3371268,
author = {Zhang, Xuefeng and Chen, Mingshuang and Ji, Guanqun},
title = {Factors influencing the crowd participation in knowledge-intensive crowdsourcing},
year = {2019},
isbn = {9781450376402},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3371238.3371268},
doi = {10.1145/3371238.3371268},
abstract = {The crowd participation is critical to the sustained development of knowledge-intensive crowdsourcing (KI-C) and its supportive online platforms. Prior studies suggested and investigated a variety of factors that influence the crowd participation in KI-C. However, to our knowledge, a structured account and integration of the important factors influencing participation is still lacking. So this study aims to fill the gap by conducting a literature review. This review classifies factors into four categories, i.e., task properties, crowdsourcing platform characteristics, motivations of the crowd, and requesters' feedbacks, according to the process of crowdsourcing and its components. Furthermore, we present eighteen factors under these categories. The findings from this study may offer guidelines for developing comprehensive models and approaches to sustain the crowd participation in KI-C.},
booktitle = {Proceedings of the 4th International Conference on Crowd Science and Engineering},
pages = {186–194},
numpages = {9},
keywords = {Knowledge-intensive crowdsourcing, influential factors, literature review, the crowd participation},
location = {Jinan, China},
series = {ICCSE'19}
}

@inproceedings{10.1145/2685553.2698999,
author = {Cavusoglu, Huseyin and Li, Zhuolun and Huang, Ke-Wei},
title = {Can Gamification Motivate Voluntary Contributions? The Case of StackOverflow Q&amp;A Community},
year = {2015},
isbn = {9781450329460},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2685553.2698999},
doi = {10.1145/2685553.2698999},
abstract = {Online communities heavily rely on voluntary participation and continued engagement from users because these sites can flourish only if there are meaningful contributions from community members. Gamifying the underlying incentive mechanism can be a solution to elicit and sustain the desired user behavior. In this paper, we develop a theory of gamification and study the impact of a hierarchical badges system, a reward mechanism based on gamification principles, on user participation and engagement at Stack Overflow Q&amp;A site. Specifically, we assess the extent to which users are incentivized by earned badges in their contributions to the answering activity. Our initial results present strong empirical evidence that confirms the value of the badges and the effectiveness of gamification in stimulating voluntary participation.},
booktitle = {Proceedings of the 18th ACM Conference Companion on Computer Supported Cooperative Work \&amp; Social Computing},
pages = {171–174},
numpages = {4},
keywords = {badges, gamification, motivation theories, online communities, q&amp;a sites, voluntary contribution},
location = {Vancouver, BC, Canada},
series = {CSCW'15 Companion}
}

@inproceedings{10.1145/3236112.3236176,
author = {Chi, Pei-Yu (Peggy) and Batra, Anurag and Hsu, Maxwell},
title = {Mobile crowdsourcing in the wild: challenges from a global community},
year = {2018},
isbn = {9781450359412},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3236112.3236176},
doi = {10.1145/3236112.3236176},
abstract = {Recent research has been devoted to designing mobile applications that encourage users to complete microtasks in everyday context, known as "mobile crowdsourcing". In this case study, we present our ongoing effort of a publicly-available mobile application, Crowdsource, that has over 540,000 global users from 200 countries or regions. Over 15 million sessions have been performed since the initial launch in August 2016. To better support users' motivations and goals, we conducted a survey with our active users and validate their feedback with a set of usability studies. Our findings suggest design considerations for crowdsourcing microtasks with mobile users at the global scale.},
booktitle = {Proceedings of the 20th International Conference on Human-Computer Interaction with Mobile Devices and Services Adjunct},
pages = {410–415},
numpages = {6},
keywords = {diversity, microtasking, mobile apps, mobile crowdsourcing},
location = {Barcelona, Spain},
series = {MobileHCI '18}
}

@inproceedings{10.1145/3424978.3425015,
author = {Zhang, Shuhong and Wang, Liyuan and Wang, Lan},
title = {Research on Crowdsourcing Mode of Internet + Rural Logistics Based on Blockchain},
year = {2020},
isbn = {9781450377720},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3424978.3425015},
doi = {10.1145/3424978.3425015},
abstract = {Blockchain is a new internet database technology, which ensures the authenticity and security of data through distributed supervision technology. It has excellent characteristics such as decentralization, non-tampering, traceability, high transparency and so on. The combination of blockchain and internet can help solve the trust problem that has always restricted the application of "internet plus", and thus speeds up the sharing economy. This paper first gives the concept of farmers' deep participation in the "internet + rural logistics" mode, then the crowdsourcing logistics operation mode based on farmers' deep participation is established by using blockchain technology. And the operation process is presented, the logical framework of smart contract under two scenarios of self-built platform and third-party platform is provided, which provides technical support and trust guarantee for the healthy development of "internet plus rural logistics" crowdsourcing mode.},
booktitle = {Proceedings of the 4th International Conference on Computer Science and Application Engineering},
articleno = {37},
numpages = {5},
keywords = {Blockchain, Farmers' deep participation, Internet plus, Rural logistics},
location = {Sanya, China},
series = {CSAE '20}
}

@inproceedings{10.1007/978-3-030-60816-3_22,
author = {Brangbour, Etienne and Bruneau, Pierrick and Tamisier, Thomas and Marchand-Maillet, St\'{e}phane},
title = {Active Learning with Crowdsourcing for the Cold Start of Imbalanced Classifiers},
year = {2020},
isbn = {978-3-030-60815-6},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-60816-3_22},
doi = {10.1007/978-3-030-60816-3_22},
abstract = {We present a novel cooperative strategy based on active learning and crowdsourcing, dedicated to provide a solution to the cold start stage, i.e. initializing the classification of a large set of data with no attached labels. The strategy is moreover designed to handle an imbalanced context in which random selection is highly inefficient. In this purpose, our method is guided by an unsupervised clustering, and the computation of cluster quality and impurity indexes, updated at each active learning step. The strategy is explained on a case study of annotating Twitter content w.r.t. a real flood event. We also show that our technique can cope with multiple heterogeneous data representations.},
booktitle = {Cooperative Design, Visualization, and Engineering: 17th International Conference, CDVE 2020, Bangkok, Thailand, October 25–28, 2020, Proceedings},
pages = {192–201},
numpages = {10},
keywords = {Active learning, Imbalanced classification, Cold start},
location = {Bangkok, Thailand}
}

@inproceedings{10.1145/3342220.3343644,
author = {Gadiraju, Ujwal and Demartini, Gianluca},
title = {Understanding Worker Moods and Reactions to Rejection in Crowdsourcing},
year = {2019},
isbn = {9781450368858},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3342220.3343644},
doi = {10.1145/3342220.3343644},
abstract = {Requesters on crowdsourcing platforms typically exercise the power to decide the fate of tasks completed by crowd workers. Rejecting work has a direct impact on workers; (i) they may not be rewarded for the work completed and for their effort that has been exerted, and (ii) rejection affects worker reputation and may limit their access to future work opportunities. This paper presents a comprehensive study that aims to understand worker moods and how workers react to rejections in microtask crowdsourcing. We experimentally investigate the effect of the mood of workers on their performance, and the interaction of their moods with their reactions to rejection. Finally, we explore techniques such as presenting social comparative explanations to foster positive reactions to rejection. We found that workers in pleasant moods significantly outperform those in unpleasant moods. Workers whose work is rejected due to narrowly failing pre-screening tests exhibited the most negative emotional responses.},
booktitle = {Proceedings of the 30th ACM Conference on Hypertext and Social Media},
pages = {211–220},
numpages = {10},
keywords = {accuracy, crowdsourcing, emotion, microtasks, mood, performance, rejection, rejection sensitivity},
location = {Hof, Germany},
series = {HT '19}
}

@inproceedings{10.1007/978-3-030-59019-2_12,
author = {Zhang, Rui and Xia, Hui and Cui, Jufu and Cheng, Xiangguo},
title = {A Novel Solution to Quality of Service Dilemma in Crowdsourcing Systems},
year = {2020},
isbn = {978-3-030-59018-5},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-59019-2_12},
doi = {10.1007/978-3-030-59019-2_12},
abstract = {Crowdsourcing recruits workers to finish complicated tasks, but it is prone to the quality of service dilemma, that is, the platform cannot guarantee the workers’ quality of service. To solve this problem, we develop a novel quality of service improvement scheme. Firstly, to promote the workers cooperation, we propose an auction screening algorithm to estimate the rational quotation range of workers for screening workers and design a task reward function to motivate the workers to complete tasks. Secondly, to promote the platforms cooperation, we divide the rewards to the workers from the platforms into three categories and punish the platform that plays the defective strategy. Finally, the detailed experimental results show that the new scheme increases worker’s reward to complete tasks and relieves the quality of service dilemma in the crowdsourcing system effectively.},
booktitle = {Wireless Algorithms, Systems, and Applications: 15th International Conference, WASA 2020, Qingdao, China, September 13–15, 2020, Proceedings, Part II},
pages = {105–112},
numpages = {8},
keywords = {Quality of service, Cooperation, Auction screening},
location = {Qingdao, China}
}

@inproceedings{10.1145/3152494.3167979,
author = {Mridha, Sankar Kumar and Bhattacharyya, Malay},
title = {Network based mechanisms for competitive crowdsourcing},
year = {2018},
isbn = {9781450363419},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3152494.3167979},
doi = {10.1145/3152494.3167979},
abstract = {The working principle of crowd in a crowdsourcing platform is either competitive or collaborative. Occasionally, the tasks submitted to crowdsourcing environments are decomposable. They are challenging to solve because decomposition and composition of tasks and proper selection of workers are difficult. We show that by appropriate inclusion of collaboration in a competitive crowdsourcing environment, we can handle decomposable-type tasks given with posted-price in a better way. We initially attempt to manage 2-decomposable tasks with appropriate mechanism design. Extending it further to n-decomposable tasks, we propose a network based mechanism to choose the best mixture of sub-tasks in a competitive environment for selecting the winners. We are currently interested in developing mechanisms to remove the participation bias from such environments.},
booktitle = {Proceedings of the ACM India Joint International Conference on Data Science and Management of Data},
pages = {318–321},
numpages = {4},
keywords = {crowdsourcing, decomposable task, posted-price mechanism},
location = {Goa, India},
series = {CODS-COMAD '18}
}

@inproceedings{10.1109/CSI-SE.2017.2,
author = {Winkler, Dietmar and Sabou, Marta and Petrovic, Sanja and Carneiro, Gisele and Kalinowski, Marcos and Biffl, Stefan},
title = {Improving model inspection with crowdsourcing},
year = {2017},
isbn = {9781538640418},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/CSI-SE.2017.2},
doi = {10.1109/CSI-SE.2017.2},
abstract = {Traditional Software Inspection is a well-established approach to identify defects in software artifacts and models early and efficiently. However, insufficient method and tool support hinder efficient defect detection in large software models. Recent Human Computation and Crowdsourcing processes may help to overcome this limitation by splitting complex inspection artifacts into smaller parts including a better control over defect detection tasks and increasing the scalability of inspection tasks. Therefore, we introduce a Crowdsourcing-Based Inspection (CSI) process with tool support with focus on inspection teams and the quality of defect detection. We evaluate the CSI process in a feasibility study involving 63 inspectors using the CSI process and 12 inspectors using a traditional best-practice inspection process. The CSI process was found useful by the participants. Although the preliminary results of the study were promising, the CSI process should be further investigated with typical large software engineering models.},
booktitle = {Proceedings of the 4th International Workshop on CrowdSourcing in Software Engineering},
pages = {30–34},
numpages = {5},
keywords = {crowdsourcing, defect detection, feasibility study, model inspection},
location = {Buenos Aires, Argentina},
series = {CSI-SE '17}
}

@inproceedings{10.1007/978-3-031-19211-1_15,
author = {Sima, Qinghua and Sun, Yu-E and Huang, He and Gao, Guoju and Wang, Yihuai},
title = {Social-Network-Assisted Task Selection for&nbsp;Online Workers in&nbsp;Spatial Crowdsourcing: A Multi-Agent Multi-Armed Bandit Approach},
year = {2022},
isbn = {978-3-031-19210-4},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-031-19211-1_15},
doi = {10.1007/978-3-031-19211-1_15},
abstract = {The popularity of smart devices and the availability of wireless networks bring considerable attention to Spatial Crowdsourcing (SC). Existing studies mainly focus on solutions to different optimization objectives of the SC platform, ignoring the entitlement of workers. This paper starts from the perspective of workers and investigates how to select suitable tasks for each online worker such that everyone can maximize their individual profit. Since the profit is related to the completion degree of tasks that is determined by the prior unknown parameter, we model the problem as a Multi-Agent Multi-Armed Bandit (MAMAB) problem. We propose a Payment-Estimation-Based Solution (PEBS), allowing workers to sequentially make decisions on task selection based on their observations and estimations. Specifically, the proposed PEBS first utilizes the social network among workers and assists workers in learning the information of tasks from the historical data. Then, it introduces the idea of probability matching in Thompson Sampling (TS) to help estimate the profit of workers and deal with the task selection problem. Finally, extensive simulations show that our proposed mechanism is efficient in optimizing the individual profit of workers.},
booktitle = {Wireless Algorithms, Systems, and Applications: 17th International Conference, WASA 2022, Dalian, China, November 24–26, 2022, Proceedings, Part III},
pages = {178–190},
numpages = {13},
keywords = {Social network, Multi-agent multi-armed bandit, Online task selection, Thompson sampling, Spatial crowdsourcing},
location = {Dalian, China}
}

@inproceedings{10.1007/978-3-030-03596-9_37,
author = {Jiang, Yun and He, Wei and Cui, Lizhen and Yang, Qian},
title = {User Location Prediction in Mobile Crowdsourcing Services},
year = {2018},
isbn = {978-3-030-03595-2},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-03596-9_37},
doi = {10.1007/978-3-030-03596-9_37},
abstract = {In recent years, mobile crowdsourcing has been integrated into people’s lives. A variety of mobile crowdsourcing services have emerged and been widely used, such as Gigwalk, Foursquare, and Uber. Due to the uncertainty of task distribution and workers’ trajectory, as well as diverse worker interests and capabilities, it is crucial to effectively predict the mobile workers’ trajectories such that they are willing to get to the location and perform their tasks with as little travel and time cost as possible. In this paper, we propose a context-sensitive prediction approach for workers’ moving path in mobile crowdsourcing services. We predict the upcoming location of workers through movement rules, real-time perception of workers’ moving path and contexts when assigning spatial tasks on a crowdsourcing platform, thereby pushing a task to the workers who will enter the region within the deadline of the task. Our location prediction method can avoid workers’ extra cost such as time and charges in performing tasks. The analysis and simulation experiments based on real data sets show that this method can effectively predict the location of a worker and achieve better results in task assignment and completion.},
booktitle = {Service-Oriented Computing: 16th International Conference, ICSOC 2018, Hangzhou, China, November 12-15, 2018, Proceedings},
pages = {515–523},
numpages = {9},
keywords = {Mobile crowdsourcing, Context, Location prediction, Task assignment},
location = {Hangzhou, China}
}

@inproceedings{10.1007/978-3-030-02610-3_13,
author = {Castano, Silvana and Ferrara, Alfio and Montanelli, Stefano},
title = {Crowdsourcing Task Assignment with Online Profile Learning},
year = {2018},
isbn = {978-3-030-02609-7},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-02610-3_13},
doi = {10.1007/978-3-030-02610-3_13},
abstract = {In this paper, we present a reference framework called Argo+ for worker-centric crowdsourcing where task assignment is characterized by feature-based representation of both tasks and workers and learning techniques are exploited to online predict the most appropriate task to execute for a requesting worker. On the task side, features are used to represent requirements expressed in terms of knowledge expertise that are asked to workers for being involved in the task execution. On the worker side, features are used to compose a profile, namely a structured description of the worker capabilities in executing tasks. Experimental results obtained on a real crowdsourcing campaign are discussed by comparing the performance of Argo+ against a baseline with conventional task assignment techniques.},
booktitle = {On the Move to Meaningful Internet Systems. OTM 2018 Conferences: Confederated International Conferences: CoopIS, C&amp;TC, and ODBASE 2018, Valletta, Malta, October 22-26, 2018, Proceedings, Part I},
pages = {226–242},
numpages = {17},
location = {Valletta, Malta}
}

@inproceedings{10.1007/978-3-030-33223-5_24,
author = {Liao, Qiao and Zhou, Xiangmin and Wang, Daling and Feng, Shi and Zhang, Yifei},
title = {Role-Based Clustering for Collaborative Recommendations in Crowdsourcing System},
year = {2019},
isbn = {978-3-030-33222-8},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-33223-5_24},
doi = {10.1007/978-3-030-33223-5_24},
abstract = {Crowdsourcing as a distributed problem-solving and business production model has attracted much attention in recent years. In crowdsourcing systems, task recommendation can help workers to select suitable tasks on crowdsourcing platforms as well as help requesters to receive good outputs. However, as one of the most successful recommendation approaches, current clustering-based models in crowdsourcing are challenged by multi-preference and cold-start problems. This paper proposes a role-based clustering model, which transforms a large-sparse worker-task rating matrix into a set of role-based clusters that are small, independent and rating intensive worker-task rating matrices, leading to better quality and performance in task recommendation. Specifically, we first cluster a worker-task rating matrix into a set of clusters in terms of the role identification and distribution operations. The clusters are further extended to include all their external worker (task) roles. Then, the task recommendation results with respect to a worker are generated by operating over the clusters involving the worker’s activities, which captures the worker’s preferences in multiple areas. Moreover, the model discovers the structure information from the clustering results and crowdsourcing datasets, by which tasks can be recommended to new workers interactively without their interest profiles. We evaluated our method over the benchmark dataset from NAACL 2010 workshop. The results show the high superiority of our proposed recommendation method over crowdsourcing platforms.},
booktitle = {Conceptual Modeling: 38th International Conference, ER 2019, Salvador, Brazil, November 4–7, 2019, Proceedings},
pages = {287–301},
numpages = {15},
keywords = {Role-based clustering, Task recommendation, Cold-start, Multi-preference, Crowdsourcing system},
location = {Salvador, Brazil}
}

@inproceedings{10.1145/3387168.3387242,
author = {Hu, Yucheng and Ou, Zhonghong and Xu, Xiangyu and Song, Meina},
title = {A Crowdsourcing Repeated Annotations System for Visual Object Detection},
year = {2020},
isbn = {9781450376259},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3387168.3387242},
doi = {10.1145/3387168.3387242},
abstract = {As a fundamental task in compute vision, object detection has been developed rapidly driven by the deep learning. The lack of a large number of images with ground truth annotations has become a chief obstacle to object detection applications in many fields. Eliciting labels from crowds is a potential way to obtain large labeled data. Nonetheless, existing crowdsourced techniques, e.g., Amazon Mechanical Turk (MTurk), often fail to guarantee the quality of the annotations, which have a bad influence on the accuracy of the deep detector. A variety of methods have been developed for ground truth inference and learning from crowds. In this paper, we study strategies to crowd-source repeated labels in support for these methods. The core challenge of building such a system is to reduce the difficulty to annotate multiple objects of interest and improve the data quality as much as possible. We present a system that adopts the turn-based annotation mechanism and consists of three simple sub-tasks: a single object annotation, a quality verification task and a coverage verification task. Experimental results demonstrate that our system is scalable, accurate and can assist the detector of obtaining higher accuracy.},
booktitle = {Proceedings of the 3rd International Conference on Vision, Image and Signal Processing},
articleno = {14},
numpages = {6},
keywords = {Crowdsourcing, Image annotation, Large scale annotation, Object detection, Repeated labels},
location = {Vancouver, BC, Canada},
series = {ICVISP 2019}
}

@inproceedings{10.1145/3318396.3318415,
author = {Shahid, Eman and Arain, Qasim and Kumari, Sandia and Farah, Isma},
title = {Images Based Indoor Positioning Using AI and Crowdsourcing},
year = {2019},
isbn = {9781450362672},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3318396.3318415},
doi = {10.1145/3318396.3318415},
abstract = {Wireless devices, QR codes, RSS technologies are usually costly and effort intensive to be implemented for real-time indoor positioning which is based on some supporting infrastructures. In this paper, we explored image based indoor positioning as an alternative to other approaches and it is cheaper than other currently using technologies. Mobile phone's camera is used to capture images of surroundings to localize or locate him/herself. No, any special arrangement is required for image-based indoor positioning.Localization is accomplished by using Microsoft Azure Cognitive Service that uses an artificial intelligence algorithm for predicting custom images with the dataset of pre-captured indoor images. First, the dataset of the environment for different locations or places is constructed from the images captured by volunteers or crowd, after preprocessing the collected data through an android application. Dataset is uploaded with tags or labels to custom vision services cloud. Microsoft Azure. Custom vision Services predict or detect the user's location through machine learning. Machine is trained on the data collected from volunteers and the trained classifier will predict the appropriate label or class. The user will be localized by capturing an image of the surrounding environment, it is then uploaded to the cloud. The classifier will match it against every label and show results in the percentage of similarity with a particular label. Label or tag is assigned based on highest percentage, only the highest percentage tag is retrieved for precise localization. Feasibility of this method is proved by the preliminary experimental results. In future, we are trying to increase accuracy to predict most accurate label for indoor positioning or localization.},
booktitle = {Proceedings of the 2019 8th International Conference on Educational and Information Technology},
pages = {97–101},
numpages = {5},
keywords = {Global Positioning System, Indoor Localization, Indoor Positioning, Point of interests, Received Signal Strength Indication},
location = {Cambridge, United Kingdom},
series = {ICEIT 2019}
}

@inproceedings{10.1007/978-3-030-59416-9_44,
author = {Li, Xiang and Zhao, Yan and Guo, Jiannan and Zheng, Kai},
title = {Group Task Assignment with Social Impact-Based Preference in Spatial Crowdsourcing},
year = {2020},
isbn = {978-3-030-59415-2},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-59416-9_44},
doi = {10.1007/978-3-030-59416-9_44},
abstract = {With the pervasiveness of GPS-enabled smart devices and increased wireless communication technologies, Spatial Crowdsourcing (SC) has drawn increasing attention in assigning location-sensitive tasks to moving workers. In real-world scenarios, for the complex tasks, SC is more likely to assign each task to more than one worker, called Group Task Assignment (GTA), for the reason that an individual worker cannot complete the task well by herself. It is a challenging issue to assign worker groups the tasks that they are interested in and willing to perform. In this paper, we propose a novel framework for group task assignment based on worker groups’ preferences, which includes two components: Social Impact-based Preference Modeling (SIPM) and Preference-aware Group Task Assignment (PGTA). SIPM employs a Bipartite Graph Embedding Model (BGEM) and the attention mechanism to learn the social impact-based preferences of different worker groups on different task categories. PGTA utilizes an optimal task assignment algorithm based on the tree-decomposition technology to maximize the overall task assignments, in which we give higher priorities to the worker groups showing more interests in the tasks. Our empirical studies based on a real-world dataset verify the practicability of our proposed framework.},
booktitle = {Database Systems for Advanced Applications: 25th International Conference, DASFAA 2020, Jeju, South Korea, September 24–27, 2020, Proceedings, Part II},
pages = {677–693},
numpages = {17},
keywords = {Spatial crowdsourcing, Group task assignment, Social impact-based preference},
location = {Jeju, Korea (Republic of)}
}

@inproceedings{10.1145/3313831.3376833,
author = {Stowell, Elizabeth and O'Leary, Teresa K. and Kimani, Everlyne and Paasche-Orlow, Michael K. and Bickmore, Timothy and Parker, Andrea G.},
title = {Investigating Opportunities for Crowdsourcing in Church-Based Health Interventions: A Participatory Design Study},
year = {2020},
isbn = {9781450367080},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3313831.3376833},
doi = {10.1145/3313831.3376833},
abstract = {Churches play a major role in providing social support to address health inequities within Black communities, in part by connecting members to key organizations and services. While public health has a history of disseminating interventions in faith communities, little work has explored the use of crowdsourcing to tailor interventions to the unique culture of each church community. Following Community Based Participatory Research principles, we partnered with two predominantly Black churches, and report on a series of three participatory design sessions with nine participants. We developed a novel storyboarding method to explore how crowdsourcing could promote health in these faith-based communities. Our findings characterize existing supports within the church community, and how church social structures impact member access to these supports. We further identify motivations to engage with a church-situated health application, and how these motivations translate to crowdsourcing tasks. Finally, we discuss considerations for public health crowdsourcing tasks.},
booktitle = {Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems},
pages = {1–12},
numpages = {12},
keywords = {african-american, crowdsourcing, faith-based communities, health promotion, mhealth, participatory design},
location = {Honolulu, HI, USA},
series = {CHI '20}
}

@inproceedings{10.1145/2898365.2899799,
author = {Pham, Long and Linehan, Conor},
title = {Crowdsourcing: Tackling Challenges in the Engagement of Citizens with Smart City Initiatives},
year = {2016},
isbn = {9781450341943},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2898365.2899799},
doi = {10.1145/2898365.2899799},
abstract = {The engagement and involvement of citizens with the design of Smart City (SC) initiatives help ensure a maximisation of benefit for all stakeholders. However, undertaking processes that facilitate citizen engagement often involves prohibitive challenges in cost, design and deployment mechanisms, particularly for small cities which have limited resources. We report on a project carried out in Cork City, a small city in Ireland, where a crowdsourcing-inspired method was used. Academics, local government, volunteers and civil organisations came together to collaboratively design and carry out a study to represent local interests around the deployment of smart city infrastructure. Our project demonstrates a new way of translating crowdsourcing for use in government problem-solving. It was three-times less in cost, creative in design, and flexible but collaborative in deployment, resulting in high volume of reliable data for project prioritisation and implementation.},
booktitle = {Proceedings of the SEACHI 2016 on Smart Cities for Better Living with HCI and UX},
pages = {28–31},
numpages = {4},
location = {San Jose, CA, USA},
series = {SEACHI 2016}
}

@inproceedings{10.1145/3382494.3410689,
author = {Mejorado, Denisse Martinez and Saremi, Razieh and Yang, Ye and Ramirez-Marquez, Jose E.},
title = {Study on Patterns and Effect of Task Diversity in Software Crowdsourcing},
year = {2020},
isbn = {9781450375801},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3382494.3410689},
doi = {10.1145/3382494.3410689},
abstract = {Context: The success of software crowdsourcing depends on steady pools of task demand and active workers supply. Existing analysis reveals an average task failure ratio of 15.7\% in software crowdsourcing market.Goal: The objective of this study is to empirically investigate patterns and effect of task diversity in software crowdsourcing platform in order to improve the success and efficiency of software crowdsourcing.Method: We first propose a conceptual task diversity model, and develop an approach to measuring and analyzing task diversity. More specifically, task diversity is characterized based on semantic similarity, dynamic competition level, and the analysis includes identifying the dominant attributes distinguishing the competition levels, and measuring the impact of task diversity on task success and worker performance in crowdsourcing platform. The empirical study is conducted on more than one year's real-world data from TopCoder, one of the leading software crowdsourcing platforms.Results: We identified that monetary prize and task complexity are the dominant attributes that differentiate among different competition levels. Based on these dominant attributes, we concluded three task diversity patterns (configurations) from workers behavior perspective: responsive-to-prize, responsive-to-prize-and-complexity and over-responsive-to-prize. This study supports that the second pattern, i.e. responsive-to-prize-and-complexity configuration, associates with the lowest task failure ratio.Conclusions: These findings are helpful for task requesters to plan for and improve task success in a more effective and efficient manner in software crowdsourcing platform.},
booktitle = {Proceedings of the 14th ACM / IEEE International Symposium on Empirical Software Engineering and Measurement (ESEM)},
articleno = {19},
numpages = {10},
keywords = {software crowdsourcing, task diversity, task failure, task success, worker performance},
location = {Bari, Italy},
series = {ESEM '20}
}

@inproceedings{10.1145/2492517.2500242,
author = {Movshovitz-Attias, Dana and Movshovitz-Attias, Yair and Steenkiste, Peter and Faloutsos, Christos},
title = {Analysis of the reputation system and user contributions on a question answering website: StackOverflow},
year = {2013},
isbn = {9781450322409},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2492517.2500242},
doi = {10.1145/2492517.2500242},
abstract = {Question answering (Q&amp;A) communities have been gaining popularity in the past few years. The success of such sites depends mainly on the contribution of a small number of expert users who provide a significant portion of the helpful answers, and so identifying users that have the potential of becoming strong contributers is an important task for owners of such communities.We present a study of the popular Q&amp;A website StackOverflow (SO), in which users ask and answer questions about software development, algorithms, math and other technical topics. The dataset includes information on 3.5 million questions and 6.9 million answers created by 1.3 million users in the years 2008--2012. Participation in activities on the site (such as asking and answering questions) earns users reputation, which is an indicator of the value of that user to the site.We describe an analysis of the SO reputation system, and the participation patterns of high and low reputation users. The contributions of very high reputation users to the site indicate that they are the primary source of answers, and especially of high quality answers. Interestingly, we find that while the majority of questions on the site are asked by low reputation users, on average a high reputation user asks more questions than a user with low reputation. We consider a number of graph analysis methods for detecting influential and anomalous users in the underlying user interaction network, and find they are effective in detecting extreme behaviors such as those of spam users. Lastly, we show an application of our analysis: by considering user contributions over first months of activity on the site, we predict who will become influential long-term contributors.},
booktitle = {Proceedings of the 2013 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining},
pages = {886–893},
numpages = {8},
location = {Niagara, Ontario, Canada},
series = {ASONAM '13}
}

@inproceedings{10.1007/978-3-030-28730-6_18,
author = {Awwad, Tarek and Bennani, Nadia and Rehn-Sonigo, Veronika and Brunie, Lionel and Kosch, Harald},
title = {CrowdED and CREX: Towards Easy Crowdsourcing Quality Control Evaluation},
year = {2019},
isbn = {978-3-030-28729-0},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-28730-6_18},
doi = {10.1007/978-3-030-28730-6_18},
abstract = {Crowdsourcing is a time- and cost-efficient web-based technique for labeling large datasets like those used in Machine Learning. Controlling the output quality in crowdsourcing is an active research domain which has yielded a fair number of methods and approaches. Due to the quantitative and qualitative limitations of the existing evaluation datasets, comparing and evaluating these methods have been very limited. In this paper, we present CrowdED (Crowdsourcing Evaluation Dataset), a rich dataset for evaluating a wide range of quality control methods alongside with CREX (CReate Enrich eXtend), a framework that facilitates the creation of such datasets and guarantees their future-proofing and re-usability through customizable extension and enrichment.},
booktitle = {Advances in Databases and Information Systems: 23rd European Conference, ADBIS 2019, Bled, Slovenia, September 8–11, 2019, Proceedings},
pages = {285–301},
numpages = {17},
keywords = {Crowdsourcing, Quality control, Dataset, Generic platform, Extendable campaign},
location = {Bled, Slovenia}
}

@inproceedings{10.1007/978-3-319-91947-8_29,
author = {Collovini, Sandra and Pereira, Bolivar and dos Santos, Henrique D. P. and Vieira, Renata},
title = {Annotating Relations Between Named Entities with Crowdsourcing},
year = {2018},
isbn = {978-3-319-91946-1},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-319-91947-8_29},
doi = {10.1007/978-3-319-91947-8_29},
abstract = {In this paper, we describe how the CrowdFlower platform was used to build an annotated corpus for Relation Extraction. The obtained data provides information on the relations between named entities in Portuguese texts.},
booktitle = {Natural Language Processing and Information Systems: 23rd International Conference on Applications of Natural Language to Information Systems, NLDB 2018, Paris, France, June 13-15, 2018, Proceedings},
pages = {290–297},
numpages = {8},
keywords = {Crowdsourcing, Semantic relations annotation, Portuguese},
location = {Paris, France}
}

@inproceedings{10.1007/978-3-030-78221-4_29,
author = {Monti, Johanna and Chiusaroli, Francesca and Sangati, Federico},
title = {Emojitaliano: A Social and Crowdsourcing Experiment of the Creation of a Visual International Language},
year = {2021},
isbn = {978-3-030-78220-7},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-78221-4_29},
doi = {10.1007/978-3-030-78221-4_29},
abstract = {Inspired by the historical models of artificial and auxiliary languages, Emojitaliano is the result of a social and crowdsourcing experiment which was conducted by a group of seventeen translators, followers of the “Scritture brevi” blog, and led to the creation of an international language based on emojis. The experiment was carried out during 2016 on Twitter in the framework of the translation into emoji of Pinocchio, the famous Italian tale. Emojitaliano consists of 1) a repertoire of stable and coherent lexical correspondences between the emoji UNICODE set and the Italian language and 2) a set of predefined simplified rules agreed on during the translation process. Emojitaliano is stored in @Emojitalianobot, an online tool and digital environment for translation into emoji, running on Telegram, the popular instant messaging platform. It is the first open and free Emoji-Italian translation bot based on UNICODE descriptions, which contains a glossary with all the senses assigned by the translators to emojis during the translation process of the famous Italian novel. This paper presents the translation projects of Emojitaliano, the background and its lexicon and grammar and finally Emojitalianobot.},
booktitle = {Design, User Experience, and Usability:  UX Research and Design: 10th International Conference, DUXU 2021, Held as Part of the 23rd HCI International Conference, HCII 2021, Virtual Event, July 24–29, 2021, Proceedings, Part I},
pages = {426–441},
numpages = {16},
keywords = {Emoji, Emojitaliano, Emojitalianobot, Pinocchio}
}

@inproceedings{10.1145/2800835.2800976,
author = {Wu, Mingsheng and Frias-Martinez, Vanessa},
title = {Crowdsourcing biking times},
year = {2015},
isbn = {9781450335751},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2800835.2800976},
doi = {10.1145/2800835.2800976},
abstract = {Urban cyclists often rely on Google's biking directions to consult routes and times. However, cyclists have reported that those estimates can sometimes be inaccurate [1]. In this paper, we explore the accuracy of Google biking times using a crowdsourced approach. Specifically, we use real biking data from a bike sharing system as ground truth and evaluate the automatic computation of Google's biking times. We analyze similarities and differences between the two as well as the role that measurable factors such as trip distance or slope might play in the temporal differences. Finally, we propose a predictive model based on a set of measurable factors that improves the accuracy of Google's biking time computations by 5\%.},
booktitle = {Adjunct Proceedings of the 2015 ACM International Joint Conference on Pervasive and Ubiquitous Computing and Proceedings of the 2015 ACM International Symposium on Wearable Computers},
pages = {1123–1131},
numpages = {9},
keywords = {bike sharing systems, predictive analytics, urban computing},
location = {Osaka, Japan},
series = {UbiComp/ISWC'15 Adjunct}
}

@inproceedings{10.1145/2976796.2988223,
author = {Segundo, Ricardo M.C. and de Amorim, Marcello N. and Santos, Celso A.S.},
title = {Crowdsourcing \&amp; Multimedia: Enhancing Multimedia Activities with the Power of Crowds},
year = {2016},
isbn = {9781450345125},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2976796.2988223},
doi = {10.1145/2976796.2988223},
abstract = {This paper summarizes some information about a 4 hours short course presented by the authors during the WebMedia'16. Our short course aims to present the concept of crowdsourcing and empower participants to implement this model of production in various activities linked with Multimedia and Web Systems, such as annotation, generation, summarization, synchronization, recommendation, retrieval, presentation and evaluation of the content quality. The idea behind crowdsourcing is to take advantage of the processing power of a multitude of employees to accomplish tasks that are "difficult for a computer", but which are apparently "simple to human intelligence". Describing the contents of an image or a video as inappropriate is an example of such difficult task, because the description of the problem by means of algorithms and automated techniques applied to parameters of this content is very complex and inaccurate. Another complex task is the subjective assessment of the quality of video encoders, the results depend on the user's perception and not only on parameters such as signal-to-noise ratio, resolution or frame rate. The crowdsourcing model tends to provide reliable results for this and other problems related to Multimedia and Web Systems. The additional content support for this short course brings the fundamental concepts of crowdsourcing, a discussion of suitable scenarios for their use within the multimedia and examples of practical use of the concept in real-world scenarios.},
booktitle = {Proceedings of the 22nd Brazilian Symposium on Multimedia and the Web},
pages = {11–12},
numpages = {2},
keywords = {audio, crowdsourcing, image, multimedia, text, video},
location = {Teresina, Piau\'{\i} State, Brazil},
series = {Webmedia '16}
}

@inproceedings{10.5555/2772879.2773305,
author = {Conitzer, Vince and Brill, Markus and Freeman, Rupert},
title = {Crowdsourcing Societal Tradeoffs},
year = {2015},
isbn = {9781450334136},
publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
address = {Richland, SC},
abstract = {It would be desirable if, as a society, we could reduce the amount of landfill trash we create, the amount of carbon dioxide we emit, the amount of forest we clear, etc. Since we cannot (or are in any case not willing to) simultaneously pursue all these objectives to their maximum extent, we must prioritize among them. Currently, this is done mostly in an ad-hoc manner, with people, companies, local governments, and other entities deciding on an individual basis which of these objectives to pursue, and to what extent.A more systematic approach would be to set, at a global level, exact numerical tradeoffs: using one gallon of gasoline is as bad as creating x bags of landfill trash. Having such tradeoffs available would greatly facilitate decision making, and reduce inefficiencies resulting from inconsistent decisions across agents. But how could we arrive at a reasonable value for x?In this paper, we argue that many techniques developed in the multiagent systems community, particularly those under economic paradigms, can be brought to bear on this question. We lay out our vision and discuss its relation to computational social choice, mechanism design, prediction markets, and related topics.},
booktitle = {Proceedings of the 2015 International Conference on Autonomous Agents and Multiagent Systems},
pages = {1213–1217},
numpages = {5},
keywords = {crowdsourcing, information markets, judgment aggregation, mechanism design, social choice},
location = {Istanbul, Turkey},
series = {AAMAS '15}
}

@inproceedings{10.1007/978-3-319-19890-3_47,
author = {Yang, Jie and Bozzon, Alessandro and Houben, Geert-Jan},
title = {Knowledge Crowdsourcing Acceleration},
year = {2015},
isbn = {9783319198897},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-319-19890-3_47},
doi = {10.1007/978-3-319-19890-3_47},
abstract = {Crowdsourcing has recently become a powerful computational tool for data collection and augmentation. Although crowdsourcing has been extensively applied in diverse domains, most tasks are of low complexity such that workers are assumed to be endless, anonymous and disposable. By unlocking the value of human knowledge-related features, e.g., experience, expertise and opinion, we envision that crowdsourcing can reach its full potential to solve complex tasks. We aim at creating a comprehensive theory of crowdsourcing for knowledge creation, i.e., knowledge crowdsourcing, with a focus on developing methods and tools to control and accelerate knowledge creation process. Inspired by previous work, we describe a reference model of knowledge crowdsourcing acceleration, together with three case studies for model validation and extension. The results of our first case study on on-line knowledge creation demonstrate the potential contribution to web engineering.},
booktitle = {Proceedings of the 15th International Conference on Engineering the Web in the Big Data Era - Volume 9114},
pages = {639–643},
numpages = {5},
keywords = {Collaborative question answering, Crowdsourcing, Enterprise expert finding, Knowledge creation, Urban computing},
location = {Rotterdam, The Netherlands},
series = {ICWE 2015}
}

@inproceedings{10.1109/HICSS.2016.543,
author = {Morschheuser, Benedikt and Hamari, Juho and Koivisto, Jonna},
title = {Gamification in Crowdsourcing: A Review},
year = {2016},
isbn = {9780769556703},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/HICSS.2016.543},
doi = {10.1109/HICSS.2016.543},
abstract = {This study investigates how different gamification implementations can increase crowdsourcees' motivation and participation in crowdsourcing (CS). To this end, we review empirical literature that has investigated the use of gamification in crowdsourcing settings. Overall, the results of the review indicate that gamification has been an effective approach for increasing crowdsourcing participation. When comparing crowdcreating, -solving, -processing and-rating CS approaches, the results show differences in the use of gamification across CS types. Crowdsourcing initiatives that provide more monotonous tasks most commonly used mere points and other simpler gamification implementations, whereas CS initiatives that seek for diverse and creative contributions have employed gamification in more manifold ways employing a richer set of mechanics. These findings provide insights for designers of gamified systems and further research on the topics of gamification and crowdsourcing.},
booktitle = {Proceedings of the 2016 49th Hawaii International Conference on System Sciences (HICSS)},
pages = {4375–4384},
numpages = {10},
series = {HICSS '16}
}

@inproceedings{10.1145/3297001.3297050,
author = {John, Indu and Bhatnagar, Shalabh},
title = {Efficient Budget Allocation and Task Assignment in Crowdsourcing},
year = {2019},
isbn = {9781450362078},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3297001.3297050},
doi = {10.1145/3297001.3297050},
abstract = {Requesters in crowdsourcing marketplaces would like to efficiently allocate a fixed budget, among the set of tasks to be completed, which are of varying difficulty levels. The uncertainty in the arrival and departure of workers and the diversity in their skill levels add to the challenge, as minimizing the overall completion time is also an important concern. Current literature focuses on sequential allocation of tasks, i.e., task assignment to one worker at a time, or assumes the task difficulties to be known in advance. In this paper, we study the problem of efficient budget allocation under dynamic worker pool in crowdsourcing. Specifically, we consider binary labeling tasks for which the budget allocation problem can be cast as one of finding the optimal policy for a Markov decision process. We present a mathematical framework for modeling the problem and propose a class of algorithms for obtaining its solution. Experiments on simulated as well as real data demonstrate the capability of these algorithms to achieve performance very close to sequential allocation in much less time and their superiority over naive allocation strategies.},
booktitle = {Proceedings of the ACM India Joint International Conference on Data Science and Management of Data},
pages = {318–321},
numpages = {4},
keywords = {budget allocation, crowdsourcing, reinforcement learning},
location = {Kolkata, India},
series = {CODS-COMAD '19}
}

@inproceedings{10.1145/3151848.3151859,
author = {Fonteles, Andr\'{e} Sales and Bouveret, Sylvain and Gensel, J\'{e}r\^{o}me},
title = {A programming framework for Spatial Crowdsourcing},
year = {2017},
isbn = {9781450353007},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3151848.3151859},
doi = {10.1145/3151848.3151859},
abstract = {Spatial crowdsourcing platforms (SCP) are systems that allow someone to publish spatial tasks in order to find a suitable workforce to achieve it. These tasks require people, often using mobile devices, to be at a given location in order to accomplish them. SCPs have been source of much interest for academy and industry. For this reason, Doan et al. [4] argued in 2011 that the race was now on "toward building general crowdsourcing platforms that can be used to develop such systems quickly". Since then, little has been done in this matter. Besides, what has been proposed does not take into account real commercial SCPs and its requirements. We propose GENIUS-C, a framework to support the development of SCPs. It is based on a generic architecture proposed by Fonteles et al. [7] designed to reduce the gap between academy and industry. GENIUS-C is meant to decrease development cost and effort and increase overall quality of SCPs. A case study SCP has been created using GENIUS-C to demonstrate its benefits and how it can be used in the developments of SCPs.},
booktitle = {Proceedings of the 15th International Conference on Advances in Mobile Computing \&amp; Multimedia},
pages = {131–140},
numpages = {10},
keywords = {crowdsourcing, framework, spatial crowdsourcing},
location = {Salzburg, Austria},
series = {MoMM2017}
}

@inproceedings{10.1145/3318464.3383127,
author = {Drutsa, Alexey and Fedorova, Valentina and Ustalov, Dmitry and Megorskaya, Olga and Zerminova, Evfrosiniya and Baidakova, Daria},
title = {Crowdsourcing Practice for Efficient Data Labeling: Aggregation, Incremental Relabeling, and Pricing},
year = {2020},
isbn = {9781450367356},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3318464.3383127},
doi = {10.1145/3318464.3383127},
abstract = {In this tutorial, we present a portion of unique industry experience in efficient data labeling via crowdsourcing shared by both leading researchers and engineers from Yandex. We will make an introduction to data labeling via public crowdsourcing marketplaces and will present the key components of efficient label collection. This will be followed by a practice session, where participants will choose one of the real label collection tasks, experiment with selecting settings for the labeling process, and launch their label collection project on one of the largest crowdsourcing marketplaces. The projects will be run on real crowds within the tutorial session. While the crowd performers are annotating the project set up by the attendees, we will present the major theoretical results in efficient aggregation, incremental relabeling, and dynamic pricing. We will also discuss their strengths and weaknesses as well as applicability to real-world tasks, summarizing our five year-long research and industrial expertise in crowdsourcing. Finally, participants will receive a feedback about their projects and practical advice on how to make them more efficient. We invite beginners, advanced specialists, and researchers to learn how to collect high quality labeled data and do it efficiently.},
booktitle = {Proceedings of the 2020 ACM SIGMOD International Conference on Management of Data},
pages = {2623–2627},
numpages = {5},
keywords = {task design, quality control, incremental relabeling, dynamic pricing, data annotation, crowdsourcing, answer aggregation},
location = {Portland, OR, USA},
series = {SIGMOD '20}
}

@inproceedings{10.1145/3284432.3284471,
author = {Charles, Jack-Antoine and Chanel, Caroline P. C. and Chauffaut, Corentin and Chauvin, Pascal and Drougard, Nicolas},
title = {Human-Agent Interaction Model Learning based on Crowdsourcing},
year = {2018},
isbn = {9781450359535},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3284432.3284471},
doi = {10.1145/3284432.3284471},
abstract = {Missions involving humans interacting with automated systems become increasingly common. Due to the non-deterministic behavior of the human and possibly high risk of failing due to human factors, such an integrated system should react smartly by adapting its behavior when necessary. A promise avenue to design an efficient interaction-driven system is the mixed-initiative paradigm. In this context, this paper proposes a method to learn the model of a mixed-initiative human-robot mission. The first step to set up a reliable model is to acquire enough data. For this aim a crowdsourcing campaign was conducted and learning algorithms were trained on the collected data in order to model the human-robot mission and to optimize a supervision policy with a Markov Decision Process (MDP). This model takes into account the actions of the human operator during the interaction as well as the state of the robot and the mission. Once such a model has been learned, the supervision strategy can be optimized according to a criterion representing the goal of the mission. In this paper, the supervision strategy concerns the robot's operating mode. Simulations based on the MDP model show that planning under uncertainty solvers can be used to adapt robot's mode according to the state of the human-robot system. The optimization of the robot's operation mode seems to be able to improve the team's performance. The dataset that comes from crowdsourcing is therefore a material that can be useful for research in human-machine interaction, that is why it has been made available on our web site.},
booktitle = {Proceedings of the 6th International Conference on Human-Agent Interaction},
pages = {20–28},
numpages = {9},
keywords = {mixed-initiative mission, markov decision process, markov chain learning, human-robot interaction, crowdsourcing, classification},
location = {Southampton, United Kingdom},
series = {HAI '18}
}

@inproceedings{10.1145/3383583.3398551,
author = {Hou, Wenjun and Han, Huijie and Hong, Liang and Yin, Wei},
title = {CHCI: A Crowdsourcing Human-computer Interaction Framework for Cultural Heritage Knowledge},
year = {2020},
isbn = {9781450375856},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3383583.3398551},
doi = {10.1145/3383583.3398551},
abstract = {This paper aims to extract knowledge including entities and relationships, from multi-source heterogeneous cultural heritage (CH) resources. The proposed crowdsourcing human-computer interaction framework utilizes museum-user-algorithm cooperation to achieve high-quality and scalable CH knowledge extraction. This paper also proposes crowdsourcing optimization mechanisms to improve participation and quality of crowdsourcing project. Finally, this paper discusses how extracted knowledge can support CH digital resource construction and knowledge-driven intelligent applications in Museum.},
booktitle = {Proceedings of the ACM/IEEE Joint Conference on Digital Libraries in 2020},
pages = {551–552},
numpages = {2},
keywords = {crowdsourcing, digital resource construction, human-computer interaction, intelligent museum},
location = {Virtual Event, China},
series = {JCDL '20}
}

@inproceedings{10.1145/3340531.3412876,
author = {F\"{a}rber, Michael and Burkard, Victoria and Jatowt, Adam and Lim, Sora},
title = {A Multidimensional Dataset Based on Crowdsourcing for Analyzing and Detecting News Bias},
year = {2020},
isbn = {9781450368599},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3340531.3412876},
doi = {10.1145/3340531.3412876},
abstract = {The automatic detection of bias in news articles can have a high impact on society because undiscovered news bias may influence the political opinions, social views, and emotional feelings of readers. While various analyses and approaches to news bias detection have been proposed, large data sets with rich bias annotations on a fine-grained level are still missing. In this paper, we firstly aggregate the aspects of news bias in related works by proposing a new annotation schema for labeling news bias. This schema covers the overall bias, as well as the bias dimensions (1) hidden assumptions, (2) subjectivity, and (3) representation tendencies. Secondly, we propose a methodology based on crowdsourcing for obtaining a large data set for news bias analysis and identification. We then use our methodology to create a dataset consisting of more than 2,000 sentences annotated with 43,000 bias and bias dimension labels. Thirdly, we perform an in-depth analysis of the collected data. We show that the annotation task is difficult with respect to bias and specific bias dimensions. While crowdworkers' labels of representation tendencies correlate with experts' bias labels for articles, subjectivity and hidden assumptions do not correlate with experts' bias labels and, thus, seem to be less relevant when creating data sets with crowdworkers. The experts' article labels better match the inferred crowdworkers' article labels than the crowdworkers' sentence labels. The crowdworkers' countries of origin seem to affect their judgements. In our study, non-Western crowdworkers tend to annotate more bias either directly or in the form of bias dimensions (e.g., subjectivity) than Western crowdworkers do.},
booktitle = {Proceedings of the 29th ACM International Conference on Information \&amp; Knowledge Management},
pages = {3007–3014},
numpages = {8},
keywords = {text mining, news articles, media bias, crowdsourcing},
location = {Virtual Event, Ireland},
series = {CIKM '20}
}

@inproceedings{10.1145/3247885,
author = {V\"{a}\"{a}t\"{a}j\"{a}, Heli},
title = {Session details: Crowdsourcing and citizen participation},
year = {2017},
isbn = {9781450354264},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3247885},
doi = {10.1145/3247885},
booktitle = {Proceedings of the 21st International Academic Mindtrek Conference},
location = {Tampere, Finland},
series = {AcademicMindtrek '17}
}

@inproceedings{10.1145/3473856.3473873,
author = {Haug, Saskia and Rietz, Tim and Maedche, Alexander},
title = {Accelerating Deductive Coding of Qualitative Data: An Experimental Study on the Applicability of Crowdsourcing},
year = {2021},
isbn = {9781450386456},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3473856.3473873},
doi = {10.1145/3473856.3473873},
abstract = {While qualitative research can produce a rich understanding of peoples’ mind, it requires an essential and strenuous data annotation process known as coding. Coding can be repetitive and time-consuming, particularly for large datasets. Crowdsourcing provides flexible access to workers all around the world, however, researchers remain doubtful about its applicability for coding. In this study, we present an interactive coding system to support crowdsourced deductive coding of semi-structured qualitative data. Through an empirical evaluation on Amazon Mechanical Turk, we assess both the quality and the reliability of crowd-support for coding. Our results show that non-expert coders provide reliable results using our system. The crowd reached a substantial agreement of up to 91\% with the coding provided by experts. Our results indicate that crowdsourced coding is an applicable strategy for accelerating a strenuous task. Additionally, we present implications of crowdsourcing to reduce biases in the interpretation of qualitative data.},
booktitle = {Proceedings of Mensch Und Computer 2021},
pages = {432–443},
numpages = {12},
keywords = {Qualitative Data, Empirical Evaluation, Crowdsourcing, Coding},
location = {Ingolstadt, Germany},
series = {MuC '21}
}

@inproceedings{10.5555/3294996.3295189,
author = {Bonald, Thomas and Combes, Richard},
title = {A minimax optimal algorithm for crowdsourcing},
year = {2017},
isbn = {9781510860964},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {We consider the problem of accurately estimating the reliability of workers based on noisy labels they provide, which is a fundamental question in crowdsourcing. We propose a novel lower bound on the minimax estimation error which applies to any estimation procedure. We further propose Triangular Estimation (TE), an algorithm for estimating the reliability of workers. TE has low complexity, may be implemented in a streaming setting when labels are provided by workers in real time, and does not rely on an iterative procedure. We prove that TE is minimax optimal and matches our lower bound. We conclude by assessing the performance of TE and other state-of-the-art algorithms on both synthetic and real-world data.},
booktitle = {Proceedings of the 31st International Conference on Neural Information Processing Systems},
pages = {4355–4363},
numpages = {9},
location = {Long Beach, California, USA},
series = {NIPS'17}
}

@inproceedings{10.1007/978-3-030-52237-7_32,
author = {Moore, Steven and Nguyen, Huy A. and Stamper, John},
title = {Evaluating Crowdsourcing and Topic Modeling in Generating Knowledge Components from Explanations},
year = {2020},
isbn = {978-3-030-52236-0},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-52237-7_32},
doi = {10.1007/978-3-030-52237-7_32},
abstract = {Associating assessment items with hypothesized knowledge components (KCs) enables us to gain fine-grained data on students’ performance within an ed-tech system. However, creating this association is a time consuming process and requires substantial instructor effort. In this study, we present the results of crowdsourcing valuable insights into the underlying concepts of problems in mathematics and English writing, as a first step in leveraging the crowd to expedite the task of generating KCs. We presented crowdworkers with two problems in each domain and asked them to provide three explanations about why one problem is more challenging than the other. These explanations were then independently analyzed through (1) a series of qualitative coding methods and (2) several topic modeling techniques, to compare how they might assist in extracting KCs and other insights from the participant contributions. Results of our qualitative coding showed that crowdworkers were able to generate KCs that approximately matched those generated by domain experts. At the same time, the topic models’ outputs were evaluated against both the domain expert generated KCs and the results of the previous coding to determine effectiveness. Ultimately we found that while the topic modeling was not up to parity with the qualitative coding methods, it did assist in identifying useful clusters of explanations. This work demonstrates a method to leverage both the crowd’s knowledge and topic modeling to assist in the process of generating KCs for assessment items.},
booktitle = {Artificial Intelligence in Education: 21st International Conference, AIED 2020, Ifrane, Morocco, July 6–10, 2020, Proceedings, Part I},
pages = {398–410},
numpages = {13},
keywords = {Intelligent tutoring systems, Topic modeling, Crowdsourcing, Knowledge component modeling, Knowledge component},
location = {Ifrane, Morocco}
}

@inproceedings{10.1145/3292500.3330879,
author = {Li, Boyang and Cheng, Yurong and Yuan, Ye and Wang, Guoren and Chen, Lei},
title = {Three-Dimensional Stable Matching Problem for Spatial Crowdsourcing Platforms},
year = {2019},
isbn = {9781450362016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3292500.3330879},
doi = {10.1145/3292500.3330879},
abstract = {The popularity of mobile Internet techniques and Online-To-Offline(O2O) business models has led to the emergence of various spatial crowdsourcing (SC) platforms in our daily life. A core issue of SC platforms is to assign tasks to suitable crowd workers. Existing approaches usually focus on the matching of two types of objects,tasks and workers, and let workers to travel to the location of usersto provide services, which is a 2D matching problem. However, recent services provided by some new platforms, such as person-alized haircut service1and station ride-sharing, need users andworkers travel together to a third workplace to complete the service, which is indeed a 3D matching problem. Approaches in the existingstudies either cannot solve such 3D matching problem, or lack aassignment plan satisfying both users' and workers' preference inreal applications. Thus, in this paper, we propose a 3-Dimensional Stable Spatial Matching(3D-SSM) for the 3D matching problem innew SC services. We prove that the 3D-SSM problem is NP-hard, and propose two baseline algorithms and two efficient approximatealgorithms with bounded approximate ratios to solve it. Finally, weconduct extensive experiment studies which verify the efficiencyand effectiveness of the proposed algorithms on real and synthetic datasets.},
booktitle = {Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery \&amp; Data Mining},
pages = {1643–1653},
numpages = {11},
keywords = {stable matching, spatial database, crowdsourcing},
location = {Anchorage, AK, USA},
series = {KDD '19}
}

@inproceedings{10.1145/3511047.3538033,
author = {Yokota, Takuya and Nakao, Yuri},
title = {Toward a decision process of the best machine learning model for multi-stakeholders: a crowdsourcing survey method},
year = {2022},
isbn = {9781450392327},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3511047.3538033},
doi = {10.1145/3511047.3538033},
abstract = {Fairness-aware machine learning (ML) technology has been developed to remove discriminatory bias, e.g., bias on race and gender. However, there are trade-offs between the metrics of accuracy and fairness in ML models, and different stakeholders prioritize these metrics differently. Hence, to form an agreement on prioritization, workshop approaches encouraging dialogue among stakeholders have been explored. However, it is practically difficult for multiple stakeholders to have conversations at the same place and time. We examined a method of extracting the prioritization of several stakeholders regarding certain metrics using an online survey. We randomly divided 739 crowdsourced participants into 4 stakeholder groups and asked them to rank 5 randomly selected ML models in terms of their metric prioritization. Through this survey, we calculated the prioritization of metrics of each stakeholder group and whether the information on three other stakeholders affects another stakeholder’s prioritization of metrics. With our method, the prioritization of each stakeholder successfully met the requirements of their role. However, metric prioritization is not affected by information on the other stakeholders. Furthermore, demographics and attitudes towards decision making scenarios affect each stakeholder’s metric prioritization differently.},
booktitle = {Adjunct Proceedings of the 30th ACM Conference on User Modeling, Adaptation and Personalization},
pages = {245–254},
numpages = {10},
keywords = {utility function, stakeholder, machine learning, crowdsourcing},
location = {Barcelona, Spain},
series = {UMAP '22 Adjunct}
}

@inproceedings{10.1145/3472163.3472279,
author = {Suzuki, Yu},
title = {Measuring Quality of Workers by Goodness-of-Fit of Machine Learning Model in Crowdsourcing},
year = {2021},
isbn = {9781450389914},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3472163.3472279},
doi = {10.1145/3472163.3472279},
abstract = {In this paper, we propose a method for predicting the quality of crowdsourcing workers using the goodness-of-fit (GoF) of machine learning models. We assume a relationship between the quality of workers and the quality of machine-learning models using the outcomes of the workers as training data. This assumption means that if worker quality is high, a machine-learning classifier constructed using the worker’s outcomes can easily predict the outcomes of the worker. If this assumption is confirmed, we can measure the worker quality without using the correct answer sets, and then the requesters can reduce the time and effort. However, if the outcomes by workers are low quality, the input tweet does not correspond to the outcomes. Therefore, if we construct a tweet classifier using input tweets and the classified results by the worker, the prediction of the outcomes by the classifier and that by the workers should differ. We assume that the GoF scores, such as accuracy and F1 scores of the test set using this classifier, correlates to worker quality. Therefore, we can predict worker quality using the GoF scores. In our experiment, we did the tweet classification task using crowdsourcing. We confirmed that the GoF scores and the quality of workers correlate. These results show that we can predict the quality of workers using the GoF scores.},
booktitle = {Proceedings of the 25th International Database Engineering \&amp; Applications Symposium},
pages = {166–172},
numpages = {7},
keywords = {Machine Learning, Crowdsourcing},
location = {Montreal, QC, Canada},
series = {IDEAS '21}
}

@inproceedings{10.5555/3191835.3191901,
author = {Bhat, Vasudev and Gokhale, Adheesh and Jadhav, Ravi and Pudipeddi, Jagat and Akoglu, Leman},
title = {Min(e)d your tags: analysis of question response time in stackoverflow},
year = {2014},
isbn = {9781479958764},
publisher = {IEEE Press},
abstract = {Given a newly posted question on a Question and Answer (Q&amp;A) site, how long will it take until an answer is received? Does response time relate to factors about how the question asker composes their question? If so, what are those factors? With advances in social media and the Web, Q&amp;A sites have become a major source of information for Internet users. Response time of a question is an important aspect in these sites as it is associated with the users' satisfaction and engagement, and thus the lifespan of these online communities. In this paper we study and estimate response time for questions in StackOverflow, a popular online Q&amp;A forum where software developers post and answer questions related to programming. We analyze a long list of factors in the data and identify those that have clear relation with response time. Our key finding is that tag-related factors, such as their "popularity" (how often the tag is used) and the number of their "subscribers" (how many users can answer questions containing the tag), provide much stronger evidence than factors not related to tags. Finally, we learn models using the identified evidential features for predicting the response time of questions, which also demonstrate the significance of tags chosen by the question asker.},
booktitle = {Proceedings of the 2014 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining},
pages = {328–335},
numpages = {8},
keywords = {collective intelligence, evidential feature analysis, human behavior, online communities, question answering sites, question response time, user engagement},
location = {Beijing, China},
series = {ASONAM '14}
}

@inproceedings{10.1109/SMC42975.2020.9283090,
author = {Schneider, Daniel and Correia, Ant\'{o}nio and Chaves, Ramon and Pimentel, Ana Paula and Antelio, Marcio and Lucas, Edson Mello and de Almeida, Marcos Antonio and Oliveira, Luiz and de Souza, Jano Moreira},
title = {Turning social news curation into microtask crowdsourcing: a vision and research agenda},
year = {2020},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/SMC42975.2020.9283090},
doi = {10.1109/SMC42975.2020.9283090},
abstract = {Over the past decade, online crowdsourcing has established itself as an emerging paradigm that industry and government have been using to harness the cognitive abilities of a multitude of users distributed around the world. In this context, microtask crowdsourcing has become the method of choice for addressing a wide range of diverse problems. Microtasks typically require a minimum of time and cognitive effort, but combined individual efforts have made it possible to accomplish great achievements. The goal of this paper is to contribute to the ongoing effort of understanding whether the same success that microtask crowdsourcing has achieved in other domains can be obtained in the field of social news curation. In particular, we ask whether it is possible to turn online news curation, typically a social and collaborative activity on the Web, into a model in which curatorial activities are mapped into microtasks to be performed by a crowd of online users.},
booktitle = {2020 IEEE International Conference on Systems, Man, and Cybernetics (SMC)},
pages = {780–787},
numpages = {8},
location = {Toronto, ON}
}

@inproceedings{10.1109/WCNC49053.2021.9417464,
author = {Pimpinella, Andrea and Marabita, Andrea and Redondi, Alessandro E. C.},
title = {Crowdsourcing or Network KPIs? A Twofold Perspective for QoE Prediction in Cellular Networks},
year = {2021},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/WCNC49053.2021.9417464},
doi = {10.1109/WCNC49053.2021.9417464},
abstract = {Monitoring the Quality of Experience (QoE) of the customer base is a key task for Mobile Network Operators (MNOs), and it is generally performed by collecting users feedbacks through directed surveys. When such feedbacks are few in number, a MNO may predict the users QoE starting from objective network measurements, gathered directly from the users equipments through crowdsourcing. In this work, we compare such a traditional approach with a different one, where the data used for predicting the users QoE is gathered directly at the network access, using Key Performance Indicators (KPI) available on each base station. Although such KPIs are aggregated by design (i.e., they refer to the distribution of a population of users rather than to a single individual), we show through experiments with a country-wide dataset that their predictive power is comparable and in some cases superior than the one of crowdsourcing. Such a result is particularly attractive for MNOs, since network KPIs are generally much easily obtainable than crowdsourcing data.},
booktitle = {2021 IEEE Wireless Communications and Networking Conference (WCNC)},
pages = {1–6},
numpages = {6},
location = {Nanjing, China}
}

@inproceedings{10.5555/3361338.3361467,
author = {Boukoros, Spyros and Humbert, Mathias and Katzenbeisser, Stefan and Troncoso, Carmela},
title = {On (the lack of) location privacy in crowdsourcing applications},
year = {2019},
isbn = {9781939133069},
publisher = {USENIX Association},
address = {USA},
abstract = {Crowdsourcing enables application developers to benefit from large and diverse datasets at a low cost. Specifically, mobile crowdsourcing (MCS) leverages users' devices as sensors to perform geo-located data collection. The collection of geolocated data raises serious privacy concerns for users. Yet, despite the large research body on location privacy-preserving mechanisms (LPPMs), MCS developers implement little to no protection for data collection or publication. To understand this mismatch, we study the performance of existing LPPMs on publicly available data from two mobile crowdsourcing projects. Our results show that well-established defenses are either not applicable or offer little protection in the MCS setting. Additionally, they have a much stronger impact on applications' utility than foreseen in the literature. This is because existing LPPMs, designed with location-based services (LBSs) in mind, are optimized for utility functions based on users' locations, while MCS utility functions depend on the values (e.g., measurements) associated with those locations. We finally outline possible research avenues to facilitate the development of new location privacy solutions that fit the needs of MCS so that the increasing number of such applications do not jeopardize their users' privacy.},
booktitle = {Proceedings of the 28th USENIX Conference on Security Symposium},
pages = {1859–1876},
numpages = {18},
location = {Santa Clara, CA, USA},
series = {SEC'19}
}

@inproceedings{10.1145/3290605.3300305,
author = {Swearngin, Amanda and Li, Yang},
title = {Modeling Mobile Interface Tappability Using Crowdsourcing and Deep Learning},
year = {2019},
isbn = {9781450359702},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3290605.3300305},
doi = {10.1145/3290605.3300305},
abstract = {Tapping is an immensely important gesture in mobile touchscreen interfaces, yet people still frequently are required to learn which elements are tappable through trial and error. Predicting human behavior for this everyday gesture can help mobile app designers understand an important aspect of the usability of their apps without having to run a user study. In this paper, we present an approach for modeling tappability of mobile interfaces at scale. We conducted large-scale data collection of interface tappability over a rich set of mobile apps using crowdsourcing and computationally investigated a variety of signifiers that people use to distinguish tappable versus not-tappable elements. Based on the dataset, we developed and trained a deep neural network that predicts how likely a user will perceive an interface element as tappable versus not tappable. Using the trained tappability model, we developed TapShoe, a tool that automatically diagnoses mismatches between the tappability of each element as perceived by a human user---predicted by our model, and the intended or actual tappable state of the element specified by the developer or designer. Our model achieved reasonable accuracy: mean precision 90.2\% and recall 87.0\%, in matching human perception on identifying tappable UI elements. The tappability model and TapShoe were well received by designers via an informal evaluation with 7 professional interaction designers.},
booktitle = {Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems},
pages = {1–11},
numpages = {11},
keywords = {tappability, mobile interfaces, deep learning, crowdsourcing},
location = {Glasgow, Scotland Uk},
series = {CHI '19}
}

@inproceedings{10.1145/3274783.3275164,
author = {Huang, Chao and Xu, Fengli and Li, Yong and Chen, Xinlei and Zhang, Pei},
title = {Locally Differentially Private Participant Recruitment for Mobile Crowdsourcing},
year = {2018},
isbn = {9781450359528},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3274783.3275164},
doi = {10.1145/3274783.3275164},
abstract = {Location-aware mobile crowdsourcing tasks like urban sensing always require exposing users' location, which lead to serious privacy breaches. In this poster, we propose a locally differentially private participants recruitment system to maximize spatial coverage of the mobile crowdsourcing task while preserving location privacy. Based on the mechanism of randomized response, our system preserves the privacy in a local way, which eliminates the need for a trusted server. With guaranteed location privacy protection, a heuristic algorithm is proposed to solve the maximum spatial coverage problem efficiently given the obfuscated reports. Extensive experiments on real-world user trajectories demonstrate the feasibility of our proposed system, which improves the spatial coverage by more than 10\% on average compared with the state-of-the-art solutions.},
booktitle = {Proceedings of the 16th ACM Conference on Embedded Networked Sensor Systems},
pages = {323–324},
numpages = {2},
location = {Shenzhen, China},
series = {SenSys '18}
}

@inproceedings{10.1109/GLOCOM.2016.7842254,
author = {Shu, Jiangang and Jia, Xiaohua},
title = {Secure Task Recommendation in Crowdsourcing},
year = {2016},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/GLOCOM.2016.7842254},
doi = {10.1109/GLOCOM.2016.7842254},
abstract = {Many crowdsourcing platforms have been developed, which enable workers to complete a broad range of complex tasks published by task requesters. Existing task recommendation systems require sensitive information such as task content and interests of workers, which has raised serious privacy concerns. In order to preserve users' privacy in crowdsourcing, we propose a secure task recommendation scheme that achieves the preservation of task privacy and worker privacy simultaneously. Based on proxy cryptography, we realize the encrypted keyword-based matching between task specification and worker interest, and the encryption and decryption of task content, both in the multi-user environment. Moreover, user revocation is also supported. Through rigorous security analysis and performance evaluation, our scheme is secure and feasible.},
booktitle = {2016 IEEE Global Communications Conference (GLOBECOM)},
pages = {1–6},
numpages = {6},
location = {Washington, DC, USA}
}

@inproceedings{10.1145/3243082.3243085,
author = {de Sousa, Peron Rezende and Lage, Marcos and de Arag\~{a}o Rocha, Antonio A.},
title = {Future Internet and Scalability Techniques in Mobile Crowdsourcing},
year = {2018},
isbn = {9781450358675},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3243082.3243085},
doi = {10.1145/3243082.3243085},
abstract = {In this paper we present a new architecture for mobile crowdsourcing systems which leverages the infrastructure of services widely scalable. We successfully developed a proof of concept and discussed an alternative architecture that uses direct communication between devices to eliminate the additional financial contribution needed in the solutions developed with elastic/cloud computing. We also presented a new incentive mechanism and evaluated its scalability with up to 1500 simultaneous accesses. Our results show that it is capable of serving one of the largest crowdsourcing systems on the internet.},
booktitle = {Proceedings of the 24th Brazilian Symposium on Multimedia and the Web},
pages = {77–84},
numpages = {8},
keywords = {Crowdsourcing, Future Internet, Scalability},
location = {Salvador, BA, Brazil},
series = {WebMedia '18}
}

@inproceedings{10.1145/3347146.3359365,
author = {Gummidi, Srinivasa Raghavendra Bhuvan and Pedersen, Torben Bach and Xie, Xike and Zim\'{a}nyi, Esteban},
title = {Push-based Spatial Crowdsourcing for Enriching Semantic Tags in OpenStreetMap},
year = {2019},
isbn = {9781450369091},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3347146.3359365},
doi = {10.1145/3347146.3359365},
abstract = {OpenStreetMap (OSM) is a popular community-driven mapping platform with voluntary contributions from (amateur) cartographers. However, it is a difficult process for the cartographer to identify the areas where she can best contribute to OSM. Furthermore, the current OSM spatial entities are missing many tags; for example, top three road network tags, Name, Source, and Surface, are available only for the 10\% of the total road segments. Our paper aims to improve the quantity and quality of the road network tags by actively pushing the nearest road segments for the cartographer to be mapped. We propose a push-based spatial crowdsourcing method to achieve this objective, and validate it by focusing on road segments in OSM. Specifically, we formally define the batch-based maximum road segment task assignment problem and suggest methods based on heuristics like travel distance and road segment task grouping. Finally, our experimental evaluation verify the applicability of our assignment solutions by comparing the resulting number of assigned tasks. With regard to the number of assigned road segments, our junctions-based and road segment-based heuristic methods, outperform the baseline methods by five and two times, respectively.},
booktitle = {Proceedings of the 27th ACM SIGSPATIAL International Conference on Advances in Geographic Information Systems},
pages = {532–535},
numpages = {4},
keywords = {OpenStreetMap, Road Network, Semantic Tags, Spatial Crowdsourcing, Task Assignment},
location = {Chicago, IL, USA},
series = {SIGSPATIAL '19}
}

@inproceedings{10.1007/978-3-030-34223-4_1,
author = {Xing, Qianli and Zhao, Weiliang and Yang, Jian and Wu, Jia and Wang, Qi and Wang, Mei},
title = {GroExpert: A Novel Group-Aware Experts Identification Approach in Crowdsourcing},
year = {2020},
isbn = {978-3-030-34222-7},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-34223-4_1},
doi = {10.1007/978-3-030-34223-4_1},
abstract = {Measuring workers’ abilities is a way to address the long standing problem of quality control in crowdsourcing. The approaches for measuring worker ability reported in recent work can be classified into two groups, i.e., upper bound-based approaches and lower bound-based approaches. Most of these works are based on two assumptions: (1) workers give their answers to a task independently and are not affected by other workers; (2) a worker’s ability for a task is a fixed value. However realistically, a worker’s ability should be evaluated as a relative value to those of others within a group. In this work, we propose an approach called GroExpert to identify experts based on their relative values in their working groups, which can be used as a basis for quality estimation in crowdsourcing. The proposed solution employs a fully connected neural network to implement the pairwise ranking method when identifying experts. Both workers’ features and groups’ features are considered in GroExpert. We conduct a set of experiments on three real-world datasets from the Amazon Mechanical Turk platform. The experimental results show that the proposed GroExpert approach outperforms the state-of-the-art in worker ability measurement.},
booktitle = {Web Information Systems Engineering – WISE 2019: 20th International Conference, Hong Kong, China, January 19–22, 2020, Proceedings},
pages = {3–17},
numpages = {15},
keywords = {Crowdsourcing, Group-aware, Worker ability},
location = {Hong Kong, China}
}

@inproceedings{10.1007/978-3-030-03098-8_35,
author = {Itoh, Yuya and Matsubara, Shigeo},
title = {Adaptive Budget Allocation for Sequential Tasks in Crowdsourcing},
year = {2018},
isbn = {978-3-030-03097-1},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-03098-8_35},
doi = {10.1007/978-3-030-03098-8_35},
abstract = {This paper proposes a new budget allocation method for crowdsourced sequential tasks. Sequential tasks mean that an output of a task becomes an input to another task, and the quality of the final artifact depends on the qualities of the preceding tasks. In crowdsourcing, the abilities of workers are often difficult to learn in advance. Thus, the fixed budget allocation to the component tasks cannot respond to the realized situation. Also, the requester is often difficult to evaluate the quality of intermediate artifacts accurately, which results in misallocating the budget and wasting a budget. To overcome these difficulties, we have developed a contingent budget allocation method, i.e., generating a conditional plan given uncertainty about the intermediate states and action effects, by formalized a problem as POMDP and introducing a quality evaluation action. The experimental results show that the proposed method can find a solution in a reasonable time and improve the quality of the final artifact.},
booktitle = {PRIMA 2018: Principles and Practice of Multi-Agent Systems: 21st International Conference, Tokyo, Japan, October 29-November 2, 2018, Proceedings},
pages = {502–509},
numpages = {8},
keywords = {Crowdsourcing, POMDP, Budget allocation, Cooperation},
location = {Tokyo, Japan}
}

@inproceedings{10.1145/3110025.3121242,
author = {Zhao, Ying and MacKinnon, Douglas J. and Zhou, Charles C.},
title = {Discovering High-Value Information from Crowdsourcing},
year = {2017},
isbn = {9781450349932},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3110025.3121242},
doi = {10.1145/3110025.3121242},
abstract = {We will demonstrate a distributed recursive method, i.e., Lexical Link Analysis (LLA) and an infrastructure, i.e., Collaborative Learning Agents (CLA) to discover high-value information. The combined system is a unified methodology of discovering high-value information from structured and unstructured heterogeneous data sources. We will demonstrate the LLA/CLA system using a crowdsourcing data source and show how it can be used to discover new knowledge for a widening range of applications and heterogeneous data types.},
booktitle = {Proceedings of the 2017 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining 2017},
pages = {585–588},
numpages = {4},
keywords = {unsupervised learning, lexical link analysis, high-value information, heterogeneous data, crowdsourcing, collaborative learning agent},
location = {Sydney, Australia},
series = {ASONAM '17}
}

@inproceedings{10.1109/GLOCOM.2018.8647377,
author = {Shao, Qi and Cheung, Man Hon and Huang, Jianwei},
title = {Crowdsourcing with Bounded Rationality: A Cognitive Hierarchy Perspective},
year = {2018},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/GLOCOM.2018.8647377},
doi = {10.1109/GLOCOM.2018.8647377},
abstract = {Previous studies in crowdsourcing systems usually regard workers as fully rational players, who have infinite cognitive capabilities when reasoning about other players' decisions. However, recent psychological studies have revealed that humans are often bounded rational with cognitive reasoning limits. In this paper, we present a first study regarding the impact of such worker bounded rationality in a crowdsourcing system, and characterize how the result obtained from this more practical assumption deviates from the fully rational benchmark. Specifically, we consider a simple two-stage crowdsourcing model, where a requester first determines the rewards for workers completing the tasks, and then workers make their task choices accordingly. First, we show that such a model is non-trivial to analyze even in the fully rational case, due to the integer constraints on workers' choices. Nevertheless, we are able to characterize the closed-form solution of the optimal rewards and Nash equilibrium with full rationality by exploiting the special structure of the problem formulation. Next, we focus on the more practical bounded rational model, and apply the cognitive hierarchy theory from behavioral economics in the modeling of workers' decisions. Comparing with the fully rational benchmark, we show that in practice the requester can receive a higher profit when considering the workers' bounded rationality, especially when the number of workers is large or the workers' average cognitive level is low. When the workers' average cognitive level is high enough, however, the practical bounded rational model converges to the benchmark fully rational model.},
booktitle = {2018 IEEE Global Communications Conference (GLOBECOM)},
pages = {206–212},
numpages = {7},
location = {Abu Dhabi, United Arab Emirates}
}

@inproceedings{10.1145/3436829.3436834,
author = {Kamel, Menna Maged and Gil-Solla, Alberto and Ramos-Carber, Manuel},
title = {Tasks Recommendation in Crowdsourcing based on Workers' Implicit Profiles and Performance History},
year = {2021},
isbn = {9781450377218},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3436829.3436834},
doi = {10.1145/3436829.3436834},
abstract = {Crowdsourcing allows to build online platforms that make use of the power of human intelligence to complete tasks that are difficult to tackle for current algorithms. Current approaches to crowdsourcing adopt a methodology where tasks are published on specialized web platforms to a group of networked workers who can pick their preferred tasks freely on a first-come-first-served basis. Although this approach has several advantages, however it doesn't consider workers differences and capabilities. With the vast number of tasks posted by the requesters every day it's a challenging issue to satisfy both workers and requesters. In this paper, a crowdsourcing recommendation approach is proposed and evaluated that is based on a push methodology. This method aims to help workers to instantly find best matching tasks according to their interests and qualifications as well help the requesters to pick from the crowd the best workers for their desired tasks.},
booktitle = {Proceedings of the 9th International Conference on Software and Information Engineering},
pages = {51–55},
numpages = {5},
keywords = {Task Recommendation, Recommendation Systems, Crowdsourcing, Classification},
location = {Cairo, Egypt},
series = {ICSIE '20}
}

@inproceedings{10.1109/INFOCOM.2019.8737541,
author = {Li, Yang and Sun, Jiachen and Huang, Wenguang and Tian, Xiaohua},
title = {Detecting Anomaly in Large-scale Network using Mobile Crowdsourcing},
year = {2019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/INFOCOM.2019.8737541},
doi = {10.1109/INFOCOM.2019.8737541},
abstract = {In this paper, we propose a tree modeling-based data mining method to detect anomalies from crowdsourced network data. We design an algorithm to extract potential network anomalies from decision trees. Moreover, we propose a criteria to evaluate the severity of anomaly in terms of three factors: standard deviation, weight sum and impurity decrease. To enhance generalization performance, we randomly generate sample subspace of the original dataset as the input for each subtree and compact detected anomalies from all subtrees. We carry out experiments based on the crowdsourced network measurement dataset containing five million samples, which contains round trip time (RTT) from more than 5,000 users. Experiments show that the proposed method can effectively detect high-latency network anomalies. Moreover, the random forest-based approach can achieve an improvement of approximately 25% of generalization performance compared to the single decision tree approach.},
booktitle = {IEEE INFOCOM 2019 - IEEE Conference on Computer Communications},
pages = {2179–2187},
numpages = {9},
location = {Paris, France}
}

@inproceedings{10.1007/978-3-030-77626-8_3,
author = {Dorton, Stephen L. and Harper, Samantha B. and Creed, Glory A. and Banta, H. George},
title = {Up for Debate: Effects of Formal Structure on Argumentation Quality in a Crowdsourcing Platform},
year = {2021},
isbn = {978-3-030-77625-1},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-77626-8_3},
doi = {10.1007/978-3-030-77626-8_3},
abstract = {We examined the use of formal structure (more specifically, the Toulmin model and the use of abstraction laddering) in argument assertion templates in a crowdsourcing platform, to determine its effects on argument quality, as rated by other peer contributors. Contrary to our hypotheses, the attempt to add rigor to asserted arguments resulted in a significant decrease in quality across several measures, including the pathos, kairos, and overall level of agreement with the assertion. We found that the way participants voted (a binary outcome of supporting or dissenting) aligned more strongly with whether they agreed with the assertion (regardless of quality) rather than with the quality of the assertion. We provide multiple potential explanations for why the use of the Toulmin model was not a reliable predictor of argument quality in a crowdsourcing application.},
booktitle = {Social Computing and Social Media: Experience Design and Social Network Analysis        : 13th International Conference, SCSM 2021, Held as Part of the 23rd HCI International Conference, HCII 2021, Virtual Event, July 24–29, 2021, Proceedings, Part  I},
pages = {36–53},
numpages = {18},
keywords = {Rhetorical analysis, Collective intelligence, Toulmin model, Argumentation, Crowdsourcing}
}

@inproceedings{10.1007/978-3-030-22744-9_52,
author = {Marques, Gon\c{c}alo and Pitarma, Rui},
title = {Noise Mapping Through Mobile Crowdsourcing for Enhanced Living Environments},
year = {2019},
isbn = {978-3-030-22743-2},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-22744-9_52},
doi = {10.1007/978-3-030-22744-9_52},
abstract = {Environmental noise pollution has a significant impact on health. The noise effects on health are related to annoyance, sleep and cognitive performance for both adults and children are reported in the literature. The smart city concept can be assumed as a strategy to mitigate the problems generated by the urban population growth and rapid urbanisation. Noise mapping is an important step for noise pollution reduction. Although, noise maps are particularly time-consuming and costly to create as they are produced with standard methodologies and are based on specific sources such as road traffic, railway traffic, aircraft and industrial. Therefore, the actual noise maps are significantly imperfect because the noise emission models and sources are extremely limited. Smartphones have incredible processing capabilities as well as several powerful sensors such as microphone and GPS. Using the resources present in a smartphone as long with participatory sensing, a crowdsourcing noise mobile application can be used to provide environmental noise supervision for enhanced living environments. Crowdsourcing techniques applied to environmental noise monitoring allow creating reliable noise maps at low-cost. This paper presents a mobile crowdsourcing solution for environmental noise monitoring named iNoiseMapping. The environmental noise data is collected through participatory sensing and stored for further analysis. The results obtained can ensure that mobile crowdsourcing offers several enhanced features for environmental noise supervision and analytics. Consequently, this mobile application is a significant decision-making tool to plan interventions for noise pollution reduction.},
booktitle = {Computational Science – ICCS 2019: 19th International Conference, Faro, Portugal, June 12–14, 2019, Proceedings, Part III},
pages = {670–679},
numpages = {10},
keywords = {Smartphones, Smart city, Participatory sensing, Mobile crowdsourcing, Environmental monitoring, Enhanced living environments},
location = {Faro, Portugal}
}

@inproceedings{10.1145/3083671.3083689,
author = {Auferbauer, D. and Tellio\u{g}lu, H.},
title = {Centralized Crowdsourcing in Disaster Management: Findings and Implications},
year = {2017},
isbn = {9781450348546},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3083671.3083689},
doi = {10.1145/3083671.3083689},
abstract = {Computer supported cooperative work (CSCW) has become an important aspect in crisis and disaster management. Volunteers undertaking relief efforts in affected areas are increasingly using information and communication technologies to coordinate their work. Relief organizations are recognizing this trend and have started to adapt new communication channels to interact with citizens. In this paper, we describe the crowdtasking approach, a centralized form of crowdsourcing for crisis and disaster management. We present a prototype implementation of the approach and report on our findings from the system's first field trial. We conclude by discussing implications of this approach for CSCW and community building in crisis and disaster management. Lastly, we give an outlook on future research based on our experience with crowdtasking.},
booktitle = {Proceedings of the 8th International Conference on Communities and Technologies},
pages = {173–182},
numpages = {10},
keywords = {volunteers, crowdtasking, crowdsourcing, computer supported cooperative work, Crisis and disaster management},
location = {Troyes, France},
series = {C&amp;T '17}
}

@inproceedings{10.1145/3019943.3020001,
author = {Mazayev, A. and Martins, J. A. and Correia, N.},
title = {Improving Accessibility through Semantic Crowdsourcing},
year = {2016},
isbn = {9781450347488},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3019943.3020001},
doi = {10.1145/3019943.3020001},
abstract = {Mainstream crowdsourcing applications for rating the accessibility of places are usually very limited in scope as only a small number of people are willing to participate, mostly the ones already interested in consuming this kind of information. We propose an extension of the schema.org ontology as a new crowdsourcing approach for engaging and motivating a larger population to participate in evaluating the accessibility of a place. An architecture of the proposed model is presented and its details are discussed. This extension has the potential to drastically improve the accessibility evaluation paradigm, as it has advantages not only for the persons who consume the data, but as well to those who describe the accessibility features of a place, especially business owners.},
booktitle = {Proceedings of the 7th International Conference on Software Development and Technologies for Enhancing Accessibility and Fighting Info-Exclusion},
pages = {408–413},
numpages = {6},
keywords = {semantic markup, schema.org, ontology, accessibility, Crowdsourcing},
location = {Vila Real, Portugal},
series = {DSAI '16}
}

@inproceedings{10.1109/SAHCN.2017.7964917,
author = {Zhang, Jianhui and Li, Zhi and Lin, Xiaojun and Jiang, Feilong},
title = {Composite Task Selection with Heterogeneous Crowdsourcing},
year = {2017},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/SAHCN.2017.7964917},
doi = {10.1109/SAHCN.2017.7964917},
abstract = {Because of the powerful mobile technology, many crowdsourcing applications emerge to implement location-dependent tasks. One common powerful function of crowdsourcing is to decompose huge or complex tasks into small sub-ones, which require users with different skills to implement. These tasks are composite and implemented completely only after all of their sub-tasks are finished. Meanwhile, users may be heterogeneous, i.e., having various skills and able to implement diverse sub-tasks with some cost. The interesting problem is how users choose the sub-tasks so as to maximize reward with cost as low as possible. \%It motivates us to consider how to design that tasks are composite. Computing the payoff maximization with multiple users tasks turn out to be an NP-complete problem. This paper focuses on the case under which users cooperatively implement composite tasks, and propose a Local Composite Task Selection (LCTS) algorithm to help the users choose their composite task selection strategies. We analyze the convergence of the algorithm, and further characterize the computation time for users' strategies updating under the algorithm. Numerical results suggest that the LCTS algorithm achieves similar payoff and task completion ratio to a greedy centralized benchmark when the number of users is large. The results also illustrate the quick convergence of the LCTS algorithm, and the convergence is quite related to the number of composite tasks and their sub-tasks.},
booktitle = {2017 14th Annual IEEE International Conference on Sensing, Communication, and Networking (SECON)},
pages = {1–9},
numpages = {9},
location = {San Diego, CA, USA}
}

@inproceedings{10.1007/978-3-030-22496-7_8,
author = {Meftah, Lakhdar and Rouvoy, Romain and Chrisment, Isabelle},
title = {FOUGERE: User-Centric Location Privacy in Mobile Crowdsourcing Apps},
year = {2019},
isbn = {978-3-030-22495-0},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-22496-7_8},
doi = {10.1007/978-3-030-22496-7_8},
abstract = {Mobile crowdsourcing is being increasingly used by industrial and research communities to build realistic datasets. By leveraging the capabilities of mobile devices, mobile crowdsourcing apps can be used to track participants’ activity and to collect insightful reports from the environment (e.g., air quality, network quality). However, most of existing crowdsourced datasets systematically tag data samples with time and location stamps, which may inevitably lead to user privacy leaks by discarding sensitive information.This paper addresses this critical limitation of the state of the art by proposing a software library that improves user privacy without compromising the overall quality of the crowdsourced datasets. We propose a decentralized approach, named Fougere, to convey data samples from user devices to third-party servers. By introducing an a priori data anonymization process, we show that Fougere defeats state-of-the-art location-based privacy attacks with little impact on the quality of crowdsourced datasets.},
booktitle = {Distributed Applications and Interoperable Systems: 19th IFIP WG 6.1 International Conference, DAIS 2019, Held as Part of the 14th International Federated Conference on Distributed Computing Techniques, DisCoTec 2019, Kongens Lyngby, Denmark, June 17–21, 2019, Proceedings},
pages = {116–132},
numpages = {17},
keywords = {LPPM, Mobile crowdsourcing, Location privacy},
location = {Kongens Lyngby, Denmark}
}

@inproceedings{10.1145/3332167.3357100,
author = {Chung, John Joon Young and Xiao, Fuhu and Banovic, Nikola and Lasecki, Walter S.},
title = {Towards Instantaneous Recovery from Autonomous System Failures via Predictive Crowdsourcing},
year = {2019},
isbn = {9781450368179},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3332167.3357100},
doi = {10.1145/3332167.3357100},
abstract = {Autonomous systems (e.g., long-distance driverless trucks) aim to reduce the need for people to complete tedious tasks. In many domains, automation is challenging because systems may fail to recognize or comprehend all relevant aspects of its current state. When an unknown or uncertain state is encountered in a mission-critical setting, recovery often requires human intervention or hand-off. However, human intervention is associated with decision (and communication, if remote) delays that prevent recovery in low-latency settings. Instantaneous crowdsourcing approaches that leverage predictive techniques reduce this latency by preparing human responses for possible near future states before they occur. Unfortunately, the number of possible future states can be vast and considering all of them is intractable in all but the simplest of settings. Instead, to reduce the number of states that must later be explored, we propose the approach that uses the crowd to first predict the most relevant or likely future states. We examine the latency and accuracy of crowd workers in a simple future state prediction task, and find that more than half of crowd workers were able to provide accurate answers within one second. Our results show that crowd predictions can filter out critical future states in tasks where decisions are required in less than three seconds.},
booktitle = {Adjunct Proceedings of the 32nd Annual ACM Symposium on User Interface Software and Technology},
pages = {16–18},
numpages = {3},
keywords = {real-time crowdsourcing, prediction, human computation},
location = {New Orleans, LA, USA},
series = {UIST '19 Adjunct}
}

@inproceedings{10.1145/3126973.3126982,
author = {Xu, Song and Liu, Lei and Cui, Lizhen and Zheng, Yongqing},
title = {Optimal Crowds Contest Model for Crowdsourcing},
year = {2017},
isbn = {9781450353755},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3126973.3126982},
doi = {10.1145/3126973.3126982},
abstract = {With the increasing frequency of participation in social networking activities, tremendous value has been created by crowds. Thus some emerging industries come along with it to collect these values. At the same time, crowds require some compensation from the these project organizers for their privacy loss or cost of activities. This paper dedicate to exploit a users incentives system, it develops a game-theoretic model of crowdsourcing or crowdsensing services base on contests. The model consists of two parts: incentives and optimizing pricing. We start from the crowds' point of view, committed to dig out their equilibrium strategies. Based on this, a bonus pool and expected rewards are demonstrated for the organizer and crowds respectively.},
booktitle = {Proceedings of the 2nd International Conference on Crowd Science and Engineering},
pages = {72–76},
numpages = {5},
keywords = {Contests, Crowdsourcing, Incentives},
location = {Beijing, China},
series = {ICCSE'17}
}

@inproceedings{10.1109/CHASE.2019.00036,
author = {Machado, Leticia Santos and Melo, Ricardo R. M. and de Souza, Cleidson R. B.},
title = {The role of platform moderators in software crowdsourcing projects},
year = {2019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/CHASE.2019.00036},
doi = {10.1109/CHASE.2019.00036},
abstract = {Crowdsourcing divides a task into small pieces that are carried out by the crowd. In Software Engineering, crowdsourcing divides the software development tasks of to be carried out online by the crowd and is simply called Software Crowdsourcing (SW CS). The goal of this paper is to investigate the role of platform moderators in the communication process during SW CS projects. The findings indicate the relevant role of the platforms moderators in facilitating communication during competitions. Our results contribute to the discussion on how communication paths can be useful in information transfer among crowd and platform.},
booktitle = {Proceedings of the 12th International Workshop on Cooperative and Human Aspects of Software Engineering},
pages = {119–122},
numpages = {4},
keywords = {topcoder, platform, crowdsourcing, communication},
location = {Montreal, Quebec, Canada},
series = {CHASE '19}
}

@inproceedings{10.1007/978-3-030-74296-6_43,
author = {Rettig, Laura and Shabani, Shaban and Sauter, Loris and Cudr\'{e}-Mauroux, Philippe and Sokhn, Maria and Schuldt, Heiko},
title = {City-Stories: Combining Entity Linking, Multimedia Retrieval, and Crowdsourcing to Make Historical Data Accessible},
year = {2021},
isbn = {978-3-030-74295-9},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-74296-6_43},
doi = {10.1007/978-3-030-74296-6_43},
abstract = {Digitized historical image collections as provided by individuals or memory institutions often suffer from limited or a complete lack of metadata In this paper, we present the City-Stories system that combines entity linking, multimedia retrieval, and crowdsourcing to make historical images searchable even across collections.},
booktitle = {Web Engineering: 21st International Conference, ICWE 2021, Biarritz, France, May 18–21, 2021, Proceedings},
pages = {521–524},
numpages = {4},
keywords = {Crowdsourcing, Semantic data, Entity linking, Multimedia retrieval},
location = {Biarritz, France}
}

@inproceedings{10.1145/3240323.3240385,
author = {Rostami, Mohammad and Huber, David and Lu, Tsai-Ching},
title = {A crowdsourcing triage algorithm for geopolitical event forecasting},
year = {2018},
isbn = {9781450359016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3240323.3240385},
doi = {10.1145/3240323.3240385},
abstract = {Predicting the outcome of geopolitical events is of huge importance to many organizations, as these forecasts may be used to make consequential decisions. Prediction polling is a common method used in crowdsourcing platforms for geopolitical forecasting, where a group of non-expert participants are asked to predict the outcome of a geopolitical event and the collected responses are aggregated to generate a forecast. It has been demonstrated that forecasts by such a crowd can be more accurate than the forecasts of experts. However, geopolitical prediction polling is challenging because participants are highly heterogeneous and diverse in terms of their skills and background knowledge and human resources are often limited. As a result, it is crucial to refer each question to the subset of participants that possess suitable skills to answer it, such that individual efforts are not wasted. In this paper, we propose an algorithm based on multitask learning to learn the skills of participants of a forecasting platform by using their performance history. The learned model then can be used to recommend suitable questions to forecasters. Our experimental results demonstrate that the prediction accuracy can be increased based on the proposed algorithm as opposed to when questions have been randomly assigned.},
booktitle = {Proceedings of the 12th ACM Conference on Recommender Systems},
pages = {377–381},
numpages = {5},
keywords = {multitask learning, metropolis\^{a}\u{A}\c{S}Hastings algorithm, markov chain monte carlo, geopolitical forecasting, biconvex optimization},
location = {Vancouver, British Columbia, Canada},
series = {RecSys '18}
}

@inproceedings{10.1007/978-3-030-33702-5_34,
author = {Liao, Zhifang and Xu, Xin and Lan, Peng and Long, Jun and Zhang, Yan},
title = {A Recommendation of Crowdsourcing Workers Based on Multi-community Collaboration},
year = {2019},
isbn = {978-3-030-33701-8},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-33702-5_34},
doi = {10.1007/978-3-030-33702-5_34},
abstract = {Currently there are problems such as fuzzy workers’ characteristics and complex human relations existing on many crowdsourcing platforms, which lead to the difficulty in the recommendation of workers to complete tasks on crowdsourcing platforms. Aiming at worker recommendations in categorical tasks on crowdsourcing platforms, this paper proposes a recommendation considering workers’ multi-community characteristics. It takes factors such as worker’s reputation, preference and activity into consideration. Finally, based on the characteristics of community intersections, it recommends Top-N workers. The results show the recommendations generated by the algorithm proposed in this paper performs the best comprehensively.},
booktitle = {Service-Oriented Computing: 17th International Conference, ICSOC 2019, Toulouse, France, October 28–31, 2019, Proceedings},
pages = {447–451},
numpages = {5},
keywords = {Crowdsourcing, Recommendation, Community discovery},
location = {Toulouse, France}
}

@inproceedings{10.1145/3290605.3300522,
author = {Cartwright, Mark and Dove, Graham and M\'{e}ndez M\'{e}ndez, Ana Elisa and Bello, Juan P. and Nov, Oded},
title = {Crowdsourcing Multi-label Audio Annotation Tasks with Citizen Scientists},
year = {2019},
isbn = {9781450359702},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3290605.3300522},
doi = {10.1145/3290605.3300522},
abstract = {Annotating rich audio data is an essential aspect of training and evaluating machine listening systems. We approach this task in the context of temporally-complex urban soundscapes, which require multiple labels to identify overlapping sound sources. Typically this work is crowdsourced, and previous studies have shown that workers can quickly label audio with binary annotation for single classes. However, this approach can be difficult to scale when multiple passes with different focus classes are required to annotate data with multiple labels. In citizen science, where tasks are often image-based, annotation efforts typically label multiple classes simultaneously in a single pass. This paper describes our data collection on the Zooniverse citizen science platform, comparing the efficiencies of different audio annotation strategies. We compared multiple-pass binary annotation, single-pass multi-label annotation, and a hybrid approach: hierarchical multi-pass multi-label annotation. We discuss our findings, which support using multi-label annotation, with reference to volunteer citizen scientists' motivations.},
booktitle = {Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems},
pages = {1–11},
numpages = {11},
keywords = {crowdsourcing, citizen science, audio annotation},
location = {Glasgow, Scotland Uk},
series = {CHI '19}
}

@inproceedings{10.1145/3491102.3501957,
author = {Tahaei, Mohammad and Vaniea, Kami},
title = {Recruiting Participants With Programming Skills: A Comparison of Four Crowdsourcing Platforms and a CS Student Mailing List},
year = {2022},
isbn = {9781450391573},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3491102.3501957},
doi = {10.1145/3491102.3501957},
abstract = {Reliably recruiting participants with programming skills is an ongoing challenge for empirical studies involving software development technologies, often leading to the use of crowdsourcing platforms and computer science (CS) students. In this work, we use five existing survey instruments to explore the programming skills, privacy and security attitudes, and secure development self-efficacy of participants from a CS student mailing list and four crowdsourcing platforms (Appen, Clickworker, MTurk, and Prolific). We recruited 613 participants who claimed to have programming skills and assessed recruitment channels regarding costs, quality, programming skills, as well as privacy and security attitudes. We find that 27\% of crowdsourcing participants, 40\% of crowdsourcing participants who self-report to be developers, and 89\% of CS students answered all programming skill questions correctly. CS students were the most cost-effective recruitment channel and rated themselves lower than crowdsourcing participants about secure development self-efficacy.},
booktitle = {Proceedings of the 2022 CHI Conference on Human Factors in Computing Systems},
articleno = {590},
numpages = {15},
keywords = {usable privacy and security, recruitment, programming, empirical software engineering, developers, datasets, crowdsourcing},
location = {New Orleans, LA, USA},
series = {CHI '22}
}

@inproceedings{10.1109/DySPAN.2018.8610482,
author = {Wang, Szu-Liang and Tsai, Tsung-Hung and Chung, Wei-Ho},
title = {The Novel Crowdsourcing Algorithm for Cooperative Spectrum Sensing},
year = {2018},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/DySPAN.2018.8610482},
doi = {10.1109/DySPAN.2018.8610482},
abstract = {Cooperative spectrum sensing with the crowdsourcing model has become an innovative and efficient technique for obtaining the spectrum state information (SSI) under the cognitive radio networks. Specifically, the secondary users sense the licensed spectrum then send the SSI back to the fusion center according to the crowdsourcing model. In practice, the users with malicious or erroneous behavior are always existing among those secondary users. Thus the accuracy of spectrum sensing would be influenced by those users. In this paper, a novel crowdsourcing algorithm for cooperative spectrum sensing is proposed to detect the malicious users and further improve the accuracy of spectrum sensing. The agreement ratio is introduced in the proposed scheme, which is an important information under the crowdsourcing framework. Simulation results show that the proposed algorithm possesses better performance than the existing approaches.},
booktitle = {2018 IEEE International Symposium on Dynamic Spectrum Access Networks (DySPAN)},
pages = {1–5},
numpages = {5},
location = {Seoul, Korea (South)}
}

@inproceedings{10.1145/3313831.3376320,
author = {Hettiachchi, Danula and Sarsenbayeva, Zhanna and Allison, Fraser and van Berkel, Niels and Dingler, Tilman and Marini, Gabriele and Kostakos, Vassilis and Goncalves, Jorge},
title = {"Hi! I am the Crowd Tasker" Crowdsourcing through Digital Voice Assistants},
year = {2020},
isbn = {9781450367080},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3313831.3376320},
doi = {10.1145/3313831.3376320},
abstract = {Inspired by the increasing prevalence of digital voice assistants, we demonstrate the feasibility of using voice interfaces to deploy and complete crowd tasks. We have developed Crowd Tasker, a novel system that delivers crowd tasks through a digital voice assistant. In a lab study, we validate our proof-of-concept and show that crowd task performance through a voice assistant is comparable to that of a web interface for voice-compatible and voice-based crowd tasks for native English speakers. We also report on a field study where participants used our system in their homes. We find that crowdsourcing through voice can provide greater flexibility to crowd workers by allowing them to work in brief sessions, enabling multi-tasking, and reducing the time and effort required to initiate tasks. We conclude by proposing a set of design guidelines for the creation of crowd tasks for voice and the development of future voice-based crowdsourcing systems.},
booktitle = {Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems},
pages = {1–14},
numpages = {14},
keywords = {crowdsourcing, digital voice assistants, smart speakers, voice user interface},
location = {Honolulu, HI, USA},
series = {CHI '20}
}

@inproceedings{10.1145/3308560.3317087,
author = {Stone, Maria},
title = {What we talk about when we talk about crowdsourcing},
year = {2019},
isbn = {9781450366755},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3308560.3317087},
doi = {10.1145/3308560.3317087},
abstract = {To understand why and how subjectivity and disagreement in label collection matters or doesn't matter I examine the history of systems evaluations and measurement performed by humans and trace the roots of human computation/crowdsourcing and the context in which it arose. &nbsp;&nbsp;Before we can begin fruitful discussions about subjectivity and disagreement we need to ask ourselves what/who it is that human raters are supposed to represent. I offer multiple different perspectives and scenarios that showcase just how varied and ill-defined the role of a human rater can be. I will conclude with some practical recommendations with respect to the questions researchers and practitioners ought to ask themselves before employing human raters, and some challenges with both methodology of such data collection and subsequent analysis of such data.},
booktitle = {Companion Proceedings of The 2019 World Wide Web Conference},
pages = {1099},
numpages = {1},
location = {San Francisco, USA},
series = {WWW '19}
}

@inproceedings{10.1145/3308558.3313749,
author = {Duan, Xiaoni and Tajima, Keishi},
title = {Improving Multiclass Classification in Crowdsourcing by Using Hierarchical Schemes},
year = {2019},
isbn = {9781450366748},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3308558.3313749},
doi = {10.1145/3308558.3313749},
abstract = {In this paper, we propose a method of improving accuracy of multiclass classification tasks in crowdsourcing. In crowdsourcing, it is important to assign appropriate workers to appropriate tasks. In multiclass classification, different workers are good at different subcategories. In our method, we reorganize a given flat classification task into a hierarchical classification task consisting of several subtasks, and assign each worker to an appropriate subtask. In this approach, it is important to choose a good hierarchy. In our method, we first post a flat classification task with a part of data and collect statistics on each worker's ability. Based on the obtained statistics, we simulate all candidate hierarchical schemes, estimate their expected accuracy, choose the best scheme, and post it with the rest of data. In our method, it is also important to allocate workers to appropriate subtasks. We designed several greedy worker allocation algorithms. The results of our experiments show that our method improves the accuracy of multiclass classification tasks.},
booktitle = {The World Wide Web Conference},
pages = {2694–2700},
numpages = {7},
keywords = {worker assignment, task design, task assignment, annotation},
location = {San Francisco, CA, USA},
series = {WWW '19}
}

@inproceedings{10.1145/3055601.3055606,
author = {Roy, H. and Kase, S. E. and Bowman, E. K.},
title = {Crowdsourcing Social Media for Military Operations},
year = {2017},
isbn = {9781450349772},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3055601.3055606},
doi = {10.1145/3055601.3055606},
abstract = {In this paper, we consider the demographics associated with social media users as a basis for determining how to interact with a population group to inform military operations such as humanitarian aid and disaster relief (HADR). With social media use increasing across most societal groups, information can be shared in a more representative manner than a decade ago. Also, crowdsourcing activities can be more productive and useful as the percentage of citizens using this technology increases. We discuss a recent experiment using the Amazon Mechanical Turk platform to investigate social bias factors associated with information transmission. 759 participants shared their social media usage characteristics as a feature of that study, and we explore those data in this paper to consider social media uses for HADR scenarios. We provide demographic characteristics for ten major social media platforms and discuss how tailored crowdsourcing would benefit decision making in traumatic and confusing conditions.},
booktitle = {Proceedings of the 2nd International Workshop on Social Sensing},
pages = {23–27},
numpages = {5},
keywords = {sociocultural understanding, military operations, demographics, crowdsourcing, Social media usage},
location = {Pittsburgh, PA, USA},
series = {SocialSens'17}
}

@inproceedings{10.1145/3151759.3151767,
author = {Kumai, Katsumi and Zhang, Jianwei and Shiraishi, Yuhki and Wakatsuki, Daisuke and Kitagawa, Hiroyuki and Morishima, Atsuyuki},
title = {Group rotation management in real-time crowdsourcing},
year = {2017},
isbn = {9781450352994},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3151759.3151767},
doi = {10.1145/3151759.3151767},
abstract = {A common workflow to perform a continuous human task stream is to divide workers into groups, have one group perform the newly-arrived task, and rotate the groups. Usually, more than one worker belongs to each group for improving the quality of task results. We call this type of workflow the group rotation. This paper addresses the problem of how to manage Group Rotation Type Crowdsourcing, the group rotation in a crowdsourcing setting. In the group-rotation type crowdsourcing, we must change the group structure dynamically because workers come in and leave frequently. However, changing the group structure will give workers psychological stress, such as surprise, confusion or irritation. This paper explores a design space for group restructuring algorithms in the group rotation type crowdsourcing and compares implemented strategies in terms of the evaluation results on psychological stress with real-world crowd workers.},
booktitle = {Proceedings of the 19th International Conference on Information Integration and Web-Based Applications \&amp; Services},
pages = {23–31},
numpages = {9},
keywords = {real-time, group management, crowdsourcing},
location = {Salzburg, Austria},
series = {iiWAS '17}
}

@inproceedings{10.1007/978-3-319-97310-4_26,
author = {Zhang, Hao and Jiang, Liangxiao and Xu, Wenqiang},
title = {Differential Evolution-Based Weighted Majority Voting for Crowdsourcing},
year = {2018},
isbn = {978-3-319-97309-8},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-319-97310-4_26},
doi = {10.1007/978-3-319-97310-4_26},
abstract = {With the rapid development of crowdsourcing learning, inferring (integrating) truth labels from multiple noisy label sets, it is also called label integration, has been a hot research topic. And many methods have been proposed for label integration. However, due to the variable uncertainty of crowdsourced labelers, inferring truth labels from multiple noisy label sets still faces great challenges. In this paper we transform the label integration problem into an optimization problem, and exploit a differential evolution-based weighted majority voting method, simply DEWMV, for label integration. DEWMV searches and weights the voting quality of each label through the designed differential evolution (DE) algorithm. In DEWMV, we define three fitness functions, including the uncertainty of the integration label, the uncertainty of the class member probability and the hybrid uncertainty, to search the optimal voting quality for each label. By theoretically analyzing their effectiveness, we choose the hybrid uncertainty as the final fitness function for DEWMV. The experimental results on 14 real-world datasets show that DEWMV is superior to standard majority voting (MV) and all the other state-of-the-art label integration methods used to compare.},
booktitle = {PRICAI 2018: Trends in Artificial Intelligence: 15th Pacific Rim International Conference on Artificial Intelligence, Nanjing, China, August 28–31, 2018, Proceedings, Part II},
pages = {228–236},
numpages = {9},
keywords = {Crowdsourcing, Multiple noisy labels, Label integration, Label quality, Differential evolution},
location = {Nanjing, China}
}

@inproceedings{10.1145/3387940.3392243,
author = {Zanatta, Alexandre and Machado, Leticia and Steinmacher, Igor and Prikladnicki, Rafael and de Souza, Cleidson R. B.},
title = {Strategies for Crowdworkers to Overcome Barriers in Competition-based Software Crowdsourcing Development},
year = {2020},
isbn = {9781450379632},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3387940.3392243},
doi = {10.1145/3387940.3392243},
abstract = {Crowdsourcing in software development uses a large pool of developers on-demand to outsource parts or the entire software project to a crowd. To succeed, this requires a continuous influx of developers, or simply crowdworkers. However, crowdworkers face many barriers when attempting to participate in software crowdsourcing. Often, these barriers lead to a low number and poor quality of submitted solutions. In our previous work, we identified several barriers faced by crowdworkers including finding a task according to his/her abilities, setting up the environment to perform the task, and managing one's personal time. We also proposed six strategies to overcome or minimize these barriers. In this paper, these six strategies are evaluated questioning Software Crowdsourcing (SW CS) experts. The results show that software crowdsourcing needs to: (i) provide a system that helps matching tasks requirements and crowdworker's profile; (ii) adopt containers or virtual machines to help crowdworkers set up their environment to perform the task, (iii) plan and control crowdworkers' personal time, and (iv) adopt communication channels to allow crowdworkers to clarify questions about the requirements and, as a consequence, finish the tasks.},
booktitle = {Proceedings of the IEEE/ACM 42nd International Conference on Software Engineering Workshops},
pages = {125–128},
numpages = {4},
keywords = {strategies, barriers, Software crowdsourcing},
location = {Seoul, Republic of Korea},
series = {ICSEW'20}
}

@inproceedings{10.1007/978-3-030-38991-8_20,
author = {Yu, Dunhui and Zhang, Xiaoxiao and Zhang, Xingsheng and Zhang, Lingli},
title = {Multitask Assignment Algorithm Based on Decision Tree in Spatial Crowdsourcing Environment},
year = {2019},
isbn = {978-3-030-38990-1},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-38991-8_20},
doi = {10.1007/978-3-030-38991-8_20},
abstract = {To improve the resource utilization rate of a platform and increase worker profit, addressing the problem of a limited suitable range in a single-task assignment in a spatial crowdsourcing environment, this paper provides a single-worker multitask assignment strategy. A candidate worker-selection algorithm based on location entropy minimum priority is proposed. Candidate tasks are selected by calculating their location entropy within a selected area. A candidate worker is obtained based on the Manhattan distance between the candidate task and the worker, completing the single-task assignment to the single worker. Then a multitask assignment algorithm based on a decision tree is designed, which builds a multitask screening decision tree and calculates the candidate tasks’ time difference, travel cost ratio, coincidence rate of route, and income growth rate of workers. We filter out the most appropriate task and assign it to a worker to complete the multitasking assignment. Experimental results show that the proposed algorithm can effectively reduce the average travel cost, reduce the idle rate of workers, and improve their income, which has better effectiveness and feasibility.},
booktitle = {Algorithms and Architectures for Parallel Processing: 19th International Conference, ICA3PP 2019, Melbourne, VIC, Australia, December 9–11, 2019, Proceedings, Part I},
pages = {300–314},
numpages = {15},
keywords = {Decision tree, Location entropy, Multitask assignment, Spatial crowdsourcing},
location = {Melbourne, VIC, Australia}
}

@inproceedings{10.1145/3068839.3068842,
author = {Cohen, Sara and Yashinski, Moran},
title = {Crowdsourcing with Diverse Groups of Users},
year = {2017},
isbn = {9781450349833},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3068839.3068842},
doi = {10.1145/3068839.3068842},
abstract = {When crowdsourcing to achieve some goal, or to gather information, there is a distinct advantage to choosing a diverse team of users. Past research has shown the advantages of diversity in the workplace, as team members bring different perspectives and points of view. Similarly, when choosing users from a crowd, user diversity must be taken into consideration. This paper studies the diverse team formation problem. More precisely, we are given a set of required skills, as wells as a large set of people, each of who has some subset of the skills. The goal is to form a team satisfying the skills, that is also diverse, as is reflected by differences in the characteristics of team members (e.g., gender, race, country of residence, economic bracket).We show that finding an optimal (diverse) team of people is an NP-complete problem. In practice, the number of candidates is likely to strongly dominate the number of skills and characteristics. Hence, we provide an algorithm that returns an optimal solution, while running in time that is indifferent to the number of candidates (but is exponential in the number of skills and characteristics). We also provide a polynomial method for approximating optimal team formation by a reduction to the problem of submodular function maximization with a matroid constraint. Extensive experimentation shows both scalability of our methods, and the quality of the solutions returned.},
booktitle = {Proceedings of the 20th International Workshop on the Web and Databases},
pages = {7–12},
numpages = {6},
location = {Chicago, IL, USA},
series = {WebDB'17}
}

@inproceedings{10.1145/3256041,
author = {Huang, Yun},
title = {Session details: Paper Session: Crowdsourcing and Funding},
year = {2018},
isbn = {9781450355629},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3256041},
doi = {10.1145/3256041},
booktitle = {Proceedings of the 2018 ACM International Conference on Supporting Group Work},
location = {Sanibel Island, Florida, USA},
series = {GROUP '18}
}

@inproceedings{10.1145/3271972.3271998,
author = {Barashev, Andrey and Li, Guoxin},
title = {Solvers' Motivation in Crowdsourcing Platform: Examining the Impacts of Reward Sensitivity, and Achievement Goals Factors},
year = {2018},
isbn = {9781450365147},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3271972.3271998},
doi = {10.1145/3271972.3271998},
abstract = {The purpose of this study is investigation of direct influence of reward sensitivity (as represented by the theory - Behavioral Approach System) on motivational outcomes (measured through effort and intrinsic motivation) of crowdsourcing platform workers. Furthermore, this paper analyses the mediating role of Approach Achievement Goals in channeling 'initial energization' to motivational outcomes. A random sample of 320 taskcn.com workers was used in this study. The findings of this study showed that reward sensitivity was effective directly and indirectly, in positively predicting all motivational outcomes. Reward sensitivity positively predicted intrinsic motivation directly and indirectly through Other-approach goals. However, other-approach goals did not channel 'initial energization' from BAS to Effort. These results are important for crowdsourcing platforms, in order to better understanding the participants that undertake tasks and find efficient solutions for motivating workers. Targeting workers with higher reward sensitivity might be beneficial for platform owners, providing the way to increase workers motivational outcomes at no additional cost, thus facilitating better results.},
booktitle = {Proceedings of the 2018 9th International Conference on E-Business, Management and Economics},
pages = {80–85},
numpages = {6},
keywords = {Reward sensitivity, Reinforcement Sensitivity theory, Effort, Crowdsourcing, Achievement goals},
location = {Waterloo, ON, Canada},
series = {ICEME '18}
}

@inproceedings{10.1145/2971648.2971741,
author = {Boutsis, Ioannis and Kalogeraki, Vana},
title = {Location privacy for crowdsourcing applications},
year = {2016},
isbn = {9781450344616},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2971648.2971741},
doi = {10.1145/2971648.2971741},
abstract = {This paper contributes to mobile crowdsourcing applications by developing a privacy preserving framework that enables users to contribute content to the community while controlling their privacy exposure. One fundamental challenge in such applications is how to preserve user privacy, as participants may end up revealing a great deal of user-identified, geo-located data, which can easily unfold user trajectories or sensitive locations (e.g., user's home or work location). In this paper we develop PROMPT, a highly efficient privacy preserving framework that runs locally on mobile devices. PROMPT relies on a novel geometric approximation approach to preserve user privacy, by evaluating the privacy exposure of users before sharing their geo-located data. Our detailed experimental evaluation using real-world datasets illustrates that our approach is effective, practical and has low overhead on smartphones.},
booktitle = {Proceedings of the 2016 ACM International Joint Conference on Pervasive and Ubiquitous Computing},
pages = {694–705},
numpages = {12},
keywords = {privacy preservation, location sharing, coresets},
location = {Heidelberg, Germany},
series = {UbiComp '16}
}

@inproceedings{10.1145/3441000.3441062,
author = {Miyata, Akihiro and Okugawa, Kazuki and Yamato, Yuki and Aibara, Megumi and Furuichi, Masakazu and Murayama, Yuko},
title = {BScanner: A Crowdsourcing Platform for Constructing Accessibility Maps to Support Multiple Participation Types},
year = {2021},
isbn = {9781450389754},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3441000.3441062},
doi = {10.1145/3441000.3441062},
abstract = {Accessibility maps enable impaired/elderly people to move more smoothly. However, very few cases satisfy both the accuracy and coverage required, due to the high cost of physically auditing roads. Though crowdsourcing approaches could solve this problem, existing studies rely on volunteers who are highly motivated and have enough free time. In this paper, we propose a crowdsourcing platform for constructing accessibility maps that supports multiple participation types: manual reporting for people who are highly motivated and have free time, walking for people who are highly motivated but do not have free time, and game playing for people who are less motivated but have free time. This design allows people to select a suitable way to participate, depending on their motivation and time. We have developed a prototype system by integrating deep learning techniques, a game design theory, and heatmap visualization.},
booktitle = {Proceedings of the 32nd Australian Conference on Human-Computer Interaction},
pages = {666–670},
numpages = {5},
keywords = {gamification, deep learning, crowdsourcing, accessibility map},
location = {Sydney, NSW, Australia},
series = {OzCHI '20}
}

@inproceedings{10.1109/SMC.2017.8122583,
author = {Li, Mengxue and Geng, Shiqiang and Gao, Yang and Peng, Shuhua and Liu, Haijing and Wang, Hao},
title = {Crowdsourcing argumentation structures in Chinese hotel reviews},
year = {2017},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/SMC.2017.8122583},
doi = {10.1109/SMC.2017.8122583},
abstract = {Argumentation mining aims at automatically extracting the premises-claim discourse structures in natural language texts. There is a great demand for argumentation corpora for customer reviews. However, due to the controversial nature of the argumentation annotation task, there exist very few large-scale argumentation corpora for customer reviews. In this work, we novelly use the crowdsourcing technique to collect argumentation annotations in Chinese hotel reviews. As the first Chinese argumentation dataset, our corpus includes 4814 argument component annotations and 411 argument relation annotations, and its annotations qualities are comparable to some widely used argumentation corpora in other languages.},
booktitle = {2017 IEEE International Conference on Systems, Man, and Cybernetics (SMC)},
pages = {87–92},
numpages = {6},
location = {Banff, AB, Canada}
}

@inproceedings{10.1145/2896338.2897727,
author = {Weber, Ingmar and Mejova, Yelena},
title = {Crowdsourcing Health Labels: Inferring Body Weight from Profile Pictures},
year = {2016},
isbn = {9781450342247},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2896338.2897727},
doi = {10.1145/2896338.2897727},
abstract = {To use social media for health-related analysis, one key step is the detection of health-related labels for users. But unlike transient conditions like flu, social media users are less vocal about chronic conditions such as obesity, as users might not tweet ``I'm still overweight''. As, however, obesity-related conditions such as diabetes, heart disease, osteoarthritis, and even cancer are on the rise, this obese-or-not label could be one of the most useful for studies in public health.In this paper we investigate the feasibility of using profile pictures to infer if a user is overweight or not. We show that this is indeed possible and further show that the fraction of labeled-as-overweight users is higher in U.S. counties with higher obesity rates. Going from public to individual health analysis, we then find differences both in behavior and social networks, for example finding users labeled as overweight to have fewer followers.},
booktitle = {Proceedings of the 6th International Conference on Digital Health Conference},
pages = {105–109},
numpages = {5},
keywords = {twitter, social network analysis, profile images, obesity, bmi},
location = {Montr\'{e}al, Qu\'{e}bec, Canada},
series = {DH '16}
}

@inproceedings{10.1145/2818048.2819957,
author = {Zhao, Qian and Huang, Zihong and Harper, F. Maxwell and Terveen, Loren and Konstan, Joseph A.},
title = {Precision CrowdSourcing: Closing the Loop to Turn Information Consumers into Information Contributors},
year = {2016},
isbn = {9781450335928},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2818048.2819957},
doi = {10.1145/2818048.2819957},
abstract = {We introduce a theoretical framework called precision crowdsourcing whose goal is to help turn online information consumers into information contributors. The framework looks at the timing and nature of the requests made of users and the feedback provided to users with the goal of increasing long-term contribution and engagement in the site or system. We present the results of a field experiment in which almost 3000 users were asked to tag movies (plus a null control group) as we varied the selection of task (popular/obscure), timing of requests (immediate or varying delays), and relational rhetoric (neutral, system reciprocal, other users reciprocal) of the requests. We found that asking increases tags provided overall, though asking generally decreases the provision of unprompted tags. Users were more likely to comply with our request when we asked them to tag obscure movies and when we used reciprocal request rhetoric.},
booktitle = {Proceedings of the 19th ACM Conference on Computer-Supported Cooperative Work \&amp; Social Computing},
pages = {1615–1625},
numpages = {11},
keywords = {Reciprocity, Online Communities, Crowdsourcing},
location = {San Francisco, California, USA},
series = {CSCW '16}
}

@inproceedings{10.1007/978-3-030-59416-9_45,
author = {Liu, Qiyu and Zheng, Libin and Shen, Yanyan and Chen, Lei},
title = {Finish Them on the Fly: An Incentive Mechanism for Real-Time Spatial Crowdsourcing},
year = {2020},
isbn = {978-3-030-59415-2},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-59416-9_45},
doi = {10.1007/978-3-030-59416-9_45},
abstract = {Proper incentive mechanism design for stimulating workers is a fundamental challenge in nowadays spatial crowdsourcing (SC) powered applications like Didi and Uber. Usually, extra monetary rewards are paid to workers as incentive to enhance their participation in the SC platform. However, deciding incentives in real-time is non-trivial as the spatial crowdsourcing market changes fast over time. Existing studies mostly assume an offline scenario where the incentives are computed considering a static market condition with the global knowledge of tasks and workers. Unfortunately, this setting does not fit the reality where the market itself would evolve gradually. In this paper, to enable online incentive determination, we formulate the problem of Real-timeMonetaryIncentive forTasks in Spatial Crowdsourcing (MIT), which computes proper reward for each task to maximize the task completion rate at real time. We propose a unified and efficient approach to the MIT problem with a theoretical effectiveness guarantee. The experimental results on real ride-sharing data show that, compared with the state-of-the-art offline algorithms, our approach decreases the total worker response time by two orders of magnitude with insignificant utility loss.},
booktitle = {Database Systems for Advanced Applications: 25th International Conference, DASFAA 2020, Jeju, South Korea, September 24–27, 2020, Proceedings, Part II},
pages = {694–710},
numpages = {17},
keywords = {Real-time spatial crowdsourcing, Competitive analysis, Incentive mechanism design},
location = {Jeju, Korea (Republic of)}
}

@inproceedings{10.1145/3366030.3366031,
author = {Kimura, Risa and Nakajima, Tatsuo},
title = {Gamifying Human Behavior in Urban Crowdsourcing for a Sustainable Smart City},
year = {2020},
isbn = {9781450371797},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366030.3366031},
doi = {10.1145/3366030.3366031},
abstract = {One of the most important topics in future crowdsourcing is how to influence and coordinate collective people to perform micro-tasks towards their common goal achieved through crowdsourcing activities. For achieving the goal, crowdsourcing needs to take into account coordinating collective people not only encouraging individual people. This paper presents the Game as Rhetoric model that is a framework for designing digital rhetoric, which is embedded in the real world and influences participants' behavior. We then present Collective Action Crowdsourcing and its IoT-based prototype system as an example using the Game as Rhetoric model.},
booktitle = {Proceedings of the 21st International Conference on Information Integration and Web-Based Applications \&amp; Services},
pages = {546–555},
numpages = {10},
keywords = {Sustainability, Social Influence, Smart City, Mobile Crowdsourcing, Game as Rhetoric},
location = {Munich, Germany},
series = {iiWAS2019}
}

@inproceedings{10.1145/3377929.3389936,
author = {Shi, Jian and Chen, Wei-Neng},
title = {Multi-objective ant colony optimization for task allocation in vehicle-based crowdsourcing},
year = {2020},
isbn = {9781450371278},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3377929.3389936},
doi = {10.1145/3377929.3389936},
abstract = {With the continuous popularization of various mobile devices, spatial crowdsourcing tasks have also developed rapidly. Recently, vehicles are being used to complete spatial crowdsourcing tasks. Drivers and passengers can provide the ability to actively complete query tasks, while sensors on the vehicle can passively complete sensing tasks. In this paper, we consider a spatial crowdsourcing scenario where a worker can complete a location-based query task and also a sensing task at the same time. The formal description of the problem is given, and then a multiobjective ant colony algorithm for vehicle-based crowdsourcing is proposed to optimize both task query reliability and sensing coverage. The performance of the algorithm is evaluated on a real-world traffic trace dataset.},
booktitle = {Proceedings of the 2020 Genetic and Evolutionary Computation Conference Companion},
pages = {67–68},
numpages = {2},
keywords = {task reliability, sensing coverage, ant colony optimization, Vehicle-based crowdsourcing},
location = {Canc\'{u}n, Mexico},
series = {GECCO '20}
}

@inproceedings{10.1109/GLOBECOM38437.2019.9013829,
author = {Chen, Xianhao and Zhang, Lan and Lin, Bin and Fang, Yuguang},
title = {Delay-Aware Incentive Mechanism for Crowdsourcing with Vehicles in Smart Cities},
year = {2019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/GLOBECOM38437.2019.9013829},
doi = {10.1109/GLOBECOM38437.2019.9013829},
abstract = {Vehicle-based crowdsourcing is becoming a powerful paradigm that can outsource intensive tasks to vehicles by exploiting their on-board resources. In this paper, we focus on the problem of motivating vehicles to join the crowdsourcing system. Considering the various delay demands of tasks in smart cities, we design a delay-aware incentive mechanism to employ vehicles based on reverse auction. Specifically, by taking task delay into consideration, we model the utility of service requester as a function closely related to when its released tasks would be completed. In our mechanism, the participating vehicles bid for their preferred tasks by submitting not only the bidding prices, but also the estimated time of completion (ETC). To maximize the utility of the service requester under a budget constraint, the proposed delay-aware mechanism is cast as a nonmonotone submodular maximization problem with a knapsack constraint. Due to the NP-hardness of the formulated problem, we develop an approximate algorithm for bid selection and payment determination, which guarantees truthfulness, budget feasibility, individual rationality, profitability, and computational efficiency. Simulation results demonstrate the effectiveness of our proposed incentive mechanism.},
booktitle = {2019 IEEE Global Communications Conference (GLOBECOM)},
pages = {1–6},
numpages = {6},
location = {Waikoloa, HI, USA}
}

@inproceedings{10.1007/978-3-030-55789-8_30,
author = {Zhao, Yunpeng and Prosperi, Mattia and Lyu, Tianchen and Guo, Yi and Zhou, Le and Bian, Jiang},
title = {Integrating Crowdsourcing and Active Learning for Classification of Work-Life Events from Tweets},
year = {2020},
isbn = {978-3-030-55788-1},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-55789-8_30},
doi = {10.1007/978-3-030-55789-8_30},
abstract = {Social media, especially Twitter, is being increasingly used for research with predictive analytics. In social media studies, natural language processing (NLP) techniques are used in conjunction with expert-based, manual and qualitative analyses. However, social media data are unstructured and must undergo complex manipulation for research use. The manual annotation is the most resource and time-consuming process that multiple expert raters have to reach consensus on every item, but is essential to create gold-standard datasets for training NLP-based machine learning classifiers. To reduce the burden of the manual annotation, yet maintaining its reliability, we devised a crowdsourcing pipeline combined with active learning strategies. We demonstrated its effectiveness through a case study that identifies job loss events from individual tweets. We used Amazon Mechanical Turk platform to recruit annotators from the Internet and designed a number of quality control measures to assure annotation accuracy. We evaluated 4 different active learning strategies (i.e., least confident, entropy, vote entropy, and Kullback-Leibler divergence). The active learning strategies aim at reducing the number of tweets needed to reach a desired performance of automated classification. Results show that crowdsourcing is useful to create high-quality annotations and active learning helps in reducing the number of required tweets, although there was no substantial difference among the strategies tested.},
booktitle = {Trends in Artificial Intelligence Theory and Applications. Artificial Intelligence Practices: 33rd International Conference on Industrial, Engineering and Other Applications of Applied Intelligent Systems, IEA/AIE 2020, Kitakyushu, Japan, September 22-25, 2020, Proceedings},
pages = {333–344},
numpages = {12},
keywords = {Active learning, Crowdsourcing, Social media},
location = {Kitakyushu, Japan}
}

@inproceedings{10.1145/2964284.2976762,
author = {Joly, Alexis and Go\"{e}au, Herv\'{e} and Champ, Julien and Dufour-Kowalski, Samuel and M\"{u}ller, Henning and Bonnet, Pierre},
title = {Crowdsourcing Biodiversity Monitoring: How Sharing your Photo Stream can Sustain our Planet},
year = {2016},
isbn = {9781450336031},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2964284.2976762},
doi = {10.1145/2964284.2976762},
abstract = {Large scale biodiversity monitoring is essential for sustainable development (earth stewardship). With the recent advances in computer vision, we see the emergence of more and more effective identification tools allowing to set-up large-scale data collection platforms such as the popular Pl@ntNet initiative that allow to reuse interaction data. Although it covers only a fraction of the world flora, this platform is already being used by more than 300K people who produce tens of thousands of validated plant observations each year. This explicitly shared and validated data is only the tip of the iceberg. The real potential relies on the millions of raw image queries submitted by the users of the mobile application for which there is no human validation. People make such requests to get information on a plant along a hike or something they find in their garden but not know anything about. Allowing the exploitation of such contents in a fully automatic way could scale up the world-wide collection of implicit plant observations by several orders of magnitude, which can complement the explicit monitoring efforts. In this paper, we first survey existing automated plant identification systems through a five-year synthesis of the PlantCLEF benchmark and an impact study of the Pl@ntNet platform. We then focus on the implicit monitoring scenario and discuss related research challenges at the frontier of computer science and biodiversity studies. Finally, we discuss the results of a preliminary study focused on implicit monitoring of invasive species in mobile search logs. We show that the results are promising but that there is room for improvement before being able to automatically share implicit observations within international platforms.},
booktitle = {Proceedings of the 24th ACM International Conference on Multimedia},
pages = {958–967},
numpages = {10},
keywords = {crowdsourcing, biodiversity monitoring},
location = {Amsterdam, The Netherlands},
series = {MM '16}
}

@inproceedings{10.1007/978-3-030-40907-4_7,
author = {Nobel, Thomas and Hoppenbrouwers, Stijn and Pleijsant, Jan Mark and Ouborg, Mats},
title = {Word Meaning, Data Semantics, Crowdsourcing, and the BKM/A-Lex Approach},
year = {2019},
isbn = {978-3-030-40906-7},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-40907-4_7},
doi = {10.1007/978-3-030-40907-4_7},
abstract = {The lexical definition of concepts is an integral part of Fact Based Modelling. More in general, structured description of term meaning, in many forms and guises, has since the early days played a role in information systems (data dictionaries, data modelling), data management (business glossaries for data governance), knowledge engineering (applied logic, rule definition and management), and the Semantic Web (RDF). We observe that at the core of many different approaches to lexical meaning lies the combination of semantic networks and textual definitions, and propose to re-appreciate these relatively simple basics as the theoretical but also, and perhaps more so, the practical core of dealing with Data Semantics. We also explore some fundamental concepts from cybernetics, providing some theoretical basis for advocating crowdsourcing as a way of taking up a continuous lexical definition in and across domain communities. We discuss and compare various combined aspects in lexical definition approaches from various relevant fields in view of the A-Lex tool, which supports a crowdsourcing approach to the lexical definition in a data management context: Business Knowledge Mapping. We explain why this approach indeed applies most of the core concepts of “word meaning as a vehicle for dealing with data semantics in and across communities”.},
booktitle = {On the Move to Meaningful Internet Systems: OTM 2019 Workshops: Confederated International Workshops: EI2N, FBM, ICSP, Meta4eS and SIAnA 2019, Rhodes, Greece, October 21–25, 2019, Revised Selected Papers},
pages = {67–78},
numpages = {12},
keywords = {Collaborative modelling, Fact Based Modelling, crowdsourcing, Lexical definition, Semantic networks, Data semantics},
location = {Rhodes, Greece}
}

@inproceedings{10.1145/3197026.3197046,
author = {Traub, Myriam C. and Samar, Thaer and van Ossenbruggen, Jacco and Hardman, Lynda},
title = {Impact of Crowdsourcing OCR Improvements on Retrievability Bias},
year = {2018},
isbn = {9781450351782},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3197026.3197046},
doi = {10.1145/3197026.3197046},
abstract = {Digitized document collections often suffer from OCR errors that may impact a document's readability and retrievability. We studied the effects of correcting OCR errors on the retrievability of documents in a historic newspaper corpus of a digital library. We computed retrievability scores for the uncorrected documents using queries from the library's search log, and found that the document OCR character error rate and retrievability score are strongly correlated. We computed retrievability scores for manually corrected versions of the same documents, and report on differences in their total sum, the overall retrievability bias, and the distribution of these changes over the documents, queries and query terms. For large collections, often only a fraction of the corpus is manually corrected. Using a mixed corpus, we assess how this mix affects the retrievability of the corrected and uncorrected documents. The correction of OCR errors increased the number of documents retrieved in all conditions. The increase contributed to a less biased retrieval, even when taking the potential lower ranking of uncorrected documents into account.},
booktitle = {Proceedings of the 18th ACM/IEEE on Joint Conference on Digital Libraries},
pages = {29–36},
numpages = {8},
keywords = {retrievability bias, ocr, digital library, data quality},
location = {Fort Worth, Texas, USA},
series = {JCDL '18}
}

@inproceedings{10.1145/3313831.3376381,
author = {Nobre, Carolina and Wootton, Dylan and Harrison, Lane and Lex, Alexander},
title = {Evaluating Multivariate Network Visualization Techniques Using a Validated Design and Crowdsourcing Approach},
year = {2020},
isbn = {9781450367080},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3313831.3376381},
doi = {10.1145/3313831.3376381},
abstract = {Visualizing multivariate networks is challenging because of the trade-offs necessary for effectively encoding network topology and encoding the attributes associated with nodes and edges. A large number of multivariate network visualization techniques exist, yet there is little empirical guidance on their respective strengths and weaknesses. In this paper, we describe a crowdsourced experiment, comparing node-link diagrams with on-node encoding and adjacency matrices with juxtaposed tables. We find that node-link diagrams are best suited for tasks that require close integration between the network topology and a few attributes. Adjacency matrices perform well for tasks related to clusters and when many attributes need to be considered. We also reflect on our method of using validated designs for empirically evaluating complex, interactive visualizations in a crowdsourced setting. We highlight the importance of training, compensation, and provenance tracking.},
booktitle = {Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems},
pages = {1–12},
numpages = {12},
keywords = {crowdsourced evaluation, multivariate networks visualization},
location = {Honolulu, HI, USA},
series = {CHI '20}
}

@inproceedings{10.1145/3132847.3132894,
author = {Zhao, Yan and Li, Yang and Wang, Yu and Su, Han and Zheng, Kai},
title = {Destination-aware Task Assignment in Spatial Crowdsourcing},
year = {2017},
isbn = {9781450349185},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3132847.3132894},
doi = {10.1145/3132847.3132894},
abstract = {With the proliferation of GPS-enabled smart devices and increased availability of wireless network, spatial crowdsourcing (SC) has been recently proposed as a framework to automatically request workers (i.e., smart device carriers) to perform location-sensitive tasks (e.g., taking scenic photos, reporting events). In this paper we study a destination-aware task assignment problem that concerns the optimal strategy of assigning each task to proper worker such that the total number of completed tasks can be maximized whilst all workers can reach their destinations before deadlines after performing assigned tasks. Finding the global optimal assignment turns out to be an intractable problem since it does not imply optimal assignment for individual worker. Observing that the task assignment dependency only exists amongst subsets of workers, we utilize tree-decomposition technique to separate workers into independent clusters and develop an efficient depth-first search algorithm with progressive bounds to prune non-promising assignments. Our empirical studies demonstrate that our proposed technique is quite effective and settle the problem nicely.},
booktitle = {Proceedings of the 2017 ACM on Conference on Information and Knowledge Management},
pages = {297–306},
numpages = {10},
keywords = {user mobility, spatial task assignment, spatial crowdsourcing},
location = {Singapore, Singapore},
series = {CIKM '17}
}

@inproceedings{10.5555/3192424.3192493,
author = {Chang, Chun-Hao and Saravia, Elvis and Chen, Yi-Shin},
title = {Subconscious crowdsourcing: a feasible data collection mechanism for mental disorder detection on social media},
year = {2016},
isbn = {9781509028467},
publisher = {IEEE Press},
abstract = {Mental disorders are currently affecting millions of people from different cultures, age groups and geographic regions. The challenge of mental disorders is that they are difficult to detect on suffering patients, thus presenting an alarming number of undetected cases and misdiagnosis. In this paper, we aim at building predictive models that leverage language and behavioral patterns, used particularly in social media, to determine whether a user is suffering from two cases of mental disorder. These predictive models are made possible by employing a novel data collection process, coined as Subconscious Crowdsourcing, which helps to collect a faster and more reliable dataset of patients. Our experiments suggest that extracting specific language patterns and social interaction features from reliable patient datasets can greatly contribute to further analysis and detection of mental disorders.},
booktitle = {Proceedings of the 2016 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining},
pages = {374–379},
numpages = {6},
keywords = {social media, sentiment analysis, mental disorder detection, data collection, crowdsourcing},
location = {Davis, California},
series = {ASONAM '16}
}

@inproceedings{10.5555/3304652.3304822,
author = {Baba, Yukino},
title = {Statistical quality control for human computation and crowdsourcing},
year = {2018},
isbn = {9780999241127},
publisher = {AAAI Press},
abstract = {Human computation is a method for solving difficult problems by combining humans and computers. Quality control is a critical issue in human computation because it relies on a large number of participants (i.e., crowds) and there is an uncertainty about their reliability. A solution for this issue is to leverage the power of the "wisdom of crowds"; for example, we can aggregate the outputs of multiple participants or ask a participant to check the output of another participant to improve its quality. In this paper, we review several statistical approaches for controlling the quality of outputs from crowds.},
booktitle = {Proceedings of the 27th International Joint Conference on Artificial Intelligence},
pages = {5667–5671},
numpages = {5},
location = {Stockholm, Sweden},
series = {IJCAI'18}
}

@inproceedings{10.1109/ICAIET.2014.30,
author = {Zakariah, Zazaleena and Janom, Norjansalika and Arshad, Noor Habibah and Salleh, Siti Salwa and Aris, Syaripah Ruzaini Syed},
title = {Crowdsourcing: The Trend of Prior Studies},
year = {2014},
isbn = {9781479979103},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/ICAIET.2014.30},
doi = {10.1109/ICAIET.2014.30},
abstract = {Crowdsourcing is view as major changes in current economy as it has revolutionary effects on organizations. Unlike previous outsourcing trends that practices closed inter-organizational relationships subject to the long-term legal contract, these new crowdsourcing practices take place in an open and virtual context, characterized with small and shortterm deals. Crowdsourcing has been recognized as new industry that able to uplift the economy and bring significant benefits to business and people in general. Although the phenomenon of crowdsourcing is fairly new and still in a formative stage, the indications and predictions are optimistic. Under the recent Malaysian government initiatives called Digital Malaysia, various crowdsourcing efforts and programs have been introduced. This paper reports on the review of prior studies pertaining crowdsourcing from a general view.},
booktitle = {Proceedings of the 2014 4th International Conference on Artificial Intelligence with Applications in Engineering and Technology},
pages = {129–133},
numpages = {5},
keywords = {Microsourcing, Crowdworkers, Crowdsourcing, Crowd, Business Model},
series = {ICAIET '14}
}

@inproceedings{10.1145/3423337.3429436,
author = {Michelin, Yves and Chadeyron, Julien},
title = {Crowdsourcing for georeferencing Napoleonic cadastre over a wide area: First methodological and practical lessons on the scale of the French Puy-de-D\^{o}me Department},
year = {2020},
isbn = {9781450381635},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3423337.3429436},
doi = {10.1145/3423337.3429436},
abstract = {The Napoleonic cadastre is a very rich collection of information concerning land use at the beginning of the 19th century, valid throughout France. Georeferencing this maps requires a specific method that we have formalized, and needs to mobilize an abundant and trained workforce if this task is to be carried out in a reasonable timescale on large areas like a wide department. A process of georeferencing of communal assembly plans involving researchers and students has been ongoing in the Puy-de-D\^{o}me department since 2017. Out of 464 communes, 287 have an accuracy of &lt; 15 m, 154 between 20 and 50 m and only about twenty remain to be completed. The first results permit to better understand how this cadastre has been made, and to characterize the landscape of this contrasted region (mountains, hills, plateaux, sedimentary basin) at this time. Even if the program is still ongoing, this experiment allows us to present some opening perspectives associating researchers and local associations in geo-history, agronomy, archaeology and landscape studies.},
booktitle = {Proceedings of the 4th ACM SIGSPATIAL Workshop on Geospatial Humanities},
pages = {10–18},
numpages = {9},
keywords = {participatory science, landscape history, land use, georeferencing, Napoleonic cadastre},
location = {Seattle, WA, USA},
series = {GeoHumanities '20}
}

@inproceedings{10.1145/3358695.3361104,
author = {Dimitriadis, Ilias and Psomiadis, Vasileios G. and Vakali, Athena},
title = {A crowdsourcing approach to advance collective awareness and social good practices},
year = {2019},
isbn = {9781450369886},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3358695.3361104},
doi = {10.1145/3358695.3361104},
abstract = {Contemporary societies are challenged by many social issues, lots of which relate with environmental threats. Plastic waste's impact on societal well being, economy, and environment has already triggered the need for its revaluation and its transformation into an asset which can inspire and mobilize innovative solutions. This work envisions trusted “wisdom of the crowds” analytics and a grassroots-driven approach to facilitate citizens and stakeholders participatory and engagement practices, under an inspiring crowdsourcing framework and model. A detailed evaluation of the filtering methodology is employed. This three-layer approach for the Data Filtering enables a Crowdsourcing Component implementation to be used as a plastic-topics insightful barometer, able to detect new trends, identify interesting content, spot influential users and generally raise awareness regarding the problem of plastic overuse.},
booktitle = {IEEE/WIC/ACM International Conference on Web Intelligence - Companion Volume},
pages = {200–207},
numpages = {8},
keywords = {social media innovation, social innovation, crowdsourcing tools and models, Web 2.0 analytics},
location = {Thessaloniki, Greece},
series = {WI '19 Companion}
}

@inproceedings{10.1109/INFOCOM.2018.8485902,
author = {Zhang, Lan and Li, Yannan and Xiao, Xiang and Li, Xiang-Yang and Wang, Junjun and Zhou, Anxin and Li, Qiang},
title = {CrowdBuy: Privacy-friendly Image Dataset Purchasing via Crowdsourcing},
year = {2018},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/INFOCOM.2018.8485902},
doi = {10.1109/INFOCOM.2018.8485902},
abstract = {In recent years, advanced machine learning techniques have demonstrated remarkable achievements in many areas. Despite the great success, one of the bottlenecks in applying machine learning techniques in real world applications lies in the lack of a large amount of high-quality training data from diverse domains. Meanwhile, massive personal data is being generated by mobile devices and is often underutilized. To bridge the gap, we propose a general dataset purchasing framework, named CrowdBuy and CrowdBuy++, based on crowdsourcing, with which a buyer can efficiently buy desired data from available mobile users with quality guarantee in a way respecting users' data ownership and privacy. We present a complete set of tools including privacy-preserving image dataset quality measurements and image selection mechanisms, which are budget feasible, truthful and highly efficient for mobile users. We conducted extensive evaluations of our framework on large-scale images and demonstrate that the system is capable of crowdsourcing high quality datasets while preserving image privacy with little computation and communication overhead.},
booktitle = {IEEE INFOCOM 2018 - IEEE Conference on Computer Communications},
pages = {2735–2743},
numpages = {9},
location = {Honolulu, HI, USA}
}

@inproceedings{10.1145/3022198.3022656,
author = {McInnis, Brian and Centivany, Alissa and Kim, Juho and Poblet, Marta and Levy, Karen and Leshed, Gilly},
title = {Crowdsourcing Law and Policy: A Design-Thinking Approach to Crowd-Civic Systems},
year = {2017},
isbn = {9781450346887},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3022198.3022656},
doi = {10.1145/3022198.3022656},
abstract = {Crowdsourcing technologies, strategies and methods offer new opportunities for bridging existing gaps among law, policymaking, and the lived experience of citizens. In recent years, a number of initiatives across the world have applied crowdsourcing to contexts including constitutional reform, drafting federal bills, and generating local policies. However, crowd-civic systems also come with challenges and risks such as socio-technical barriers, marginalization of specific groups, silencing of interests, etc. Using a design-thinking approach, this workshop will address both opportunities and challenges of crowd-civic systems to develop best practices for increasing public engagement with law and policy. The workshop organizers will suggest an initial framework explicitly intended to be criticized by participants and reconfigured through a series of iterative cooperative small-group activities focusing on ``diagnosing'' the failures of past crowd-civic system efforts and the successes of online action around social issues. While the ultimate objective of the workshop is to develop a best practices guide, we see iterations on the guide as a mechanism for fostering community and collaboration among policymakers, technologists, and researchers around crowd-civic systems for law and policy.},
booktitle = {Companion of the 2017 ACM Conference on Computer Supported Cooperative Work and Social Computing},
pages = {355–361},
numpages = {7},
keywords = {online action, law and policy, crowdsourcing, civic engagement, best practices},
location = {Portland, Oregon, USA},
series = {CSCW '17 Companion}
}

@inproceedings{10.1007/978-3-319-93037-4_11,
author = {Jin, Yuan and Du, Lan and Zhu, Ye and Carman, Mark},
title = {Leveraging Label Category Relationships in Multi-class Crowdsourcing},
year = {2018},
isbn = {978-3-319-93036-7},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-319-93037-4_11},
doi = {10.1007/978-3-319-93037-4_11},
abstract = {Current quality control methods for crowdsourcing largely account for variations in worker responses to items by interactions between item difficulty and worker expertise. Few have taken into account the semantic relationships that can exist between the response label categories. When the number of the label categories is large, these relationships are naturally indicative of how crowd-workers respond to items, with expert workers tending to respond with more semantically related categories to the categories of true labels, and with difficult items tending to see greater spread in the responded labels. Based on these observations, we propose a new statistical model which contains a latent real-valued matrix for capturing the relatedness of response categories alongside variables for worker expertise, item difficulty and item true labels. The model can be easily extended to incorporate prior knowledge about the semantic relationships between response labels in the form of a hierarchy over them. Experiments show that compared with numerous state-of-the-art baselines, our model (both with and without the prior knowledge) yields superior true label prediction performance on four new crowdsourcing datasets featuring large sets of label categories.},
booktitle = {Advances in Knowledge Discovery and Data Mining: 22nd Pacific-Asia Conference, PAKDD 2018, Melbourne, VIC, Australia, June 3-6, 2018, Proceedings, Part II},
pages = {128–140},
numpages = {13},
location = {Melbourne, VIC, Australia}
}

@inproceedings{10.1145/3340531.3411913,
author = {Shen, Wei and He, Xiaonan and Zhang, Chuheng and Ni, Qiang and Dou, Wanchun and Wang, Yan},
title = {Auxiliary-task Based Deep Reinforcement Learning for Participant Selection Problem in Mobile Crowdsourcing},
year = {2020},
isbn = {9781450368599},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3340531.3411913},
doi = {10.1145/3340531.3411913},
abstract = {In mobile crowdsourcing (MCS), the platform selects participants to complete location-aware tasks from the recruiters aiming to achieve multiple goals (e.g., profit maximization, energy efficiency, and fairness). However, different MCS systems have different goals and there are possibly conflicting goals even in one MCS system. Therefore, it is crucial to design a participant selection algorithm that applies to different MCS systems to achieve multiple goals. To deal with this issue, we formulate the participant selection problem as a reinforcement learning problem and propose to solve it with a novel method, which we call auxiliary-task based deep reinforcement learning (ADRL). We use transformers to extract representations from the context of the MCS system and a pointer network to deal with the combinatorial optimization problem. To improve the sample efficiency, we adopt an auxiliary-task training process that trains the network to predict the imminent tasks from the recruiters, which facilitates the embedding learning of the deep learning model. Additionally, we release a simulated environment on a specific MCS task, the ride-sharing task, and conduct extensive performance evaluations in this environment. The experimental results demonstrate that ADRL outperforms and improves sample efficiency over other well-recognized baselines in various settings.},
booktitle = {Proceedings of the 29th ACM International Conference on Information \&amp; Knowledge Management},
pages = {1355–1364},
numpages = {10},
keywords = {reinforcement learning, participant selection problem, mobile crowdsourcing},
location = {Virtual Event, Ireland},
series = {CIKM '20}
}

@inproceedings{10.1109/INFOCOM.2018.8486433,
author = {Tian, Yulong and Wei, Wei and Li, Qun and Xu, Fengyuan and Zhong, Sheng},
title = {MobiCrowd: Mobile Crowdsourcing on Location-based Social Networks},
year = {2018},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/INFOCOM.2018.8486433},
doi = {10.1109/INFOCOM.2018.8486433},
abstract = {The great potential of mobile crowdsourcing has started to attract attention of both industries and the research community. However, current commercial mobile crowdsourcing marketplaces are unsatisfactory because of the limited worker base and functionality. In this paper, we first revisit the foundation of performing mobile crowdsourcing on location-based social networks (LBSNs) through specially designed survey studies and comparison experiments involving hundreds of users. Our results reveal that active check-ins are good indicators of picking a right user to perform tasks, and LBSN could be an ideal platform for mobile crowdsourcing given proper services provided. We then propose both the centralized and decentralized design of MobiCrowd, a mobile crowdsourcing service built on LBSNs. Our evaluation, through trace-driven simulation and real-world experiments, demonstrates that the proposed schemes can effectively find workers for mobile crowdsourcing tasks associated with different venues by analyzing their location check-in histories.},
booktitle = {IEEE INFOCOM 2018 - IEEE Conference on Computer Communications},
pages = {2726–2734},
numpages = {9},
location = {Honolulu, HI, USA}
}

@inproceedings{10.1109/ICSE-SEIP.2017.2,
author = {Saremi, Razieh L and Yang, Ye and Ruhe, Guenther and Messinger, David},
title = {Leveraging crowdsourcing for team elasticity: an empirical evaluation at TopCoder},
year = {2017},
isbn = {9781538627174},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE-SEIP.2017.2},
doi = {10.1109/ICSE-SEIP.2017.2},
abstract = {There is an emergent trend in software development projects that mini-tasks can be crowdsourced to achieve rapid development and delivery. For software managers requesting crowdsourcing services, it is beneficial to be able to evaluate and assure the availability and performance of trustable workers on their tasks. However, existing rating systems are facing challenges such as providing limited information regarding worker's abilities as well as potential threats from workers' gaming or cheating the systems. To develop better understanding of worker performance in software crowdsourcing, this paper reports an empirical study at TopCoder, one of the primary software crowdsourcing platforms.We aim at investigating the following questions: How diverse are crowd workers in terms of skill and experience? How fast do crowd workers respond to a task call? How reliable are crowd workers in submitting tasks? And how much does CSD benefit schedule reduction? The main results of this study showed that on average, (i) 59\% of workers respond to a task call in the first 24 hours; (ii) 24\% of the workers who registered early will make submissions to tasks, and 76\% of them exceeding the acceptance criteria; and (iii) an overall average of 1.82 schedule acceleration rate is observed through organizing mass parallel development in 4 software crowdsourcing projects. Such empirical evidences are beneficial to help exploring resourcing options and improve team elasticity in adaptive software development.},
booktitle = {Proceedings of the 39th International Conference on Software Engineering: Software Engineering in Practice Track},
pages = {103–112},
numpages = {10},
keywords = {worker performance, worker availability, velocity, topcoder, elasticity, crowdsourced software development, agility},
location = {Buenos Aires, Argentina},
series = {ICSE-SEIP '17}
}

@inproceedings{10.1145/2488388.2488403,
author = {Bozzon, Alessandro and Brambilla, Marco and Ceri, Stefano and Mauri, Andrea},
title = {Reactive crowdsourcing},
year = {2013},
isbn = {9781450320351},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2488388.2488403},
doi = {10.1145/2488388.2488403},
abstract = {An essential aspect for building effective crowdsourcing com- putations is the ability of "controlling the crowd", i.e. of dynamically adapting the behaviour of the crowdsourcing systems as response to the quantity and quality of completed tasks or to the availability and reliability of performers. Most crowdsourcing systems only provide limited and predefined controls; in contrast, we present an approach to crowdsourcing which provides fine-level, powerful and flexible controls. We model each crowdsourcing application as composition of elementary task types and we progressively transform these high level specifications into the features of a reactive execution environment that supports task planning, assignment and completion as well as performer monitoring and exclusion. Controls are specified as active rules on top of data structures which are derived from the model of the application; rules can be added, dropped or modified, thus guaranteeing maximal flexibility with limited effort.We also report on our prototype platform that implements the proposed framework and we show the results of our experimentations with different rule sets, demonstrating how simple changes to the rules can substantially affect time, effort and quality involved in crowdsourcing activities.},
booktitle = {Proceedings of the 22nd International Conference on World Wide Web},
pages = {153–164},
numpages = {12},
keywords = {social computation, reactive rules, crowdsourcing, control},
location = {Rio de Janeiro, Brazil},
series = {WWW '13}
}

@inproceedings{10.1145/3250978,
author = {Dow, Steven},
title = {Session details: Crowdsourcing},
year = {2014},
isbn = {9781450324731},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3250978},
doi = {10.1145/3250978},
booktitle = {Proceedings of the SIGCHI Conference on Human Factors in Computing Systems},
location = {Toronto, Ontario, Canada},
series = {CHI '14}
}

@inproceedings{10.1145/3485447.3512256,
author = {Kou, Ziyi and Shang, Lanyu and Zhang, Yang and Duan, Siyu and Wang, Dong},
title = {Can I only share my eyes? A Web Crowdsourcing based Face Partition Approach Towards Privacy-Aware Face Recognition},
year = {2022},
isbn = {9781450390965},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3485447.3512256},
doi = {10.1145/3485447.3512256},
abstract = {Human face images represent a rich set of visual information for online social media platforms to optimize the machine learning (ML)/AI models in their data-driven facial applications (e.g., face detection, face recognition). However, there exists a growing privacy concern from social media users to share their online face images that will be annotated by unknown crowd workers and analyzed by ML/AI researchers in the model training and optimization process. In this paper, we focus on a privacy-aware face recognition problem where the goal is to empower the facial applications to train their face recognition models with images shared by social media users while protecting the identity of the users. Our problem is motivated by the limitation of current privacy-aware face recognition approaches that mainly prevent algorithmic attacks by manipulating face images but largely ignore the potential privacy leakage related to human activities (e.g., crowdsourcing annotation). To address such limitations, we develop FaceCrowd, a web crowdsourcing based face partition approach to improve the performance of current face recognition models by designing a novel crowdsourced partial face graph generated from privacy-preserved social media face images. We evaluate the performance of FaceCrowd using two real-world human face datasets that consist of large-scale human face images. The results show that FaceCrowd not only improves the accuracy of the face recognition models but also effectively protects the identity information of the social media users who share their face images.},
booktitle = {Proceedings of the ACM Web Conference 2022},
pages = {3611–3622},
numpages = {12},
keywords = {Crowdsourcing, Face Recognition, Privacy-aware},
location = {Virtual Event, Lyon, France},
series = {WWW '22}
}

@inproceedings{10.1007/978-3-030-60259-8_57,
author = {Cui, Lizhen and Chen, Jing and He, Wei and Li, Hui and Guo, Wei},
title = {A Pruned DOM-Based Iterative Strategy for Approximate Global Optimization in Crowdsourcing&nbsp;Microtasks},
year = {2020},
isbn = {978-3-030-60258-1},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-60259-8_57},
doi = {10.1007/978-3-030-60259-8_57},
abstract = {Crowdsourcing can solve many challenging problems for machines. The ability and knowledge background of employees on the Internet are unknown and different, the answers collected from the crowd are ambiguous. The choice of employee quality control strategy is really important to ensure the crowdsourcing results. In previous works, Expectation-Maximization (EM) was mainly used to estimate the real answer and quality of workers. Unfortunately, EM provides a local optimal solution, and the estimation results are often affected by the initial parameters. In this paper, an iterative optimization method based on EM local optimal results is designed to improve the quality estimation of workers for crowdsourcing micro-tasks (which has binary answers). The iterative search method works on the dominance ordering model (DOM) we proposed, which prunes the dominated task-response sequences while preserving the dominating ones, to iteratively search for the approximate global optimal estimation in a reduced space. We evaluate the proposed approach through extensive experiments on both simulated and real-world datasets, the experimental results illustrate that this strategy has higher performence than EM-based algorithm.},
booktitle = {Web and Big Data: 4th International Joint Conference, APWeb-WAIM 2020, Tianjin, China,  September 18-20, 2020, Proceedings, Part I},
pages = {779–793},
numpages = {15},
keywords = {Maximum likelihood estimation, Optimization strategy, Quality management, Crowdsourcing},
location = {Tianjin, China}
}

@inproceedings{10.1145/2702123.2702151,
author = {Otterbacher, Jahna},
title = {Crowdsourcing Stereotypes: Linguistic Bias in Metadata Generated via GWAP},
year = {2015},
isbn = {9781450331456},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2702123.2702151},
doi = {10.1145/2702123.2702151},
abstract = {Games with a Purpose (GWAP) is a popular approach for metadata creation, enabling institutions to collect descriptions of digital artifacts on a mass scale. Creating metadata is challenging not only because one must recognize the artifact; the description must then be encoded into natural language. Language behaviors are influenced by many social factors, particularly when we are asked to describe other people. We consider labels for images of people generated via the ESP Game. While ESP has been shown to produce relevant labels, critics claim they are obvious and stereotypical. Based on theories of linguistic biases, we examine whether there are systematic differences in the ways players describe images of men versus women. Our first analysis considers images of people generally, and reveals a tendency for women to be described with subjective adjectives. A second analysis compares images depicting men and women within each of six occupational roles. Images of women receive more labels related to appearance, whereas those depicting men receive more occupation-related labels. Our work exposes the presence of gender-based stereotypes through linguistic biases, illustrates the forms in which they manifest, and raises important implications for those who design systems or train algorithms using data produced via GWAP.},
booktitle = {Proceedings of the 33rd Annual ACM Conference on Human Factors in Computing Systems},
pages = {1955–1964},
numpages = {10},
keywords = {subjective language, stereotypes, metadata, linguistic bias, gender, games with a purpose, crowdsourcing},
location = {Seoul, Republic of Korea},
series = {CHI '15}
}

@inproceedings{10.5555/3382225.3382303,
author = {Zhang, Daniel (Yue) and Li, Qi and Tong, Herman and Badilla, Jose and Zhang, Yang and Wang, Dong},
title = {Crowdsourcing-based copyright infringement detection in live video streams},
year = {2020},
isbn = {9781538660515},
publisher = {IEEE Press},
abstract = {With the increasing popularity of online video sharing platforms (such as YouTube and Twitch), the detection of content that infringes copyright has emerged as a new critical problem in online social media. In contrast to the traditional copyright detection problem that studies the static content (e.g., music, films, digital documents), this paper focuses on a much more challenging problem: one in which the content of interest is from live videos. We found that the state-of-the-art commercial copyright infringement detection systems, such as the ContentID from YouTube, did not solve this problem well: large amounts of copyright-infringing videos bypass the detector while many legal videos are taken down by mistake. In addressing the copyright infringement detection problem for live videos, we identify several critical challenges: i) live streams are generated in real-time and the original copyright content from the owner may not be accessible; ii) streamers are getting more and more sophisticated in bypassing the copyright detection system (e.g., by modifying the title, tweaking the presentation of the video); iii) similar video descriptions and visual contents make it difficult to distinguish between legal streams and copyright-infringing ones. In this paper, we develop a crowdsourcing-based copyright infringement detection (CCID) scheme to address the above challenges by exploring a rich set of valuable clues from live chat messages. We evaluate CCID on two real world live video datasets collected from YouTube. The results show our scheme is significantly more effective and efficient than ContentID in detecting copyright-infringing live videos on YouTube.},
booktitle = {Proceedings of the 2018 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining},
pages = {367–374},
numpages = {8},
location = {Barcelona, Spain},
series = {ASONAM '18}
}

@inproceedings{10.1145/2808435.2808443,
author = {Salomoni, P. and Prandi, C. and Roccetti, M. and Nisi, V. and Nunes, N. Jardim},
title = {Crowdsourcing Urban Accessibility: Some Preliminary Experiences with Results},
year = {2015},
isbn = {9781450336840},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2808435.2808443},
doi = {10.1145/2808435.2808443},
abstract = {This short paper presents some preliminary results (both quantitative and qualitative) gathered from field trials with three different mobile apps that allow walkers to map urban accessibility barriers/facilities, while wandering around. The three apps were designed based on different gamification mechanisms, respectively exploiting: i) intrinsic (i.e., altruistic) motivations, ii) extrinsic motivations expressed in terms of a concrete reward, and iii) extrinsic motivations expressed in terms of fun/entertainment. These preliminary results reveal that the apps designed on the basis of extrinsic motivations are able to drive users to provide a larger amount of contributions. Interesting differences between concrete rewards and fun used as effective means to motivate contributors are discussed.},
booktitle = {Proceedings of the 11th Biannual Conference of the Italian SIGCHI Chapter},
pages = {130–133},
numpages = {4},
keywords = {urban accessibility, field trials with Results, crowdsourcing, Gamification},
location = {Rome, Italy},
series = {CHItaly '15}
}

@inproceedings{10.1145/2818048.2819997,
author = {Goncalves, Jorge and Kukka, Hannu and S\'{a}nchez, Iv\'{a}n and Kostakos, Vassilis},
title = {Crowdsourcing Queue Estimations in Situ},
year = {2016},
isbn = {9781450335928},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2818048.2819997},
doi = {10.1145/2818048.2819997},
abstract = {We present the development and evaluation of a situated crowdsourcing mechanism that estimates queue length in real time. The system relies on public interactive kiosks to collect human estimations about their queue waiting time. The system has been designed as a standalone tool that can be retrospectively embedded in a variety of locations without interfacing with billing or customer systems. An initial study was conducted in order to determine whether people who just joined the queue would differ in their estimates from people who were at the front of the queue. We then present our system's evaluation in four different restaurants over 19 weekdays. Our analysis shows how our system is perceived by users, and we develop 2 ways to optimise the waiting time estimation: by correcting the estimations based on the position of the input mechanism, and by changing the sliding window considered inputs to provide better prediction. Our analysis shows that approximately 7\% of restaurant customers provided estimations, but even so our system can provide predictions with up to 2 minute mean absolute error.},
booktitle = {Proceedings of the 19th ACM Conference on Computer-Supported Cooperative Work \&amp; Social Computing},
pages = {1040–1051},
numpages = {12},
keywords = {waiting time, tablets, situated, restaurants, queuing, public displays., estimations, Crowdsourcing},
location = {San Francisco, California, USA},
series = {CSCW '16}
}

@inproceedings{10.1145/2983323.2983716,
author = {Bansal, Piyush and Eickhoff, Carsten and Hofmann, Thomas},
title = {Active Content-Based Crowdsourcing Task Selection},
year = {2016},
isbn = {9781450340731},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2983323.2983716},
doi = {10.1145/2983323.2983716},
abstract = {Crowdsourcing has long established itself as a viable alternative to corpus annotation by domain experts for tasks such as document relevance assessment. The crowdsourcing process traditionally relies on high degrees of label redundancy in order to mitigate the detrimental effects of individually noisy worker submissions. Such redundancy comes at the cost of increased label volume, and, subsequently, monetary requirements. In practice, especially as the size of datasets increases, this is undesirable. In this paper, we focus on an alternate method that exploits document information instead, to infer relevance labels for unjudged documents. We present an active learning scheme for document selection that aims at maximising the overall relevance label prediction accuracy, for a given budget of available relevance judgements by exploiting system-wide estimates of label variance and mutual information.Our experiments are based on TREC 2011 Crowdsourcing Track data and show that our method is able to achieve state-of-the-art performance while requiring 17\% - 25\% less budget.},
booktitle = {Proceedings of the 25th ACM International on Conference on Information and Knowledge Management},
pages = {529–538},
numpages = {10},
keywords = {relevance assessment, crowdsourcing, active learning},
location = {Indianapolis, Indiana, USA},
series = {CIKM '16}
}

@inproceedings{10.1145/3251708,
author = {Suh, Bongwon},
title = {Session details: Evaluating Crowdsourcing},
year = {2015},
isbn = {9781450331456},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3251708},
doi = {10.1145/3251708},
booktitle = {Proceedings of the 33rd Annual ACM Conference on Human Factors in Computing Systems},
location = {Seoul, Republic of Korea},
series = {CHI '15}
}

@inproceedings{10.1145/3277139.3277169,
author = {Barashev, Andrey and Li, Guoxin},
title = {Worker's reward sensitivity predicting motivation in crowdsourcing: self-approach achievement goals perspective},
year = {2018},
isbn = {9781450364867},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3277139.3277169},
doi = {10.1145/3277139.3277169},
abstract = {Current paper investigates direct and indirect effects of workers' reward sensitivity (as represented by Behavioral Approach System) on motivation (measured with work engagement and levels of effort) in crowdsourcing platform. Furthermore, this paper analyses the mediating role of Self-Approach Achievement Goals in channeling 'initial energization' to motivational outcomes. A random sample of 320 taskcn.com workers was used in this study.The findings of this study showed that reward sensitivity was effective directly and indirectly, in positively predicting all motivational outcomes. Interestingly, the predictive relationship only related to self-based approach Achievement Goals and not in relation to task-based approach Achievement Goals. However, workers adopting self-approach and task-approach goals have higher Effort and Work Engagement exerted in crowdsourcing.These results could help crowdsourcing platform managers better understand the participants that working on the tasks, as well as find efficient ways of motivating workers. Targeting workers with higher reward sensitivity is beneficial for platform owners, providing the way to increase workers motivational outcomes at no additional cost, thus facilitating better results.},
booktitle = {Proceedings of the 1st International Conference on Information Management and Management Science},
pages = {180–184},
numpages = {5},
keywords = {work engagement, reward sensitivity, reinforcement sensitivity theory, effort, crowdsourcing, achievement goals},
location = {Chengdu, China},
series = {IMMS '18}
}

@inproceedings{10.1145/2567948.2578835,
author = {Brambilla, Marco and Ceri, Stefano and Mauri, Andrea and Volonterio, Riccardo},
title = {Community-based crowdsourcing},
year = {2014},
isbn = {9781450327459},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2567948.2578835},
doi = {10.1145/2567948.2578835},
abstract = {This paper is focused on community-based crowdsourcing applications, i.e. the ability of spawning crowdsourcing tasks upon multiple communities of performers, thus leveraging the peculiar characteristics and capabilities of the community members. We show that dynamic adaptation of crowdsourcing campaigns to community behaviour is particularly relevant. We demonstrate that this approach can be very effective for obtaining answers from communities, with very different size, precision, delay and cost, by exploiting the social networking relations and the features of the crowdsourcing task. We show the approach at work within the CrowdSearcher platform, which allows configuring and dynamically adapting crowdsourcing campaigns tailored to different communities. We report on an experiment demonstrating the effectiveness of the approach.},
booktitle = {Proceedings of the 23rd International Conference on World Wide Web},
pages = {891–896},
numpages = {6},
keywords = {social computation, crowdsourcing, community},
location = {Seoul, Korea},
series = {WWW '14 Companion}
}

@inproceedings{10.1145/3437120.3437330,
author = {Xanthopoulos, Theodoros and Anagnostopoulos, Theodoros and Kytagias, Christos and Psaromiligkos, Yannis},
title = {A smartphone-enabled crowdsensing and crowdsourcing system for predicting municipality resource allocation stochastic requirements},
year = {2021},
isbn = {9781450388979},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3437120.3437330},
doi = {10.1145/3437120.3437330},
abstract = {Smart Cities is the future of human habitation, which is evangelized by the Internet of Things (IoT) technology. We study the municipality of Papagos, which is located in the Smart City of Athens, Greece. In Papagos is developed a technical infrastructure, which enable citizens to act as human sensors by exploiting their smartphones to report malfunctions in the municipality infrastructure. Using Citify software app municipality problematic situations annotated and submitted to the system for further processing exploiting crowdsourcing and crowdsensing technology. When a report arrives to the municipality control center the system allocates certain department to serve the problem. Since incidents are served by a certain number of departments with limited resources, the early planning and allocation of a department's resources before the incident emerges is significant. To handle such situations, we incorporate an inference engine model, which is based on a Long Short-Term Memory (LSTM) Neural Network to learn stochastically examples of incidence occurrence. Based on the LSTM classification model the proposed system is able to predict a similar event in the future thus allocates efficiently a municipality department resource before the problem emerges.},
booktitle = {Proceedings of the 24th Pan-Hellenic Conference on Informatics},
pages = {305–310},
numpages = {6},
location = {Athens, Greece},
series = {PCI '20}
}

@inproceedings{10.1007/978-3-030-77091-4_16,
author = {Grassi, Lucrezia and Recchiuto, Carmine T. and Sgorbissa, Antonio},
title = {Knowledge-Driven Conversation for Social Robots: Exploring Crowdsourcing Mechanisms for&nbsp;Improving the System Capabilities},
year = {2020},
isbn = {978-3-030-77090-7},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-77091-4_16},
doi = {10.1007/978-3-030-77091-4_16},
abstract = {Social robots and artificial agents should be able to interact with the user in the most natural way possible. This work describes the basic principles of a conversation system designed for social robots and artificial agents, which relies on knowledge encoded in the form of an Ontology. Given the knowledge-driven approach, the possibility of expanding the Ontology in run-time, during the verbal interaction with the users is of the utmost importance: this paper also deals with the implementation of a system for the run-time expansion of the knowledge base, thanks to a crowdsourcing approach.},
booktitle = {AIxIA 2020 – Advances in Artificial Intelligence: XIXth International Conference of the Italian Association for Artificial Intelligence, Virtual Event,  November 25–27, 2020, Revised Selected Papers},
pages = {249–259},
numpages = {11},
keywords = {Crowdsourcing, NLP, Knowledge-driven conversation, Ontology, Autonomous conversation, Social robots},
location = {Milan, Italy}
}

@inproceedings{10.1007/978-3-319-97310-4_19,
author = {Qiu, Chen and Jiang, Liangxiao and Cai, Zhihua},
title = {Using Differential Evolution to Estimate Labeler Quality for Crowdsourcing},
year = {2018},
isbn = {978-3-319-97309-8},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-319-97310-4_19},
doi = {10.1007/978-3-319-97310-4_19},
abstract = {Crowdsourcing has emerged as an effective paradigm for accomplishing various intelligent tasks at low costs. However, the labels provided by non-expert crowdsourcing labelers often appear various quality as labelers possess wide-ranging levels of competence. This raises the significant challenges of estimating the true answers for tasks and the reliability of the labelers. Of numerous approaches to estimating labeler quality, expectation-maximization (EM) is widely used by maximizing the likelihood estimates of labeler quality from the observed multiple labels. However, EM-based approaches are easily trapped into local optima. In this paper we use a weight vector to represent the quality (reliability) of corresponding labelers and then using differential evolution (DE) to search optimal weights for different labelers. The experimental results validate the effectiveness of the proposed approach.},
booktitle = {PRICAI 2018: Trends in Artificial Intelligence: 15th Pacific Rim International Conference on Artificial Intelligence, Nanjing, China, August 28–31, 2018, Proceedings, Part II},
pages = {165–173},
numpages = {9},
keywords = {Crowdsourcing, Labeler quality, Differential evolution},
location = {Nanjing, China}
}

@inproceedings{10.5555/3171642.3171676,
author = {Hu, Zehong and Zhang, Jie},
title = {Optimal posted-price mechanism in microtask crowdsourcing},
year = {2017},
isbn = {9780999241103},
publisher = {AAAI Press},
abstract = {Posted-price mechanisms are widely-adopted to decide the price of tasks in popular microtask crowdsourcing. In this paper, we propose a novel postedprice mechanism which not only outperforms existing mechanisms on performance but also avoids their need of a finite price range. The advantages are achieved by converting the pricing problem into a multi-armed bandit problem and designing an optimal algorithm to exploit the unique features of microtask crowdsourcing. We theoretically show the optimality of our algorithm and prove that the performance upper bound can be achieved without the need of a prior price range. We also conduct extensive experiments using real price data to verify the advantages and practicability of our mechanism.},
booktitle = {Proceedings of the 26th International Joint Conference on Artificial Intelligence},
pages = {228–234},
numpages = {7},
location = {Melbourne, Australia},
series = {IJCAI'17}
}

@inproceedings{10.1145/3007120.3011076,
author = {Ernst, Christoph and Mladenow, Andreas and Strauss, Christine},
title = {Location-based Crowdsourcing in Disaster Response},
year = {2016},
isbn = {9781450348065},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3007120.3011076},
doi = {10.1145/3007120.3011076},
abstract = {This paper discusses approaches to identify location-dependent information for emergency managers, in order to make quick but solid decisions, and improve the coordination of activities performed by crowdsourcees during disaster response. In this regard, the idea of considering location as a driver for flexibility in the design of business processes is prepared, and the meaning of location-dependent tasks for volunteers is investigated. In terms of emergency management, the paper sheds light on what may have to be considered by emergency managers when coordinating activities performed by volunteers, followed by an illustration of how they can be aware of location-based information using crowdsourcing, as well as how visualization of this information can support decision-making.},
booktitle = {Proceedings of the 14th International Conference on Advances in Mobile Computing and Multi Media},
pages = {28–34},
numpages = {7},
keywords = {social crowd, mobile interaction, mobile applications, location-based services, emergency management, crowdsourcing, Disaster response},
location = {Singapore, Singapore},
series = {MoMM '16}
}

@inproceedings{10.1145/3148330.3148342,
author = {Jiang, Yu and Sun, Yuling and Yang, Jing and Lin, Xin and He, Liang},
title = {Enabling Uneven Task Difficulty in Micro-Task Crowdsourcing},
year = {2018},
isbn = {9781450355629},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3148330.3148342},
doi = {10.1145/3148330.3148342},
abstract = {In micro-task crowdsourcing markets such as Amazon's Mechanical Turk, how to obtain high quality result without exceeding the limited budgets is one main challenge. The existing theory and practice of crowdsourcing suggests that uneven task difficulty plays a crucial role to task quality. Yet, it lacks a clear identifying method to task difficulty, which hinders effective and efficient execution of micro-task crowdsourcing. This paper explores the notion of task difficulty and its influence to crowdsourcing, and presents a difficulty-based crowdsourcing method to optimize the crowdsourcing process. We firstly identify task difficulty feature based on a local estimation method in the real crowdsourcing context, followed by proposing an optimization method to improve the accuracy of results, while reducing the overall cost. We conduct a series of experimental studies to evaluate our method, which show that our difficulty-based crowdsourcing method can accurately identify the task difficulty feature, improve the quality of task performance and reduce the cost significantly, and thus demonstrate the effectiveness of task difficulty as task modeling property.},
booktitle = {Proceedings of the 2018 ACM International Conference on Supporting Group Work},
pages = {12–21},
numpages = {10},
keywords = {task feature, task difficulty, quality, micro tasks, crowdsourcing, context, budget, assignment},
location = {Sanibel Island, Florida, USA},
series = {GROUP '18}
}

@inproceedings{10.1007/978-3-319-25013-7_22,
author = {Gong, Yiwei},
title = {Enabling Flexible IT Services by Crowdsourcing: A Method for Estimating Crowdsourcing Participants},
year = {2015},
isbn = {978-3-319-25012-0},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-319-25013-7_22},
doi = {10.1007/978-3-319-25013-7_22},
abstract = {Crowdsourcing has become an increasingly attractive practice for companies to execute business processes in open contexts with on-demand workforce and higher level of flexibility. One of the challenges is the identification of the best-fit crowdsourcing participant from a group of online candidates. This paper presents a method of AHP-TOPSIS based on Grey Relation Analysis for estimating participants of a crowdsourcing task based on their online profiles and proposals. This method is tested by an experiment on a dataset of 348 completed IT service crowdsourcing tasks. An analysis on the matching between the test result and the actual selection result reveals the accuracy and efficiency of this method. Companies can use this method to facilitate the quality control at the beginning of crowdsourcing and keeps the selection of participants easy. This paper contributes to the design of a software agent for crowdsourcing platforms to automatically rank the participants of a task.},
booktitle = {Open and Big Data Management and Innovation : 14th IFIP WG 6.11 Conference on e-Business, e-Services, and e-Society, I3E 2015, Delft, The Netherlands, October 13-15, 2015, Proceedings},
pages = {275–286},
numpages = {12},
keywords = {Grey relation analysis, TOPSIS, AHP, Flexibility, Crowdsourcing},
location = {Delft, The Netherlands}
}

@inproceedings{10.1145/2601248.2601300,
author = {Sherief, Nada and Jiang, Nan and Hosseini, Mahmood and Phalp, Keith and Ali, Raian},
title = {Crowdsourcing software evaluation},
year = {2014},
isbn = {9781450324762},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2601248.2601300},
doi = {10.1145/2601248.2601300},
abstract = {Crowdsourcing is an emerging online paradigm for problem solving which involves a large number of people often recruited on a voluntary basis and given, as a reward, some tangible or intangible incentives. It harnesses the power of the crowd for minimizing costs and, also, to solve problems which inherently require a large, decentralized and diverse crowd. In this paper, we advocate the potential of crowdsourcing for software evaluation. This is especially true in the case of complex and highly variable software systems, which work in diverse, even unpredictable, contexts. The crowd can enrich and keep the timeliness of the developers' knowledge about software evaluation via their iterative feedback. Although this seems promising, crowdsourcing evaluation introduces a new range of challenges mainly on how to organize the crowd and provide the right platforms to obtain and process their input. We focus on the activity of obtaining evaluation feedback from the crowd and conduct two focus groups to understand the various aspects of such an activity. We finally report on a set of challenges to address and realize correct and efficient crowdsourcing mechanisms for software evaluation.},
booktitle = {Proceedings of the 18th International Conference on Evaluation and Assessment in Software Engineering},
articleno = {19},
numpages = {4},
keywords = {users feedback, software evaluation, crowdsourcing},
location = {London, England, United Kingdom},
series = {EASE '14}
}

@inproceedings{10.1145/3077136.3080666,
author = {Abad, Azad and Nabi, Moin and Moschitti, Alessandro},
title = {Autonomous Crowdsourcing through Human-Machine Collaborative Learning},
year = {2017},
isbn = {9781450350228},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3077136.3080666},
doi = {10.1145/3077136.3080666},
abstract = {In this paper, we introduce a general iterative human-machine collaborative method for training crowdsource workers: the classifier (i.e., the machine) selects the highest quality examples for training the crowdsource workers (i.e., the humans). Then, the latter annotate the lower quality examples such that the classifier can be re-trained with more accurate examples. This process can be iterated several times. We tested our approach on two different tasks, Relation Extraction and Community Question Answering, which are also in two different languages, English and Arabic, respectively. Our experimental results show a significant improvement for creating Gold Standard data over distant supervision or just crowdsourcing without worker training. At the same time, our method approach the performance than state-of-the-art methods using expensive Gold Standard for training workers},
booktitle = {Proceedings of the 40th International ACM SIGIR Conference on Research and Development in Information Retrieval},
pages = {873–876},
numpages = {4},
keywords = {self-training, relation extraction, human in the loop, crowdsourcing, community question answering},
location = {Shinjuku, Tokyo, Japan},
series = {SIGIR '17}
}

@inproceedings{10.1145/3278252.3278279,
author = {Peng, Fei and Liu, Yu and Lu, Bin},
title = {Research on Development Tendency and Strategy of Crowdsourcing Platform},
year = {2018},
isbn = {9781450365451},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3278252.3278279},
doi = {10.1145/3278252.3278279},
abstract = {This paper summarizes the development history and current situation crowdsourcing industry, describes some typical representatives of crowdsourcing platform, analysis the competition situation with Potter's five forces model. Then this paper predicts the development tendency using the website index and word frequency/emotional analysis, and puts forward some suggestions for the crowdsourcing platform.},
booktitle = {Proceedings of the 2nd International Conference on Business and Information Management},
pages = {39–43},
numpages = {5},
keywords = {Word frequency and emotional analysis, Website index, Development proposals, Crowdsourcing, Competitive analysis},
location = {Barcelona, Spain},
series = {ICBIM '18}
}

@inproceedings{10.5555/3091125.3091431,
author = {Xu, Pan and Srinivasan, Aravind and Sarpatwar, Kanthi K. and Wu, Kun-Lung},
title = {Budgeted Online Assignment in Crowdsourcing Markets: Theory and Practice},
year = {2017},
publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
address = {Richland, SC},
abstract = {We consider the following budgeted online assignment (BOA) problem motivated by crowdsourcing. We are given a set of offline tasks that need to be assigned to workers who come online from the pool of types {1, 2, ..., n}. For a given time horizon {1, 2, ..., T}, at each instant of time t, a worker j arrives from the pool in accordance with a known probability distribution [pjt] such that ∑j pjt ≤ 1; j has a known subset N(j) of the tasks that it can complete, and an assignment of one task i to j (if we choose to do so) should be done before task i's deadline. The assignment e = (i,j) (of task i ∈ N(j) to worker j) yields a profit we to the crowdsourcing provider and requires different quantities of K distinct resources, as specified by a cost vector ae ∈ [0, 1]K; these resources could be client-centric (such as their budget) or worker-centric (e.g., a driver's limitation on the total distance traveled or number of hours worked in a period). The goal is to design an online-assignment policy such that the total expected profit is maximized subject to the budget and deadline constraints.We propose and analyze two simple linear programming (LP)-based algorithms and achieve a competitive ratio of nearly 1/(l + 1), where l is an upper bound on the number of non-zero elements in any ae. This is nearly optimal among all LP-based approaches.},
booktitle = {Proceedings of the 16th Conference on Autonomous Agents and MultiAgent Systems},
pages = {1763–1765},
numpages = {3},
keywords = {online algorithms, crowdsourcing market, approximation algorithms},
location = {S\~{a}o Paulo, Brazil},
series = {AAMAS '17}
}

@inproceedings{10.1145/2487788.2487915,
author = {Langhans, Philipp and Wieser, Christoph and Bry, Fran\c{c}ois},
title = {Crowdsourcing MapReduce: JSMapReduce},
year = {2013},
isbn = {9781450320382},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2487788.2487915},
doi = {10.1145/2487788.2487915},
abstract = {JSMapReduce is an implementation of MapReduce which exploits the computing power available in the computers of the users of a web platform by giving tasks to the JavaScript engines of their web browsers. This article describes the implementation of JSMapReduce exploiting HTML 5 features, the heuristics it uses for distributing tasks to workers, and reports on an experimental evaluation of JSMapReduce.},
booktitle = {Proceedings of the 22nd International Conference on World Wide Web},
pages = {253–256},
numpages = {4},
keywords = {mapreduce, javascript, crowdsourcing},
location = {Rio de Janeiro, Brazil},
series = {WWW '13 Companion}
}

@inproceedings{10.1007/978-3-030-22649-7_23,
author = {Saremi, Razieh and Yang, Ye and Khanfor, Abdullah},
title = {Ant Colony Optimization to Reduce Schedule Acceleration in Crowdsourcing Software Development},
year = {2019},
isbn = {978-3-030-22648-0},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-22649-7_23},
doi = {10.1007/978-3-030-22649-7_23},
abstract = {The complexity of software tasks and the variety of developer skill sets requires to accomplish the tasks, provides a challenge in the planning process for software project managers. Uncertainty based on crowd workers’ different time zone and first language adds a layer of complexity to the CSD task scheduling. Therefore, accessing a scheduling model which can ease task allocation to improve task success and decrease project duration is essential. Existing models are either focused on the task allocation based on workers quality, or task availability in the crowdsourced platform. To create a flexible and effective model in CSD, we present an Ant Colony Optimization algorithm. The proposed approach shows a plan based on a list of available tasks in the platform and available workers based on their performance and rating metrics. The presented model is composed of four components: task fitness, workers’ attraction, task-worker availability, and task scheduler. Experimental results on 408 projects demonstrate that the proposed method reduced project duration on average 74 days.},
booktitle = {Human Interface and the Management of Information. Information in Intelligent Systems: Thematic Area, HIMI 2019, Held as Part of the 21st HCI International Conference, HCII 2019, Orlando, FL, USA, July 26-31, 2019, Proceedings, Part II},
pages = {286–300},
numpages = {15},
keywords = {Topcoder, Workers’ availability, Task similarity, Task fitness, Ant Colony Optimization, Crowdsourced software development},
location = {Orlando, FL, USA}
}

@inproceedings{10.5555/3060832.3060962,
author = {Zhou, Yao and He, Jingrui},
title = {Crowdsourcing via tensor augmentation and completion},
year = {2016},
isbn = {9781577357704},
publisher = {AAAI Press},
abstract = {Nowadays, the rapid proliferation of data makes it possible to build complex models for many real applications. Such models, however, usually require large amount of labeled data, and the labeling process can be both expensive and tedious for domain experts. To address this problem, researchers have resorted to crowdsourcing to collect labels from non-experts with much less cost. The key challenge here is how to infer the true labels from the large number of noisy labels provided by non-experts.Different from most existing work on crowdsourcing, which ignore the structure information in the labeling data provided by non-experts, in this paper, we propose a novel structured approach based on tensor augmentation and completion. It uses tensor representation for the labeled data, augments it with a ground truth layer, and explores two methods to estimate the ground truth layer via low rank tensor completion. Experimental results on 6 real data sets demonstrate the superior performance of the proposed approach over state-of-the-art techniques.},
booktitle = {Proceedings of the Twenty-Fifth International Joint Conference on Artificial Intelligence},
pages = {2435–2441},
numpages = {7},
location = {New York, New York, USA},
series = {IJCAI'16}
}

@inproceedings{10.1145/3003819.3003820,
author = {To, Hien},
title = {Task assignment in spatial crowdsourcing: challenges and approaches},
year = {2016},
isbn = {9781450345842},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3003819.3003820},
doi = {10.1145/3003819.3003820},
abstract = {Spatial crowdsourcing (a.k.a mobile crowdsourcing) is a new paradigm of data collection, which has been emerged in the last few years to enable workers to perform tasks in the physical world. The objective of spatial crowdsourcing is to outsource a set of location-specific tasks to a set of workers, in which the workers are required to physically be at the task locations to complete them, i.e., taking pictures or collecting air quality information at specified locations of interest. In this paper, we discuss the unique challenges of spatial crowdsourcing: task assignment, incentive mechanism, worker's location privacy and the absence of real-world datasets. Thereafter, we present our current approaches to those issues.},
booktitle = {Proceedings of the 3rd ACM SIGSPATIAL PhD Symposium},
articleno = {1},
numpages = {4},
location = {Burlingame, California},
series = {SIGSPATIAL PhD '16}
}

@inproceedings{10.1145/3334480.3383099,
author = {Liang, Qianhui and Wang, Meijie and Nagakura, Takehiko},
title = {Urban Immersion: A Web-based Crowdsourcing Platform for Collecting Urban Space Perception Data},
year = {2020},
isbn = {9781450368193},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3334480.3383099},
doi = {10.1145/3334480.3383099},
abstract = {We introduce Urban Immersion, a web-based platform for collecting crowdsourcing immersive perception data of urban space. Current research based on crowdsourcing data mainly utilize 2D representation tools, and it has limitations in the studies involving spatial features. Thus, in our design and implementation of the platform, which aims to help architects, urban researchers, or people in spatial management to understand their users' preferences, we incorporate webVR and 360-video techniques to display both realistic and abstract representation of the urban environment. We chose the city Shanghai for the first round of the experiment to test our crowdsourcing platform. We first did internal testing for the prototyping of the platform and then published it in social media and invited 771 people to participate in rating their perception preference. We did a rating analysis and visual mapping of the 5735 valid data we collected. We evaluated this round of crowdsourcing data collection on Urban Immersion platform. We checked the effectiveness of applying 3D representation techniques to crowdsourcing platforms and proposed how we could improve the platform in future work.},
booktitle = {Extended Abstracts of the 2020 CHI Conference on Human Factors in Computing Systems},
pages = {1–8},
numpages = {8},
keywords = {crowdsourcing, urban perception, web-based interaction},
location = {Honolulu, HI, USA},
series = {CHI EA '20}
}

@inproceedings{10.1109/CDC.2017.8264132,
author = {Liu, Xiangyang and Baras, John S.},
title = {Crowdsourcing with multi-dimensional trust and active learning},
year = {2017},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/CDC.2017.8264132},
doi = {10.1109/CDC.2017.8264132},
abstract = {We consider a typical crowdsourcing task that aggregates input from multiple workers as a problem in information fusion. To cope with the issue of noisy and sometimes malicious input from users, trust is used to model workers expertise. We propose a probabilistic model to jointly infer multi-dimensional trust of workers, multi-domain properties of questions, and true labels of questions. Our model is flexible and extensible to incorporate metadata associated with questions. To show that, we further propose two extended models, one of which handles input tasks with real-valued features and the other handles tasks with text features by incorporating topic models. In order to decrease entropies and reduce error rates more quickly with fewer annotations from workers, we further propose strategies for selecting which questions to ask and which workers to assign the questions to based on multidimension characteristics of questions and workers trust values in those dimensions. We evaluate our models and algorithms on real-world data sets. These results can be applied for fusion of information from multiple data sources like sensors, human input, machine learning results, or a hybrid of them.},
booktitle = {2017 IEEE 56th Annual Conference on Decision and Control (CDC)},
pages = {3224–3231},
numpages = {8},
location = {Melbourne, Australia}
}

@inproceedings{10.1145/2678025.2700997,
author = {Weld, Daniel S.},
title = {Intelligent Control of Crowdsourcing},
year = {2015},
isbn = {9781450333061},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2678025.2700997},
doi = {10.1145/2678025.2700997},
abstract = {Crowd-sourcing labor markets (e.g., Amazon Mechanical Turk) are booming, because they enable rapid construction of complex workflows that seamlessly mix human computation with computer automation. Example applications range from photo tagging to audio-visual transcription and interlingual translation. Similarly, workflows on citizen science sites (e.g. GalaxyZoo) have allowed ordinary people to pool their effort and make interesting discoveries. Unfortunately, constructing a good workflow is difficult, be- cause the quality of the work performed by humans is highly variable. Typically, a task designer will experiment with several alternative workflows to accomplish a task, varying the amount of redundant labor, until she devises a control strategy that delivers acceptable performance. Fortunately, this control challenge can often be formulated as an automated planning problem ripe for algorithms from the probabilistic planning and reinforcement learning literature. I describe our recent work on the decision-theoretic control of crowd sourcing and suggest open problems for future research.},
booktitle = {Proceedings of the 20th International Conference on Intelligent User Interfaces},
pages = {1},
numpages = {1},
keywords = {planning, crowdsourcing, adaptive interfaces},
location = {Atlanta, Georgia, USA},
series = {IUI '15}
}

@inproceedings{10.1007/978-3-030-23207-8_43,
author = {Reisert, Paul and Vallejo, Gisela and Inoue, Naoya and Gurevych, Iryna and Inui, Kentaro},
title = {An Annotation Protocol for Collecting User-Generated Counter-Arguments Using Crowdsourcing},
year = {2019},
isbn = {978-3-030-23206-1},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-23207-8_43},
doi = {10.1007/978-3-030-23207-8_43},
abstract = {Constructive feedback is important for improving critical thinking skills. However, little work has been done to automatically generate such feedback for an argument. In this work, we experiment with an annotation protocol for collecting user-generated counter-arguments via crowdsourcing. We conduct two parallel crowdsourcing experiments, where workers are instructed to produce (i) a counter-argument, and (ii) a counter-argument after identifying a fallacy. Our analysis indicates that we can collect counter-arguments that are useful as constructive feedback, especially when workers are first asked to identify a fallacy type.},
booktitle = {Artificial Intelligence in Education: 20th International Conference, AIED 2019, Chicago, IL, USA, June 25-29, 2019, Proceedings, Part II},
pages = {232–236},
numpages = {5},
keywords = {Critical thinking, Counter-argument, Fallacy, Crowdsourcing, Annotation study, Constructive feedback},
location = {Chicago, IL, USA}
}

@inproceedings{10.1145/3326285.3329043,
author = {Li, Juan and Wu, Jie and Zhu, Yanmin},
title = {Selecting optimal mobile users for long-term environmental monitoring by crowdsourcing},
year = {2019},
isbn = {9781450367783},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3326285.3329043},
doi = {10.1145/3326285.3329043},
abstract = {Urban environmental monitoring related to such issues as air pollution and noise helps people understand their living environments and promotes urban construction. It is more and more important nowadays. By crowdsourcing, we can get mobile users at a low cost to collect measurement at different locations. This paper studies how to select optimal mobile users to construct an accurate monitoring map under a limited budget. We extend the noise Gaussian Process model to construct the data utility model. Because the monitoring map is updated in each time slot, we try to maximize the time-averaged data utility under the time-averaged budget constraint. This problem is particularly challenging given the unknown future information and the difficulty of solving the one-slot problem: maximizing a non-monotone sub-modular objective under the budget constraint. To address these challenges, we first make use of Lyapunov optimization to decompose the long-term optimization problem into a series of real-time problems which do not require a priori knowledge about the future information. We then propose a time-efficient online algorithm to solve the NP-hard one-slot problem. As long as the algorithm for the one-slot problem has a competitive ratio e, the time-averaged data utility of our online algorithm has a small gap compared with e times the optimal one. Evaluations based on the real air pollution data in Beijing [2] and real human trajectory data [1] show the efficiency of our approach.},
booktitle = {Proceedings of the International Symposium on Quality of Service},
articleno = {8},
numpages = {10},
keywords = {non-monotone submodular function, long-term problem, gaussian process, environmental monitoring, crowdsourcing},
location = {Phoenix, Arizona},
series = {IWQoS '19}
}

@inproceedings{10.5555/3060832.3060916,
author = {Wang, Lu and Zhou, Zhi-Hua},
title = {Cost-saving effect of crowdsourcing learning},
year = {2016},
isbn = {9781577357704},
publisher = {AAAI Press},
abstract = {Crowdsourcing is widely adopted in many domains as a popular paradigm to outsource work to individuals. In the machine learning community, crowdsourcing is commonly used as a cost-saving way to collect labels for training data. While a lot of effort has been spent on developing methods for inferring labels from a crowd, few work concentrates on the theoretical foundation of crowdsourcing learning. In this paper, we theoretically study the cost-saving effect of crowdsourcing learning, and present an upper bound for the minimally-sufficient number of crowd labels for effective crowdsourcing learning. Our results provide an understanding about how to allocate crowd labels efficiently, and are verified empirically.},
booktitle = {Proceedings of the Twenty-Fifth International Joint Conference on Artificial Intelligence},
pages = {2111–2117},
numpages = {7},
location = {New York, New York, USA},
series = {IJCAI'16}
}

@inproceedings{10.1145/3092305.3092309,
author = {Acer, Utku Gunay and van den Broeck, Marc and Godon, Marc and Forlivesi, Claudio and Kawsar, Fahim},
title = {Can Mobile Workforce Revolutionize Country-Scale Crowdsourcing?},
year = {2017},
isbn = {9781450349581},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3092305.3092309},
doi = {10.1145/3092305.3092309},
abstract = {Traditional urban-scale crowdsourcing approaches suffer from three caveats - lack of complete spatiotemporal coverage, lack of accurate information and lack of sustained engagement of crowd workers. We argue that mobile workforces roaming around the city (and the larger country) can overcome all three caveats if their daily activity routines embed crowdsourcing tasks. To this end, in this paper, we report a first-of-its-kind study in which we explore behavioral attributes of mobile postal workers both quantitatively (6.3K) and qualitatively (6) to assess the opportunities of leveraging them for country-scale crowdsourcing tasks. Based on our observations, we develop a crowdsourcing infrastructure with carefully designed data collection strategies, and a corresponding wearable data collection application. We briefly present this solution and discuss its potential in country-scale crowdsourcing applications.},
booktitle = {Proceedings of the 4th International on Workshop on Physical Analytics},
pages = {31–36},
numpages = {6},
keywords = {mobile computing, crowdsourcin},
location = {Niagara Falls, New York, USA},
series = {WPA '17}
}

@inproceedings{10.1109/ICASSP.2017.7952910,
author = {Pag\`{e}s-Zamora, Alba and Giannakis, Georgios B. and L\'{o}pez-Valcarce, Roberto and Gim\'{e}nez-Febrer, Pere},
title = {Robust clustering of data collected via crowdsourcing},
year = {2017},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICASSP.2017.7952910},
doi = {10.1109/ICASSP.2017.7952910},
abstract = {Crowdsourcing approaches rely on the collection of multiple individuals to solve problems that require analysis of large data sets in a timely accurate manner. The inexperience of participants or annotators motivates well robust techniques. Focusing on clustering setups, the data provided by all annotators is suitably modeled here as a mixture of Gaussian components plus a uniformly distributed random variable to capture outliers. The proposed algorithm is based on the expectation-maximization algorithm and allows for soft assignments of data to clusters, to rate annotators according to their performance, and to estimate the number of Gaussian components in the non-Gaussian/Gaussian mixture model, in a jointly manner.},
booktitle = {2017 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
pages = {4014–4018},
numpages = {5},
location = {New Orleans, LA, USA}
}

@inproceedings{10.5555/3306127.3332087,
author = {Yang, Yi and Bai, Quan and Liu, Qing},
title = {Modeling Random Guessing and Task Difficulty for Truth Inference in Crowdsourcing},
year = {2019},
isbn = {9781450363099},
publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
address = {Richland, SC},
abstract = {This paper addresses the challenge of truth inference in crowdsourcing applications. We propose a generative method that jointly models tasks' difficulties, workers' abilities and guessing behavior to estimate the truths of crowdsourced tasks, which leads to a more accurate estimation on the workers' abilities and tasks' truths. Experiments demonstrate that the proposed method is more effective for estimating truths of crowdsourced tasks compared with the state-of-art methods.},
booktitle = {Proceedings of the 18th International Conference on Autonomous Agents and MultiAgent Systems},
pages = {2288–2290},
numpages = {3},
keywords = {crowdsourcing truth inference, crowdsourcing},
location = {Montreal QC, Canada},
series = {AAMAS '19}
}

@inproceedings{10.1145/2889160.2889225,
author = {Dwarakanath, Anurag and Shrikanth, N. C. and Abhinav, Kumar and Kass, Alex},
title = {Trustworthiness in enterprise crowdsourcing: a taxonomy \&amp; evidence from data},
year = {2016},
isbn = {9781450342056},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2889160.2889225},
doi = {10.1145/2889160.2889225},
abstract = {In this paper we study the trustworthiness of the crowd for crowdsourced software development. Through the study of literature from various domains, we present the risks that impact the trustworthiness in an enterprise context. We survey known techniques to mitigate these risks. We also analyze key metrics from multiple years of empirical data of actual crowdsourced software development tasks from two leading vendors. We present the metrics around untrustworthy behavior and the performance of certain mitigation techniques. Our study and results can serve as guidelines for crowdsourced enterprise software development.},
booktitle = {Proceedings of the 38th International Conference on Software Engineering Companion},
pages = {41–50},
numpages = {10},
keywords = {upwork, trustworthiness, crowdsourcing, TopCoder},
location = {Austin, Texas},
series = {ICSE '16}
}

@inproceedings{10.1145/3297156.3297239,
author = {Gu, Yonggen and Chen, Jiashen and Wu, Xiaohong},
title = {An Implement of Smart Contract Based Decentralized Online Crowdsourcing Mechanism},
year = {2018},
isbn = {9781450366069},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3297156.3297239},
doi = {10.1145/3297156.3297239},
abstract = {With the gradual promotion, crowdsourcing has become an efficient way to solve problems that are very complicated for computers and simple for human crowd intelligence in recent years. Traditional crowdsourcing is based on a central system where requesters post tasks on a crowdsourcing central server or platform, however, this centralized model currently faces various challenges such as prohibitive cost, single point of failure, and vulnerability to malicious attacks. To this end, this paper proposes a smart contract-based decentralized online crowdsourcing mechanism, which includes task assignment rules and reward payment rules, etc. The mechanism has the characteristics like decentralization, unalterable, truthfulness and so on. In addition, the corresponding smart contract is designed, so that the mechanism can really run and process the actual data, and the effectiveness is shown by experiments. In this way, the entire crowdsourcing process no longer requires the participation of trusted third-party agencies, information and privacy security is guaranteed, and the cost is lower.},
booktitle = {Proceedings of the 2018 2nd International Conference on Computer Science and Artificial Intelligence},
pages = {195–199},
numpages = {5},
keywords = {smart contract, decentralization, blockchain, Ethereum, Crowdsourcing},
location = {Shenzhen, China},
series = {CSAI '18}
}

@inproceedings{10.1145/3323503.3360634,
author = {Yagui, Marcela Mayumi Mauricio and Vivacqua, Adriana S.},
title = {A crowdsourcing web system for curating empirical knowledge in linked open data},
year = {2019},
isbn = {9781450367639},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3323503.3360634},
doi = {10.1145/3323503.3360634},
abstract = {Traditional and Empirical knowledge databases are becoming essential for the preservation of a region's culture. Without proper curation this knowledge might be lost over time. The goal of this work is to present a web system to support the co-curation of data derived from empirical knowledge. Our proposal also provides the interconnection between visitors' contributions with data already available in Linked Open Data repositories. The web system was evaluated by specialists from the Rio de Janeiro Botanical Garden, who verified the impact of the system implantation in cultural environments related to medicinal plants.},
booktitle = {Proceedings of the 25th Brazillian Symposium on Multimedia and the Web},
pages = {441–444},
numpages = {4},
keywords = {linked open data, cultural heritage, crowdsourcing, content curation, collaborative systems},
location = {Rio de Janeiro, Brazil},
series = {WebMedia '19}
}

@inproceedings{10.1007/978-3-030-58805-2_22,
author = {Tanaka, Kohei and Wakatsuki, Daisuke and Minagawa, Hiroki},
title = {A Study Examining a Real-Time Sign Language-to-Text Interpretation System Using Crowdsourcing},
year = {2020},
isbn = {978-3-030-58804-5},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-58805-2_22},
doi = {10.1007/978-3-030-58805-2_22},
abstract = {The current study examined how to use crowdsourcing to convert sign language-to-text. Generally in Japan, a sign language interpreter reads and vocalizes the sign language of the speaker, and caption typists generate captions from the vocalization. However, this method doubles labor costs and delays caption provision. Therefore, we developed a system that interprets sign language-to-caption text via crowdsourcing, with non-experts performing interpretations. While many individuals classified as deaf/hard-of-hearing (DHH) who can read sign language are suitable for this task, not all of them possess adequate typing skills. To address this, our system divides live sign language video into shorter segments, distributing them to workers. After the worker interprets and types the segments to text, the system generates captions through integration of these texts. Furthermore, we provide a user interface for playback speed control and one second rewinding in order to improve the ease with which tasks are completed. Our system can establish an environment that not only allows the interpretation of sign language-to-caption text, but also provides an opportunity for DHH individuals to assist those that are unable read sign language. We conducted a test using our prototype system for sign language-to-text interpretation. The mean time it took a worker to finish a task was 26&nbsp;s for a 9&nbsp;s segment. The combined total rate of missing text and collision between segments was 66\%. Analysis of questionnaire responses found that workers assigned fewer tasks considered the tasks more enjoyable.},
booktitle = {Computers Helping People with Special Needs: 17th International Conference, ICCHP 2020, Lecco, Italy, September 9–11, 2020, Proceedings, Part II},
pages = {186–194},
numpages = {9},
keywords = {Real-time sign language-to-text interpretation, Deaf and hard-of-hearing, Crowdsourcing},
location = {Lecco, Italy}
}

@inproceedings{10.1109/GLOCOM.2018.8647346,
author = {Shu, Jiangang and Liu, Ximeng and Yang, Kan and Zhang, Yinghui and Jia, Xiaohua and Deng, Robert H.},
title = {SybMatch: Sybil Detection for Privacy-Preserving Task Matching in Crowdsourcing},
year = {2018},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/GLOCOM.2018.8647346},
doi = {10.1109/GLOCOM.2018.8647346},
abstract = {The past decade has witnessed the rise of crowdsourcing, and privacy in crowdsourcing has also gained rising concern in the meantime. In this paper, we focus on the privacy leaks and sybil attacks during the task matching, and propose a privacy-preserving task matching scheme, called SybMatch. The SybMatch scheme can simultaneously protect the privacy of publishers and subscribers against semi-honest crowdsourcing service provider, and meanwhile support the sybil detection against greedy subscribers and efficient user revocation. Detailed security analysis and thorough performance evaluation show that the SybMatch scheme is secure and efficient.},
booktitle = {2018 IEEE Global Communications Conference (GLOBECOM)},
pages = {1–6},
numpages = {6},
location = {Abu Dhabi, United Arab Emirates}
}

@inproceedings{10.1109/GLOCOM.2018.8647174,
author = {Li, Weiwei and Zhang, Kuan and Su, Zhou and Lu, Rongxing and Wang, Ying},
title = {Anomalous Path Detection for Spatial Crowdsourcing-Based Indoor Navigation System},
year = {2018},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/GLOCOM.2018.8647174},
doi = {10.1109/GLOCOM.2018.8647174},
abstract = {Indoor navigation system provides customized path planning for requesters who are unfamiliar with the indoor environment, such as shopping mall and airport. Spatial crowd-sourcing technology can be applied to indoor navigation to offer fundamental services related to location. However, spatial crowdsourcing-based indoor navigation is vulnerable to the intrusion of injected anomalous paths from attackers. In this paper, we propose an anomalous path detection (APD) scheme to classify attackers according to their reputation management and abnormal trajectory sequence. Specifically, we first develop a crowdsourcing system to support the indoor location service using the fog as the spatial crowdsourcing server. Then, we identify two levels of attackers, i.e., the malicious responders and the semi-honest responders in the indoor environment according to their attacking purposes. Through the responders' historical records from the fog server, we analyze a series of trajectory sequences consisting of the distance between the current position and the destination to distinguish the semi-honest responders from the normal. In addition, we propose a semi-supervised learning with hidden Markov model (HMM) to detect the semi-honest responders. Finally, the extensive simulations show that the APD scheme can achieve higher accuracy with the acceptable false rate.},
booktitle = {2018 IEEE Global Communications Conference (GLOBECOM)},
pages = {1–7},
numpages = {7},
location = {Abu Dhabi, United Arab Emirates}
}

@inproceedings{10.1145/2837185.2837242,
author = {Mladenow, Andreas and Bauer, Christine and Strauss, Christine},
title = {Crowdsourcing in logistics: concepts and applications using the social crowd},
year = {2015},
isbn = {9781450334914},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2837185.2837242},
doi = {10.1145/2837185.2837242},
abstract = {The introduction of crowdsourcing offers numerous business opportunities. In recent years, manifold forms of crowdsourcing have emerged on the market -- also in logistics. Thereby, the ubiquitous availability and sensor-supported assistance functions of mobile devices support crowdsourcing applications, which promotes contextual interactions between users at the right place at the right time. This paper presents the results of an in-depth-analysis on crowdsourcing in logistics in the course of ongoing research in the field of location-based crowdsourcing (LBCS). This paper analyzes LBCS for both, 'classic' logistics as well as 'information' logistics. Real-world examples of crowdsourcing applications are used to underpin the two evaluated types of logistics using crowdsourcing. Potential advantages and challenges of logistics with the crowd ('crowd-logistics') are discussed. Accordingly, this paper aims to provide the necessary basis for a novel interdisciplinary research field.},
booktitle = {Proceedings of the 17th International Conference on Information Integration and Web-Based Applications \&amp; Services},
articleno = {30},
numpages = {8},
keywords = {innovative e-Applications, e-Applications, crowdsourcing, crowd-logistics, collaboration, business model},
location = {Brussels, Belgium},
series = {iiWAS '15}
}

@inproceedings{10.1145/3394171.3413619,
author = {Li, Jing and Ling, Suiyi and Wang, Junle and Le Callet, Patrick},
title = {A Probabilistic Graphical Model for Analyzing the Subjective Visual Quality Assessment Data from Crowdsourcing},
year = {2020},
isbn = {9781450379885},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3394171.3413619},
doi = {10.1145/3394171.3413619},
abstract = {The swift development of the multimedia technology has raised dramatically the users' expectation on the quality of experience. To obtain the ground-truth perceptual quality for model training, subjective assessment is necessary. Crowdsourcing platform provides us a convenient and feasible way to run large-scale experiments. However, the obtained perceptual quality labels are generally noisy. In this paper, we propose a probabilistic graphical annotation model to infer the underlying ground truth and discovering the annotator's behavior. In the proposed model, the ground truth quality label is considered following a categorical distribution rather than a unique number, i.e., different reliable opinions on the perceptual quality are allowed. In addition, different annotator's behaviors in crowdsourcing are modeled, which allows us to identify the possibility that the annotator makes noisy labels during the test. The proposed model has been tested on both simulated data and real-world data, where it always shows superior performance than the other state-of-the-art models in terms of accuracy and robustness.},
booktitle = {Proceedings of the 28th ACM International Conference on Multimedia},
pages = {3339–3347},
numpages = {9},
keywords = {quality assessment, probabilistic graphic model, ground truth, crowdsourcing, annotator behavior},
location = {Seattle, WA, USA},
series = {MM '20}
}

@inproceedings{10.1145/3155133.3155153,
author = {Minh, Quang Tran and Chi, Thanh Nguyen and Toulouse, Michel},
title = {Toward a Crowdsourcing-Based Urban Flood Mitigation Platform},
year = {2017},
isbn = {9781450353281},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3155133.3155153},
doi = {10.1145/3155133.3155153},
abstract = {Urban flood management and mitigation is one of the critical missions to maintain a stable city development. Most of the current available flood mitigation approaches are using data from fixed-site sensor systems which are not only costly but also limited in their coverage. Crowdsourcing is one of the potential approaches for addressing the coverage issue where flood data is collected and shared by crowds available everywhere. However, existing crowdsourcing-based approaches to flood mitigation systems lack thorough solutions on data validity. This paper proposes an appropriate crowdsourced data validity scheme which relies on both the scores evaluated by other users in the crowds and the accumulated reputation obtained by the user who shares the data. The paper also presents a practical mechanism to cluster reported data based on spatial and temporal information to improve the effectiveness of the data analytics. The proposed approaches have been implemented in an urban flood mitigation platform prototype running on both the IOS and the Android mobile devices. This prototype preliminarily shows the appropriateness and the usefulness of the proposed solutions.},
booktitle = {Proceedings of the 8th International Symposium on Information and Communication Technology},
pages = {301–308},
numpages = {8},
keywords = {user reputation, mobile systems, data validity, crowdsourcing, Urban flood mitigation},
location = {Nha Trang City, Viet Nam},
series = {SoICT '17}
}

@inproceedings{10.1145/2998181.2998311,
author = {Kandappu, Thivya and Misra, Archan and Tandriansyah, Randy},
title = {Collaboration Trumps Homophily in Urban Mobile Crowdsourcing},
year = {2017},
isbn = {9781450343350},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2998181.2998311},
doi = {10.1145/2998181.2998311},
abstract = {This paper establishes the power of dynamic collaborative task completion among workers for urban mobile crowd-sourcing. Collaboration is defined via the notion of peer referrals, whereby a worker who has accepted a location-specific task, but is unlikely to visit that location, offloads the task to a willing friend. Such a collaborative framework might be particularly useful for task bundles, especially for bundles that have higher geographic dispersion. The challenge, however, comes from the high similarity observed in the spatio-temporal pattern of task completion among friends. Using extensive real-world crowd-sourcing studies conducted over 7 weeks and 1000+ workers on a campus-based crowd-sourcing platform, we quantify the effect of such "task completion homophily", and show that incorporating such peer-preferences can improve worker-specific models of task preferences by over 30\%. We then show that such collaborative offloading works in spite of such spatio-temporal similarity, primarily because workers refer tasks to their close friends, who in turn perform such peer-requested tasks (with over 95\% completion rate) even if they experience detours that are significantly larger (often more than twice) than what they normally tolerate for platform-recommended tasks.},
booktitle = {Proceedings of the 2017 ACM Conference on Computer Supported Cooperative Work and Social Computing},
pages = {902–915},
numpages = {14},
keywords = {social-ties, homophily, crowd-sourcing, collaboration},
location = {Portland, Oregon, USA},
series = {CSCW '17}
}

@inproceedings{10.1145/3018661.3018688,
author = {Li, Yaliang and Du, Nan and Liu, Chaochun and Xie, Yusheng and Fan, Wei and Li, Qi and Gao, Jing and Sun, Huan},
title = {Reliable Medical Diagnosis from Crowdsourcing: Discover Trustworthy Answers from Non-Experts},
year = {2017},
isbn = {9781450346757},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3018661.3018688},
doi = {10.1145/3018661.3018688},
abstract = {Nowadays, increasingly more people are receiving medical diagnoses from healthcare-related question answering platforms as people can get diagnoses quickly and conveniently. However, such diagnoses from non-expert crowdsourcing users are noisy or even wrong due to the lack of medical domain knowledge, which can cause serious consequences. To unleash the power of crowdsourcing on healthcare question answering, it is important to identify trustworthy answers and filter out noisy ones from user-generated data. Truth discovery methods estimate user reliability degrees and infer trustworthy information simultaneously, and thus these methods can be adopted to discover trustworthy diagnoses from crowdsourced answers. However, existing truth discovery methods do not take into account the rich semantic meanings of the answers. In the light of this challenge, we propose a method to automatically capture the semantic meanings of answers, where answers are represented as real-valued vectors in the semantic space. To learn such vector representations from noisy user-generated data, we tightly combine the truth discovery and vector learning processes. In this way, the learned vector representations enable truth discovery method to model the semantic relations among answers, and the information trustworthiness inferred by truth discovery can help the procedure of vector representation learning. To demonstrate the effectiveness of the proposed method, we collect a large-scale real-world dataset that involves 219,527 medical diagnosis questions and 23,657 non-expert users. Experimental results show that the proposed method improves the accuracy of identified trustworthy answers due to the successful consideration of answers' semantic meanings. Further, we demonstrate the fast convergence and good scalability of the proposed method, which makes it practical for real-world applications.},
booktitle = {Proceedings of the Tenth ACM International Conference on Web Search and Data Mining},
pages = {253–261},
numpages = {9},
keywords = {truth discovery, semantic meanings, medical question answering},
location = {Cambridge, United Kingdom},
series = {WSDM '17}
}

@inproceedings{10.1109/WCNC.2018.8377007,
author = {Chen, Xiao},
title = {Task trading for crowdsourcing in opportunistic mobile social networks},
year = {2018},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/WCNC.2018.8377007},
doi = {10.1109/WCNC.2018.8377007},
abstract = {With the explosive proliferation of mobile devices, mobile crowdsourcing has become a new paradigm involving a crowd of mobile users to collectively take large-scale tasks from requesters in mobile social networks (MSNs). In this paper, we study task allocation in crowdsourcing in Opportunistic Mobile Social Networks (OMSNs) which are formed opportunistically when people gather together at social events. Specifically, we aim to minimize the total working hours of the users to finish these tasks. Different from other algorithms, we hope to raise the efficiency of the whole network by task trading inspired by the comparative advantage in macroeconomy. We first prove that our defined problem is NP-hard and then propose a heuristic task trading algorithm TTA by which users can trade when they meet opportunistically. Simulation results comparing our proposed algorithm with the one without considering trading and the brute force algorithm to find the minimum total number of hours show that our proposed algorithm can substantially reduce the total number of hours to finish all the allocated tasks and is very close to the benchmark brute force algorithm.},
booktitle = {2018 IEEE Wireless Communications and Networking Conference (WCNC)},
pages = {1–6},
numpages = {6},
location = {Barcelona, Spain}
}

@inproceedings{10.5555/3192424.3192461,
author = {Choi, Hongkyu and Lee, Kyumin and Webb, Steve},
title = {Detecting malicious campaigns in crowdsourcing platforms},
year = {2016},
isbn = {9781509028467},
publisher = {IEEE Press},
abstract = {Crowdsourcing systems enable new opportunities for requesters with limited funds to accomplish various tasks using human computation. However, the power of human computation is abused by malicious requesters who create malicious campaigns to manipulate information in web systems such as social networking sites, online review sites, and search engines. To mitigate the impact and reach of these malicious campaigns to targeted sites, we propose and evaluate a machine learning based classification approach for detecting malicious campaigns in crowdsourcing platforms as a first line of defense. Specifically, we (i) conduct a comprehensive analysis to understand the characteristics of malicious campaigns and legitimate campaigns in crowdsourcing platforms, (ii) propose various features to distinguish between malicious campaigns and legitimate campaigns, and (iii) evaluate a classification approach against baselines. Our experimental results show that our proposed approaches effectively detect malicious campaigns with low false negative and false positive rates.},
booktitle = {Proceedings of the 2016 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining},
pages = {197–202},
numpages = {6},
location = {Davis, California},
series = {ASONAM '16}
}

@inproceedings{10.1007/978-3-030-77750-0_32,
author = {Rapp, Maximilian and Kr\"{o}ger, Niclas and Scheerer, Samira},
title = {Inside-Out: How Internal Social Media Platforms Can Accelerate Innovation and Push External Crowdsourcing Towards New Frontiers},
year = {2021},
isbn = {978-3-030-77749-4},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-77750-0_32},
doi = {10.1007/978-3-030-77750-0_32},
abstract = {Continuous Improvement Processes (CIP) in companies and organizations alike have been part of a widespread metamorphosis to a more strategic internal crowdsourcing process with professional campaigns as well as sophisticated ideation platforms to gather knowledge and experiences from the organizations’ employees and stakeholders. While its counterpart, namely external crowdsourcing with users, customers, or external stakeholders is a matter of myriad research, the use, process, metamorphosis, and environments of internal crowds are lacking a deeper understanding through in-depth analysis. In this paper, we will answer 1) why most organizations use either internal or external crowdsourcing, and 2) what key success factors exist for effective internal campaigns. In order to answer these questions, we accompanied 10 organizations using an active research approach based on a variety of data, including interviews. We sum up by consolidating all findings in managerial implications for practical execution.},
booktitle = {HCI in Business, Government and Organizations: 8th International Conference, HCIBGO 2021, Held as Part of the 23rd HCI International Conference, HCII 2021, Virtual Event, July 24–29, 2021, Proceedings},
pages = {500–514},
numpages = {15},
keywords = {Open innovation, Co-creation, Ideation, Community, Crowdsourcing, Continuous Improvement Process, Employee integration}
}

@inproceedings{10.1145/3379157.3391304,
author = {Shekh.Khalil, Naziha and Dogruer, Ecem and Elosta, Abdulmohimen K. O. and Eraslan, Sukru and Yesilada, Yeliz and Harper, Simon},
title = {EyeCrowdata: Towards a Web-based Crowdsourcing Platform for Web-related Eye-Tracking Data},
year = {2020},
isbn = {9781450371353},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3379157.3391304},
doi = {10.1145/3379157.3391304},
abstract = {Eye-tracking studies are commonly used for identifying the usability problems of Web pages and gaining insights into how the design of Web pages can be improved for better user experience. Similar to other user studies, eye-tracking studies should be carefully designed and conducted by considering ethical issues and confounding factors, and therefore these studies typically require a considerable amount of time. Recruiting a large number of participants is also an important issue as eye-tracking sessions may not be conducted in parallel in case of limited resources such as equipment and researchers. Previous work highlighted the need for a Web-based platform to crowdsource Web-related eye-tracking data and facilitate data sharing, thus allowing the replication of existing analysis. Previous work also presented a preliminary structured literature review on what kinds of metrics are required for such a platform. In this paper, we also focus on Web-related eye-tracking studies, and we present an overview of the extended version of the structured literature review along with a prototype for a Web-based platform for crowdsourcing Web-related eye-tracking data called EyeCrowdata.},
booktitle = {ACM Symposium on Eye Tracking Research and Applications},
articleno = {31},
numpages = {6},
keywords = {replicability, eye tracking, data sharing, crowdsourcing, Web},
location = {Stuttgart, Germany},
series = {ETRA '20 Adjunct}
}

@inproceedings{10.5555/3327345.3327455,
author = {Hu, Zehong and Liang, Yitao and Zhang, Jie and Li, Zhao and Liu, Yang},
title = {Inference aided reinforcement learning for incentive mechanism design in crowdsourcing},
year = {2018},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Incentive mechanisms for crowdsourcing are designed to incentivize financially self-interested workers to generate and report high-quality labels. Existing mechanisms are often developed as one-shot static solutions, assuming a certain level of knowledge about worker models (expertise levels, costs of exerting efforts, etc.). In this paper, we propose a novel inference aided reinforcement mechanism that learns to incentivize high-quality data sequentially and requires no such prior assumptions. Specifically, we first design a Gibbs sampling augmented Bayesian inference algorithm to estimate workers' labeling strategies from the collected labels at each step. Then we propose a reinforcement incentive learning (RIL) method, building on top of the above estimates, to uncover how workers respond to different payments. RIL dynamically determines the payment without accessing any ground-truth labels. We theoretically prove that RIL is able to incentivize rational workers to provide high-quality labels. Empirical results show that our mechanism performs consistently well under both rational and non-fully rational (adaptive learning) worker models. Besides, the payments offered by RIL are more robust and have lower variances compared to the existing one-shot mechanisms.},
booktitle = {Proceedings of the 32nd International Conference on Neural Information Processing Systems},
pages = {5512–5522},
numpages = {11},
location = {Montr\'{e}al, Canada},
series = {NIPS'18}
}

@inproceedings{10.5555/3021955.3022041,
author = {Quirino, Wancharle S. and Santos, Celso A.S. and Calles, Juan X.E.A. and F., Fernando Tinelli},
title = {Crowdsourcing strategies for smart cities applications},
year = {2016},
isbn = {9788576693178},
publisher = {Brazilian Computer Society},
address = {Porto Alegre, BRA},
abstract = {Crowdsourcing is a problem-solving model through the contribution of a large number of people and has a low cost among its main advantages. On the other hand, smart cities today comprise a multidisciplinary challenge, where they have the objective of sustainable development and improving the quality of life of its inhabitants. Thus, we see in crowdsourcing a resource capable of contributing to building smart cities. This paper investigates the existing intersection between the fields of smart cities and crowdsourcing and discover the gaps and challenges that characterize this applications context. As a result, we propose some strategies to facilitate the development of crowdsourcing applications. These strategies are then applied to the construction of several applications of this type, two of which are discussed at the end of the article.},
booktitle = {Proceedings of the XII Brazilian Symposium on Information Systems on Brazilian Symposium on Information Systems: Information Systems in the Cloud Computing Era - Volume 1},
pages = {510–517},
numpages = {8},
keywords = {smart cities, platform, applications, Crowdsourcing},
location = {Florianopolis, Santa Catarina, Brazil},
series = {SBSI '16}
}

@inproceedings{10.3233/978-1-61499-672-9-1573,
author = {Liu, Siyuan and Fan, Xiuyi and Miao, Chunyan},
title = {Identifying and rewarding subcrowds in crowdsourcing},
year = {2016},
isbn = {9781614996712},
publisher = {IOS Press},
address = {NLD},
url = {https://doi.org/10.3233/978-1-61499-672-9-1573},
doi = {10.3233/978-1-61499-672-9-1573},
abstract = {Identifying and rewarding truthful workers are key to the sustainability of crowdsourcing platforms. In this paper, we present a clustering based rewarding mechanism that rewards workers based on their truthfulness while accommodating the differences in workers' preferences. Experimental results show that the proposed approach can effectively discover subcrowds under various conditions, and truthful workers are better rewarded than less truthful ones.},
booktitle = {Proceedings of the Twenty-Second European Conference on Artificial Intelligence},
pages = {1573–1574},
numpages = {2},
location = {The Hague, The Netherlands},
series = {ECAI'16}
}

@inproceedings{10.1145/3127404.3127443,
author = {Sun, Yong and Wang, Jun and Tan, Wenan},
title = {Online Algorithms of Task Allocation in Spatial Crowdsourcing},
year = {2017},
isbn = {9781450353526},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3127404.3127443},
doi = {10.1145/3127404.3127443},
abstract = {Recently, spatial collaborations1 and crowdsourcing has emerged as a novel typical pattern for applying to a range of problems. A key problem of spatial collaboration is to allocate suitable workers to nearby tasks in a real-time online way. Traditional crowdsourcing algorithms always consider the quality of worker with prior knowledge. However, in online crowdsourcing context, the quality of crowd-workers is unknown and uncertain. It is so hard for such task crowdsourcing process in an inherently online and dynamic environment. To solve this spatial crowdsourcing problem, the branch-and-bound R-tree data structure is employed in our algorithms to prune the search tree of the nearby crowd-workers. Furthermore, we introduce a new online algorithm to deal with the uncertain crowdsourcing problems. Theoretical analysis and extensive experiments are conducted for validation purpose; and the experimental results show that our algorithms outperform several existing algorithms in terms of computation time in dealing with the increasing number of crowdsourcing task executing candidates.},
booktitle = {Proceedings of the 12th Chinese Conference on Computer Supported Cooperative Work and Social Computing},
pages = {205–208},
numpages = {4},
keywords = {task allocation, online algorithms, collaborative computing, Spatial crowdsourcing},
location = {Chongqing, China},
series = {ChineseCSCW '17}
}

@inproceedings{10.1145/3313831.3376473,
author = {Wang, Yihong and Papangelis, Konstantinos and Saker, Michael and Lykourentzou, Ioanna and Chamberlain, Alan and Khan, Vassilis-Javed},
title = {Crowdsourcing in China: Exploring the Work Experiences of Solo Crowdworkers and Crowdfarm Workers},
year = {2020},
isbn = {9781450367080},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3313831.3376473},
doi = {10.1145/3313831.3376473},
abstract = {Recent research highlights the potential of crowdsourcing in China. Yet very few studies explore the workplace context and experiences of Chinese crowdworkers. Those that do, focus mainly on the work experiences of solo crowdworkers but do not deal with issues pertaining to the substantial amount of people working in 'crowdfarms'. This article addresses this gap as one of its primary concerns. Drawing on a study that involves 48 participants, our research explores, compares and contrasts the work experiences of solo crowdworkers to those of crowdfarm workers. Our findings illustrate that the work experiences and context of the solo workers and crowdfarm workers are substantially different, with regards to their motivations, the ways they engage with crowdsourcing, the tasks they work on, and the crowdsourcing platforms they utilize. Overall, our study contributes to furthering the understandings on the work experiences of crowdworkers in China.},
booktitle = {Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems},
pages = {1–13},
numpages = {13},
keywords = {crowdfarms, crowdsourcing, crowdworkers, motivations and attitudes, platform satisfaction, reputation management, tasks, work experience, work life balance},
location = {Honolulu, HI, USA},
series = {CHI '20}
}

@inproceedings{10.1145/3173574.3173641,
author = {Jang, Esther and Barela, Mary Claire and Johnson, Matt and Martinez, Philip and Festin, Cedric and Lynn, Margaret and Dionisio, Josephine and Heimerl, Kurtis},
title = {Crowdsourcing Rural Network Maintenance and Repair via Network Messaging},
year = {2018},
isbn = {9781450356206},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3173574.3173641},
doi = {10.1145/3173574.3173641},
abstract = {Repair and maintenance requirements limit the successful operation of rural infrastructure. Current best practices are centralized management, which requires travel from urban areas and is prohibitively expensive, or intensively training community members, which limits scaling. We explore an alternative model: crowdsourcing repair from the community. Leveraging a Community Cellular Network in the remote Philippines, we sent SMS to all active network subscribers (n = 63) requesting technical support. From the pool of physical respondents, we explored their ability to repair through mock failures and conducted semi-structured interviews about their experiences with repair. We learned that community members would be eager to practice repair if allowed, would network to recruit more expertise, and seemingly have the collective capacity to resolve some common failures. They are most successful when repairs map directly to their lived experiences. We suggest infrastructure design considerations that could make repairs more tractable and argue for an inclusive approach.},
booktitle = {Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems},
pages = {1–12},
numpages = {12},
keywords = {technology for development, rural development, internet access, ictd, crowdsourcing},
location = {Montreal QC, Canada},
series = {CHI '18}
}

@inproceedings{10.1007/978-3-319-68059-0_46,
author = {Riganova, Michaela and Balata, Jan and Mikovec, Zdenek},
title = {Crowdsourcing of Accessibility Attributes on Sidewalk-Based Geodatabase},
year = {2017},
isbn = {9783319680583},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-319-68059-0_46},
doi = {10.1007/978-3-319-68059-0_46},
abstract = {Although the issue of limited mobility affects a large portion of the population, current navigation systems working with roadway-based geodatabases are designed primarily for cars and therefore cannot efficiently help. Usage of the professionally created sidewalk-based geodatabase is a solution. However, the professional geographical "on-site reconnaissance" is labor demanding. In this poster, we report on results of preliminary research focused on a design of the gamified collection of accessibility attributes by non-expert crowd, which will reduce the data collection cost. Preliminary results suggest the feasibility of the approach supported by a proper guidance of non-experts and creativity of achieving precise measurements.},
booktitle = {16th IFIP TC 13 International Conference on Human-Computer Interaction --- INTERACT 2017 - Volume 10516},
pages = {436–440},
numpages = {5}
}

@inproceedings{10.1145/2487575.2487593,
author = {Mo, Kaixiang and Zhong, Erheng and Yang, Qiang},
title = {Cross-task crowdsourcing},
year = {2013},
isbn = {9781450321747},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2487575.2487593},
doi = {10.1145/2487575.2487593},
abstract = {Crowdsourcing is an effective method for collecting labeled data for various data mining tasks. It is critical to ensure the veracity of the produced data because responses collected from different users may be noisy and unreliable. Previous works solve this veracity problem by estimating both the user ability and question difficulty based on the knowledge in each task individually. In this case, each single task needs large amounts of data to provide accurate estimations. However, in practice, budgets provided by customers for a given target task may be limited, and hence each question can be presented to only a few users where each user can answer only a few questions. This data sparsity problem can cause previous approaches to perform poorly due to the overfitting problem on rare data and eventually damage the data veracity. Fortunately, in real-world applications, users can answer questions from multiple historical tasks. For example, one can annotate images as well as label the sentiment of a given title. In this paper, we employ transfer learning, which borrows knowledge from auxiliary historical tasks to improve the data veracity in a given target task. The motivation is that users have stable characteristics across different crowdsourcing tasks and thus data from different tasks can be exploited collectively to estimate users' abilities in the target task. We propose a hierarchical Bayesian model, TLC (Transfer Learning for Crowdsourcing), to implement this idea by considering the overlapping users as a bridge. In addition, to avoid possible negative impact, TLC introduces task-specific factors to model task differences. The experimental results show that TLC significantly improves the accuracy over several state-of-the-art non-transfer-learning approaches under very limited budget in various labeling tasks.},
booktitle = {Proceedings of the 19th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {677–685},
numpages = {9},
keywords = {transfer learning, crowdsourcing},
location = {Chicago, Illinois, USA},
series = {KDD '13}
}

@inproceedings{10.1109/MDM.2015.55,
author = {Alfarrarjeh, Abdullah and Emrich, Tobias and Shahabi, Cyrus},
title = {Scalable Spatial Crowdsourcing: A Study of Distributed Algorithms},
year = {2015},
isbn = {9781479999729},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/MDM.2015.55},
doi = {10.1109/MDM.2015.55},
abstract = {Recently spatial crowd sourcing was introduced as a natural extension to traditional crowd sourcing allowing for tasks to have a geospatial component, i.e., A task can only be performed if a worker is physically present at the location of the task. The problem of assigning spatial tasks to workers in a spatial crowd sourcing system can be formulated as a weighted bipartite b-matching graph problem that can be solved optimally by existing methods for the minimum cost maximum flow problem. However, these methods are still too complex to run repeatedly for an online system, especially when the number of incoming workers and tasks increases. Hence, we propose a class of approaches that utilizes an online partitioning method to reduce the problem space across a set of cloud servers to construct independent bipartite graphs and solve the assignment problem in parallel. Our approaches solve the spatial task assignment approximately but competitive to the exact solution. We experimentally verify that our approximate approaches outperform the centralized and Map Reduce version of the exact approach with acceptable accuracy and thus suitable for online spatial crowd sourcing at scale.},
booktitle = {Proceedings of the 2015 16th IEEE International Conference on Mobile Data Management - Volume 01},
pages = {134–144},
numpages = {11},
keywords = {spatial task assignment, spatial crowdsouring, online partitioning, distributed spatial task assignment},
series = {MDM '15}
}

@inproceedings{10.1145/3357155.3360478,
author = {Amorim, Ana Maria and Vieira, Vaninha},
title = {Exploratory study on the motivation of brazilian elderly people in crowdsourcing systems},
year = {2019},
isbn = {9781450369718},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3357155.3360478},
doi = {10.1145/3357155.3360478},
abstract = {The number of elderly people in the world has been growing every year as well as their interest in using computer and internet. Crowdsourcing systems can benefit from the participation of these elderly people, however, little is known about the use of these systems by this public. The aim of our research is to propose a motivation model to support understanding crowdsourcing usage by elderly people. In this article, we present preliminary results of an exploratory study performed with interviews and observations with 6 old-aged people performing micro-tasks crowdsourcing known as Human Intelligence Task (HIT). The results indicate the potential of crowdsourcing to become a fun activity and pastime for elderly, as well as supporting the increase of self-esteem, and social engagement.},
booktitle = {Proceedings of the 18th Brazilian Symposium on Human Factors in Computing Systems},
articleno = {65},
numpages = {4},
keywords = {senior workforce, motivation, crowdsourcing},
location = {Vit\'{o}ria, Esp\'{\i}rito Santo, Brazil},
series = {IHC '19}
}

@inproceedings{10.1145/3334480.3382999,
author = {Seong, Eunjin and Kim, Seungjun},
title = {Designing a Crowdsourcing System for the Elderly: A Gamified Approach to Speech Collection},
year = {2020},
isbn = {9781450368193},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3334480.3382999},
doi = {10.1145/3334480.3382999},
abstract = {Despite the need for representation of older adults in crowdsourced data, crowd work is generally not designed for older adults and participation by older adults is low. In this paper, we demonstrate a process for designing crowd work for older adults; identifying their needs, designing an approach to foster their participation, and verifying its effectiveness. We found when older people feel connected to others while doing crowd work, they are highly motivated. Furthermore, gamification is an effective tool for fostering their engagement when aligned with their needs and values, as opposed to the needs and values of younger participants. Lastly, we suggest important considerations and opportunities for designing crowd work approaches for senior citizens.},
booktitle = {Extended Abstracts of the 2020 CHI Conference on Human Factors in Computing Systems},
pages = {1–9},
numpages = {9},
keywords = {aging society, crowd work, crowdsourcing, gamification, older adults, speech collection},
location = {Honolulu, HI, USA},
series = {CHI EA '20}
}

@inproceedings{10.1145/3334480.3375228,
author = {Khan, Vassilis-Javed and Papangelis, Konstantinos and Markopoulos, Panos},
title = {Completing a Crowdsourcing Task Instead of an Assignment; What do University Students Think?},
year = {2020},
isbn = {9781450368193},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3334480.3375228},
doi = {10.1145/3334480.3375228},
abstract = {University educators actively seek realistic projects to include in their educational activities. However, finding an actually realistic project is not trivial. The rise of crowdsourcing platforms, in which a variety of tasks are offered in the form of an open call, might be an alternative source to help educators scale up project- based learning. But how do university students feel about executing crowdsourcing tasks instead of their typical assignments? In a study with 24 industrial design students we investigate students' attitudes on introducing crowdsourcing tasks as assignments. Based on our study we offer four suggestions to universities that consider integrating crowdsourcing tasks in their educational activities.},
booktitle = {Extended Abstracts of the 2020 CHI Conference on Human Factors in Computing Systems},
pages = {1–8},
numpages = {8},
keywords = {crowdsourcing, higher education, project based learning, university students' attitude},
location = {Honolulu, HI, USA},
series = {CHI EA '20}
}

@inproceedings{10.1145/2858036.2858115,
author = {Krishna, Ranjay A. and Hata, Kenji and Chen, Stephanie and Kravitz, Joshua and Shamma, David A. and Fei-Fei, Li and Bernstein, Michael S.},
title = {Embracing Error to Enable Rapid Crowdsourcing},
year = {2016},
isbn = {9781450333627},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2858036.2858115},
doi = {10.1145/2858036.2858115},
abstract = {Microtask crowdsourcing has enabled dataset advances in social science and machine learning, but existing crowdsourcing schemes are too expensive to scale up with the expanding volume of data. To scale and widen the applicability of crowdsourcing, we present a technique that produces extremely rapid judgments for binary and categorical labels. Rather than punishing all errors, which causes workers to proceed slowly and deliberately, our technique speeds up workers' judgments to the point where errors are acceptable and even expected. We demonstrate that it is possible to rectify these errors by randomizing task order and modeling response latency. We evaluate our technique on a breadth of common labeling tasks such as image verification, word similarity, sentiment analysis and topic classification. Where prior work typically achieves a 0.25x to 1x speedup over fixed majority vote, our approach often achieves an order of magnitude (10x) speedup.},
booktitle = {Proceedings of the 2016 CHI Conference on Human Factors in Computing Systems},
pages = {3167–3179},
numpages = {13},
keywords = {human computation, crowdsourcing, RSVP},
location = {San Jose, California, USA},
series = {CHI '16}
}

@inproceedings{10.1007/978-3-319-26148-5_30,
author = {Slaimi, Fatma and Sellami, Sana and Boucelma, Omar and Hassine, Ahlem Ben},
title = {Crowdsourcing for Web Service Discovery},
year = {2015},
isbn = {9783319261478},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-319-26148-5_30},
doi = {10.1007/978-3-319-26148-5_30},
abstract = {Over last decade, research in Web service discovery has brought a variety of techniques to find out responses for a Web service request. While the accuracy of matchmaking approaches has continuously improved, human contributions remain a key ingredient of the process. In this paper, we propose an approach called Crowd4WS Crowdsourcing for Web service discovery to complement and refine matchmaking approaches by using crowdsourcing techniques. We describe our approach and present the results of experiments on a known collection of RESTful services described with hRESTS.},
booktitle = {Proceedings of the Confederated International Conferences on On the Move to Meaningful Internet Systems: OTM 2015 Conferences - Volume 9415},
pages = {451–464},
numpages = {14},
keywords = {Web services discovery, Matchmaking, Crowdsourcing}
}

@inproceedings{10.1145/3357729.3357748,
author = {Lee, Helena H. and Achananuparp, Palakorn and Liu, Yue and Lim, Ee-Peng and Varshney, Lav R.},
title = {Estimating Glycemic Impact of Cooking Recipes via Online Crowdsourcing and Machine Learning},
year = {2019},
isbn = {9781450372084},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3357729.3357748},
doi = {10.1145/3357729.3357748},
abstract = {Consumption of diets with low glycemic impact is highly recommended for diabetics and pre-diabetics as it helps maintain their blood glucose levels. However, laboratory analysis of dietary glycemic potency is time-consuming and expensive. In this paper, we explore a data-driven approach utilizing online crowdsourcing and machine learning to estimate the glycemic impact of cooking recipes. We show that a commonly used healthiness metric may not always be effective in determining recipes suitable for diabetics, thus emphasizing the importance of the glycemic-impact estimation task. Our best classification model, trained on nutritional and crowdsourced data obtained from Amazon Mechanical Turk (AMT), can accurately identify recipes which are unhealthful for diabetics.},
booktitle = {Proceedings of the 9th International Conference on Digital Public Health},
pages = {31–35},
numpages = {5},
keywords = {recipe embeddings, recipe classification, glycemic impact},
location = {Marseille, France},
series = {DPH2019}
}

@inproceedings{10.5555/3178876.3258514,
author = {Demartini, Gianluca and Bozzon, Alessandro},
title = {Session details: Crowdsourcing and Human Computation for the Web},
year = {2018},
isbn = {9781450356398},
publisher = {International World Wide Web Conferences Steering Committee},
address = {Republic and Canton of Geneva, CHE},
booktitle = {Proceedings of the 2018 World Wide Web Conference},
location = {Lyon, France},
series = {WWW '18}
}

@inproceedings{10.1109/GLOCOM.2018.8647916,
author = {Li, Zan and Zhao, Xiaohui and Zhao, Zhongliang and Hu, Fengye and Liang, Hui and Braun, Torsten},
title = {Crowdsensing Indoor Walking Paths with Massive Noisy Crowdsourcing User Traces},
year = {2018},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/GLOCOM.2018.8647916},
doi = {10.1109/GLOCOM.2018.8647916},
abstract = {Crowdsensing indoor walking paths based on crowdsourcing traces collected from normal users has recently become an emerging topic for indoor positioning, which can reduce the labor effort of building radio maps and improve positioning accuracy when a floor plan is unavailable. In this work, we design an indoor walking path crowdsensing system with massive noisy crowdsourcing traces. In this system, we propose a robust iterative trace merging algorithm based on WiFi access points as markers (named 'WiFi-RITA') to merge massive noisy traces. The algorithm formulates the trace merging problem as an optimization problem in which each trace is controlled to translate and rotate to minimize the limitation of distances among traces defined by WiFi access points as markers. WiFi-RITA is robust to the rotation errors and uncertain absolute locations of user traces, and can efficiently work for a large number of user traces. We further adopt a landmark matching algorithm to match the merged traces to the target building and take a 2- dimensional histogram approach to remove outlier traces. With such procedures, we generate walking paths of a large-scale building with a mean accuracy of 2.1m.},
booktitle = {2018 IEEE Global Communications Conference (GLOBECOM)},
pages = {1–6},
numpages = {6},
location = {Abu Dhabi, United Arab Emirates}
}

@inproceedings{10.1109/INCoS.2015.74,
author = {Mladenow, Andreas and Bauer, Christine and Strauss, Christine and Gregus, Michal},
title = {Collaboration and Locality in Crowdsourcing},
year = {2015},
isbn = {9781467376952},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/INCoS.2015.74},
doi = {10.1109/INCoS.2015.74},
abstract = {As novel forms of crowdsourcing emerge on the market, we emphasize that the important aspect of location-dependency is more complex than assumed and, thus, suggest a typology along two dimensions of locality: the first dimension refers to whether or not the crowdsourcees interact while being collocated or dispersed, the second dimension refers to the locality of the crowdsourcees in relation to the crowdsourcer's locality (local vs. remote crowd). The resulting four types of crowdsourcing are underpinned by real-world examples. Potential advantages and challenges of the four types are discussed, particularly with respect to motivation and value. The suggested categorization shall provide the necessary basis for future research, as a systematic approach is essential to enable, yield and foster sustainability in a novel interdisciplinary research field like location-based crowdsourcing.},
booktitle = {Proceedings of the 2015 International Conference on  Intelligent Networking and Collaborative Systems},
pages = {1–6},
numpages = {6},
keywords = {Web-based Communities, Tournament-based Crowdsourcing, Taxonomy, Social Networks, Location-based Crowdsourcing, Crowdsourcing, Collaborative Systems, Collaboration-based Crowdsourcing, Collaboration},
series = {INCOS '15}
}

@inproceedings{10.1145/3411764.3445477,
author = {Chiang, Chia-En and Chen, Yu-Chun and Lin, Fang-Yu and Feng, Felicia and Wu, Hao-An and Lee, Hao-Ping and Yang, Chang-Hsuan and Chang, Yung-Ju},
title = {“I Got Some Free Time”: Investigating Task-execution and Task-effort Metrics in Mobile Crowdsourcing Tasks},
year = {2021},
isbn = {9781450380966},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3411764.3445477},
doi = {10.1145/3411764.3445477},
abstract = {Using a mixed-methods approach over six weeks, we studied 30 smartphone users’ task choices, task execution and effort devoted to two commercial mobile crowdsourcing platforms in the wild. We focused on the influence of activity contexts, characterized by breakpoint situations and activity attributes. In line with their stated preferences, the participants were more likely to proactively perform mobile crowdsourcing tasks during transitions between activities than during an ongoing activity and during long breaks, respectively. Their task choices were influenced by various activity attributes, and more impacted by their current and preceding activities than their upcoming ones. Two of our three target outcomes, task execution and task choice, were also influenced by individuals’ stress and energy levels. Our qualitative data provide further insights into participants’ decisions about which crowdsourcing tasks to perform and when; and our results’ implications for the design of future mobile crowdsourcing task-prompting mechanisms are also discussed.},
booktitle = {Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems},
articleno = {648},
numpages = {14},
keywords = {qualitative analysis, notification, mixed-effect logistic regression, interruption, Mobile crowdsourcing, ESM},
location = {Yokohama, Japan},
series = {CHI '21}
}

@inproceedings{10.1007/978-3-030-62056-1_53,
author = {Bevins, Alisha and McPhaul, Nina and Duncan, Brittany A.},
title = {Content Is King: Impact of Task Design for Eliciting Participant Agreement in&nbsp;Crowdsourcing for HRI},
year = {2020},
isbn = {978-3-030-62055-4},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-62056-1_53},
doi = {10.1007/978-3-030-62056-1_53},
abstract = {This work investigates how the design of crowdsourced tasks can influence responses. As a formative line of inquiry, this study sought to understand how users would respond either through movement, response, or shift of focus to varying flight paths from a drone. When designing an experiment, running several proto-studies can help with generating a dataset that is actionable, but it has been unclear how differences in things such as phrasing or pre- and post-surveys can impact the results. Leveraging methods from psychology, computer-supported cooperative work, and the human-robot interaction communities this work explored the best practices and lessons learned for crowdsourcing to reduce time to actionable data for defining new communication paradigms. The lessons learned in this work will be applicable broadly within the human-robot interaction community, even outside those who are interested in defining flight paths, because they provide a scaffold on which to build future experiments seeking to communicate using non-anthropomorphic robots. Important results and recommendations include: increased negative affect with increased question quantity, completion time being relatively consistent based on total number of responses rather than number of videos, responses being more related to the video than the question, and necessity of varying question lengths to maintain engagement.},
booktitle = {Social Robotics: 12th International Conference, ICSR 2020, Golden, CO, USA, November 14–18, 2020, Proceedings},
pages = {640–651},
numpages = {12},
keywords = {Aerial vehicle, Gesture, Crowdsourced},
location = {Golden, CO, USA}
}

@inproceedings{10.1145/3208903.3208927,
author = {Nasirifard, Pezhman and Rivera, Jose and Zhou, Qunjie and Schreiber, Klaus Bernd and Jacobsen, Hans-Arno},
title = {A Crowdsourcing Approach for the Inference of Distribution Grids},
year = {2018},
isbn = {9781450357678},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3208903.3208927},
doi = {10.1145/3208903.3208927},
abstract = {Maintaining a complete and up-to-date model of the distribution grid is a challenging task, and the scarcity of open models represents a significant bottleneck for researchers in this area. In this work, we address these challenges by introducing a crowdsourcing framework for the collection of open data on distribution grid devices and an algorithm to infer the topological model of the distribution grids. We use the crowd and smartphones to collect an image and the geographical position of power distribution grid devices. Since power distribution lines are usually underground and cannot be mapped, we use spatial data analytics on the collected data in combination with other open data sources to infer the topology of the distribution grid. This paper describes and evaluates our crowdsourcing and inference approach. To evaluate our approach, we organized and conducted a crowdsourcing campaign to map and infer a sizeable district in Munich, Germany. The results are compared with the ground truth of the distribution system operator. Our field experiments show that using the crowd to recognize power distribution elements, a precision of up to 82\% and a recall of up to 65\% can be obtained. The numerical evaluation of our inference algorithm demonstrates that the model we inferred based on the acquired official DSO grid dataset achieves a power length accuracy of 88\% compared to the ground truth. These results confirm our approach as a practical method to infer real power distribution grid models.},
booktitle = {Proceedings of the Ninth International Conference on Future Energy Systems},
pages = {187–199},
numpages = {13},
keywords = {Power grids, Power distribution, Geographic information systems, Distribution grid inference, Crowdsourcing},
location = {Karlsruhe, Germany},
series = {e-Energy '18}
}

@inproceedings{10.1145/3012709.3018007,
author = {Pipelidis, Georgios and Su, Xiang and Prehofer, Christian},
title = {Generation of indoor navigable maps with crowdsourcing},
year = {2016},
isbn = {9781450348607},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3012709.3018007},
doi = {10.1145/3012709.3018007},
abstract = {This paper presents our research in developing a model for the dynamic generation of indoor maps with crowdsourcing. With approximation of the user traces, we generate a point cloud and develop the topology of the space from time based segmentation of the traces. Moreover, we add semantic information for navigation and localization enabled maps. We discuss motivation, research objectives, and detailed research methods in this paper.},
booktitle = {Proceedings of the 15th International Conference on Mobile and Ubiquitous Multimedia},
pages = {385–387},
numpages = {3},
keywords = {semantic annotation, indoor mapping, crowdsourcing},
location = {Rovaniemi, Finland},
series = {MUM '16}
}

@inproceedings{10.1145/3106426.3106501,
author = {Ashikawa, Masayuki and Kawamura, Takahiro and Ohsuga, Akihiko},
title = {Crowdsourcing worker development based on probabilistic task network},
year = {2017},
isbn = {9781450349512},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3106426.3106501},
doi = {10.1145/3106426.3106501},
abstract = {Crowdsourcing platforms provide an attractive solution for processing numerous tasks at low cost. However, insufficient quality control remains a major concern. In the present study, we propose a grade-based training method for workers. Our training method utilizes probabilistic networks to estimate correlations between tasks based on workers' records for 18.5 million tasks and then allocates pre-learning tasks to the workers to raise the accuracy of target tasks according to the task correlations. In an experiment, the method automatically allocated 31 pre-learning task categories for 9 target task categories, and after the training of the pre-learning tasks, we confirmed that the accuracy of the target tasks was raised by 7.8 points on average. We thus confirmed that the task correlations can be estimated using a large amount of worker records, and that these are useful for the grade-based training of low-quality workers.},
booktitle = {Proceedings of the International Conference on Web Intelligence},
pages = {855–862},
numpages = {8},
keywords = {education, crowdsourcing, bayesian network},
location = {Leipzig, Germany},
series = {WI '17}
}

@inproceedings{10.1145/3308560.3317081,
author = {K. Chaithanya Manam, V. and Jampani, Dwarakanath and Zaim, Mariam and Wu, Meng-Han and J. Quinn, Alexander},
title = {TaskMate: A Mechanism to Improve the Quality of Instructions in Crowdsourcing},
year = {2019},
isbn = {9781450366755},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3308560.3317081},
doi = {10.1145/3308560.3317081},
abstract = {Developing instructions for microtask crowd workers requires time to ensure consistent interpretations by crowd workers. Even with substantial effort, workers may still misinterpret the instructions due to ambiguous language and structure in the task design. Prior work demonstrated methods for facilitating iterative improvement with help from the requester. However, any participation by the requester reduces the time saved by delegating the work—and hence the utility of using crowdsourcing. We present TaskMate, a system for facilitating worker-led refinement of task instructions with minimal involvement by the requester. Small teams of workers search for ambiguities and vote on the interpretation they believe the requester intended. This paper describes the workflow, our implementation, and our preliminary evaluation.},
booktitle = {Companion Proceedings of The 2019 World Wide Web Conference},
pages = {1121–1130},
numpages = {10},
keywords = {workflow, task instructions, ambiguities, Crowdsourcing},
location = {San Francisco, USA},
series = {WWW '19}
}

@inproceedings{10.1145/2818048.2835202,
author = {Celis, L. Elisa and Reddy, Sai Praneeth and Singh, Ishaan Preet and Vaya, Shailesh},
title = {Assignment Techniques for Crowdsourcing Sensitive Tasks},
year = {2016},
isbn = {9781450335928},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2818048.2835202},
doi = {10.1145/2818048.2835202},
abstract = {Protecting the privacy of crowd workers has been an important topic in crowdsourcing, however, task privacy has largely been ignored despite the fact that many tasks, e.g., form digitization, live audio transcription or image tagging often contain sensitive information. Although assigning an entire job to a worker may leak private information, jobs can often be split into small components that individually do not. We study the problem of distributing such tasks to workers with the goal of maximizing task privacy using such an approach.We introduce information loss functions to formally measure the amount of private information leaked as a function of the task assignment. We then design assignment mechanisms for three different assignment settings: PUSH, PULL and a new setting Tug Of War (TOW), which is an intermediate approach that balances flexibility for both workers and requesters. Our assignment algorithms have zero privacy loss for PUSH, and tight theoretical guarantees for PULL. For TOW, our assignment algorithm provably outperforms PULL; importantly the privacy loss is independent of the number of tasks, even when workers collude. We further analyze the performance and privacy tradeoffs empirically on simulated and real-world collusion networks and find that our algorithms outperform the theoretical guarantees.},
booktitle = {Proceedings of the 19th ACM Conference on Computer-Supported Cooperative Work \&amp; Social Computing},
pages = {836–847},
numpages = {12},
keywords = {Social Networks, Privacy, Microtasks, Crowdsourcing},
location = {San Francisco, California, USA},
series = {CSCW '16}
}

@inproceedings{10.1145/3106426.3106446,
author = {Kang, Qiyu and Tay, Wee Peng},
title = {Sequential multi-class labeling in crowdsourcing: a ulam-renyi game approach},
year = {2017},
isbn = {9781450349512},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3106426.3106446},
doi = {10.1145/3106426.3106446},
abstract = {We consider a crowdsourcing platform where workers are posed questions by a crowdsourcer, who then uses their responses to determine the hidden state of a multi-class labeling problem. Workers may be unreliable, therefore by designing the questions using error correction coding approaches, the crowdsourcer can achieve a more reliable overall result. We propose to perform sequential questioning in which workers are asked q-ary questions sequentially, and questions are determined based on the workers' previous responses. We propose an optimization framework to determine the best q and questioning strategy to use, subject to a crowdsourcer budget constraint. For a fixed q, this problem is equivalent to finding an optimal questioning strategy to a q-ary Ulam-R\'{e}nyi game, which is in general intractable. We propose a heuristic to find a suboptimal strategy, and demonstrate through simulations that our solution outperforms another error correction coding strategy that does not utilize previous workers' responses. Simulations also suggest that q can in general be chosen to be much smaller than the number of classes in the multi-class labeling problem.},
booktitle = {Proceedings of the International Conference on Web Intelligence},
pages = {245–251},
numpages = {7},
keywords = {ulam-r\'{e}nyi game, question design, multi-class labeling, crowdsourcing, cooperative work},
location = {Leipzig, Germany},
series = {WI '17}
}

@inproceedings{10.1145/3286606.3286837,
author = {El Khaili, Mohamed and Bakkoury, Jamila and Khiat, Azeddine and Alloubane, Abdelkarim},
title = {Crowdsourcing by IoT using LabVIEW for Measuring the Air Quality},
year = {2018},
isbn = {9781450365628},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3286606.3286837},
doi = {10.1145/3286606.3286837},
abstract = {Our challenge has two dimensions: social and technological. We want to solve a serious problem that is assessing the air quality in cities. To inform and sensitize people to the air pollution problem, our project will bring the locals in participatory situation and actor for the improvement of air quality.We hear more and more talk about the Internet of Things, connected objects, or even connected world, or even intelligent home; new concepts that invade the world and enhance our way of life. Internet of Things called the third industrial revolution will profoundly change the lives of people with home automation, health and recreation, energy, distribution and our environment with intelligent cities or transport connected. The collection of information remains a major challenge without the participation of a large group of people or partners. The Crowdsourcing allows obtaining information due to a large group of people by the internet.},
booktitle = {Proceedings of the 3rd International Conference on Smart City Applications},
articleno = {60},
numpages = {8},
keywords = {Smart city, LabView, Internet of things, Crowdsourcing, Air quality},
location = {Tetouan, Morocco},
series = {SCA '18}
}

@inproceedings{10.1145/3126973.3126988,
author = {Cui, Lizhen and Zhao, Xudong and Liu, Lei and Yu, Han and Miao, Yuan},
title = {Learning Complex Crowdsourcing Task Allocation Strategies from Humans},
year = {2017},
isbn = {9781450353755},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3126973.3126988},
doi = {10.1145/3126973.3126988},
abstract = {Efficient allocation of complex tasks, which typically include heterogeneous attributes such as value, difficulty, skill required, effort required and deadline, is a challenging open problem in crowdsourcing. Existing approaches are mostly designed based on expert knowledge and fail to leverage on user generated data to capture the complex interaction of crowdsourcing participants' behaviours. In this paper, we propose a data-driven learning approach to address this challenge. The proposed approach combines supervised learning and reinforcement learning to enable agents to imitate human task allocation strategies which have shown good performance. The policy network component selects task allocation strategies and the reputation network component calculates the trends of worker reputation fluctuations. The two networks have been trained and evaluated using a large-scale real human task allocation strategy dataset derived from the Agile Manager game. Extensive experiments based on this dataset demonstrate the validity and efficiency of our approach.},
booktitle = {Proceedings of the 2nd International Conference on Crowd Science and Engineering},
pages = {33–37},
numpages = {5},
keywords = {task allocation, reinforcement learning, Crowdsourcing},
location = {Beijing, China},
series = {ICCSE'17}
}

@inproceedings{10.1109/GLOCOM.2018.8647720,
author = {Li, Shu and Zhang, Jie and Xie, Dongqing and Yu, Shui and Dou, Wanchun},
title = {High Quality Participant Recruitment of Mobile Crowdsourcing over Big Data},
year = {2018},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/GLOCOM.2018.8647720},
doi = {10.1109/GLOCOM.2018.8647720},
abstract = {With the rich set of embedded sensors installed in smart-phones, an increasing number of applications have been designed based on these mobile sensors rather than on static sensors in urban areas. In Mobile Crowdsourcing (MCS), participant selection is promoted to save energy and entire incentives. Nevertheless, most of the current researches on this problem assume that the system should get the entire information about the participants. As a result, the suitable tasks are always not allocated to the suitable participants. This practice contributes an inaccurate match between a task and participants, which leads to energy and incentives waste. In view of this challenge, we aim to select participants under a more accurate prediction model, rather than assuming that the information of each participant should be obtained in advance. The prediction model is enabled by the big data of participants' historic evaluation, which are used to predict the user action. Furthermore, a greedy method based on an improved singular value decomposition (SVD), named as SVD_G, is proposed to solve this problem. Finally, the proposed SVD_G method is validated by using the large-scale dataset collected from a real-world project (DaZhongDianPing APP).},
booktitle = {2018 IEEE Global Communications Conference (GLOBECOM)},
pages = {1–6},
numpages = {6},
location = {Abu Dhabi, United Arab Emirates}
}

@inproceedings{10.5555/2891460.2891668,
author = {Baba, Yukino and Kashima, Hisashi and Kinoshita, Kei and Yamaguchi, Goushi and Akiyoshi, Yosuke},
title = {Leveraging crowdsourcing to detect improper tasks in crowdsourcing marketplaces},
year = {2013},
publisher = {AAAI Press},
abstract = {Controlling the quality of tasks is a major challenge in crowdsourcing marketplaces. Most of the existing crowdsourcing services prohibit requesters from posting illegal or objectionable tasks. Operators in the marketplaces have to monitor the tasks continuously to find such improper tasks; however, it is too expensive to manually investigate each task. In this paper, we present the reports of our trial study on automatic detection of improper tasks to support the monitoring of activities by marketplace operators. We perform experiments using real task data from a commercial crowdsourcing marketplace and show that the classifier trained by the operator judgments achieves high accuracy in detecting improper tasks. In addition, to reduce the annotation costs of the operator and improve the classification accuracy, we consider the use of crowdsourcing for task annotation. We hire a group of crowdsourcing (non-expert) workers to monitor posted tasks, and incorporate their judgments into the training data of the classifier. By applying quality control techniques to handle the variability in worker reliability, our results show that the use of non-expert judgments by crowdsourcing workers in combination with expert judgments improves the accuracy of detecting improper crowdsourcing tasks.},
booktitle = {Proceedings of the Twenty-Seventh AAAI Conference on Artificial Intelligence},
pages = {1487–1492},
numpages = {6},
location = {Bellevue, Washington},
series = {AAAI'13}
}

@inproceedings{10.5555/3016387.3016480,
author = {Yu, Han and Miao, Chunyan and Shen, Zhiqi and Lin, Jun and Leung, Cyril and Yang, Qiang},
title = {Infusing human factors into Algorithmic Crowdsourcing},
year = {2016},
publisher = {AAAI Press},
booktitle = {Proceedings of the Thirtieth AAAI Conference on Artificial Intelligence},
pages = {4062–4063},
numpages = {2},
location = {Phoenix, Arizona},
series = {AAAI'16}
}

@inproceedings{10.1109/ISI.2018.8587316,
author = {da Silva, M\^{o}nica and Viterbo, Jos\'{e} and Bernardini, Flavia and Maciel, Cristiano},
title = {Identifying Privacy Functional Requirements for Crowdsourcing Applications in Smart Cities},
year = {2018},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ISI.2018.8587316},
doi = {10.1109/ISI.2018.8587316},
abstract = {Information and Communication Technologies are indispensable components of smart cities. Its applications are present in several areas, such as urban mobility, environmental issues and medical systems. In this scenario, the use of crowdsourcing technologies comes to help people to contribute to the development and improvement of the urban digital services. However, using crowdsourced data in smart cities solutions can lead to problems with the security and the privacy of user’s data. The setting of comprehensive Functional Requirements (FR) to ensure data privacy is an approach for preventing the occurrence of such issues. In this work, we intend to identify, from a literature review the main privacy requirements that have been observed in the development of applications that make use of crowdsourced data in Smart Cities scenarios.},
booktitle = {2018 IEEE International Conference on Intelligence and Security Informatics (ISI)},
pages = {106–111},
numpages = {6},
location = {Miami, FL, USA}
}

@inproceedings{10.1109/WCNC.2018.8377240,
author = {Cui, Jingmei and Sun, Yu-E and Huang, He and Guo, Hansong and Du, Yang and Yang, Wenjian and Li, Meixuan},
title = {TCAM: A truthful combinatorial auction mechanism for crowdsourcing systems},
year = {2018},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/WCNC.2018.8377240},
doi = {10.1109/WCNC.2018.8377240},
abstract = {Crowdsourcing has shown its efficiency in obtaining information by harnessing the intelligence of a large crowd of human workers. It is essential to employ incentive mechanisms, typically auction, to motivate workers and collect sufficient data, since performing crowdsourcing tasks will always consume considerable resources, e.g., CPU or battery resource. To this end, we focus on the problem of heterogeneous task allocation with budget constraint in the crowdsourcing systems and propose a truthful auction mechanism which can maximize the profit of the task requester. In this paper, we first prove the NP-hardness of the studied problem and design a near-optimal task allocation mechanism with partial enumeration which can maximize the profit of the requester. Then, we judiciously design a bid-independent payment calculation mechanism to ensure the truthfulness of the participants. Finally, we prove that the proposed crowdsourcing task auction mechanism can achieve truthfulness and individual rationality. The extensive simulation results also corroborate with our theoretical analysis.},
booktitle = {2018 IEEE Wireless Communications and Networking Conference (WCNC)},
pages = {1–6},
numpages = {6},
location = {Barcelona, Spain}
}

@inproceedings{10.1109/INFOCOM.2016.7524546,
author = {Chen, Yanjiao and Li, Baochun and Zhang, Qian},
title = {Incentivizing crowdsourcing systems with network effects},
year = {2016},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/INFOCOM.2016.7524546},
doi = {10.1109/INFOCOM.2016.7524546},
abstract = {In a crowdsourcing system, it is important for the crowdsourcer to engineer extrinsic rewards to incentivize the participants. With mobile social networking, a user enjoys an intrinsic benefit when she aligns her behavior with the behavior of others. Referred to as network effects, such an intrinsic benefit becomes more significant as the number of users grows in the crowdsourcing system. But should a crowdsourcer design her extrinsic rewards differently when such network effects are taken into account? In this paper, we, for the first time, consider network effects as a contributing factor to intrinsic rewards, and study its influence on the design of extrinsic rewards. Rather than assuming a fixed participant population, we show that the number of participating users evolves to a steady equilibrium, thanks to subtle interactions between intrinsic rewards due to network effects and extrinsic rewards offered by the crowdsourcer. Taken network effects into consideration, we design progressively more sophisticated extrinsic reward mechanisms, and propose new and optimal strategies for a crowdsourcer to obtain a higher utility. Via extensive simulations, we demonstrate that with our new strategies, a crowdsourcer is able to attract more participants with higher contributed efforts; and participants gain higher utilities from both intrinsic and extrinsic rewards.},
booktitle = {IEEE INFOCOM 2016 - The 35th Annual IEEE International Conference on Computer Communications},
pages = {1–9},
numpages = {9},
location = {San Francisco, CA, USA}
}

@inproceedings{10.1109/RO-MAN53752.2022.9900685,
author = {Gonzalez, Antonio Galiza Cerdeira and Lo, WingSum and Mizuuchi, Ikuo},
title = {Talk to Kotaro: a web crowdsourcing study on the impact of phone and prosody choice for synthesized speech on human impression},
year = {2022},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/RO-MAN53752.2022.9900685},
doi = {10.1109/RO-MAN53752.2022.9900685},
abstract = {During the COVID-19 pandemic, many research areas that require in person experiments with human volunteers have been impacted due to lockdowns and other activity-restricting policies. The field of robotics is no exception, and specially human-robot interaction research has been severely impacted. In order to circumvent the difficulty of gathering volunteers in person to interact with a robot, we have decided to build a novel crowdsourcing web platform for hosting our "Talk to Kotaro" experiment. The experiment consists of volunteers talking to a robot avatar and reacting to its semantic-free utterances. The developed web platform, which was built using the Python Flask framework, allows for such interactions while recording audio and video and other relevant data, which will be used for studying human impression estimation on gibberish speech. This paper describes not only the experiment and its preliminary results, but the developed platform itself; such tool is essential during pandemics and very useful for regular times, because it enables crowdsourcing data from all over the world.},
booktitle = {2022 31st IEEE International Conference on Robot and Human Interactive Communication (RO-MAN)},
pages = {244–251},
numpages = {8},
location = {Napoli, Italy}
}

@inproceedings{10.1145/3251806,
author = {Lawson, Shaun},
title = {Session details: Crowdsourcing Fans \&amp; Friends},
year = {2015},
isbn = {9781450331456},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3251806},
doi = {10.1145/3251806},
booktitle = {Proceedings of the 33rd Annual ACM Conference on Human Factors in Computing Systems},
location = {Seoul, Republic of Korea},
series = {CHI '15}
}

@inproceedings{10.1007/978-3-030-01391-2_1,
author = {Chai, Yan-sheng and Ma, Huang-lei and Xing, Lin-quan and Wang, Xu and Li, Bo-han},
title = {Implementation of Bus Value-Added Service Platform via Crowdsourcing Incentive},
year = {2018},
isbn = {978-3-030-01390-5},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-01391-2_1},
doi = {10.1007/978-3-030-01391-2_1},
abstract = {Sharing economy is prevailing. The network of cars and shared bicycles is convenient for people to travel. We investigate the issue of value-added service based on crowdsourcing for campus shuttles. We can provide diverse services between users by solving matching problems. The service concludes positioning and location services, requesting designating. The efficient incentive mechanisms make the shuttle bus transportation parcel convenient. We use KNN algorithm to establish KD tree to index different parcels nodes. In our app demo, we show how the application execute and how to improve the user experience who involve the orders.},
booktitle = {Advances in Conceptual Modeling: ER 2018 Workshops Emp-ER, MoBiD, MREBA, QMMQ, SCME, Xi’an, China, October 22-25, 2018, Proceedings},
pages = {3–6},
numpages = {4},
keywords = {Spatio-temporal, KD-tree, Crowdsourcing, Incentive},
location = {Xi'an, China}
}

@inproceedings{10.1007/978-3-642-41338-4_30,
author = {Mortensen, Jonathan M.},
title = {Crowdsourcing Ontology Verification},
year = {2013},
isbn = {9783642413377},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-642-41338-4_30},
doi = {10.1007/978-3-642-41338-4_30},
abstract = {As the scale and complexity of ontologies increases, so too do errors and engineering challenges. It is frequently unclear, however, to what degree extralogical ontology errors negatively affect the application that the ontology underpins. For example, "Shoe SubClassOf Foot" may be correct logically, but not in a human interpretation. Indeed, such errors, not caught by reasoning, are likely to be domain-specific, and thus identifying salient ontology errors requires consideration of the domain. There are both automated and manual methods that provide ontology quality assurance. Nevertheless, these methods do not readily scale as ontology size increases, and do not necessarily identify the most salient extralogical errors. Recently, crowdsourcing has enabled solutions to complex problems that computers alone cannot solve. For instance, human workers can quickly and more accurately identify objects in images at scale. Crowdsourcing presents an opportunity to develop methods for ontology quality assurance that overcome the current limitations of scalability and applicability. In this work, I aim (1) to determine the effect of extralogical ontology errors in an example domain, (2) to develop a scalable framework for crowdsourcing ontology verification that overcomes current ontology Q/A method limitations, and (3) to apply this framework to ontologies in use. I will then evaluate the method itself and also its effect in the context of a specific domain. As an example domain, I will use biomedicine, which applies many large-scale ontologies. Thus, this work will enable scalable quality assurance for extralogical errors in biomedical ontologies. Terminology},
booktitle = {Proceedings of the 12th International Semantic Web Conference - Part II},
pages = {448–455},
numpages = {8},
series = {ISWC '13}
}

@inproceedings{10.1145/2897659.2897664,
author = {Weidema, Edgar R. Q. and L\'{o}pez, Consuelo and Nayebaziz, Sahand and Spanghero, Fernando and van der Hoek, Andr\'{e}},
title = {Toward microtask crowdsourcing software design work},
year = {2016},
isbn = {9781450341585},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2897659.2897664},
doi = {10.1145/2897659.2897664},
abstract = {The use of crowdsourcing as an approach for performing software engineering work is slowly but surely gaining foothold. Different models of crowdsourcing, however, have had varying levels of success to date. This paper contributes to the discussion a preliminary exploration of microtask crowdsourcing and its potential to generate design solutions. We specifically report on an experiment with Amazon Mechanical Turk workers, who each provided one or more solution alternatives to a small, partial user interface design problem. Early analysis of the results indicates that: (1) it is feasible for a crowd to generate a broad range of alternative solutions, (2) quality of those solutions varies considerably, and (3) the task, despite being small, is seen as difficult by many workers.},
booktitle = {Proceedings of the 3rd International Workshop on CrowdSourcing in Software Engineering},
pages = {41–44},
numpages = {4},
keywords = {software design, microtasks, crowdsourcing, alternatives},
location = {Austin, Texas},
series = {CSI-SE '16}
}

@inproceedings{10.5555/3017447.3017508,
author = {Goh, Dion Hoe-Lian and Pe-Than, Ei Pa Pa and Lee, Chei Sian},
title = {Crowdsourcing mobile content through games: an analysis of contribution patterns},
year = {2016},
publisher = {American Society for Information Science},
address = {USA},
abstract = {Crowdsourcing of mobile content has become a major way of populating information-rich online environments. One approach to motivate participation is via games. That is, a crowdsourcing game is built upon the desire of individuals to be entertained while generating useful outputs as byproducts of gameplay. A gap in current research is that actual usage patterns of crowdsourcing games have not been investigated adequately. We address this gap by comparing content creation patterns in a game for crowdsourcing mobile content against a non-game version. Our analysis of 3024 contributions in both apps reveal 10 categories, divided into: (1) those that conform more to the notion of mobile content utilized to learn about a specific place or for navigational purposes; and (2) those that were about the content creator himself/herself, or in relation to other users or other non-playing individuals, with the location as a backdrop, similar to status updates in social media platforms like Twitter. We argue that both categories are potentially useful in that they meet different needs, and together could serve to recruit and sustain participation in the longer term. Further, the distribution of categories varied across the apps, indicating that the features afforded by games shape behavior differently from non-game-based approaches to crowdsourcing.},
booktitle = {Proceedings of the 79th ASIS&amp;T Annual Meeting: Creating Knowledge, Enhancing Lives through Information \&amp; Technology},
articleno = {61},
numpages = {10},
keywords = {mobile content, human computation, evaluation, crowdsourcing games, content analysis},
location = {Copenhagen, Denmark},
series = {ASIST '16}
}

@inproceedings{10.1145/2998181.2998197,
author = {Law, Edith and Gajos, Krzysztof Z. and Wiggins, Andrea and Gray, Mary L. and Williams, Alex},
title = {Crowdsourcing as a Tool for Research: Implications of Uncertainty},
year = {2017},
isbn = {9781450343350},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2998181.2998197},
doi = {10.1145/2998181.2998197},
abstract = {Numerous crowdsourcing platforms are now available to support research as well as commercial goals. However, crowdsourcing is not yet widely adopted by researchers for generating, processing or analyzing research data. This study develops a deeper understanding of the circumstances under which crowdsourcing is a useful, feasible or desirable tool for research, as well as the factors that may influence researchers' decisions around adopting crowdsourcing technology. We conducted semi-structured interviews with 18 researchers in diverse disciplines, spanning the humanities and sciences, to illuminate how research norms and practitioners' dispositions were related to uncertainties around research processes, data, knowledge, delegation and quality. The paper concludes with a discussion of the design implications for future crowdsourcing systems to support research.},
booktitle = {Proceedings of the 2017 ACM Conference on Computer Supported Cooperative Work and Social Computing},
pages = {1544–1561},
numpages = {18},
keywords = {interviews, crowdsourcing for research, citizen science},
location = {Portland, Oregon, USA},
series = {CSCW '17}
}

@inproceedings{10.5555/3016387.3016616,
author = {Yu, Han and Miao, Chunyan and Liu, Siyuan and Pan, Zhengxiang and Khalid, N. Syahidah B. and Shen, Zhiqi and Leung, Cyril},
title = {Productive aging through intelligent personalized crowdsourcing},
year = {2016},
publisher = {AAAI Press},
abstract = {The current generation of senior citizens are enjoying unparalleled levels of good health than previous generations. The need for personal fulfilment after retirement has driven many of them to participate in productive aging activities such as volunteering. This paper outlines the Silver Productive (SP) mobile app, a system powered by the RTS-P intelligent personalized task sub-delegation approach with dynamic worker effort pricing functions. It provides an algorithmic crowd-sourcing platform to enable seniors to contribute their effort through productive aging activities and help organizations efficiently utilize seniors' collective productivity.},
booktitle = {Proceedings of the Thirtieth AAAI Conference on Artificial Intelligence},
pages = {4405–4406},
numpages = {2},
location = {Phoenix, Arizona},
series = {AAAI'16}
}

@inproceedings{10.1145/3340531.3411863,
author = {Qiu, Chenxi and Squicciarini, Anna and Li, Zhuozhao and Pang, Ce and Yan, Li},
title = {Time-Efficient Geo-Obfuscation to Protect Worker Location Privacy over Road Networks in Spatial Crowdsourcing},
year = {2020},
isbn = {9781450368599},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3340531.3411863},
doi = {10.1145/3340531.3411863},
abstract = {To promote cost-effective task assignment in Spatial Crowdsourcing (SC), workers are required to report their location to servers, which raises serious privacy concerns. As a solution, geo-obfuscation has been widely used to protect the location privacy of SC workers, where workers are allowed to report perturbed location instead of the true location. Yet, most existing geo-obfuscation methods consider workers? mobility on a 2 dimensional (2D) plane, wherein workers can move in arbitrary directions. Unfortunately, 2D-based geo-obfuscation is likely to generate high traveling cost for task assignment over roads, as it cannot accurately estimate the traveling costs distortion caused by location obfuscation. In this paper, we tackle the SC worker location privacy problem over road networks. Considering the network-constrained mobility features of workers, we describe workers? mobility by a weighted directed graph, which considers the dynamic traffic condition and road network topology. Based on the graph model, we design a geo-obfuscation (GO) function for workers to maximize the workers? overall location privacy without compromising the task assignment efficiency. We formulate the problem of deriving the optimal GO function as a linear programming (LP) problem. By using the angular block structure of the LP's constraint matrix, we apply Dantzig-Wolfe decomposition to improve the time-efficiency of the GO function generation. Our experimental results in the real-trace driven simulation and the real-world experiment demonstrate the effectiveness of our approach in terms of both privacy and task assignment efficiency.},
booktitle = {Proceedings of the 29th ACM International Conference on Information \&amp; Knowledge Management},
pages = {1275–1284},
numpages = {10},
keywords = {spatial crowdsourcing, location privacy, geo-obfuscation},
location = {Virtual Event, Ireland},
series = {CIKM '20}
}

@inproceedings{10.1145/2910896.2925455,
author = {Goh, Dion Hoe-Lian and Pe-Than, Ei Pa Pa and Lee, Chei Sian},
title = {Games for Crowdsourcing Mobile Content: An Analysis of Contribution Patterns},
year = {2016},
isbn = {9781450342292},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2910896.2925455},
doi = {10.1145/2910896.2925455},
abstract = {Crowdsourcing of mobile content through games is becoming a major way of populating information-rich online environments. A current research gap is that actual usage patterns of crowdsourcing games has been inadequately investigated. We address this gap by comparing content creation patterns in a game for crowdsourcing mobile content against a non-game version. Results show distinct differences in the types and distribution of content created.},
booktitle = {Proceedings of the 16th ACM/IEEE-CS on Joint Conference on Digital Libraries},
pages = {249–250},
numpages = {2},
keywords = {mobile content, crowdsourcing games, content analysis},
location = {Newark, New Jersey, USA},
series = {JCDL '16}
}

@inproceedings{10.1145/3459043.3459059,
author = {Zhu, Mei-Li},
title = {Project-based learning model design based on crowdsourcing— Take“Game Project Development Practice”as an example∗},
year = {2021},
isbn = {9781450389617},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3459043.3459059},
doi = {10.1145/3459043.3459059},
abstract = {In the process of practical teaching, it needs to include the enthusiasm of students, teachers and teaching environment. Based on the concept of crowdsourcing, combined with the characteristics of digital media professional practice education, this paper studies and constructs the project-based learning teaching model based on crowdsourcing. Teachers and students are regarded as crowdsourcing users. By designing appropriate crowdsourcing strategies and assessment and reward mechanisms, teachers' participation and integration in practical courses can be improved, and students' enthusiasm for learning and practice can be stimulated.},
booktitle = {Proceedings of the 2021 2nd International Conference on Education Development and Studies},
pages = {63–66},
numpages = {4},
keywords = {Project based learning, Practice teaching, Crowdsourcing},
location = {Hilo, HI, USA},
series = {ICEDS '21}
}

@inproceedings{10.1145/2968219.2968591,
author = {Hosio, Simo and Goncalves, Jorge and van Berkel, Niels and Klakegg, Simon},
title = {Crowdsourcing situated \&amp; subjective knowledge for decision support},
year = {2016},
isbn = {9781450344623},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2968219.2968591},
doi = {10.1145/2968219.2968591},
abstract = {In this paper we present a study on crowdsourcing subjective knowledge. We introduce a mobile app that was built for this purpose, and compare results from two datasets collected using the app. One dataset was collected during a workshop and the other one during a one-week long field trial. We present interview findings on mobile knowledge collection. Further, we discuss the types of information that should optimally be collected on the go, and show how our data analysis supports the qualitative findings. This work directly continues our earlier efforts on creating a platform that encapsulates wisdom of the crowd for decision support.},
booktitle = {Proceedings of the 2016 ACM International Joint Conference on Pervasive and Ubiquitous Computing: Adjunct},
pages = {1478–1483},
numpages = {6},
keywords = {wisdom of the crowd, smartphones, mobile crowdsourcing, decision support, crowdsourcing, android},
location = {Heidelberg, Germany},
series = {UbiComp '16}
}

@inproceedings{10.1109/HICSS.2016.235,
author = {Straub, Tim and Teubner, Timm and Weinhardt, Christof},
title = {Risk Taking in Online Crowdsourcing Tournaments},
year = {2016},
isbn = {9780769556703},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/HICSS.2016.235},
doi = {10.1109/HICSS.2016.235},
abstract = {Rankings and tournaments are often used to incentivize task completion and participation in online innovation and design contests and prediction markets. One of the main challenges for platform operators is to encourage high quality contributions and effort. In this study we illustrate that in such tournaments, the participants' ranks interfere with risk taking behavior. We present an online experiment accompanying the FIFA World Cup 2014, considering the interplay of different tournament modes (individual and team rankings), the relative rank, tournament progress, and risk taking. We find that subjects take higher risk as the tournament progresses, where this increase is stronger for subjects competing individually, compared to those competing as teams.},
booktitle = {Proceedings of the 2016 49th Hawaii International Conference on System Sciences (HICSS)},
pages = {1851–1860},
numpages = {10},
series = {HICSS '16}
}

@inproceedings{10.1145/3415088.3415094,
author = {Kahasha, Emmanuella Iranga and Zuva, Tranos},
title = {Mobile crowdsourcing in crop production for farmers in rural areas of the south kivu (DRC)},
year = {2020},
isbn = {9781450375580},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3415088.3415094},
doi = {10.1145/3415088.3415094},
abstract = {A lack of an effective way to collect farm produce data, record farm input expenses, as well as expenditure on farm chemicals and receive information from other stakeholders (e.g. agriculture advisers) are some challenges encountered by farmers on their daily basis [1]. Gaining access to information and communication technologies can alleviate some of their problems and allow them to benefit from earlier unexploited opportunities [2]. In this research, we studied the factors that influenced farmers in the adoption of mobile crowdsourcing portals for agriculture purposes. A model was used to measure the perception of farmers about the technology after having used the technology for one season. The results showed that there is a strong relationship between the multiple independent factors in the model find the dependent variable "intention to use" for mobile crowdsourcing portals for agriculture. We conclude that mobile crowdsourcing application is perceived highly in enhancing the agricultural development in remote areas with regards to data accessibility, agriculture development in crop production and providing support in decision-making processes.},
booktitle = {Proceedings of the 2nd International Conference on Intelligent and Innovative Computing Applications},
articleno = {6},
numpages = {6},
keywords = {mobile crowdsourcing, mobile application, crop production},
location = {Plaine Magnien, Mauritius},
series = {ICONIC '20}
}

@inproceedings{10.1007/978-3-642-30284-8_43,
author = {Karampinas, Dimitris and Triantafillou, Peter},
title = {Crowdsourcing taxonomies},
year = {2012},
isbn = {9783642302831},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-642-30284-8_43},
doi = {10.1007/978-3-642-30284-8_43},
abstract = {Taxonomies are great for organizing and searching web content. As such, many popular classes of web applications, utilize them. However, their manual generation and maintenance by experts is a time-costly procedure, resulting in static taxonomies. On the other hand, mining and statistical approaches may produce low quality taxonomies. We thus propose a drastically new approach, based on the proven, increased human involvement and desire to tag/annotate web content. We define the required input from humans in the form of explicit structural, e.g., supertype-subtype relationships between concepts. Hence we harvest, via common annotation practices, the collective wisdom of users with respect to the (categorization of) web content they share and access. We further define the principles upon which crowdsourced taxonomy construction algorithms should be based. The resulting problem is NP-Hard. We thus provide and analyze heuristic algorithms that aggregate human input and resolve conflicts. We evaluate our approach with synthetic and real-world crowdsourcing experiments and on a real-world taxonomy.},
booktitle = {Proceedings of the 9th International Conference on The Semantic Web: Research and Applications},
pages = {545–559},
numpages = {15},
keywords = {taxonomy, tagging, crowdsourcing, collective intelligence},
location = {Heraklion, Crete, Greece},
series = {ESWC'12}
}

@inproceedings{10.1109/EATIS.2016.7520143,
author = {Barroso, Bruno L. K. and de Oliveira, Rodolfo R. and Macedo, Hendrik T.},
title = {Mobile crowdsourcing app for smart cities},
year = {2016},
isbn = {9781509024360},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/EATIS.2016.7520143},
doi = {10.1109/EATIS.2016.7520143},
abstract = {As current changes in population disposition are increasingly focused on cities and mega-cities, problems associated with rapid urbanization and high population density are becoming very evident and are affecting the lives of millions of people. Thus, the development of solutions that make the organization and use of urban space more efficient is highly necessary. From this need arises the concept of Smart Cities. Upon searching for applications that embodied the Smart Cities concept in the context of reporting issues in public spaces, a new application was developed. GO! Cidade is an innovative application for iOS which uses the power of mobile crowdsourcing to enable collaborative mapping of the problems in public spaces such as parks, streets or roads. To verify its effectiveness, this application was tested for two months by four people in the city of Aracaju-SE, Brazil.},
booktitle = {Proceedings of the 2016 8th Euro American Conference on Telematics and Information Systems (EATIS)},
pages = {49}
}

@inproceedings{10.1145/2556288.2556996,
author = {Vaish, Rajan and Wyngarden, Keith and Chen, Jingshu and Cheung, Brandon and Bernstein, Michael S.},
title = {Twitch crowdsourcing: crowd contributions in short bursts of time},
year = {2014},
isbn = {9781450324731},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2556288.2556996},
doi = {10.1145/2556288.2556996},
abstract = {To lower the threshold to participation in crowdsourcing, we present twitch crowdsourcing: crowdsourcing via quick contributions that can be completed in one or two seconds. We introduce Twitch, a mobile phone application that asks users to make a micro-contribution each time they unlock their phone. Twitch takes advantage of the common habit of turning to the mobile phone in spare moments. Twitch crowdsourcing activities span goals such as authoring a census of local human activity, rating stock photos, and extracting structured data from Wikipedia pages. We report a field deployment of Twitch where 82 users made 11,240 crowdsourcing contributions as they used their phone in the course of everyday life. The median Twitch activity took just 1.6 seconds, incurring no statistically distinguishable costs to unlock speed or cognitive load compared to a standard slide-to-unlock interface.},
booktitle = {Proceedings of the SIGCHI Conference on Human Factors in Computing Systems},
pages = {3645–3654},
numpages = {10},
keywords = {mobile crowdsourcing, microtasking, crowdsourcing},
location = {Toronto, Ontario, Canada},
series = {CHI '14}
}

@inproceedings{10.1145/3077136.3080679,
author = {Zhang, Jing and Sheng, Victor S. and Li, Tao},
title = {Label Aggregation for Crowdsourcing with Bi-Layer Clustering},
year = {2017},
isbn = {9781450350228},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3077136.3080679},
doi = {10.1145/3077136.3080679},
abstract = {This paper proposes a novel general label aggregation method for both binary and multi-class labeling in crowdsourcing, namely Bi-Layer Clustering (BLC), which clusters two layers of features - the conceptual-level and the physical-level features - to infer true labels of instances. BLC first clusters the instances using the conceptual-level features extracted from their multiple noisy labels and then performs clustering again using the physical-level features. It can facilitate tracking the uncertainty changes of the instances, so that the integrated labels that are likely to be falsely inferred on the conceptual layer can be easily corrected using the estimated labels on the physical layer. Experimental results on two real-world crowdsourcing data sets show that BLC outperforms seven state-of-the-art methods.},
booktitle = {Proceedings of the 40th International ACM SIGIR Conference on Research and Development in Information Retrieval},
pages = {921–924},
numpages = {4},
keywords = {label aggregation, inference, crowdsourcing, clustering},
location = {Shinjuku, Tokyo, Japan},
series = {SIGIR '17}
}

@inproceedings{10.1145/3205651.3205796,
author = {Wang, Han and Ren, Zhilei and Li, Xiaochen and Chen, Xin and Jiang, He},
title = {Solving team making problem for crowdsourcing with hybrid metaheuristic algorithm},
year = {2018},
isbn = {9781450357647},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3205651.3205796},
doi = {10.1145/3205651.3205796},
abstract = {For a typical crowdsourcing process, a task publisher first publishes a task with an acceptable budget. Then hundreds of crowdsourced workers apply for the task with their desired bids. To recruit an adequate Crowdsourced Virtual Team (CVT) while balancing the profits of the task publisher and crowdsourced workers, previous studies proposed various algorithms, including Genetic Algorithm (GA), Alternating Variable Method (AVM), etc. However, the performance is still limited. In this study, we propose a novel hybrid metaheuristic algorithm CVTMaker to help publishers identify ideal CVTs. CVTMaker is effective which combines (1+1) Evolutionary Strategy ((1+1)-ES) and AVM to search solutions. Experimental results show that CVTMaker significantly outperforms GA and AVM over 3,117 and 5,642 of the 6,000 instances respectively.},
booktitle = {Proceedings of the Genetic and Evolutionary Computation Conference Companion},
pages = {318–319},
numpages = {2},
keywords = {virtual team making, local search, hybrid meta-heuristic algorithm, evolution strategy, crowdsourcing},
location = {Kyoto, Japan},
series = {GECCO '18}
}

@inproceedings{10.1145/3151759.3151844,
author = {Al-Matham, Rawan N. and Al-Khalifa, Hend S.},
title = {A crowdsourcing web-based system for reporting predatory publishers},
year = {2017},
isbn = {9781450352994},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3151759.3151844},
doi = {10.1145/3151759.3151844},
abstract = {With the increasing number of predatory publishers that involves charging publication fees to authors without providing proper peer-review or editorial refereeing to qualify papers for publication; The need for a web-based system that involves the participation of authors and researchers in reporting such publishers is becoming a must.In this paper, we present the design and implementation of a web-based system for reporting predatory publishers. The system utilizes crowdsourcing to populate its database of predatory publishers. Also, the system helps researchers avoid the pitfall of publishing their research work in untrustworthy venues. Finally, the system provides browser add-ons (for Chrome and Firefox) to seamlessly notify researchers while browsing the web of predatory publishers' websites.},
booktitle = {Proceedings of the 19th International Conference on Information Integration and Web-Based Applications \&amp; Services},
pages = {573–576},
numpages = {4},
keywords = {web-based systems, predatory publishers, open access publishers, crowdsourcing, browser add-ons},
location = {Salzburg, Austria},
series = {iiWAS '17}
}

@inproceedings{10.1145/2740908.2741747,
author = {AlShehry, Majid Ali and Ferguson, Bruce Walker},
title = {A Taxonomy of Crowdsourcing Campaigns},
year = {2015},
isbn = {9781450334730},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2740908.2741747},
doi = {10.1145/2740908.2741747},
abstract = {Crowdsourcing serves different needs of different sets of users. Most existing definitions and taxonomies of crowdsourcing address platform purpose while paying little attention to other parameters of this novel social phenomenon. In this paper, we analyze 41 crowdsourcing campaigns on 21 crowdsourcing platforms to derive 9 key parameters of successful crowdsourcing campaigns and introduce a comprehensive taxonomy of crowdsourcing. Using this taxonomy, we identify crowdsourcing trends in two parameters, platform purpose and contributor motivation. The paper highlights important advantages of using this conceptual model in planning crowdsourcing campaigns and concludes with a discussion of emerging challenges to such campaigns.},
booktitle = {Proceedings of the 24th International Conference on World Wide Web},
pages = {475–479},
numpages = {5},
keywords = {crowdsourcing, crowdfunding, collaboration platform},
location = {Florence, Italy},
series = {WWW '15 Companion}
}

@inproceedings{10.1145/3491101.3519744,
author = {Lin, Fang-Yu and Lee, Chia-Yi and Ho, Yi-Ting and Chen, Yao-Kuang and Yen, Grace Yu-Chun and Chang, Yung-Ju},
title = {What Kinds of Experiences Do You Desire? A Preliminary Study of the Desired Experiences of Contributors to Location-Based Mobile Crowdsourcing},
year = {2022},
isbn = {9781450391566},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3491101.3519744},
doi = {10.1145/3491101.3519744},
abstract = {Mobile crowdsourcing enables people to learn location-related information from others with diverse experiences and opinions. However, little research has investigated the expected quality of the location-related information users of mobile-crowdsourcing platforms, and the levels and types of relevant experience such users expect crowd members to possess, respectively. To fill this gap, we first conducted an interview study with 22 participants, which yielded five key information properties of the answers to location-based questions: objectivity, relativity, specificity, temporal regularity, and variability. Based on his//her stated perceptions of these properties of the requested information, we deemed each participant to desire at least one, and up to 10 main qualities of the information, and seven main aspects of contributors’ experience. A follow-up survey study was then used to quantify the characteristics of a list of location-related information according to the information properties that the 139 respondents perceived that information to have.},
booktitle = {Extended Abstracts of the 2022 CHI Conference on Human Factors in Computing Systems},
articleno = {220},
numpages = {7},
keywords = {review, mobile crowdsourcing, location-based, information quality},
location = {New Orleans, LA, USA},
series = {CHI EA '22}
}

@inproceedings{10.5555/3295222.3295339,
author = {T\'{a}nczos, Ervin and Nowak, Robert and Mankoff, Bob},
title = {A KL-LUCB bandit algorithm for large-scale crowdsourcing},
year = {2017},
isbn = {9781510860964},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {This paper focuses on best-arm identification in multi-armed bandits with bounded rewards. We develop an algorithm that is a fusion of lil-UCB and KL-LUCB, offering the best qualities of the two algorithms in one method. This is achieved by proving a novel anytime confidence bound for the mean of bounded distributions, which is the analogue of the LIL-type bounds recently developed for sub-Gaussian distributions. We corroborate our theoretical results with numerical experiments based on the New Yorker Cartoon Caption Contest.},
booktitle = {Proceedings of the 31st International Conference on Neural Information Processing Systems},
pages = {5896–5905},
numpages = {10},
location = {Long Beach, California, USA},
series = {NIPS'17}
}

@inproceedings{10.1145/3512576.3512617,
author = {Lawas, Leodivino and Dalino Gorro, Ken and Ranolo, Elmo and Ilano, Anthony},
title = {Exploring and Analyzing Facebook as crowdsourcing platform for traffic updates using Selenium Support Vector Machine and Non-parametric LDA},
year = {2022},
isbn = {9781450384971},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3512576.3512617},
doi = {10.1145/3512576.3512617},
abstract = {Traffic is a major problem in the Philippines. Facebook is one of the social media platforms that is commonly used by Filipinos. Machine learning is a field of computer science that allows computers to perform tasks like human beings. In this study, the proponents explored Facebook as a source of traffic updates and as a source of traffic information. In this paper, as a partial result, a machine learning model was created to classify Facebook posts as related to traffic. To gather Facebook posts, a total of 1000 respondents were asked for consent to scrape their public post using the username link and selenium. The Support vector machine model was trained with 3000 Facebook posts. The SVM model was only trained to 3 classes {Road accident, Road activities and Other}. The SVM model was evaluated using 10-cross fold validation. The result shows that the accuracy is 76\% and the recall is 69\%. To analyze the narrative of the corpus, the Hierarchical Dirichlet Process model was created with the log-likelihood of -4.06 with 10 topic models. The following are the narratives of the corpus: {Traffic Management, Immediate Emergency Response, Seeking help, Busses causes majority of accidents.}},
booktitle = {Proceedings of the 2021 9th International Conference on Information Technology: IoT and Smart City},
pages = {226–230},
numpages = {5},
keywords = {Social Media, LDA, HDP, Facebook},
location = {Guangzhou, China},
series = {ICIT '21}
}

@inproceedings{10.1007/978-3-030-19274-7_15,
author = {Chen, Rong and Li, Bo and Xing, Hu and Wang, Yijing},
title = {CrowDIY: How to Design and Adapt Collaborative Crowdsourcing Workflows Under Budget Constraints},
year = {2019},
isbn = {978-3-030-19273-0},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-19274-7_15},
doi = {10.1007/978-3-030-19274-7_15},
abstract = {Workflow quality is a key determinant of crowdsourcing complex work, but finding ways to task design and plan has proved illusive. Instead, we formulate it as an optimization problem with budget constraints and fewer decision variables to set. We propose a two-staged approach CrowDIY that can not only estimate task attributes based on previous tasks but also optimize them with budget constraints in order to publish tasks more wisely in a timely manner. Several experimental studies have been conducted, and the results show compelling evidence that, under different conditions, the proposed approach can effectively reduce the workload of workflow design and plan, while avoiding commonly encountered trial-and-error in crowdsourcing workflows and leading up to successful complex outcomes.},
booktitle = {Web Engineering: 19th International Conference, ICWE 2019, Daejeon, South Korea, June 11–14, 2019, Proceedings},
pages = {203–210},
numpages = {8},
keywords = {Crowdsourcing workflow, Workflow design and plan, Task publishing, Optimization},
location = {Daejeon, Korea (Republic of)}
}

@inproceedings{10.1145/3210240.3210804,
author = {Walelgne, Ermias A. and Asrese, Alemnew S. and Bajpai, Vaibhav and Ott, J\"{o}rg and Manner, Jukka},
title = {Using Crowdsourcing Data for Adaptive Video Streaming in Cellular Network},
year = {2018},
isbn = {9781450357203},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3210240.3210804},
doi = {10.1145/3210240.3210804},
booktitle = {Proceedings of the 16th Annual International Conference on Mobile Systems, Applications, and Services},
pages = {512},
numpages = {1},
location = {Munich, Germany},
series = {MobiSys '18}
}

@inproceedings{10.5555/2772879.2773301,
author = {Kamar, Ece and Horvitz, Eric},
title = {Planning for Crowdsourcing Hierarchical Tasks},
year = {2015},
isbn = {9781450334136},
publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
address = {Richland, SC},
abstract = {We show how machine vision, learning, and planning can be combined to solve hierarchical consensus tasks. Hierarchical consensus tasks seek correct answers to a hierarchy of subtasks, where branching depends on answers at preceding levels of the hierarchy. We construct a set of hierarchical classification models that aggregate machine and human effort on different subtasks and use these inferences in planning. Optimal solution of hierarchical tasks is intractable due to the branching of task hierarchy and the long horizon of these tasks. We study Monte Carlo planning procedures that can exploit task structure to constrain the policy space for tractability. We evaluate the procedures on data collected from Galaxy Zoo II in allocating human effort and show that significant gains can be achieved.},
booktitle = {Proceedings of the 2015 International Conference on Autonomous Agents and Multiagent Systems},
pages = {1191–1199},
numpages = {9},
keywords = {monte carlo planning, mdps, crowdsourcing, consensus tasks, complementary computing},
location = {Istanbul, Turkey},
series = {AAMAS '15}
}

@inproceedings{10.1145/3265689.3265717,
author = {Jiang, Yun and Cui, Lizhen and Cao, Yiming and Liu, Lei and He, Wei and Pan, Li and Zheng, Yongqing and Li, Qingzhong},
title = {Spatial Crowdsourcing Task Assignment Based on the Quality of Workers},
year = {2018},
isbn = {9781450365871},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3265689.3265717},
doi = {10.1145/3265689.3265717},
abstract = {With the rapid development of mobile Internet, a variety of spatial crowdsourcing platforms have emerged and been widely applied. Task assignment is the core issue of spatial crowdsourcing. The existing methods of task assignment aim at assigning tasks to workers as much as possible, which lacks the guarantee of the quality of the tasks' answer. In this paper, two kinds of task assignment strategy based on the quality of workers are proposed to ensure the accuracy of the answer submitted by the workers as high as possible. The classical quality control algorithm, Incremental Quality Inference, is used to obtain the quality of workers. Capable worker strategy and maximum worker distance-quality strategy are proposed and compared with nearest work strategy to carry out task assignment based on the quality of workers computed by Incremental Quality Inference. Experimental results with discounted data in the offline shopping mall from crowdsourcing platform demonstrate the effectiveness of our approach.},
booktitle = {Proceedings of the 3rd International Conference on Crowd Science and Engineering},
articleno = {28},
numpages = {6},
keywords = {worker quality, task assignment strategy, spatial crowdsourcing},
location = {Singapore, Singapore},
series = {ICCSE'18}
}

@inproceedings{10.1145/3255630,
author = {Karahalios, Karrie},
title = {Session details: Crowdsourcing complexity},
year = {2014},
isbn = {9781450325400},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3255630},
doi = {10.1145/3255630},
booktitle = {Proceedings of the 17th ACM Conference on Computer Supported Cooperative Work \&amp; Social Computing},
location = {Baltimore, Maryland, USA},
series = {CSCW '14}
}

@inproceedings{10.1145/3132525.3134827,
author = {Gleason, Cole},
title = {Crowdsourcing the Installation and Maintenance of Indoor Navigation Infrastructure},
year = {2017},
isbn = {9781450349260},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3132525.3134827},
doi = {10.1145/3132525.3134827},
abstract = {Indoor navigation systems, especially for people with visual impairments, often require intensive data collection and instrumentation to install and maintain over time. The most accurate navigation systems require experts to install many Bluetooth beacons or similar technologies throughout the environment. After hardware installation, data samples must be collected to construct a signal model of the environment to be navigated. The demands of this intense upfront workload and ongoing updates pose barriers for widespread adoption of navigation systems. This document describes LuzDeploy, a system to break these installation and maintenance workflows into small, simple tasks that be completed by volunteers with little to no training. A Facebook Messenger bot coordinates the overall deployment by assigning volunteers to beacon placement, data collection, or quality assurance tasks that take only a few minutes to complete. These tasks may be batched to do more or less work, depending on the amount of time the volunteer is able to contribute. LuzDeploy allows building owners and managers to distribute the difficult task of installation and maintenance. By making these workloads easier for non-experts, LuzDeploy aims to increase adoption of indoor navigation systems and make unfamiliar spaces more accessible to people who wish to navigate independently.},
booktitle = {Proceedings of the 19th International ACM SIGACCESS Conference on Computers and Accessibility},
pages = {411–412},
numpages = {2},
keywords = {physical crowdsourcing, indoor navigation assistance, individuals with visual impairments},
location = {Baltimore, Maryland, USA},
series = {ASSETS '17}
}

@inproceedings{10.1145/3308560.3317083,
author = {Aroyo, Lora and Dixon, Lucas and Thain, Nithum and Redfield, Olivia and Rosen, Rachel},
title = {Crowdsourcing Subjective Tasks: The Case Study of Understanding Toxicity in Online Discussions},
year = {2019},
isbn = {9781450366755},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3308560.3317083},
doi = {10.1145/3308560.3317083},
abstract = {Discussing things you care about can be difficult, especially via online platforms, where sharing your opinion leaves you open to the real and immediate threats of abuse and harassment. Due to these threats, people stop expressing themselves and give up on seeking different opinions. Recent research efforts focus on examining the strengths and weaknesses (e.g. potential unintended biases) of using machine learning as a support tool to facilitate safe space for online discussions; for example, through detecting various types of negative online behaviors such as hate speech, online harassment, or cyberbullying. Typically, these efforts build upon sentiment analysis or spam detection in text. However, the toxicity of the language could be a strong indicator for the intensity of the negative behavior. In this paper, we study the topic of toxicity in online conversations by addressing the problems of subjectivity, bias, and ambiguity inherent in this task. We start with an analysis of the characteristics of subjective assessment tasks (e.g. relevance judgment, toxicity judgment, sentiment assessment, etc). Whether we perceive something as relevant or as toxic can be influenced by almost infinite amounts of prior or current context, e.g. culture, background, experiences, education, etc. We survey recent work that tries to understand this phenomenon, and we outline a number of open questions and challenges which shape the research perspectives in this multi-disciplinary field.},
booktitle = {Companion Proceedings of The 2019 World Wide Web Conference},
pages = {1100–1105},
numpages = {6},
keywords = {toxicity, subjectivity, crowdsourcing, ACM proceedings},
location = {San Francisco, USA},
series = {WWW '19}
}

@inproceedings{10.1145/3063955.3063989,
author = {Fu, Ningjia and Zhang, Jianzhong and Yu, Wenping and Wang, Changhai},
title = {Crowdsourcing-based wifi fingerprint update for indoor localization},
year = {2017},
isbn = {9781450348737},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3063955.3063989},
doi = {10.1145/3063955.3063989},
abstract = {Researches on indoor localization become more and more popular because human spend more life time indoors than outdoors. Among all of the present indoor localization technologies, WiFi fingerprint localization is the most widely used. The method of fingerprint is based on matching the current received signal strength with fingerprints stored in the database to get user's position. This method can get high precision with the simple operation, but it's a labor-intensive work to acquire the fingerprint database which costs much time and human resources. In this paper, we proposed a crowdsourcing method to build an auto-update fingerprint database using the data fed back by numerous users. First we detect the user's step sequence using inertial sensors built in smartphones. Then we establish a Hidden Markov Model (HMM) and propose a Ratio-based Map Matching(RMM) algorithm to match the step sequence with the real path in the map. After the successful match, we bind each fingerprint collected during a walk to its corresponding position, so the auto-update fingerprint database is generated. We did some experiments in a teaching building to evaluate our proposed method, and the results show the accuracy achieved by the method is related to the length of the step sequence. If the step sequence is long enough, the database we generated is very close to the manual measuring results.},
booktitle = {Proceedings of the ACM Turing 50th Celebration Conference - China},
articleno = {34},
numpages = {9},
keywords = {wifi fingerprint, step sequence, indoor localization, crowdsourcing, RMM, HMM},
location = {Shanghai, China},
series = {ACM TURC '17}
}

@inproceedings{10.1109/INFOCOM.2018.8486277,
author = {Zhang, Jianhui and Lu, Pengqian and Li, Zhi and Gan, Jiayu},
title = {Distributed Trip Selection Game for Public Bike System with Crowdsourcing},
year = {2018},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/INFOCOM.2018.8486277},
doi = {10.1109/INFOCOM.2018.8486277},
abstract = {Public Bike Systems (PBSs) offer convenient and green travel service and become popular around the world. In many cities, the local governments build thousands of fixed stations for PBS to alleviate the city traffic jam and solve the last-mile problem. However, the increasing use of PBSs leads to new congestion problems in the form that users have, such as no bike to rent or no dock to return the bike. Further, users wish to receive assistance on deciding how to select bike trips with minimal time cost while taking congestion into account. Meanwhile, crowdsourcing attracted increasing attention in recent years. This paper applies it to help users share information and select bike trips before the bikes or docks are occupied. An interesting and important problem is how to help users select bike trips so that the time consumed on the trips can be minimized. We model the problem as a Bike Trip Selection (BTS) game which is shown to be equivalent to the symmetric network congestion game. This equivalence allows us to design a BTS algorithm by which the users can find at least one Nash Equilibria (NE) distributively. Furthermore, this paper evaluates the algorithm based on real datasets collected from the PBS of Hangzhou City in China. We also design a BTS system including an Android APP and a server to conduct the experiment for the distributed BTS algorithm in practice,},
booktitle = {IEEE INFOCOM 2018 - IEEE Conference on Computer Communications},
pages = {2717–2725},
numpages = {9},
location = {Honolulu, HI, USA}
}

@inproceedings{10.1109/HICSS.2015.197,
author = {Whitaker, Roger M. and Chorley, Martin and Allen, Stuart M.},
title = {New Frontiers for Crowdsourcing: The Extended Mind},
year = {2015},
isbn = {9781479973675},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/HICSS.2015.197},
doi = {10.1109/HICSS.2015.197},
abstract = {We introduce the concept of extended mind crowd sourcing (EMC) which capitalises on the way in which humans naturally extend their cognition into the environment, using external objects such as smartphones and applications to augment their mental capacity. This phenomenon means that human computation is embedded in data and devices, representing a new way through which human cognition can be accessed for collective discoveries. We relate EMC to existing sociological and psychological concepts and argue that it lies at the intersection of human computation, social computing and crowd sourcing. EMC is a way in which new problems and discoveries can be tackled, for example as necessitated by "wicked" problems, ethnography and culture. We relate EMC to diverse disciplines and point to ways in which the concept may develop in future. We exemplify EMC by presenting a case study where participation in location-based social networks is used to discover the correlation between mobility and human personality traits. This has involved participation from 43 countries and resulted in analysis of over half a million check-ins at street-level locations.},
booktitle = {Proceedings of the 2015 48th Hawaii International Conference on System Sciences},
pages = {1635–1644},
numpages = {10},
keywords = {social computing, personality, participatory computing, networked individualism, location-based social networks, extended mind, distributed cognition, crowdsourcing, Foursquare},
series = {HICSS '15}
}

@inproceedings{10.1145/3254659,
author = {Demartini, Gianluca},
title = {Session details: Crowdsourcing 1},
year = {2014},
isbn = {9781450327442},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3254659},
doi = {10.1145/3254659},
booktitle = {Proceedings of the 23rd International Conference on World Wide Web},
location = {Seoul, Korea},
series = {WWW '14}
}

@inproceedings{10.1145/3183713.3183732,
author = {Xin, Hao and Meng, Rui and Chen, Lei},
title = {Subjective Knowledge Base Construction Powered By Crowdsourcing and Knowledge Base},
year = {2018},
isbn = {9781450347037},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3183713.3183732},
doi = {10.1145/3183713.3183732},
abstract = {Knowledge base construction (KBC) has become a hot and in-time topic recently with the increasing application need of large-scale knowledge bases (KBs), such as semantic search, QA systems, the Google Knowledge Graph and IBM Watson QA System. Existing KBs mainly focus on encoding the factual facts of the world, e.g., city area and company product, which are regarded as the objective knowledge, whereas the subjective knowledge, which is frequently mentioned in Web queries, has been neglected. The subjective knowledge has no documented ground truth, instead, the truth relies on people's dominant opinion, which can be solicited from online crowd workers. In our work, we propose a KBC framework for subjective knowledge base construction taking advantage of the knowledge from the crowd and existing KBs. We develop a two-staged framework for subjective KB construction which consists of core subjective KB construction and subjective KB enrichment. Firstly, we try to build a core subjective KB mined from existing KBs, where every instance has rich objective properties. Then, we populate the core subjective KB with instances extracted from existing KBs, in which the crowd is leverage to annotate the subjective property of the instances. In order to optimize the crowd annotation process, we formulate the problem of subjective KB enrichment procedure as a cost-aware instance annotation problem and propose two instance annotation algorithms, i.e., adaptive instance annotation and batch-mode instance annotation algorithms. We develop a two-stage system for subjective KB construction which consists of core subjective KB construction and subjective knowledge enrichment. We evaluate our framework on real knowledge bases and a real crowdsourcing platform, the experimental results show that we can derive high quality subjective knowledge facts from existing KBs and crowdsourcing techniques through our proposed framework.},
booktitle = {Proceedings of the 2018 International Conference on Management of Data},
pages = {1349–1361},
numpages = {13},
keywords = {subjective knowledge, knowledge base construction, crowdsourcing},
location = {Houston, TX, USA},
series = {SIGMOD '18}
}

@inproceedings{10.1145/3152494.3152495,
author = {Bhattacharyya, Malay and Mridha, Sankar Kumar},
title = {Studying the influence of requesters in posted-price crowdsourcing},
year = {2018},
isbn = {9781450363419},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3152494.3152495},
doi = {10.1145/3152494.3152495},
abstract = {Crowd-powered systems have recently emerged as useful models for solving complex tasks online by combining machine intelligence with crowd intelligence. These models are mainly of two types - collaborative and competitive. Studying the behavior of the participating crowd workers and requester experiences might yield useful insights about both these models. Analyzing the behaviors of crowd workers has been in major focus for the past several years, whereas requester behaviors have rarely been studied. In this paper, we study the activity of requesters in Flightfox, a competitive crowdsourcing environment, that works on posted-price mechanism to gain new insights about the model. We analyze the task completion data from Flightfox to investigate the patterns of interest and activity of the requesters. It came into view that requesters have an important role in the crowd-powered systems. Again, the behavioral psychology of the requesters appear to inspire the sustainability of the studied model. The global pattern of the requester activities is found to reflect scale-free behavior. Overall, we highlighted some interesting influential characteristics of the requesters in crowdsourcing platforms that were hitherto unknown.},
booktitle = {Proceedings of the ACM India Joint International Conference on Data Science and Management of Data},
pages = {1–10},
numpages = {10},
keywords = {network analysis, competitive crowdsourcing, behavioral analysis, Flightfox},
location = {Goa, India},
series = {CODS-COMAD '18}
}

@inproceedings{10.5555/3045390.3045455,
author = {Gao, Chao and Lu, Yu and Zhou, Dengyong},
title = {Exact exponent in optimal rates for crowdsourcing},
year = {2016},
publisher = {JMLR.org},
abstract = {In many machine learning applications, crowd-sourcing has become the primary means for label collection. In this paper, we study the optimal error rate for aggregating labels provided by a set of non-expert workers. Under the classic Dawid-Skene model, we establish matching upper and lower bounds with an exact exponent mI(π) in which m is the number of workers and I(π) the average Chernoff information that characterizes the workers' collective ability. Such an exact characterization of the error exponent allows us to state a precise sample size requirement m &gt; 1/I(π) log 1/ε in order to achieve an ε misclassification error. In addition, our results imply the optimality of various EM algorithms for crowd-sourcing initialized by consistent estimators.},
booktitle = {Proceedings of the 33rd International Conference on International Conference on Machine Learning - Volume 48},
pages = {603–611},
numpages = {9},
location = {New York, NY, USA},
series = {ICML'16}
}

@inproceedings{10.1145/3243082.3267455,
author = {Silva, Ramon and Fonseca, Augusto and Goldschmidt, Ronaldo and dos Santos, Joel and Bezerra, Eduardo},
title = {A Crowdsourcing Tool for Data Augmentation in Visual Question Answering Tasks},
year = {2018},
isbn = {9781450358675},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3243082.3267455},
doi = {10.1145/3243082.3267455},
abstract = {Visual Question Answering (VQA) is a task that connects the fields of Computer Vision and Natural Language Processing. Taking as input an image I and a natural language question Q about I, a VQA model must be able to produce a coherent answer R (also in natural language) to Q. A particular type of visual question is one in which the question is binary (i.e., a question whose answer belongs to the set {yes, no}). Currently, deep neural networks correspond to the state of the art technique for training of VQA models. Despite its success, the application of neural networks to the VQA task requires a very large amount of data in order to produce models with adequate precision. Datasets currently used for the training of VQA models are the result of laborious manual labeling processes (i.e., made by humans). This context makes relevant the study of approaches to augment these datasets in order to train more accurate prediction models. This paper describes a crowdsourcing tool which can be used in a collaborative manner to augment an existing VQA dataset for binary questions. Our tool actively integrates candidate items from an external data source in order to optimize the selection of queries to be presented to curators.},
booktitle = {Proceedings of the 24th Brazilian Symposium on Multimedia and the Web},
pages = {137–140},
numpages = {4},
keywords = {Image Annotation, Human Computation, Data Augmentation, Crowdsourcing},
location = {Salvador, BA, Brazil},
series = {WebMedia '18}
}

@inproceedings{10.1145/3394486.3403125,
author = {Li, Ang and Duan, Yixiao and Yang, Huanrui and Chen, Yiran and Yang, Jianlei},
title = {TIPRDC: Task-Independent Privacy-Respecting Data Crowdsourcing Framework for Deep Learning with Anonymized Intermediate Representations},
year = {2020},
isbn = {9781450379984},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3394486.3403125},
doi = {10.1145/3394486.3403125},
abstract = {The success of deep learning partially benefits from the availability of various large-scale datasets. These datasets are often crowdsourced from individual users and contain private information like gender, age, etc. The emerging privacy concerns from users on data sharing hinder the generation or use of crowdsourcing datasets and lead to hunger of training data for new deep learning applications. One naive solution is to pre-process the raw data to extract features at the user-side, and then only the extracted features will be sent to the data collector. Unfortunately, attackers can still exploit these extracted features to train an adversary classifier to infer private attributes. Some prior arts leveraged game theory to protect private attributes. However, these defenses are designed for known primary learning tasks, the extracted features work poorly for unknown learning tasks. To tackle the case where the learning task may be unknown or changing, we present TIPRDC, a task-independent privacy-respecting data crowdsourcing framework with anonymized intermediate representation. The goal of this framework is to learn a feature extractor that can hide the privacy information from the intermediate representations; while maximally retaining the original information embedded in the raw data for the data collector to accomplish unknown learning tasks. We design a hybrid training method to learn the anonymized intermediate representation: (1) an adversarial training process for hiding private information from features; (2) maximally retain original information using a neural-network-based mutual information estimator. We extensively evaluate TIPRDC and compare it with existing methods using two image datasets and one text dataset. Our results show that TIPRDC substantially outperforms other existing methods. Our work is the first task-independent privacy-respecting data crowdsourcing framework.},
booktitle = {Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery \&amp; Data Mining},
pages = {824–832},
numpages = {9},
keywords = {privacy-respecting data crowdsourcing, deep learning, anonymized intermediate representations},
location = {Virtual Event, CA, USA},
series = {KDD '20}
}

@inproceedings{10.1109/ICST.2013.44,
author = {Dolstra, Eelco and Vliegendhart, Raynor and Pouwelse, Johan},
title = {Crowdsourcing GUI Tests},
year = {2013},
isbn = {9780769549682},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/ICST.2013.44},
doi = {10.1109/ICST.2013.44},
abstract = {Graphical user interfaces are difficult to test: automated tests are hard to create and maintain, while manual tests are time-consuming, expensive and hard to integrate in a continuous testing process. In this paper, we show that it is possible to crowd source GUI tests, that is, to outsource them to individuals drawn from a large pool of workers on the Internet, by instantiating virtual machines (VMs) running the system under test and letting testers access the VMs through their web browsers. This enables semi-automated continuous testing of GUIs and usability experiments with large numbers of participants at low cost. Several large experiments on the Amazon Mechanical Turk demonstrate that our approach is technically feasible and sufficiently reliable.},
booktitle = {Proceedings of the 2013 IEEE Sixth International Conference on Software Testing, Verification and Validation},
pages = {332–341},
numpages = {10},
keywords = {virtualization, usability studies, crowdsourcing, continuous testing, Mechanical Turk, GUI testing},
series = {ICST '13}
}

@inproceedings{10.1145/2800835.2800966,
author = {Goncalves, Jorge and Hosio, Simo and Kostakos, Vassilis and Vukovic, Maja and Konomi, Shin'ichi},
title = {Workshop on mobile and situated crowdsourcing},
year = {2015},
isbn = {9781450335751},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2800835.2800966},
doi = {10.1145/2800835.2800966},
abstract = {Crowdsourcing beyond the desktop is increasingly attracting interest due to the rapid proliferation of smart phones and other ubiquitous technologies, such as public displays. This workshop seeks to investigate the current state of the art of mobile and situated crowdsourcing by bringing together researchers of this thriving research agenda.},
booktitle = {Adjunct Proceedings of the 2015 ACM International Joint Conference on Pervasive and Ubiquitous Computing and Proceedings of the 2015 ACM International Symposium on Wearable Computers},
pages = {1339–1342},
numpages = {4},
keywords = {ubiquitous technologies, situated crowdsourcing, mobile crowdsourcing},
location = {Osaka, Japan},
series = {UbiComp/ISWC'15 Adjunct}
}

@inproceedings{10.1145/2897659.2897666,
author = {Machado, Leticia and Prikladnicki, Rafael and Meneguzzi, Felipe and de Souza, Cleidson R. B. and Carmel, Erran},
title = {Task allocation for crowdsourcing using AI planning},
year = {2016},
isbn = {9781450341585},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2897659.2897666},
doi = {10.1145/2897659.2897666},
abstract = {Crowdsourcing is a relatively new phenomenon in computer science and software engineering. In crowdsourcing a task is delivered to a crowd of participants who will work on this task. Task allocation is then an important aspect in the context of crowdsourcing. If done properly, it delivers successful results based on the answers provided by the crowd. However, task allocation in crowdsourcing is not a trivial problem. Factors like a task's requirements, the knowledge required for its resolution, and the size and heterogeneity of the participants in the crowd all impact task allocation, and therefore, the expected quality of the task results. In this case, the execution of actions from a plan, which assist the dynamic tasks' allocation in crowdsourcing systems, become relevant as an alternative solution. This paper formalizes task allocation in crowdsourcing scenarios as an artificial intelligence planning problem. Our results suggest that task allocation has several challenges when it is observed in distributed, undefined and dynamic environments, like in crowdsourcing scenarios. Our goal is to evaluate if automated planning is appropriate for providing a plan to match skills of crowd workers for the right tasks in software engineering projects. Preliminary results are presented in this paper.},
booktitle = {Proceedings of the 3rd International Workshop on CrowdSourcing in Software Engineering},
pages = {36–40},
numpages = {5},
keywords = {task allocation, software engineering, crowdsourcing, automated planning},
location = {Austin, Texas},
series = {CSI-SE '16}
}

@inproceedings{10.5555/3237383.3238050,
author = {Luo, Yuan and Jennings, Nicholas R.},
title = {A Differential Privacy Mechanism with Network Effects for Crowdsourcing Systems},
year = {2018},
publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
address = {Richland, SC},
abstract = {In crowdsourcing systems, it is important for the crowdsource campaign initiator to incentivize users to share their data to produce results of the desired computational accuracy. This problem becomes especially challenging when users are concerned about the privacy of their data. To overcome this challenge, existing work often aims to provide users with differential privacy guarantees to incentivize privacy-sensitive users to share their data. However, this work neglects the network effect that a user enjoys greater privacy protection when he aligns his participation behaviour with that of other users. To explore the network effect and provide a suitable differential privacy guarantee, we design PINE (Privacy Incentivization with Network Effects). PINE is a mechanism that maximizes the initiator's payoff while providing participating users with privacy protections.},
booktitle = {Proceedings of the 17th International Conference on Autonomous Agents and MultiAgent Systems},
pages = {1998–2000},
numpages = {3},
keywords = {network effects, incentive mechanism, differential privacy, crowdsourcing systems},
location = {Stockholm, Sweden},
series = {AAMAS '18}
}

@inproceedings{10.1109/ICSM.2012.6405249,
author = {Sillito, Jonathan and Maurer, Frank and Nasehi, Seyed Mehdi and Burns, Chris},
title = {What makes a good code example? A study of programming Q&amp;A in StackOverflow},
year = {2012},
isbn = {9781467323130},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/ICSM.2012.6405249},
doi = {10.1109/ICSM.2012.6405249},
abstract = {Programmers learning how to use an API or a programming language often rely on code examples to support their learning activities. However, what makes for an effective ode example remains an open question. Finding the haracteristics of the effective examples is essential in improving the appropriateness of these learning aids. To help answer this question we have onducted a qualitative analysis of the questions and answers posted to a programming Q&amp;A web site called StackOverflow. On StackOverflow answers can be voted on, indicating which answers were found helpful by users of the site. By analyzing these well-received answers we identified haracteristics of effective examples. We found that the explanations acompanying examples are as important as the examples themselves. Our findings have implications for the way the API documentation and example set should be developed and evolved as well as the design of the tools assisting the development of these materials.},
booktitle = {Proceedings of the 2012 IEEE International Conference on Software Maintenance (ICSM)},
pages = {25–34},
numpages = {10},
keywords = {social learning, documentation, code example, Web sites, Software maintenance, Programming, Java, Documentation, Conferences, Best practices, API},
series = {ICSM '12}
}

@inproceedings{10.1109/ICDM.2015.119,
author = {Rahman, Habibur and Roy, Senjuti Basu and Thirumuruganathan, Saravanan and Amer-Yahia, Sihem and Das, Gautam},
title = {Task Assignment Optimization in Collaborative Crowdsourcing},
year = {2015},
isbn = {9781467395045},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/ICDM.2015.119},
doi = {10.1109/ICDM.2015.119},
abstract = {A number of emerging applications, such as, collaborative document editing, sentence translation, and citizen journalism require workers with complementary skills and expertise to form groups and collaborate on complex tasks. While existing research has investigated task assignment for knowledge intensive crowdsourcing, they often ignore the aspect of collaboration among workers, that is central to the success of such tasks. Research in behavioral psychology has indicated that large groups hinder successful collaboration. Taking that into consideration, our work is one of the first to investigate and formalize the notion of collaboration among workers and present theoretical analyses to understand the hardness of optimizing task assignment. We propose efficient approximation algorithms with provable theoretical guarantees and demonstrate the superiority of our algorithms through a comprehensive set of experiments using real-world and synthetic datasets. Finally, we conduct a real world collaborative sentence translation application using Amazon Mechanical Turk that we hope provides a template for evaluating collaborative crowdsourcing tasks in micro-task based crowdsourcing platforms.},
booktitle = {Proceedings of the 2015 IEEE International Conference on Data Mining (ICDM)},
pages = {949–954},
numpages = {6},
series = {ICDM '15}
}

@inproceedings{10.1145/3173574.3173898,
author = {Agapie, Elena and Chinh, Bonnie and Pina, Laura R. and Oviedo, Diana and Welsh, Molly C. and Hsieh, Gary and Munson, Sean},
title = {Crowdsourcing Exercise Plans Aligned with Expert Guidelines and Everyday Constraints},
year = {2018},
isbn = {9781450356206},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3173574.3173898},
doi = {10.1145/3173574.3173898},
abstract = {Exercise plans help people implement behavior change. Crowd workers can help create exercise plans for clients, but their work may result in lower quality plans than produced by experts. We built CrowdFit, a tool that provides feedback about compliance with exercise guidelines and leverages strengths of crowdsourcing to create plans made by non-experts. We evaluated CrowdFit in a comparative study with 46 clients using exercise plans for two weeks. Clients received plans from crowd planners using CrowdFit, crowd planners without CrowdFit, or from expert planners. Compared to crowd planners not using CrowdFit, crowd planners using CrowdFit created plans that are more actionable and more aligned with exercise guidelines. Compared to experts, crowd planners created more actionable plans, and plans that are not significantly different with respect to tailoring, strength and aerobic principles. They struggled, however, to satisfy exercise requirements of amount of exercise. We discuss opportunities for designing technology supporting physical activity planning by non-experts.},
booktitle = {Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems},
pages = {1–13},
numpages = {13},
keywords = {exercise plans, crowdsourcing},
location = {Montreal QC, Canada},
series = {CHI '18}
}

@inproceedings{10.1145/2851581.2892312,
author = {Manojlovic, Stefan and Gavrilo, Katerina and de Wit, Jan and Khan, Vassilis-Javed and Markopoulos, Panos},
title = {Exploring the Potential of Children in Crowdsourcing},
year = {2016},
isbn = {9781450340823},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2851581.2892312},
doi = {10.1145/2851581.2892312},
abstract = {Recently, companies and academia have turned to crowdsourcing to stimulate creativity and innovation. Although children's creative nature has been well documented in the design process in co-creation for new products and/or services, this has not yet extended to crowdsourcing. With this paper, we investigate -- through crowdsourcing -- the gap between children and crowdsourcing. To gather a diverse sample of participants we used CrowdFlower, a crowdsourcing platform, to generate, evaluate and rank ideas and concepts. Results show that 93\% of parents and 80\% of non-parents would involve children in crowdsourcing. The most valued concept of the crowd was the collaboration between parents and children, who are innovating for companies. This concept involves publishing companies requesting drawings from children for book illustrations.},
booktitle = {Proceedings of the 2016 CHI Conference Extended Abstracts on Human Factors in Computing Systems},
pages = {1250–1256},
numpages = {7},
keywords = {interaction design with children, crowdsourcing},
location = {San Jose, California, USA},
series = {CHI EA '16}
}

@inproceedings{10.1007/978-3-030-29908-8_7,
author = {Truong, Nhat Van-Quoc and Stein, Sebastian and Tran-Thanh, Long and Jennings, Nicholas R.},
title = {What Prize Is Right? How to Learn the Optimal Structure for Crowdsourcing Contests},
year = {2019},
isbn = {978-3-030-29907-1},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-29908-8_7},
doi = {10.1007/978-3-030-29908-8_7},
abstract = {In crowdsourcing, one effective method for encouraging par-ticipants to perform tasks is to run contests where participants compete against each other for rewards. However, there are numerous ways to implement such contests in specific projects. They could vary in their structure (e.g., performance evaluation and the number of prizes) and parameters (e.g., the maximum number of participants and the amount of prize money). Additionally, with a given budget and a time limit, choosing incentives (i.e., contest structures with specific parameter values) that maximise the overall utility is not trivial, as their respective effectiveness in a specific project is usually unknown a priori. Thus, in this paper, we propose a novel algorithm, BOIS (Bayesian-optimisation-based incentive selection), to learn the optimal structure and tune its parameters effectively. In detail, the learning and tuning problems are solved simultaneously by using online learning in combination with Bayesian optimisation. The results of our extensive simulations show that the performance of our algorithm is up&nbsp;to 85\% of the optimal and up&nbsp;to 63\% better than state-of-the-art benchmarks.},
booktitle = {PRICAI 2019: Trends in Artificial Intelligence: 16th Pacific Rim International Conference on Artificial Intelligence, Cuvu, Yanuca Island, Fiji, August 26–30, 2019, Proceedings, Part I},
pages = {85–97},
numpages = {13},
keywords = {Incentive, Crowdsourcing, Bayesian optimisation},
location = {Cuvu, Yanuka Island, Fiji}
}

@inproceedings{10.1145/2660460.2660486,
author = {Almishari, Mishari and Oguz, Ekin and Tsudik, Gene},
title = {Fighting authorship linkability with crowdsourcing},
year = {2014},
isbn = {9781450331982},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2660460.2660486},
doi = {10.1145/2660460.2660486},
abstract = {Massive amounts of contributed content -- including traditional literature, blogs, music, videos, reviews and tweets -- are available on the Internet today, with authors numbering in many millions. Textual information, such as product or service reviews, is an important and increasingly popular type of content that is being used as a foundation of many trendy community-based reviewing sites, such as TripAdvisor and Yelp. Some recent results have shown that, due partly to their specialized/topical nature, sets of reviews authored by the same person are readily linkable based on simple stylometric features. In practice, this means that individuals who author more than a few reviews under different accounts (whether within one site or across multiple sites) can be linked, which represents a significant loss of privacy.In this paper, we start by showing that the problem is actually worse than previously believed. We then explore ways to mitigate authorship linkability in community-based reviewing. We first attempt to harness the global power of crowdsourcing by engaging random strangers into the process of re-writing reviews. As our empirical results (obtained from Amazon Mechanical Turk) clearly demonstrate, crowdsourcing yields impressively sensible reviews that reflect sufficiently different stylometric characteristics such that prior stylometric linkability techniques become largely ineffective. We also consider using machine translation to automatically re-write reviews. Contrary to what was previously believed, our results show that translation decreases authorship linkability as the number of intermediate languages grows. Finally, we explore the combination of crowdsourcing and machine translation and report on results.},
booktitle = {Proceedings of the Second ACM Conference on Online Social Networks},
pages = {69–82},
numpages = {14},
keywords = {stylometry, crowdsourcing, authorship attribution, author linkability, author identification, author anonymization},
location = {Dublin, Ireland},
series = {COSN '14}
}

@inproceedings{10.1145/3445815.3445818,
author = {Aslan Oguz, Evin and Kosir, Andrej},
title = {An Online Crowdsourcing Experiment to Model the Effects of a Commercial on a User's Consumption Behavior},
year = {2021},
isbn = {9781450388436},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3445815.3445818},
doi = {10.1145/3445815.3445818},
abstract = {Measuring multimedia exposure is challenging, since the impact of multimedia-content on a user is dependent on the user, the multimedia-content, the context, previous exposure, etc. We focus on measuring the impact of a commercial on a user's consumption behavior. We aim to understand the relation between a commercial and its effects on a consumer; whether watching a particular commercial affects an individual's consumption behavior regarding the advertised product. This paper covers a segment of the larger study which is building a model for measuring the multimedia exposure of the users caused by a commercial. The goal of this paper is to test the developed instrument via an online crowdsourcing study, and analyze the participation date and time, and the attentiveness of the users. The aim is to evaluate the applicability of crowdsourcing platforms as an alternative to testing real users in the laboratory. We evaluate the psychometric characteristics (validity and reliability) of the developed instrument. The reliability coefficient α shows good reliability and coefficient ω shows good saturation model.},
booktitle = {Proceedings of the 2020 4th International Conference on Computer Science and Artificial Intelligence},
pages = {15–23},
numpages = {9},
keywords = {online crowdsourcing experiment applicability, multimedia exposure model, instrument reliability},
location = {Zhuhai, China},
series = {CSAI '20}
}

@inproceedings{10.1145/3209582.3209599,
author = {Gong, Xiaowen and Shroff, Ness},
title = {Incentivizing Truthful Data Quality for Quality-Aware Mobile Data Crowdsourcing},
year = {2018},
isbn = {9781450357708},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3209582.3209599},
doi = {10.1145/3209582.3209599},
abstract = {Mobile data crowdsourcing has found a broad range of applications (e.g., spectrum sensing, environmental monitoring) by leveraging the "wisdom" of a potentially large crowd of "workers" (i.e., mobile users). A key metric of crowdsourcing is data accuracy, which relies on the quality of the participating workers' data (e.g., the probability that the data is equal to the ground truth). However, the data quality of a worker can be its own private information (which the worker learns, e.g., based on its location) that it may have incentive to misreport, which can in turn mislead the crowdsourcing requester about the accuracy of the data. This issue is further complicated by the fact that the worker can also manipulate its effort made in the crowdsourcing task and the data reported to the requester, which can also mislead the requester. In this paper, we devise truthful crowdsourcing mechanisms for Quality, Effort, and Data Elicitation (QEDE), which incentivize strategic workers to truthfully report their private worker quality and data to the requester, and make truthful effort as desired by the requester. The truthful design of the QEDE mechanisms overcomes the lack of ground truth and the coupling in the joint elicitation of worker quality, effort, and data. Under the QEDE mechanisms, we characterize the socially optimal and the requester's optimal task assignments, and analyze their performance. We show that the requester's optimal assignment is determined by the largest "virtual valuation" rather than the highest quality among workers, which depends on the worker's quality and the quality's distribution. We evaluate the QEDE mechanisms using simulations which demonstrate the truthfulness of the mechanisms and the performance of the optimal task assignments.},
booktitle = {Proceedings of the Eighteenth ACM International Symposium on Mobile Ad Hoc Networking and Computing},
pages = {161–170},
numpages = {10},
keywords = {incentive mechanism, data quality, Mobile data crowdsourcing},
location = {Los Angeles, CA, USA},
series = {Mobihoc '18}
}

@inproceedings{10.1145/3335550.3335577,
author = {Li, Ruixue and Peng, Can and Sun, Huiliang},
title = {Product Selection Strategy Analysis of Crowdsourcing Platform from the Full Cost Perspective},
year = {2019},
isbn = {9781450362641},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3335550.3335577},
doi = {10.1145/3335550.3335577},
abstract = {From the perspective of full cost, this paper uses Coase's transaction cost theory to analyze the causes of crowdsourcing, and on this basis to analyze the applicability of crowdsourcing platform products. At the same time, based on the crowdsourcing platform--zbj.com, we use the big data technology to grasp and analyze the related data of the crowdsourcing platform's successful cases in the past five months, and use the relevant statistical analysis method to categorize and analyze the industry attributes of the top five orders of the success cases of the zbj.com, in order to verify the theory mentioned in the article.},
booktitle = {Proceedings of the 2019 International Conference on Management Science and Industrial Engineering},
pages = {92–97},
numpages = {6},
keywords = {Selection Strategy Analysis, Full cost, Crowdsourcing platform, Appropriate products},
location = {Phuket, Thailand},
series = {MSIE '19}
}

@inproceedings{10.5555/2832249.2832366,
author = {Sun, Yuyin and Singla, Adish and Fox, Dieter and Krause, Andreas},
title = {Building hierarchies of concepts via crowdsourcing},
year = {2015},
isbn = {9781577357384},
publisher = {AAAI Press},
abstract = {Hierarchies of concepts are useful in many applications from navigation to organization of objects. Usually, a hierarchy is created in a centralized manner by employing a group of domain experts, a time-consuming and expensive process. The experts often design one single hierarchy to best explain the semantic relationships among the concepts, and ignore the natural uncertainty that may exist in the process. In this paper, we propose a crowdsourcing system to build a hierarchy and furthermore capture the underlying uncertainty. Our system maintains a distribution over possible hierarchies and actively selects questions to ask using an information gain criterion. We evaluate our methodology on simulated data and on a set of real world application domains. Experimental results show that our system is robust to noise, efficient in picking questions, cost-effective, and builds high quality hierarchies.},
booktitle = {Proceedings of the 24th International Conference on Artificial Intelligence},
pages = {844–851},
numpages = {8},
location = {Buenos Aires, Argentina},
series = {IJCAI'15}
}

@inproceedings{10.1109/GLOCOM.2017.8255048,
author = {Yadav, Akash and Sairam, Ashok Singh and Kumar, Anand},
title = {Concurrent Team Formation for Multiple Tasks in Crowdsourcing Platform},
year = {2017},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/GLOCOM.2017.8255048},
doi = {10.1109/GLOCOM.2017.8255048},
abstract = {The tremendous growth of social media technologies has inspired research communities as well as industries to extend the horizon of organizations by recruiting workers available on freelancing sites. Most of the tasks usually require expertise of workers from diverse domains, thus the problem can be reduced to that of team formation. In this work, we address the problem of assigning workers to tasks, where each task requires a set of skills and thus may require more than one worker to successfully complete the task. Given a set of tasks, and a set of workers each with a cost, the objective is to find mutually exclusive set of workers for each task, who can accomplish the task in the most cost-effective manner. The problem being NP-hard, we propose an approximation algorithm that attempts to find the best fit workers based on their collective intelligence for a single task. This approach selects workers in a manner that their expertise complements each other, hence maintaining a balance among the required skills. Such balanced assignment sets require lesser number of workers and reduce the overall cost. The approach is then extended to a set of N tasks. We show that the solution is (2+ α) approximate. The proposed algorithm is evaluated against different assignment schemes. Experimental results using real data show that our approach performs well.},
booktitle = {GLOBECOM 2017 - 2017 IEEE Global Communications Conference},
pages = {1–7},
numpages = {7},
location = {Singapore}
}

@inproceedings{10.1145/3126673.3126677,
author = {O'Leary, Anthony and O'Raghallaigh, Paidi and Nagle, Tadhg and Sammon, David},
title = {Crowdsourcing from the Community to Resolve Complex Service Requests},
year = {2017},
isbn = {9781450354172},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3126673.3126677},
doi = {10.1145/3126673.3126677},
abstract = {The VMware Community may provide an opportunity to VMware to tap into the collective intelligence of its 2.4 million strong members to generate intelligent responses to complex Service Requests (SRs). Bill Joy, cofounder of Sun Microsystems, put it well when he said: "No matter who you are, most of the smartest people work for someone else..!". The data generated from the vSlua project using an Action Design Research approach shows that the Community resolves lower complexity SR issues efficiently, but begins to struggle as the complexity increases. Almost 50\% of all the SRs were answered and over 50\% of the answered SRs were resolved in under 6 hours.},
booktitle = {Proceedings of the 13th International Symposium on Open Collaboration Companion},
articleno = {5},
numpages = {5},
keywords = {Service Requests, Industry-academia Collaboration, Design Research, Crowdsourcing, Community, Action Research},
location = {Galway, Ireland},
series = {OpenSym '17}
}

@inproceedings{10.5220/0007811005780585,
author = {Hussain, Nasir},
title = {Categorical Classification of Factors Effecting Knowledge Management in Software Crowdsourcing: Hypothetical Framework},
year = {2019},
isbn = {9789897583759},
publisher = {SCITEPRESS - Science and Technology Publications, Lda},
address = {Setubal, PRT},
url = {https://doi.org/10.5220/0007811005780585},
doi = {10.5220/0007811005780585},
abstract = {Within Software Crowdsourcing, Knowledge Management has a great significance in both academia and industry as a valuable tool used to manage knowledge from the crowd. The aim of this research is to ascertain which success are and which are failure factors of Knowledge Management in Software Crowdsourcing. Literature review techniques and Quantitative Research techniques were applied in order to establish the success and failure factors. By utilizing the literature review method a total of twelve success factors were established of which seven is supported. Eight failure factures were established out of which six are supported. Subsequent to the analysis, a framework is presented in which the factors are further linked to the implementation of Knowledge Management in Software Crowdsourcing. This research and its suggested framework will also prove useful for academics to further gain a comprehensive view of Knowledge Management factors in Software Crowdsourcing for use in future studies.},
booktitle = {Proceedings of the 14th International Conference on Evaluation of Novel Approaches to Software Engineering},
pages = {578–585},
numpages = {8},
keywords = {Success and Failure Factors, Software Crowdsourcing, Quantitative Research., Knowledge Management},
location = {Heraklion, Crete, Greece},
series = {ENASE 2019}
}

@inproceedings{10.1007/978-3-319-91458-9_18,
author = {Tao, Qian and Zeng, Yuxiang and Zhou, Zimu and Tong, Yongxin and Chen, Lei and Xu, Ke},
title = {Multi-Worker-Aware Task Planning in&nbsp;Real-Time Spatial Crowdsourcing},
year = {2018},
isbn = {978-3-319-91457-2},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-319-91458-9_18},
doi = {10.1007/978-3-319-91458-9_18},
abstract = {Spatial crowdsourcing emerges as a new computing paradigm with the development of mobile Internet and the ubiquity of mobile devices. The core of many real-world spatial crowdsourcing applications is to assign suitable tasks to proper workers in real time. Many works only assign a set of tasks to each worker without making the plan how to perform the assigned tasks. Others either make task plans only for a single worker or are unable to operate in real time. In this paper, we propose a new problem called the Multi-Worker-AwareTaskPlanning (MWATP) problem in the online scenario, in which we not only assign tasks to workers but also make plans for them, such that the total utility (revenue) is maximized. We prove that the offline version of MWATP problem is NP-hard, and no online algorithm has a constant competitive ratio on the MWATP problem. Two heuristic algorithms, called Delay-Planning and Fast-Planning, are proposed to solve the problem. Extensive experiments on synthetic and real datasets verify the effectiveness and efficiency of the two proposed algorithms.},
booktitle = {Database Systems for Advanced Applications: 23rd International Conference, DASFAA 2018, Gold Coast, QLD, Australia, May 21-24, 2018, Proceedings, Part II},
pages = {301–317},
numpages = {17},
keywords = {Spatial crowdsourcing, Task assignment, Task planning},
location = {Gold Coast, QLD, Australia}
}

@inproceedings{10.1145/2800835.2801628,
author = {Loke, Seng W.},
title = {On crowdsourcing information maps: cornucopia of the commons for the city},
year = {2015},
isbn = {9781450335751},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2800835.2801628},
doi = {10.1145/2800835.2801628},
abstract = {Information maps about urban phenomena can be constructed via crowdsourcing, creating a potential Cornucopia of the Commons, as information can be reused by many. We present a stylised model of crowdsourcing for building urban information maps, and make observations about the behaviour of contributors.},
booktitle = {Adjunct Proceedings of the 2015 ACM International Joint Conference on Pervasive and Ubiquitous Computing and Proceedings of the 2015 ACM International Symposium on Wearable Computers},
pages = {1527–1533},
numpages = {7},
location = {Osaka, Japan},
series = {UbiComp/ISWC'15 Adjunct}
}

@inproceedings{10.1145/1868914.1868921,
author = {Alt, Florian and Shirazi, Alireza Sahami and Schmidt, Albrecht and Kramer, Urs and Nawaz, Zahid},
title = {Location-based crowdsourcing: extending crowdsourcing to the real world},
year = {2010},
isbn = {9781605589343},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1868914.1868921},
doi = {10.1145/1868914.1868921},
abstract = {The WWW and the mobile phone have become an essential means for sharing implicitly and explicitly generated information and a communication platform for many people. With the increasing ubiquity of location sensing included in mobile devices we investigate the arising opportunities for mobile crowdsourcing making use of the real world context. In this paper we assess how the idea of user-generated content, web-based crowdsourcing, and mobile electronic coordination can be combined to extend crowdsourcing beyond the digital domain and link it to tasks in the real world. To explore our concept we implemented a crowd-sourcing platform that integrates location as a parameter for distributing tasks to workers. In the paper we describe the concept and design of the platform and discuss the results of two user studies. Overall the findings show that integrating tasks in the physical world is useful and feasible. We observed that (1) mobile workers prefer to pull tasks rather than getting them pushed, (2) requests for pictures were the most favored tasks, and (3) users tended to solve tasks mainly in close proximity to their homes. Based on this, we discuss issues that should be considered during designing mobile crowdsourcing applications.},
booktitle = {Proceedings of the 6th Nordic Conference on Human-Computer Interaction: Extending Boundaries},
pages = {13–22},
numpages = {10},
keywords = {mobile phone, location, crowdsourcing, context},
location = {Reykjavik, Iceland},
series = {NordiCHI '10}
}

@inproceedings{10.5555/3204094.3204121,
author = {Xue, Qinghan and Chuah, Mooi Choo},
title = {Incentivising high quality crowdsourcing clinical data for disease prediction},
year = {2017},
isbn = {9781509047215},
publisher = {IEEE Press},
abstract = {Predictive modeling is fundamental in transforming large clinical data sets into actionable knowledge which can guide clinical decision making and personalized medicine. Although several studies have merged data mining techniques with statistical analysis to extract hidden patterns from large database, these proposed mechanisms are excessively complex for practical use. Therefore, it is essential that a better tool is developed for disease progression and survival rate predictions. In this paper, we first present how carefully chosen clinical features with our proper data cleaning method improves the accuracy of the the Amyotrophic Lateral Sclerosis (ALS) disease progression and survival rate predictions. In addition, we present an incentive model which provides individual rationality and platform profitability features to encourage hospitals to share high quality data for such predictions. Our evaluation results show promising outcomes for our proposed approaches.},
booktitle = {Proceedings of the Second IEEE/ACM International Conference on Connected Health: Applications, Systems and Engineering Technologies},
pages = {185–194},
numpages = {10},
keywords = {survival rate prediction, platform profitability, individual rationality, incentive, feature selection, ALS},
location = {Philadelphia, Pennsylvania},
series = {CHASE '17}
}

@inproceedings{10.1145/3126673.3126683,
author = {Cullina, Eoin},
title = {A Crowdsourcing Practices Framework for Science Funding Call Processes},
year = {2017},
isbn = {9781450354172},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3126673.3126683},
doi = {10.1145/3126673.3126683},
abstract = {Public scientific research funding agencies (funding agencies) are charged with the task of implementing government science policy and identifying research projects worthy of funding. They play an important role in creating value for society through funding research and informing research policy. However, the work of funding agencies in recent years has been hampered by various challenges in call processes. This research proposes crowdsourcing as a potential solution for funding agencies. Information systems research has engaged with crowdsourcing and the open innovation phenomenon. Crowdsourcing has been utilised by both private organisations and governments in the seeking solutions to similar types of challenges. Despite this fact, no crowdsourcing frameworks have been adapted to address the types of challenges faced by funding agencies in call processes. This research seeks to identify challenges faced by funding agencies for the purposes adapting a crowdsourcing practices framework to address these challenges.},
booktitle = {Proceedings of the 13th International Symposium on Open Collaboration Companion},
articleno = {10},
numpages = {5},
keywords = {scientific research funding agencies, open innovation, Crowdsourcing},
location = {Galway, Ireland},
series = {OpenSym '17}
}

@inproceedings{10.5555/3172077.3172083,
author = {Augustin, Alexandry and Venanzi, Matteo and Rogers, Alex and Jennings, Nicholas R.},
title = {Bayesian aggregation of categorical distributions with applications in crowdsourcing},
year = {2017},
isbn = {9780999241103},
publisher = {AAAI Press},
abstract = {A key problem in crowdsourcing is the aggregation of judgments of proportions. For example, workers might be presented with a news article or an image, and be asked to identify the proportion of each topic, sentiment, object, or colour present in it. These varying judgments then need to be aggregated to form a consensus view of the document's or image's contents. Often, however, these judgments are skewed by workers who provide judgments randomly. Such spammers make the cost of acquiring judgments more expensive and degrade the accuracy of the aggregation. For such cases, we provide a new Bayesian framework for aggregating these responses (expressed in the form of categorical distributions) that for the first time accounts for spammers. We elicit 796 judgments about proportions of objects and colours in images. Experimental results show comparable aggregation accuracy when 60\% of the workers are spammers, as other state of the art approaches do when there are no spammers.},
booktitle = {Proceedings of the 26th International Joint Conference on Artificial Intelligence},
pages = {1411–1417},
numpages = {7},
location = {Melbourne, Australia},
series = {IJCAI'17}
}

@inproceedings{10.1145/3178876.3186033,
author = {Yang, Jie and Drake, Thomas and Damianou, Andreas and Maarek, Yoelle},
title = {Leveraging Crowdsourcing Data for Deep Active Learning An Application: Learning Intents in Alexa},
year = {2018},
isbn = {9781450356398},
publisher = {International World Wide Web Conferences Steering Committee},
address = {Republic and Canton of Geneva, CHE},
url = {https://doi.org/10.1145/3178876.3186033},
doi = {10.1145/3178876.3186033},
abstract = {This paper presents a generic Bayesian framework that enables any deep learning model to actively learn from targeted crowds. Our framework inherits from recent advances in Bayesian deep learning, and extends existing work by considering the targeted crowdsourcing approach, where multiple annotators with unknown expertise contribute an uncontrolled amount (often limited) of annotations. Our framework leverages the low-rank structure in annotations to learn individual annotator expertise, which then helps to infer the true labels from noisy and sparse annotations. It provides a unified Bayesian model to simultaneously infer the true labels and train the deep learning model in order to reach an optimal learning efficacy. Finally, our framework exploits the uncertainty of the deep learning model during prediction as well as the annotators» estimated expertise to minimize the number of required annotations and annotators for optimally training the deep learning model. We evaluate the effectiveness of our framework for intent classification in Alexa (Amazon»s personal assistant), using both synthetic and real-world datasets. Experiments show that our framework can accurately learn annotator expertise, infer true labels, and effectively reduce the amount of annotations in model training as compared to state-of-the-art approaches. We further discuss the potential of our proposed framework in bridging machine learning and crowdsourcing towards improved human-in-the-loop systems.},
booktitle = {Proceedings of the 2018 World Wide Web Conference},
pages = {23–32},
numpages = {10},
keywords = {deep active learning, crowdsourcing, conversational agents},
location = {Lyon, France},
series = {WWW '18}
}

@inproceedings{10.1145/3251626,
author = {Bustamante, Fabi\'{a}n E.},
title = {Session details: On Designing Crowdsourcing Systems},
year = {2015},
isbn = {9781450335393},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3251626},
doi = {10.1145/3251626},
booktitle = {Proceedings of the 2015 ACM SIGCOMM Workshop on Crowdsourcing and Crowdsharing of Big (Internet) Data},
location = {London, United Kingdom},
series = {C2B(1)D '15}
}

@inproceedings{10.1145/3369740.3372730,
author = {Basak, Jayanta and Bhaumik, Parama and Roy, Siuli and Bandyopadhyay, Somprakash},
title = {A Crowdsourcing based Information System Framework for Coordinated Disaster Management and Building Community Resilience},
year = {2020},
isbn = {9781450377515},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3369740.3372730},
doi = {10.1145/3369740.3372730},
abstract = {Disaster management involves intensive coordination among multiple agencies like police, fire departments, public health, non-govt. agencies, including local volunteers/field workers. Accurate situational information about damage, resource needs, available resources etc., in the affected areas help the disaster management agencies in proper damage and need assessment and prepare suitable resource deployment plan. Crowdsourcing has become a popular approach for information collection where open crowds of people share multimodal situational information (text, images, audio, video etc.) about any event through social media posts. However, the authenticity and reliability of such posts are still debatable. Gathering situational data directly from the affected community (community-sourcing) can supplement social media posts to generate effective insights. In this paper, we attempt to design and develop a multiplatform disaster management information system where both social media-based crowdsourcing and community sourcing techniques are used to accumulate location-specific situational information. Subsequently, a coherent picture of the disaster situation is evolved through the integration of these local snapshots. Here, we explore how community participation, in the context of disaster management, can be enhanced through collaborative knowledge transaction, which eventually will lead towards the development of a resilient community. A field trial of our system is conducted involving a remote village community at Namkhana, West Bengal.},
booktitle = {Proceedings of the 21st International Conference on Distributed Computing and Networking},
articleno = {33},
numpages = {6},
keywords = {Information Crowdsourcing, Disaster Management Information System, Community Resilience},
location = {Kolkata, India},
series = {ICDCN '20}
}

@inproceedings{10.1145/2884781.2884865,
author = {Fava, Daniel and Shapiro, Dan and Osborn, Joseph and Sch\"{a}ef, Martin and Whitehead, E. James},
title = {Crowdsourcing program preconditions via a classification game},
year = {2016},
isbn = {9781450339001},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2884781.2884865},
doi = {10.1145/2884781.2884865},
abstract = {Invariant discovery is one of the central problems in software verification. This paper reports on an approach that addresses this problem in a novel way; it crowdsources logical expressions for likely invariants by turning invariant discovery into a computer game. The game, called Binary Fission, employs a classification model. In it, players compose preconditions by separating program states that preserve or violate program assertions. The players have no special expertise in formal methods or programming, and are not specifically aware they are solving verification tasks. We show that Binary Fission players discover concise, general, novel, and human readable program preconditions. Our proof of concept suggests that crowdsourcing offers a feasible and promising path towards the practical application of verification technology.},
booktitle = {Proceedings of the 38th International Conference on Software Engineering},
pages = {1086–1096},
numpages = {11},
location = {Austin, Texas},
series = {ICSE '16}
}

@inproceedings{10.1145/3126973.3126994,
author = {Cao, Yiming and Liu, Lei and Cui, Lizhen and Li, Qingzhong},
title = {Empirical Study on Assessment Algorithms with Confidence in Crowdsourcing},
year = {2017},
isbn = {9781450353755},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3126973.3126994},
doi = {10.1145/3126973.3126994},
abstract = {Evaluating the quality of workers is very important in crowdsourcing system and impactful methods are required in order to obtain the most appropriate quality. Previous work have introduced confidence intervals to estimate the quality of workers. However, we have found the size of the confidence interval is wide through analysis of experimental results, which leads to inaccurate worker error rates. In this paper, we propose an optimized algorithm of confidence interval to reduce the size of the confidence interval as narrow as possible and to estimate the quality of workers more precise. We verify our algorithm using the simulated data from our own crowdsourcing platform under realistic settings.},
booktitle = {Proceedings of the 2nd International Conference on Crowd Science and Engineering},
pages = {100–104},
numpages = {5},
keywords = {quality of worker, confidence interval, Crowdsourcing},
location = {Beijing, China},
series = {ICCSE'17}
}

@inproceedings{10.1145/3230467.3230470,
author = {Peng, Fei and Liu, Yu and Lu, Bin},
title = {Research and Application of Block Chain Technology in Crowdsourcing Platform},
year = {2018},
isbn = {9781450364300},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3230467.3230470},
doi = {10.1145/3230467.3230470},
abstract = {Crowdsourcing has become an important mean of value realization in the modern economic environment. Because of the characteristics of decentralization, mutual-trust and non-tampering, block chain technology can establish a distributed trust environment, which is quite suitable for crowdsourcing transactions. Based on the requirements of crowdsourcing transactions and the basic conception of block chain technology, the crowdsourcing trading collaboration mechanism under block chain model is proposed to optimize transaction process and consensus process. Further, the architecture of crowdsourcing platform using the block chain technology is built, providing a framework for the development of the crowdsourcing platform.},
booktitle = {Proceedings of the 2018 International Conference on E-Business and Mobile Commerce},
pages = {1–5},
numpages = {5},
keywords = {Platform architecture, Crowdsourcing, Collaboration mechanism, Block chain technology},
location = {Chengdu, China},
series = {ICEMC '18}
}

@inproceedings{10.1007/978-3-319-03260-3_35,
author = {Kavaler, David and Posnett, Daryl and Gibler, Clint and Chen, Hao and Devanbu, Premkumar and Filkov, Vladimir},
title = {Using and Asking: APIs Used in the Android Market and Asked about in StackOverflow},
year = {2013},
isbn = {9783319032597},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-319-03260-3_35},
doi = {10.1007/978-3-319-03260-3_35},
abstract = {Programming is knowledge intensive. While it is well understood that programmers spend lots of time looking for information, with few exceptions, there is a significant lack of data on what information they seek, and why. Modern platforms, like Android, comprise complex APIs that often perplex programmers. We ask: which elements are confusing, and why? Increasingly, when programmers need answers, they turn to StackOverflow. This provides a novel opportunity. There are a vast number of applications for Android devices, which can be readily analyzed, and many traces of interactions on StackOverflow. These provide a complementary perspective on using and asking, and allow the two phenomena to be studied together. How does the market demand for the USE of an API drive the market for knowledge about it? Here, we analyze data from Android applications and StackOverflow together, to find out what it is that programmers want to know and why.},
booktitle = {Proceedings of the 5th International Conference on Social Informatics - Volume 8238},
pages = {405–418},
numpages = {14},
location = {Kyoto, Japan},
series = {SocInfo 2013}
}

@inproceedings{10.1145/3018896.3018916,
author = {Alabduljabbar, Reham and Al-Dossari, Hmood},
title = {Towards a classification model for tasks in crowdsourcing},
year = {2017},
isbn = {9781450347747},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3018896.3018916},
doi = {10.1145/3018896.3018916},
abstract = {Crowdsourcing is an increasingly popular approach for utilizing the power of the crowd in performing tasks that cannot be solved sufficiently by machines. Text annotation and image labeling are two examples of crowdsourcing tasks that are difficult to automate and human knowledge is often required. However, the quality of the obtained outcome from the crowdsourcing is still problematic. To obtain high-quality results, different quality control mechanisms should be applied to evaluate the different type of tasks. In a previous work, we present a task ontology-based model that can be utilized to identify which quality mechanism is most appropriate based on the task type. In this paper, we complement our previous work by providing a categorization of crowdsourcing tasks. That is, we define the most common task types in the crowdsourcing context. Then, we show how machine learning algorithms can be used to infer automatically the type of the crowdsourced task.},
booktitle = {Proceedings of the Second International Conference on Internet of Things, Data and Cloud Computing},
articleno = {23},
numpages = {7},
keywords = {task, quality control, crowdsourcing, classification, amazon Mturk},
location = {Cambridge, United Kingdom},
series = {ICC '17}
}

@inproceedings{10.1007/978-3-319-68786-5_21,
author = {Sun, Yue and Liu, An and Li, Zhixu and Liu, Guanfeng and Zhao, Lei and Zheng, Kai},
title = {Anonymity-Based Privacy-Preserving Task Assignment in Spatial Crowdsourcing},
year = {2017},
isbn = {978-3-319-68785-8},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-319-68786-5_21},
doi = {10.1007/978-3-319-68786-5_21},
abstract = {The ubiquity of mobile device and wireless networks flourishes the market of Spatial Crowdsourcing (SC), in which location constrained tasks are sent to workers and expected to be performed in some designated locations. To obtain a global optimal task assignment scheme, the SC-server usually needs to collect location information of all workers. During this process, there is a significant security concern, that is, SC-server may not be trustworthy, so it brings about a threat to workers location privacy. In this paper, we focus on the privacy-preserving task assignment in SC. By introducing a semi-honest third party, we present an approach for task assignment in which location privacy of workers can be protected in a k-anonymity manner. We theoretically show that the proposed model is secure against semi-honest adversaries. Experimental results show that our approach is efficient and can scale to real SC applications.},
booktitle = {Web Information Systems Engineering – WISE 2017: 18th International Conference, Puschino, Russia, October 7-11, 2017, Proceedings, Part II},
pages = {263–277},
numpages = {15},
keywords = {Privacy-preserving, Spatial crowdsourcing, Spatial task assignment},
location = {Puschino, Russia}
}

@inproceedings{10.1145/3411764.3445399,
author = {Rubya, Sabirat and Numainville, Joseph and Yarosh, Svetlana},
title = {Comparing Generic and Community-Situated Crowdsourcing for Data Validation in the Context of Recovery from Substance Use Disorders},
year = {2021},
isbn = {9781450380966},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3411764.3445399},
doi = {10.1145/3411764.3445399},
abstract = {Targeting the right group of workers for crowdsourcing often achieves better quality results. One unique example of targeted crowdsourcing is seeking community-situated workers whose familiarity with the background and the norms of a particular group can help produce better outcome or accuracy. These community-situated crowd workers can be recruited in different ways from generic online crowdsourcing platforms or from online recovery communities. We evaluate three different approaches to recruit generic and community-situated crowd in terms of the time and the cost of recruitment, and the accuracy of task completion. We consider the context of Alcoholics Anonymous (AA), the largest peer support group for recovering alcoholics, and the task of identifying and validating AA meeting information. We discuss the benefits and trade-offs of recruiting paid vs. unpaid community-situated workers and provide implications for future research in the recovery context and relevant domains of HCI, and for the design of crowdsourcing ICT systems.},
booktitle = {Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems},
articleno = {449},
numpages = {17},
keywords = {community-situated crowd, Crowdsourcing, Alcoholics Anonymous},
location = {Yokohama, Japan},
series = {CHI '21}
}

@inproceedings{10.1145/2647868.2654908,
author = {Seetharaman, Prem and Pardo, Bryan},
title = {Crowdsourcing a Reverberation Descriptor Map},
year = {2014},
isbn = {9781450330633},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2647868.2654908},
doi = {10.1145/2647868.2654908},
abstract = {Audio production is central to every kind of media that involves sound, such as film, television, and music and involves transforming audio into a state ready for consumption by the public. One of the most commonly-used audio production tools is the reverberator. Current interfaces are often complex and hard-to-understand. We seek to simplify these interfaces by letting users communicate their audio production objective with descriptive language (e.g. "Make the drums sound bigger."). To achieve this goal, a system must be able to tell whether the stated goal is appropriate for the selected tool (e.g. making the violin warmer using a panning tool does not make sense). If the goal is appropriate for the tool, it must know what actions lead to the goal. Further, the tool should not impose a vocabulary on users, but rather understand the vocabulary users prefer. In this work, we describe SocialReverb, a project to crowdsource a vocabulary of audio descriptors that can be mapped onto concrete actions using a parametric reverberator. We deployed SocialReverb, on Mechanical Turk, where 513 unique users described 256 instances of reverberation using 2861 unique words. We used this data to build a concept map showing which words are popular descriptors, which ones map consistently to specific reverberation types, and which ones are synonyms. This promises to enable future interfaces that let the user communicate their production needs using natural language.},
booktitle = {Proceedings of the 22nd ACM International Conference on Multimedia},
pages = {587–596},
numpages = {10},
keywords = {interfaces, human computation, audio synonyms, audio engineering, audio descriptors},
location = {Orlando, Florida, USA},
series = {MM '14}
}

@inproceedings{10.1145/2911451.2914756,
author = {Moshfeghi, Yashar and Huertas-Rosero, Alvaro F. and Jose, Joemon M.},
title = {Identifying Careless Workers in Crowdsourcing Platforms: A Game Theory Approach},
year = {2016},
isbn = {9781450340694},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2911451.2914756},
doi = {10.1145/2911451.2914756},
abstract = {In this paper we introduce a game scenario for crowdsourcing (CS) using incentives as a bait for careless (gambler) workers, who respond to them in a characteristic way. We hypothesise that careless workers are risk-inclined and can be detected in the game scenario by their use of time, and test this hypothesis in two steps: first, we formulate and prove a theorem stating that a risk-inclined worker will react to competition with shorter Task Completion Time (TCT) than a risk-neutral or risk-averse worker. Second, we check if the game scenario introduces a link between TCT and performance, by performing a crowdsourced evaluation using 35 topics from the TREC-8 collection. Experimental evidence confirms our hypothesis, showing that TCT can be used as a powerful discrimination factor to detect careless workers. This is a valuable result in the quest for quality assurance in CS-based micro tasks such as relevance assessment.},
booktitle = {Proceedings of the 39th International ACM SIGIR Conference on Research and Development in Information Retrieval},
pages = {857–860},
numpages = {4},
keywords = {chicken game, crowdsourcing, game theory, relevance assessment},
location = {Pisa, Italy},
series = {SIGIR '16}
}

@inproceedings{10.1145/3242587.3242621,
author = {Ali, Abdullah X. and Morris, Meredith Ringel and Wobbrock, Jacob O.},
title = {Crowdsourcing Similarity Judgments for Agreement Analysis in End-User Elicitation Studies},
year = {2018},
isbn = {9781450359481},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3242587.3242621},
doi = {10.1145/3242587.3242621},
abstract = {End-user elicitation studies are a popular design method, but their data require substantial time and effort to analyze. In this paper, we present Crowdsensus, a crowd-powered tool that enables researchers to efficiently analyze the results of elicitation studies using subjective human judgment and automatic clustering algorithms. In addition to our own analysis, we asked six expert researchers with experience running and analyzing elicitation studies to analyze an end-user elicitation dataset of 10 functions for operating a web-browser, each with 43 voice commands elicited from end-users for a total of 430 voice commands. We used Crowdsensus to gather similarity judgments of these same 430 commands from 410 online crowd workers. The crowd outperformed the experts by arriving at the same results for seven of eight functions and resolving a function where the experts failed to agree. Also, using Crowdsensus was about four times faster than using experts.},
booktitle = {Proceedings of the 31st Annual ACM Symposium on User Interface Software and Technology},
pages = {177–188},
numpages = {12},
keywords = {online crowds, mechanical turk, human computation, end-user elicitation study, crowdsourcing, agreement rate},
location = {Berlin, Germany},
series = {UIST '18}
}

@inproceedings{10.1145/3185089.3185152,
author = {Li, Boshu and Wu, Wenjun and Hu, Zhenhui},
title = {Evaluation of Software Quality for Competition-based Software Crowdsourcing Projects},
year = {2018},
isbn = {9781450354141},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3185089.3185152},
doi = {10.1145/3185089.3185152},
abstract = {Crowdsourcing-based Software Development (CSSD) performs as: many software practitioners use their own experience and technology to participate software development related tasks, through the open platform such as TopCoder. Crowdsourcing software quality issue has caught some researchers' attention, but it is still far from enough, and no work has been done on evaluating crowdsourcing software projects from a macro point of view. In the paper, we apply traditional quality evaluation practice and theory into the evaluation of crowdsourcing-based software quality by proper modification. The main contributions of this paper are: evaluate TopCoder software quality from the perspective of Project Rating and Project Effort respectively, and explore their aggregation strategies. In order to explore the relationship between them, we introduce the definition of quality assurance effort. We believe the final project rating indicator and quality assurance effort can help a project manager to make reasonable decisions on crowdsourcing-based software development tasks.},
booktitle = {Proceedings of the 2018 7th International Conference on Software and Computer Applications},
pages = {102–109},
numpages = {8},
keywords = {TopCoder, Software Quality, Software Competition, Quality Assurance Effort, Project Rating, Project Effort, Crowdsourcing-based Software Development},
location = {Kuantan, Malaysia},
series = {ICSCA '18}
}

@inproceedings{10.1007/978-3-319-96893-3_2,
author = {Li, Jian and Liu, An and Wang, Weiqi and Li, Zhixu and Liu, Guanfeng and Zhao, Lei and Zheng, Kai},
title = {Towards Privacy-Preserving Travel-Time-First Task Assignment in Spatial Crowdsourcing},
year = {2018},
isbn = {978-3-319-96892-6},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-319-96893-3_2},
doi = {10.1007/978-3-319-96893-3_2},
abstract = {With the ubiquity of mobile devices and wireless networks, spatial crowdsourcing (SC) has gained considerable popularity and importance as a new tool of problem-solving. It enables complex tasks at specific locations to be performed by a crowd of nearby workers. In this paper, we study the privacy-preserving travel-time-first task assignment problem where tasks are assigned to workers who can arrive at the required locations first and no private information are revealed to unauthorized parties. Compared with existing work on privacy-preserving task assignment, this problem is novel as tasks are allocated according to travel time rather than travel distance. Moreover, it is challenging as secure computation of travel time requires secure division which is still an open problem nowadays. Observing that current solutions for secure division do not scale well, we propose an efficient algorithm to securely calculate the least common multiple (LCM) of every workers speed, based on which expensive division operation on ciphertexts can be avoided. We formally prove that our protocol is secure against semi-honest adversaries. Through extensive experiments over real datasets, we demonstrate the efficiency and effectiveness of our proposed protocol.},
booktitle = {Web and Big Data: Second International Joint Conference, APWeb-WAIM 2018, Macau, China, July 23-25, 2018, Proceedings, Part II},
pages = {19–34},
numpages = {16},
keywords = {Spatial crowdsourcing, Privacy-preserving, Task assignment},
location = {Macau, China}
}

@inproceedings{10.1145/3084041.3084058,
author = {Han, Kai and He, Yuntian and Tan, Haisheng and Tang, Shaojie and Huang, He and Luo, Jun},
title = {Online Pricing for Mobile Crowdsourcing with Multi-Minded Users},
year = {2017},
isbn = {9781450349123},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3084041.3084058},
doi = {10.1145/3084041.3084058},
abstract = {Mobile crowdsourcing has been proposed as a promising approach for urban data collection, but it has also brought the critical problem of designing proper mechanisms to incentivize user participation. Most previous work on crowdsourcing incentivization has assumed that each user holds a single private cost for participation or behaves in a "win all or nothing" (a.k.a. "single-minded") manner. However, in some crowdsourcing applications such as Amazon's Mechanical Turk, the users are usually "multi-minded" in the sense that each of them holds heterogeneous private costs for different tasks and only performs a portion of her/his interested tasks according to the announced payments. To address this problem, we propose LIME, an onLine prIcing mechanism to incentivize Multi-minded usErs under the scenario where the users arrive sequentially in an arbitrary order. We show that the design of LIME involves solving a "dummy semi-bandits with multiple knapsacks and random costs" problem, which has not been investigated before, and we also prove that LIME achieves several desirable properties including computational efficiency, budget feasibility, truthfulness, individual rationality and low regret on the utility. Finally, the effectiveness of LIME as well as its superiorities over prior related work are demonstrated through extensive simulations.},
booktitle = {Proceedings of the 18th ACM International Symposium on Mobile Ad Hoc Networking and Computing},
articleno = {18},
numpages = {10},
keywords = {truthful, regret, multi-minded, mobile crowdsourcing, Online pricing},
location = {Chennai, India},
series = {Mobihoc '17}
}

@inproceedings{10.1145/2658861.2658948,
author = {Hsu, Jane Yung-jen},
title = {Crowdsourcing agents for smart IoT},
year = {2014},
isbn = {9781450330350},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2658861.2658948},
doi = {10.1145/2658861.2658948},
abstract = {Activity recognition is a key capability for a smart environment to offer timely services and intelligent interactions with people, especially with the growing number of connected devices. While logging data from connected sensors is no longer beyond reach, it is still quite difficult to collect the labels required by machine learning approaches to activity recognition. In this research, crowdsourcing agents are designed to acquire status labels from people situated in the environment. Experiments on crowdsourcing in a typical building on campus have been conducted to improve air conditioning and space utilization. In particular, we will discuss how crowdsourcing agents in the form of simple physical objects can significantly improve user engagement as well as data quality. Collaboration among cyber-physical agents can lead to better user experience and overall performance.},
booktitle = {Proceedings of the Second International Conference on Human-Agent Interaction},
pages = {103},
numpages = {1},
keywords = {internet of things, intelligent agents, cyber-physical systems, crowdsourcing, activity recognition},
location = {Tsukuba, Japan},
series = {HAI '14}
}

@inproceedings{10.1007/978-3-030-27523-5_11,
author = {Falc\~{a}o, Ana Gabrielle Ramos and Wanderley, Pedro Farias and da Silva Leite, Tiago Henrique and de Souza Baptista, Cl\'{a}udio and de Queiroz, Jos\'{e} Eust\'{a}quio Rangel and de Oliveira, Maxwell Guimar\~{a}es and Rocha, J\'{u}lio Henrique},
title = {Crowdsourcing Urban Issues in Smart Cities: A Usability Assessment of the Crowd4City System},
year = {2019},
isbn = {978-3-030-27522-8},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-27523-5_11},
doi = {10.1007/978-3-030-27523-5_11},
abstract = {Geosocial networks gather large amounts of voluntarily generated information that can be explored in different contexts, including urban areas. In this sense, we developed the Crowd4City system, which puts together city authorities and citizens focusing on the improvement of their urban spaces. In order to ensure the effectiveness of our proposal, we carried out a usability assessment following the ISO 9241, which covers ergonomics of human-computer interaction. For such, this paper describes a case study using Crow4City in a 3-stage evaluation, involving human volunteers, pre-defined tasks, survey analysis and conformity analysis. The statistical indicators show the usability levels which are useful in the analysis of the user’s challenge and motivation on using such a system.},
booktitle = {Electronic Government and the Information Systems Perspective: 8th International Conference, EGOVIS 2019, Linz, Austria, August 26–29, 2019, Proceedings},
pages = {147–159},
numpages = {13},
keywords = {Crowdsourcing, Smart cities, Data reliability, Geosocial networks, Urban issues},
location = {Linz, Austria}
}

@inproceedings{10.1145/3266237.3266265,
author = {Vaz, Luis and Marczak, Sabrina and Steinmacher, Igor},
title = {An empirical study on task documentation in software crowdsourcing: the case of the topcoder platform},
year = {2018},
isbn = {9781450365031},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3266237.3266265},
doi = {10.1145/3266237.3266265},
abstract = {Software Crowdsourcing, the act of outsourcing software development tasks to a crowd in the form of an open call, happens mediated by a platform and is based on tasks. In the competitive model, the members of the crowd seek for tasks and submit solutions attempting to receive financial rewards In this context, task description plays a relevant role since its understanding supports the choice and development of a task. Little is known about the role of task description as support for these processes. In order to contribute to fill this gap, this paper presents an empirical study exploring the role of documentation when developers select and develop tasks in software crowdsourcing. The TopCoder platform was studied in two stages: a case study with newcomers to crowdsourcing (in the classroom); and a study based on interviews with industry professionals. We identified that the documentation quality influences task selection. Tasks with unclear objective description, without specifying required technologies or environment setup instructions, discourage developers from selecting the task. We also found that poorly specified or incomplete tasks lead developers to look for supplementary material or invest more time and effort than initially estimated. The results provide a better understanding about the importance of task documentation in software crowdsourcing and point out what information is important to the crowd.},
booktitle = {Proceedings of the XXXII Brazilian Symposium on Software Engineering},
pages = {62–71},
numpages = {10},
keywords = {topcoder, software crowdsourcing, documentation},
location = {Sao Carlos, Brazil},
series = {SBES '18}
}

@inproceedings{10.1145/3173574.3173884,
author = {Alallah, Fouad and Neshati, Ali and Sheibani, Nima and Sakamoto, Yumiko and Bunt, Andrea and Irani, Pourang and Hasan, Khalad},
title = {Crowdsourcing vs Laboratory-Style Social Acceptability Studies? Examining the Social Acceptability of Spatial User Interactions for Head-Worn Displays},
year = {2018},
isbn = {9781450356206},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3173574.3173884},
doi = {10.1145/3173574.3173884},
abstract = {The use of crowdsourcing platforms for data collection in HCI research is attractive in their ability to provide rapid access to large and diverse participant samples. As a result, several researchers have conducted studies investigating the similarities and differences between data collected through crowdsourcing and more traditional, laboratory-style data collection. We add to this body of research by examining the feasibility of conducting social acceptability studies via crowdsourcing. Social acceptability can be a key determinant for the early adoption of emerging technologies, and as such, we focus our investigation on social acceptability for Head-Worn Display (HWD) input modalities. Our results indicate that data collected via a crowdsourced experiment and a laboratory-style setting did not differ at a statistically significant level. These results provide initial support for crowdsourcing platforms as viable options for conducting social acceptability research.},
booktitle = {Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems},
pages = {1–7},
numpages = {7},
keywords = {social acceptance, input modalities, head-worn displays, crowdsourcing},
location = {Montreal QC, Canada},
series = {CHI '18}
}

@inproceedings{10.1145/3167918.3167965,
author = {Kumar, Pinky P. and Rashid, Mahmood A.},
title = {Crowdsourcing based social awareness for taboo diseases like HIV/AIDS},
year = {2018},
isbn = {9781450354363},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3167918.3167965},
doi = {10.1145/3167918.3167965},
abstract = {Creating social awareness for sexually transmitted diseases, such as HIV/AIDS is critical. The 2015 UNAIDS statistics shows that newly infected cases of HIV/AIDS has fallen down by 35\% since 2000, worldwide. However, the statistics of Fiji tells a different story. The newly infected HIV/AIDS cases in Fiji has been increasing every year since 2000. Different HIV/AIDS awareness programs have been launched previously such as, workshops and seminars. Despite these efforts, there has been no reduction in the newly infected HIV/AIDS cases. Fiji's health sector is still using traditional approaches for building public awareness. Therefore, the primary purpose of this study is to explore the benefits of crowdsourcing in developing the social awareness on taboo diseases among Fijians. Data for this study were collected through questionnaire and experimental methods from the people living with HIV as well as from the young Fijians, such as secondary and tertiary students. The result clearly showed that crowdsourcing can be an effective means of assisting Fiji's public health by reaching out to the remote areas, reducing the program costs, and assisting thousands of people simultaneously.},
booktitle = {Proceedings of the Australasian Computer Science Week Multiconference},
articleno = {25},
numpages = {9},
keywords = {taboo disease, social awareness, public health, crowdsourcing, HIV/AIDS},
location = {Brisband, Queensland, Australia},
series = {ACSW '18}
}

@inproceedings{10.1145/3078714.3078746,
author = {Sethi, Ricky J.},
title = {Crowdsourcing the Verification of Fake News and Alternative Facts},
year = {2017},
isbn = {9781450347082},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3078714.3078746},
doi = {10.1145/3078714.3078746},
abstract = {Fake news and alternative facts have dominated the news cycle of late. In this paper, we present a prototype system that uses social argumentation to verify the validity of proposed alternative facts and help in the detection of fake news. We utilize fundamental argumentation ideas in a graph-theoretic framework that also incorporates semantic web and linked data principles. The argumentation structure is crowdsourced and mediated by expert moderators in a virtual community.},
booktitle = {Proceedings of the 28th ACM Conference on Hypertext and Social Media},
pages = {315–316},
numpages = {2},
keywords = {social argumentation, fake news, alternative facts},
location = {Prague, Czech Republic},
series = {HT '17}
}

@inproceedings{10.5555/3017447.3017460,
author = {Zhitomirsky-Geffet, Maayan and Hajibayova, Lala and Kwa\'{s}nik, Barbara H. and Hamari, Juho and Bullard, Julia and Bowman, Timothy},
title = {Crowdsourcing approaches for knowledge organization systems: crowd collaboration or crowd work?},
year = {2016},
publisher = {American Society for Information Science},
address = {USA},
abstract = {Development of Internet technologies has empowered ordinary users to create, contribute, share and connect with other members of the community. As users learn to exploit the potential of networked communications, they participate in a process, which facilitates a shift from individual to collective contributions and introduces an opportunity for multi-vocal and multi-faceted representation of cultural heritage. Open access to crowdsourced collections requires reconsideration of the traditional authoritative approach of cultural heritage institutions. The arduous nature of the work rendered voluntarily in cultural heritage crowdsourcing initiatives calls for reconsideration of power relationships and giving power to devoted contributors supported by modern "intelligent" technology to regulate the process of representation and organization. Taking into consideration the fact that crowdsourced data are not without flaws, the question is how to better utilize the collective intelligence to create quality information. In this context, various issues such as power, control, trust, inter-contributor consensus, heterogeneity of opinions will be raised and discussed by the panelists. Each of the panelists comes from a different field of expertise (Computer science, Information science, Economics, Communication studies, cultural heritage) and various cultural backgrounds and geographical locations (United States, Europe and Israel). This diversity will be reflected in the presented perspectives on the crowdsourcing topic.},
booktitle = {Proceedings of the 79th ASIS&amp;T Annual Meeting: Creating Knowledge, Enhancing Lives through Information \&amp; Technology},
articleno = {13},
numpages = {6},
keywords = {wisdom of crowds, ontologies, crowdsourcing, crowd work, crowd collaboration, collaborative knowledge organization},
location = {Copenhagen, Denmark},
series = {ASIST '16}
}

@inproceedings{10.1109/MDM.2014.77,
author = {Zadorozhny, Vladimir and Lewis, Michael},
title = {Fusing Information, Crowdsourcing and Mobility},
year = {2014},
isbn = {9781479957057},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/MDM.2014.77},
doi = {10.1109/MDM.2014.77},
abstract = {In this seminar we will consider how concepts of information fusion, crowdsourcing and mobility complement each other and accelerate novel advanced research directions in mobile data management. We will elaborate on each of those concepts and explore their synergy under a prominent scenario of situation assessment in multi-robot search and rescue missions.},
booktitle = {Proceedings of the 2014 IEEE 15th International Conference on Mobile Data Management - Volume 02},
pages = {4–6},
numpages = {3},
series = {MDM '14}
}

@inproceedings{10.1109/MDM.2012.21,
author = {Madria, Sanjay Kumar and Mondal, Anirban},
title = {Crowdsourcing: Dynamic Data Management in Mobile P2P Networks},
year = {2012},
isbn = {9780769547138},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/MDM.2012.21},
doi = {10.1109/MDM.2012.21},
abstract = {Not applicable},
booktitle = {Proceedings of the 2012 IEEE 13th International Conference on Mobile Data Management (Mdm 2012)},
pages = {364–367},
numpages = {4},
keywords = {data replication, crowdsourcing, caching, Mobile P2P},
series = {MDM '12}
}

@inproceedings{10.1007/978-3-319-96893-3_19,
author = {Dong, Zhaoan and Fan, Ju and Lu, Jiaheng and Du, Xiaoyong and Ling, Tok Wang},
title = {Using Crowdsourcing for Fine-Grained Entity Type Completion in Knowledge Bases},
year = {2018},
isbn = {978-3-319-96892-6},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-319-96893-3_19},
doi = {10.1007/978-3-319-96893-3_19},
abstract = {Recent years have witnessed the proliferation of large-scale Knowledge Bases (KBs). However, many entities in KBs have incomplete type information, and some are totally untyped. Even worse, fine-grained types (e.g., BasketballPlayer) containing rich semantic meanings are more likely to be incomplete, as they are more difficult to be obtained. Existing machine-based algorithms use predicates (e.g., birthPlace) of entities to infer their missing types, and they have limitations that the predicates may be insufficient to infer fine-grained types. In this paper, we utilize crowdsourcing to solve the problem, and address the challenge of controlling crowdsourcing cost. To this end, we propose a hybrid machine-crowdsourcing approach for fine-grained entity type completion. It firstly determines the types of some “representative” entities via crowdsourcing and then infers the types for remaining entities based on the crowdsourcing results. To support this approach, we first propose an embedding-based influence for type inference which considers not only the distance between entity embeddings but also the distances between entity and type embeddings. Second, we propose a new difficulty model for entity selection which can better capture the uncertainty of the machine algorithm when identifying the entity types. We demonstrate the effectiveness of our approach through experiments on real crowdsourcing platforms. The results show that our method outperforms the state-of-the-art algorithms by improving the effectiveness of fine-grained type completion at affordable crowdsourcing cost.},
booktitle = {Web and Big Data: Second International Joint Conference, APWeb-WAIM 2018, Macau, China, July 23-25, 2018, Proceedings, Part II},
pages = {248–263},
numpages = {16},
keywords = {Crowdsourcing, Entity type completion, Knowledge base},
location = {Macau, China}
}

@inproceedings{10.1145/2502081.2502234,
author = {Soleymani, Mohammad and Larson, Martha},
title = {Crowdsourcing for multimedia research},
year = {2013},
isbn = {9781450324045},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2502081.2502234},
doi = {10.1145/2502081.2502234},
abstract = {Crowdsourcing techniques make use of intelligent contributions of large number of human crowdmembers. This tutorial introduces researchers to the applications of crowdsourcing to multimedia analysis with the aim of allowing them to understand the potentials and limitations of crowdsourcing tools and techniques. We emphasize the fact that crowdsourcing represents a further development along a pre-existing continuum of techniques, and discuss the added advantages that new developments offer. We provide a basic overview of human computation, with an emphasis on example cases in which crowdsourcing has been applied to generate data sets, to improve automatic multimedia content analysis, and to elicit user needs or multimedia system requirements. Different techniques and considerations in using human computation methods to acquire high-quality data and annotations are discussed and demonstrated.},
booktitle = {Proceedings of the 21st ACM International Conference on Multimedia},
pages = {1111–1112},
numpages = {2},
keywords = {user studies, testing, multimedia annotation, crowdsourcing},
location = {Barcelona, Spain},
series = {MM '13}
}

@inproceedings{10.5555/3235838.3235861,
author = {Panjwani, Saurabh and Prakash, Achintya},
title = {Crowdsourcing attacks on biometric systems},
year = {2014},
isbn = {9781931971133},
publisher = {USENIX Association},
address = {USA},
abstract = {We introduce a new approach for attacking and analyzing biometric-based authentication systems, which involves crowdsourcing the search for potential impostors to the system. Our focus is on voice-based authentication, or speaker verification (SV), and we propose a generic method to use crowdsourcing for identifying candidate "mimics" for speakers in a given target population. We then conduct a preliminary analysis of this method with respect to a well-known text-independent SV scheme (the GMM-UBM scheme) using Mechanical Turk as the crowdsourcing platform.Our analysis shows that the new attack method can identify mimics for target speakers with high impersonation success rates: from a pool of 176 candidates, we identified six with an overall false acceptance rate of 44\%, which is higher than what has been reported for professional mimics in prior voice-mimicry experiments. This demonstrates that na\"{\i}ve, untrained users have the potential to carry out impersonation attacks against voice-based systems, although good imitators are rare to find. (We also implement our method with a crowd of amateur mimicry artists and obtain similar results for them.) Match scores for our best mimics were found to be lower than those for automated attacks but, given the relative difficulty of detecting mimicry attacks vis-\'{a}-vis automated ones, our method presents a potent threat to real systems. We discuss implications of our results for the security analysis of SV systems (and of biometric systems, in general) and highlight benefits and challenges associated with the use of crowdsourcing in such analysis.},
booktitle = {Proceedings of the Tenth USENIX Conference on Usable Privacy and Security},
pages = {257–269},
numpages = {13},
location = {Menlo Park, CA},
series = {SOUPS '14}
}

@inproceedings{10.1109/GLOCOM.2018.8647268,
author = {Wang, Huiyang and Nguyen, Diep N. and Hoang, Dinh Thai and Dutkiewicz, Eryk and Cheng, Qingqing},
title = {Real-Time Crowdsourcing Incentive for Radio Environment Maps: A Dynamic Pricing Approach},
year = {2018},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/GLOCOM.2018.8647268},
doi = {10.1109/GLOCOM.2018.8647268},
abstract = {To effectively utilize&amp;#x002F;harvest short-lived whitespace that accounts for more than 30\% of the cellular bands, it is critical to build a real-time radio environment map. Note that existing radio spectrum maps&amp;#x002F;databases (e.g., Google Spectrum Database) are updated on a daily or weekly basis. In this paper, we introduce a novel real-time crowdsourcing incentive solution that rewards mobile users who contribute their qualified spectrum sensing data to a radio environment map. First, we develop a feature-based model based on advanced machine learning techniques in order to estimate model parameters of the radio environment map. Based on the prediction model, we then propose a smart dynamic pricing strategy including prepaid and postpaid pricing schemes. The prepaid scheme is to guarantee the minimum payment for participants, and the postpaid scheme is to reward the participants according to their contributions. Importantly, in our model, the postpaid scheme will be adjusted iteratively in a real-time manner based on the contributions of participants to the spectrum map. After that we carry out real experiments through a mobile application and a cloud spectrum database. The experiment results show that our proposed solution can achieve not only better users' utilities, but also a lower overall system cost compared with those of some existing works.},
booktitle = {2018 IEEE Global Communications Conference (GLOBECOM)},
pages = {1–6},
numpages = {6},
location = {Abu Dhabi, United Arab Emirates}
}

@inproceedings{10.1145/3055601.3055607,
author = {Li, Qunwei and Varshney, Pramod K.},
title = {Does Confidence Reporting from the Crowd Benefit Crowdsourcing Performance?},
year = {2017},
isbn = {9781450349772},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3055601.3055607},
doi = {10.1145/3055601.3055607},
abstract = {We explore the design of an effective crowdsourcing system for an M-ary classification task. Crowd workers complete simple binary microtasks whose results are aggregated to give the final classification decision. We consider the scenario where the workers have a reject option so that they are allowed to skip microtasks when they are unable to or choose not to respond to binary microtasks. Additionally, the workers report quantized confidence levels when they are able to submit definitive answers. We present an aggregation approach using a weighted majority voting rule, where each worker's response is assigned an optimized weight to maximize crowd's classification performance. We obtain a couterintuitive result that the classification performance does not benefit from workers reporting quantized confidence. Therefore, the crowdsourcing system designer should employ the reject option without requiring confidence reporting.},
booktitle = {Proceedings of the 2nd International Workshop on Social Sensing},
pages = {49–54},
numpages = {6},
keywords = {reject option, information fusion, distributed inference, crowdsourcing, confidence reporting, Classification},
location = {Pittsburgh, PA, USA},
series = {SocialSens'17}
}

@inproceedings{10.1145/2642918.2647409,
author = {Retelny, Daniela and Robaszkiewicz, S\'{e}bastien and To, Alexandra and Lasecki, Walter S. and Patel, Jay and Rahmati, Negar and Doshi, Tulsee and Valentine, Melissa and Bernstein, Michael S.},
title = {Expert crowdsourcing with flash teams},
year = {2014},
isbn = {9781450330695},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2642918.2647409},
doi = {10.1145/2642918.2647409},
abstract = {We introduce flash teams, a framework for dynamically assembling and managing paid experts from the crowd. Flash teams advance a vision of expert crowd work that accomplishes complex, interdependent goals such as engineering and design. These teams consist of sequences of linked modular tasks and handoffs that can be computationally managed. Interactive systems reason about and manipulate these teams' structures: for example, flash teams can be recombined to form larger organizations and authored automatically in response to a user's request. Flash teams can also hire more people elastically in reaction to task needs, and pipeline intermediate output to accelerate completion times. To enable flash teams, we present Foundry, an end-user authoring platform and runtime manager. Foundry allows users to author modular tasks, then manages teams through handoffs of intermediate work. We demonstrate that Foundry and flash teams enable crowdsourcing of a broad class of goals including design prototyping, course development, and film animation, in half the work time of traditional self-managed teams.},
booktitle = {Proceedings of the 27th Annual ACM Symposium on User Interface Software and Technology},
pages = {75–85},
numpages = {11},
keywords = {flash teams, expert crowd work, crowdsourcing},
location = {Honolulu, Hawaii, USA},
series = {UIST '14}
}

@inproceedings{10.5555/3045118.3045121,
author = {Shah, Nihar B. and Zhou, Dengyong and Peres, Yuval},
title = {Approval voting and incentives in crowdsourcing},
year = {2015},
publisher = {JMLR.org},
abstract = {The growing need for labeled training data has made crowdsourcing an important part of machine learning. The quality of crowdsourced labels is, however, adversely affected by three factors: (1) the workers are not experts; (2) the incentives of the workers are not aligned with those of the requesters; and (3) the interface does not allow workers to convey their knowledge accurately, by forcing them to make a single choice among a set of options. In this paper, we address these issues by introducing approval voting to utilize the expertise of workers who have partial knowledge of the true answer, and coupling it with a ("strictly proper") incentive-compatible compensation mechanism. We show rigorous theoretical guarantees of optimality of our mechanism together with a simple axiomatic characterization. We also conduct preliminary empirical studies on Amazon Mechanical Turk which validate our approach.},
booktitle = {Proceedings of the 32nd International Conference on International Conference on Machine Learning - Volume 37},
pages = {10–19},
numpages = {10},
location = {Lille, France},
series = {ICML'15}
}

@inproceedings{10.1145/2556288.2556967,
author = {Forlines, Clifton and Miller, Sarah and Guelcher, Leslie and Bruzzi, Robert},
title = {Crowdsourcing the future: predictions made with a social network},
year = {2014},
isbn = {9781450324731},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2556288.2556967},
doi = {10.1145/2556288.2556967},
abstract = {Researchers have long known that aggregate estimations built from the collected opinions of a large group of people often outperform the estimations of individual experts. This phenomenon is generally described as the "Wisdom of Crowds". This approach has shown promise with respect to the task of accurately forecasting future events. Previous research has demonstrated the value of utilizing meta-forecasts (forecasts about what others in the group will predict) when aggregating group predictions. In this paper, we describe an extension to meta-forecasting and demonstrate the value of modeling the familiarity among a population's members (its social network) and applying this model to forecast aggregation. A pair of studies demonstrates the value of taking this model into account, and the described technique produces aggregate forecasts for future events that are significantly better than the standard Wisdom of Crowds approach as well as previous meta-forecasting techniques.},
booktitle = {Proceedings of the SIGCHI Conference on Human Factors in Computing Systems},
pages = {3655–3664},
numpages = {10},
keywords = {social network, meta-forecast, forecasting, crowd-sourcing, bayesian truth serum, aggregation},
location = {Toronto, Ontario, Canada},
series = {CHI '14}
}

@inproceedings{10.5555/3157382.3157639,
author = {Khetan, Ashish and Oh, Sewoong},
title = {Achieving budget-optimality with adaptive schemes in crowdsourcing},
year = {2016},
isbn = {9781510838819},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Adaptive schemes, where tasks are assigned based on the data collected thus far, are widely used in practical crowdsourcing systems to efficiently allocate the budget. However, existing theoretical analyses of crowdsourcing systems suggest that the gain of adaptive task assignments is minimal. To bridge this gap, we investigate this question under a strictly more general probabilistic model, which has been recently introduced to model practical crowdsourcing datasets. Under this generalized Dawid-Skene model, we characterize the fundamental trade-off between budget and accuracy. We introduce a novel adaptive scheme that matches this fundamental limit. A given budget is allocated over multiple rounds. In each round, a subset of tasks with high enough confidence are classified, and increasing budget is allocated on remaining ones that are potentially more difficult. On each round, decisions are made based on the leading eigenvector of (weighted) non-backtracking operator corresponding to the bipartite assignment graph. We further quantify the gain of adaptivity, by comparing the tradeoff with the one for non-adaptive schemes, and confirm that the gain is significant and can be made arbitrarily large depending on the distribution of the difficulty level of the tasks at hand.},
booktitle = {Proceedings of the 30th International Conference on Neural Information Processing Systems},
pages = {4851–4859},
numpages = {9},
location = {Barcelona, Spain},
series = {NIPS'16}
}

@inproceedings{10.1145/2882903.2882953,
author = {Das Sarma, Akash and Parameswaran, Aditya and Widom, Jennifer},
title = {Towards Globally Optimal Crowdsourcing Quality Management: The Uniform Worker Setting},
year = {2016},
isbn = {9781450335317},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2882903.2882953},
doi = {10.1145/2882903.2882953},
abstract = {We study crowdsourcing quality management, that is, given worker responses to a set of tasks, our goal is to jointly estimate the true answers for the tasks, as well as the quality of the workers. Prior work on this problem relies primarily on applying Expectation-Maximization (EM) on the underlying maximum likelihood problem to estimate true answers as well as worker quality. Unfortunately, EM only provides a locally optimal solution rather than a globally optimal one. Other solutions to the problem (that do not leverage EM) fail to provide global optimality guarantees as well. In this paper, we focus on filtering, where tasks require the evaluation of a yes/no predicate, and rating, where tasks elicit integer scores from a finite domain. We design algorithms for finding the global optimal estimates of correct task answers and worker quality for the underlying maximum likelihood problem, and characterize the complexity of these algorithms. Our algorithms conceptually consider all mappings from tasks to true answers (typically a very large number), leveraging two key ideas to reduce, by several orders of magnitude, the number of mappings under consideration, while preserving optimality. We also demonstrate that these algorithms often find more accurate estimates than EM-based algorithms. This paper makes an important contribution towards understanding the inherent complexity of globally optimal crowdsourcing quality management.},
booktitle = {Proceedings of the 2016 International Conference on Management of Data},
pages = {47–62},
numpages = {16},
keywords = {rating, quality management, maximum likelihood, human computation, filtering, expectation-maximization, crowdsourcing, EM},
location = {San Francisco, California, USA},
series = {SIGMOD '16}
}

@inproceedings{10.1145/3027063.3053356,
author = {Homan, Christopher Michael and Schull, Jon I. and Prabhu, Akshai},
title = {On the Genesis of an Assistive Technology Crowdsourcing Community},
year = {2017},
isbn = {9781450346566},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3027063.3053356},
doi = {10.1145/3027063.3053356},
abstract = {The e-NABLE movement is a global confederation that designs, builds, and distributes free, 3D-printed, upper limb assistive devices to children born without fingers and hands. It has been called one of the most inspiring philanthropic efforts of the 21st century. We use social network analysis and natural language processing on the original e-NABLE Google+ community to understand the challenges and opportunities in organizing a rapidly growing real-world social entrepreneurship venture via social media. Our results provide important lessons and benchmarks for similar communities.},
booktitle = {Proceedings of the 2017 CHI Conference Extended Abstracts on Human Factors in Computing Systems},
pages = {1003–1011},
numpages = {9},
keywords = {time-series analysis, social network analysis, social media, micromanufacturing, crowdsourcing, computational linguistics.},
location = {Denver, Colorado, USA},
series = {CHI EA '17}
}

@inproceedings{10.1007/978-3-030-18576-3_16,
author = {Zhai, Dongjun and Liu, An and Chen, Shicheng and Li, Zhixu and Zhang, Xiangliang},
title = {SeqST-ResNet: A Sequential Spatial Temporal ResNet for Task Prediction in Spatial Crowdsourcing},
year = {2019},
isbn = {978-3-030-18575-6},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-18576-3_16},
doi = {10.1007/978-3-030-18576-3_16},
abstract = {Task appearance prediction has great potential to improve task assignment in spatial crowdsourcing platforms. The main challenge of this prediction problem is to model the spatial dependency among neighboring regions and the temporal dependency at different time scales (e.g., hourly, daily, and weekly). A recent model ST-ResNet predicts traffic flow by capturing the spatial and temporal dependencies in historical data. However, the data fragments are concatenated as one tensor fed to the deep neural networks, rather than learning the temporal dependencies in a sequential manner. We propose a novel deep learning model, called SeqST-ResNet, which well captures the temporal dependencies of historical task appearance in sequences at several time scales. We validate the effectiveness of our model via experiments on a real-world dataset. The experimental results show that our SeqST-ResNet model significantly outperforms ST-ResNet when predicting tasks at hourly intervals and also during weekday and weekends, more importantly, in regions with intensive task requests.},
booktitle = {Database Systems for Advanced Applications: 24th International Conference, DASFAA 2019, Chiang Mai, Thailand, April 22–25, 2019, Proceedings, Part I},
pages = {260–275},
numpages = {16},
keywords = {Task prediction, Spatial crowdsourcing, Deep neural network},
location = {Chiang Mai, Thailand}
}

@inproceedings{10.5555/2343776.2343793,
author = {Cavallo, Ruggiero and Jain, Shaili},
title = {Efficient crowdsourcing contests},
year = {2012},
isbn = {0981738125},
publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
address = {Richland, SC},
abstract = {A principal seeks production of a good within a limited timeframe with a hard deadline, after which any good procured has no value. There is inherent uncertainty in the production process, which in light of the deadline may warrant simultaneous production of multiple goods by multiple producers despite there being no marginal value for extra goods beyond the maximum quality good produced. This motivates a crowdsourcing model of procurement. We address efficient execution of such procurement from a social planner's perspective, taking account of and optimally balancing the value to the principal with the costs to producers (modeled as effort expenditure) while, crucially, contending with self-interest on the part of all players. A solution to this problem involves both an algorithmic aspect that determines an optimal effort level for each producer given the principal's value, and also an incentive mechanism that achieves equilibrium implementation of the socially optimal policy despite the principal privately observing his value, producers privately observing their skill levels and effort expenditure, and all acting selfishly to maximize their own individual welfare. In contrast to popular "winner take all" contests, the efficient mechanism we propose involves a payment to every producer that expends non-zero effort in the efficient policy.},
booktitle = {Proceedings of the 11th International Conference on Autonomous Agents and Multiagent Systems - Volume 2},
pages = {677–686},
numpages = {10},
keywords = {social welfare, mechanism design, crowdsourcing, contests},
location = {Valencia, Spain},
series = {AAMAS '12}
}

@inproceedings{10.1007/978-3-319-18818-8_43,
author = {Dumitrache, Anca},
title = {Crowdsourcing Disagreement for Collecting Semantic Annotation},
year = {2015},
isbn = {9783319188171},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-319-18818-8_43},
doi = {10.1007/978-3-319-18818-8_43},
abstract = {This paper proposes an approach to gathering semantic annotation, which rejects the notion that human interpretation can have a single ground truth, and is instead based on the observation that disagreement between annotators can signal ambiguity in the input text, as well as how the annotation task has been designed. The purpose of this research is to investigate whether disagreement-aware crowdsourcing is a scalable approach to gather semantic annotation across various tasks and domains. We propose a methodology for answering this question that involves, for each task and domain: defining the crowdsourcing setup, experimental data collection, and evaluating both the setup and the results. We present initial results for the task of medical relation extraction, and propose an evaluation plan for crowdsourcing semantic annotation for several tasks and domains.},
booktitle = {Proceedings of the 12th European Semantic Web Conference on The Semantic Web. Latest Advances and New Domains - Volume 9088},
pages = {701–710},
numpages = {10},
keywords = {Semantic annotation, Semantic ambiguity, Natural language processing, Human computation, Ground truth, Crowdsourcing}
}

@inproceedings{10.1145/3254783,
author = {Lane, Nic},
title = {Session details: Crowdsourcing II},
year = {2013},
isbn = {9781450317702},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3254783},
doi = {10.1145/3254783},
booktitle = {Proceedings of the 2013 ACM International Joint Conference on Pervasive and Ubiquitous Computing},
location = {Zurich, Switzerland},
series = {UbiComp '13}
}

@inproceedings{10.1145/3139958.3139973,
author = {Li, Wei and Chen, Haiquan and Ku, Wei-Shinn and Qin, Xiao},
title = {Scalable Spatiotemporal Crowdsourcing for Smart Cities based on Particle Filtering},
year = {2017},
isbn = {9781450354905},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3139958.3139973},
doi = {10.1145/3139958.3139973},
abstract = {In mobile crowdsourcing, workers are financially motivated to perform as many self-selected tasks as possible to maximize their revenue. Unfortunately, the existing task scheduling approaches in mobile crowdsourcing fail to consider task execution duration and do not scale for massive tasks and large geographic areas (e.g., a whole city). In this paper, we study on the geo-task scheduling problem (GTS) under the various spatial and temporal constraints in real-world mobile crowdsourcing applications, including task execution duration and task expiration time. Given the location of a worker, the goal of our study is to find an optimal task execution sequence that maximizes the number of tasks that could be finished. Since the exact solution to the maximum task scheduling is computationally intractable, we propose two sub-optimal approaches (LCPF and NUD-IC) based on the particle filtering and the DBSCAN clustering.},
booktitle = {Proceedings of the 25th ACM SIGSPATIAL International Conference on Advances in Geographic Information Systems},
articleno = {63},
numpages = {4},
keywords = {Particle Filtering, Mobile Crowdsourcing, DBSCAN},
location = {Redondo Beach, CA, USA},
series = {SIGSPATIAL '17}
}

@inproceedings{10.1145/2872427.2883035,
author = {Wilson, Shomir and Schaub, Florian and Ramanath, Rohan and Sadeh, Norman and Liu, Fei and Smith, Noah A. and Liu, Frederick},
title = {Crowdsourcing Annotations for Websites' Privacy Policies: Can It Really Work?},
year = {2016},
isbn = {9781450341431},
publisher = {International World Wide Web Conferences Steering Committee},
address = {Republic and Canton of Geneva, CHE},
url = {https://doi.org/10.1145/2872427.2883035},
doi = {10.1145/2872427.2883035},
abstract = {Website privacy policies are often long and difficult to understand. While research shows that Internet users care about their privacy, they do not have time to understand the policies of every website they visit, and most users hardly ever read privacy policies. Several recent efforts aim to crowdsource the interpretation of privacy policies and use the resulting annotations to build more effective user interfaces that provide users with salient policy summaries. However, very little attention has been devoted to studying the accuracy and scalability of crowdsourced privacy policy annotations, the types of questions crowdworkers can effectively answer, and the ways in which their productivity can be enhanced. Prior research indicates that most Internet users often have great difficulty understanding privacy policies, suggesting limits to the effectiveness of crowdsourcing approaches. In this paper, we assess the viability of crowdsourcing privacy policy annotations. Our results suggest that, if carefully deployed, crowdsourcing can indeed result in the generation of non-trivial annotations and can also help identify elements of ambiguity in policies. We further introduce and evaluate a method to improve the annotation process by predicting and highlighting paragraphs relevant to specific data practices.},
booktitle = {Proceedings of the 25th International Conference on World Wide Web},
pages = {133–143},
numpages = {11},
keywords = {privacy policies, privacy, machine learning, crowdsourcing, HCI},
location = {Montr\'{e}al, Qu\'{e}bec, Canada},
series = {WWW '16}
}

@inproceedings{10.1145/2968219.2968585,
author = {Ludwig, Thomas and Kotthaus, Christoph and Pipek, Volkmar},
title = {Situated and ubiquitous crowdsourcing with volunteers during disasters},
year = {2016},
isbn = {9781450344623},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2968219.2968585},
doi = {10.1145/2968219.2968585},
abstract = {Although emergency services have already recognized the importance of citizen-initiated activities during disasters, still questions with regard to the coordination of spontaneous volunteers and their activities arise. Situated and ubiquitous crowdsourcing seem to be appropriate concepts for supporting the management of voluntary activities during emergencies. Within this paper we present two tools that encompass both types of crowdsourcing mechanisms.},
booktitle = {Proceedings of the 2016 ACM International Joint Conference on Pervasive and Ubiquitous Computing: Adjunct},
pages = {1441–1447},
numpages = {7},
keywords = {volunteers, ubiquitous crowdsourcing, situated crowdsourcing, crisis management},
location = {Heidelberg, Germany},
series = {UbiComp '16}
}

@inproceedings{10.1109/JCDL.2019.00093,
author = {Han, Wenting and Song, Shijie and Zhao, Yuxiang and Zhu, Qinghua},
title = {The role of self-efficacy and familiarity in digital humanity crowdsourcing: a preliminary study from transcribe-sheng project},
year = {2020},
isbn = {9781728115474},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/JCDL.2019.00093},
doi = {10.1109/JCDL.2019.00093},
abstract = {In this work, we employed regression analysis based on Transcribe-Sheng Project, a typical Chinese crowdsourcing project in culture heritage, to explore the influencing effects of the two predicting factors---self-efficacy and familiarity, on the volunteers' task completion and task performance respectively. The preliminary study found that: 1) Familiarity (including familiarity of background knowledge and familiarity of transcription platform) was the main factor that influenced task completion; 2) Familiarity of background knowledge significantly influenced the task performance; 3) The effects of self-efficacy on both task completion and task performance were not significant.},
booktitle = {Proceedings of the 18th Joint Conference on Digital Libraries},
pages = {408–409},
numpages = {2},
keywords = {transcribe sheng project, self-efficacy, familiarity, digital humanity, crowdsourcing},
location = {Champaign, Illinois},
series = {JCDL '19}
}

@inproceedings{10.1145/2691195.2691223,
author = {Zambrano, Ra\'{u}l and Eymann, Simone},
title = {Crowdsourcing and human development: the role of governments},
year = {2014},
isbn = {9781605586113},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2691195.2691223},
doi = {10.1145/2691195.2691223},
abstract = {This paper explores in which way crowdsourcing and other new technologies can help governments in developing countries work more closely with stakeholders to improve public policy making and allocate public resources in a more responsive fashion vis-a-vis people's needs and priorities. The paper first sets a general background to frame the issues followed by a short literature review of the latest research in this area. It then proposes a new analytical framework which is used to study several cases studies from which it draws conclusions and suggests areas for further research.},
booktitle = {Proceedings of the 8th International Conference on Theory and Practice of Electronic Governance},
pages = {170–177},
numpages = {8},
keywords = {state capacity, public policy making, public policy, institutional development, human development, empowerment, e-participation, e-government, e-governance, e-democracy, democratic governance, crowdsourcing, citizensourcing, ICT for development},
location = {Guimaraes, Portugal},
series = {ICEGOV '14}
}

@inproceedings{10.1145/2989238.2989243,
author = {Abhinav, Kumar and Dubey, Alpana and Virdi, Gurdeep and Kass, Alex},
title = {Analyzing on-boarding time in context of crowdsourcing},
year = {2016},
isbn = {9781450343954},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2989238.2989243},
doi = {10.1145/2989238.2989243},
abstract = {Crowdsourcing is an emerging area which leverages collective intelligence of the crowd. Although crowdsourcing provides several benefits, it also brings uncertainty in any project execution. The uncertainty may be because of the time taken in on-boarding workers and lack of confidence in workers. The On-boarding time specifically becomes important when tasks are of short duration as it is not worth spending too much of time in on-boarding a worker for short task. In this paper, we empirically analyze 59,597 tasks data from Upwork, an online marketplace, to understand major factors that impact On-boarding time. We identified that certain factors, such as Feedback, Hiring rate, Total hours spent, Length of requirement etc., affect the On-boarding time. We applied two predictive models to predict the On-boarding time. Our study provides insights for researchers, organizations, etc. who are looking to accomplish their tasks through crowdsourcing and helps them to better understand factors which influence the On-boarding time.},
booktitle = {Proceedings of the 2nd International Workshop on Software Analytics},
pages = {29–35},
numpages = {7},
keywords = {Flash Teams, Data Analytics, Crowdsourcing},
location = {Seattle, WA, USA},
series = {SWAN 2016}
}

@inproceedings{10.1145/2751957.2755504,
author = {Taylor, Joseph},
title = {Crowdsourcing IT Work: A Three-Fold Perspective from the Workers, Buyers, and Platform Providers},
year = {2015},
isbn = {9781450335577},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2751957.2755504},
doi = {10.1145/2751957.2755504},
abstract = {This paper will present a proposal for research in the area of crowdsourcing. The proposal will highlight the need for research in the area of crowdsourcing as a mechanism to enhance and expand the technology workforce. It does so by examining the technology crowdsourcing phenomenon from three perspectives: the worker (or labor supply), the buyer of technology services (or labor demand) and the marketplaces that facilitates the buyer-seller transaction. It will explore how workforce development and enterprise readiness theories can be applied in explaining how crowdsourcing can be applied to technology tasks. This dissertation will be structured in a three study format. Study one will explore the technology crowdsourcing phenomenon from a "crowdworker" perspective. This study will examine technology crowdwork from a career anchors perspective, and will highlight the potential role of crowdsourcing in expanding the technology workforce to additional sources of worker capacity. This study will establish the theories that describe the motivations and outcomes achieved by workers in crowdsourcing project engagements, and utilize Schein's Career Anchors (Schein 1990) to examine the motivations of workers technology enabled collaborative work environments. Study two will focus on the perceptions and readiness for crowdsourcing labor on the part of buyers of IT services. The research will collect survey data regarding enterprise readiness, and will examine the current state of enterprise readiness to adopt new development techniques. Study three will utilize a design science perspective to examine the ability of crowdsourcing marketplace platforms to meet the needs of IT service buyers and IT service workers as identified in Study's one and two.},
booktitle = {Proceedings of the 2015 ACM SIGMIS Conference on Computers and People Research},
pages = {1–2},
numpages = {2},
keywords = {it services, enterprise readiness, crowdsourcing, career anchors},
location = {Newport Beach, California, USA},
series = {SIGMIS-CPR '15}
}

@inproceedings{10.1007/978-3-319-04244-2_21,
author = {Challiol, Cecilia and Firmenich, Sergio and Bosetti, Gabriela Alejandra and Gordillo, Silvia E. and Rossi, Gustavo},
title = {Crowdsourcing Mobile Web Applications},
year = {2013},
isbn = {9783319042435},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-319-04244-2_21},
doi = {10.1007/978-3-319-04244-2_21},
abstract = {Building Mobile Web or Hypermedia Applications is usually difficult since there is a myriad of issues to take into account. Moreover adding support for personalized or context-aware behaviors goes far beyond the possibilities of many kinds of organizations that intend to build this kind of software (museums, city halls, etc). In this article we present a novel approach to delegate part of the effort in building mobile Web software to developers outside those organizations or even to final users. We show that this approach is feasible, light and practical and present a set of experiments we developed to verify our claims.},
booktitle = {Revised Selected Papers of the ICWE 2013 International Workshops on Current Trends in Web Engineering - Volume 8295},
pages = {223–237},
numpages = {15},
keywords = {Mobile Web Applications, Mobile Hypermedia, Crowdsoursing, Client-Side Adaptation}
}

@inproceedings{10.1145/3152896.3152898,
author = {Nguyen, Q. N. and Frisiello, A. and Rossi, C.},
title = {Co-design of a crowdsourcing solution for disaster risk reduction},
year = {2017},
isbn = {9781450354240},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3152896.3152898},
doi = {10.1145/3152896.3152898},
abstract = {Disaster Risk Reduction (DRR) is a complex field in which a huge amount of data is used to plan preventive measures, get prepared to natural disasters, and effectively respond when they strike. This work focuses on the definition of a co-design methodology to integrate a crowdsourcing solution in the DRR processes. We define the proposed methodology, and implement it involving operators and experts in the DRR domain (crisis managers, technical services, first responders). We show how a participatory design approach helps in the design of a crowdsourcing solution that experts are willing to integrate into their DRR procedures.},
booktitle = {Proceedings of the First CoNEXT Workshop on ICT Tools for Emergency Networks and DisastEr Relief},
pages = {7–12},
numpages = {6},
keywords = {visual analysis, user interface, user experience, prototyping, human-centred design, disaster risk reduction, crowdsourcing, co-design},
location = {Incheon, Republic of Korea},
series = {I-TENDER '17}
}

@inproceedings{10.1109/ATNAC.2015.7366785,
author = {Schwartz, Christian and Borchert, Kathrin and Hirth, Matthias and Tran-Gia, Phuoc},
title = {Modeling crowdsourcing platforms to enable workforce dimensioning},
year = {2015},
isbn = {9781467393485},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/ATNAC.2015.7366785},
doi = {10.1109/ATNAC.2015.7366785},
abstract = {Crowdsourcing platforms provide an easy and scalable access to human workforce that can, e.g., provide subjective judgements, tagging information, or even generate knowledge. In conjunction with machine clouds offering scalable access to computing resources, these human cloud providers offer numerous possibilities for creating new applications which would not have been possible a few years ago. However, in order to build sustainable services on top of this inter-cloud environment, scalability considerations have to be made. While cloud computing systems are already well studied in terms of dimensioning of the hardware resources, there still exists little work on the appropriate scaling of crowdsourcing platforms. This is especially challenging, as the complex interaction between all involved stakeholders, platform providers, workers and employers has to be considered. The contribution of this work is threefold. First, we develop a model for common crowdsourcing platforms and implement the model using a simulative approach, which is validated with a comparison to an analytic M[X]/M/c system. In a second step, we evaluate inter-arrival times as well as campaign size distributions based on a dataset of a large commercial crowdsourcing platform to derive realistic model parameters and illustrate the differences to the analytic approximation. Finally, we perform a parameter study using the simulation model to derive guidelines for dimensioning crowdsourcing platforms, while considering relevant parameters for the involved stakeholders, i.e., the delay before work on a task begins and the work load of the workers.},
booktitle = {Proceedings of the 2015 International Telecommunication Networks and Applications Conference (ITNAC)},
pages = {30–37},
numpages = {8},
series = {ITNAC '15}
}

@inproceedings{10.1145/3309700.3338418,
author = {Naruse, Kana and Takamichi, Shinnosuke and Tanikawa, Tomohiro and Yoshida, Shigeo and Narumi, Takuji and Hirose, Michitaka},
title = {Estimating confidence in voices using crowdsourcing for alleviating tension with altered auditory feedback},
year = {2020},
isbn = {9781450366793},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3309700.3338418},
doi = {10.1145/3309700.3338418},
abstract = {People tend to face difficulties while speaking, owing to excessive tension. To relieve this tension, we propose a method to alter their voice such that it appears more confident and feed this voice back to the speaker in real time. As determining the appropriate parameters for voice processing is difficult, we gathered data on the perception of confidence in voices through crowdsourcing and constructed a model for estimating confidence scores from voice-processing parameters. An analysis of the model showed that although the coefficient of determination was not considerably large, the inflection of speech tended to affect the perception of confidence.},
booktitle = {Proceedings of Asian CHI Symposium 2019: Emerging HCI Research Collection},
pages = {15–22},
numpages = {8},
keywords = {tension, speech, self-perception, emotion, crowdsourcing, confidence, auditory feedback},
location = {Glasgow, Scotland Uk},
series = {AsianHCI '19}
}

@inproceedings{10.1109/ICPADS.2015.42,
author = {Haijiang Xie and Li Lin and Zhiping Jiang and Wei Xi and Kun Zhao and Meiyong Ding and Jizhong Zhao},
title = {Accelerating Crowdsourcing Based Indoor Localization Using CSI},
year = {2015},
isbn = {9780769557854},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/ICPADS.2015.42},
doi = {10.1109/ICPADS.2015.42},
abstract = {Indoor localization is of importance for many applications. Crowdsourcing individual users' measurements can provide accurate localization without costly site-survey. However, crowdsourcing based approaches suffer from the cold start problem, in which at the beginning of system deployment, there are insufficient users to contribute their measurements, resulting in inaccurate and time-inefficient localization. In this paper, we propose a hybrid indoor localization method to solve such problem, called ACIL. We first employ the inertial navigation technique to localize some core positions or paths. To tackle the inaccuracy problem, we propose an effective method that utilizes the channel state information (CSI) of wireless signals for accurate distance estimation. This method is based on a new observation: there is a ripple-like fading pattern in wireless signals upon moving objects. Leveraging this observation, our system is capable of calculating the distance of human's movement and his/her direction. We also propose a graph-matching algorithm to setup the correlation between the trajectory and floor map. With those extra obtained location information, the impact of cold start issue will be significantly mitigated, while the LBS can be guaranteed with high localization accuracy. Extensive experiments show that the effectiveness in the human localization and movement detection. Extensive experiments validate the great performance of our protocol in case of various human locations and diverse channel conditions.},
booktitle = {Proceedings of the 2015 IEEE 21st International Conference on Parallel and Distributed Systems (ICPADS)},
pages = {274–281},
numpages = {8},
series = {ICPADS '15}
}

@inproceedings{10.5555/2887007.2887188,
author = {Yu, Han and Miao, Chunyan and Shen, Zhiqi and Leung, Cyril and Chen, Yiqiang and Yang, Qiang},
title = {Efficient task sub-delegation for crowdsourcing},
year = {2015},
isbn = {0262511290},
publisher = {AAAI Press},
abstract = {Reputation-based approaches allow a crowdsourcing system to identify reliable workers to whom tasks can be delegated. In crowdsourcing systems that can be modeled as multi-agent trust networks consist of resource constrained trustee agents (i.e., workers), workers may need to further sub-delegate tasks to others if they determine that they cannot complete all pending tasks before the stipulated deadlines. Existing reputation-based decision-making models cannot help workers decide when and to whom to sub-delegate tasks. In this paper, we proposed a reputation aware task sub-delegation (RTS) approach to bridge this gap. By jointly considering a worker's reputation, workload, the price of its effort and its trust relationships with others, RTS can be implemented as an intelligent agent to help workers make sub-delegation decisions in a distributed manner. The resulting task allocation maximizes social welfare through efficient utilization of the collective capacity of a crowd, and provides provable performance guarantees. Experimental comparisons with state-of-the-art approaches based on the Epinions trust network demonstrate significant advantages of RTS under high workload conditions.},
booktitle = {Proceedings of the Twenty-Ninth AAAI Conference on Artificial Intelligence},
pages = {1305–1311},
numpages = {7},
location = {Austin, Texas},
series = {AAAI'15}
}

@inproceedings{10.1109/GLOCOM.2017.8254121,
author = {Tang, Ming and Gao, Lin and Huang, Jianwei},
title = {A General Framework for Crowdsourcing Mobile Communication, Computation, and Caching},
year = {2017},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/GLOCOM.2017.8254121},
doi = {10.1109/GLOCOM.2017.8254121},
abstract = {Today's mobile devices are capable of tackling various complicated tasks that may require a large amount of communication, computation, and caching (3C) resources. Due to users' heterogeneous resources and service requirements, it is challenging for each user to always accomplish his task satisfactorily. To alleviate this issue, mobile users can exploit the heterogeneity and crowdsource their resources to enhance the task execution performance. In this paper, we propose a general 3C framework that enables mobile users to share all three types of resources through device- to-device connections. Such a framework generalizes many existing 1C/2C resource sharing models (that only shares one or two types of resources among users). To quantify the benefit of the proposed framework, we focus on an energy minimization problem, and show that the 3C framework always achieves a smaller total energy consumption, comparing with other 1C/2C models. Furthermore, we show that the energy reduction is maximized, when user connection probability and content caching ratio are neither too large nor too small. Our numerical results show that, when ignoring device-to-device transmission energy, the general 3C framework can reduce the total energy consumption by 82.98\%, comparing with the 1C/2C models.},
booktitle = {GLOBECOM 2017 - 2017 IEEE Global Communications Conference},
pages = {1–6},
numpages = {6},
location = {Singapore}
}

@inproceedings{10.1145/1864431.1864504,
author = {Vukovic, Maja and Kumara, Soundar and Greenshpan, Ohad},
title = {Ubiquitous crowdsourcing},
year = {2010},
isbn = {9781450302838},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1864431.1864504},
doi = {10.1145/1864431.1864504},
abstract = {Web 2.0 provides the technological foundations upon which the crowdsourcing paradigm evolves and operates, enabling networked experts to work on various problem solving and data-intensive tasks. During the past decade crowdsourcing grew from a number of purpose-built initiatives, such as Wikipedia and Mechanical Turk, to a technique that today attracts and engages over 2 million people worldwide. As the computing systems are becoming more intimately embedded in physical and social contexts, promising truly ubiquitous computing, crowdsourcing takes new forms. Increasingly, crowds are engaged through mobile devices, to capture, share and validate sheer amount data (e.g. reporting security threats or capturing social events).This workshop challenges researchers and practitioners to think about three key aspects of ubiquitous crowdsourcing. Firstly, to establish technological foundations, what are the interaction models and protocols between the ubiquitous computing systems and the crowd? Secondly, how is crowdsourcing going to face the challenges in quality assurance, while providing valuable incentive frameworks that enable honest contributions? Finally, what are the novel applications of crowdsourcing enabled by ubiquitous computing systems?},
booktitle = {Proceedings of the 12th ACM International Conference Adjunct Papers on Ubiquitous Computing - Adjunct},
pages = {523–526},
numpages = {4},
keywords = {ubiquitous, mobile, crowdsourcing},
location = {Copenhagen, Denmark},
series = {UbiComp '10 Adjunct}
}

@inproceedings{10.1145/3021460.3021483,
author = {S., Lalit Mohan and Raman, Priya and Choppella, Venkatesh and Reddy, Y. R.},
title = {A Crowdsourcing Approach for Quality Enhancement of eLearning Systems},
year = {2017},
isbn = {9781450348560},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3021460.3021483},
doi = {10.1145/3021460.3021483},
abstract = {In India, a large number of engineering undergraduates are adopting eLearning as it provides access to best faculty and reduces concerns on inadequate physical infrastructure at colleges. Virtual Labs is a Government of India eLearning initiative containing simulation and remote triggered labs for engineering students. Virtual Labs developed over a period of 6 years is used by more than a million undergraduate students across nine engineering disciplines. The software used for developing these experiments requires substantial effort for maintenance due to deprecation, compatibility, etc. We propose a targeted crowdsourcing approach for maintenance of Virtual Labs with sustainable quality. The targeted crowdsourcing involves the large number of engineering students who are also the major stakeholders of these labs. Our quality enhancement using crowdsourcing approach was validated for 14 labs and would be extended to 191 labs based on encouraging results.},
booktitle = {Proceedings of the 10th Innovations in Software Engineering Conference},
pages = {188–194},
numpages = {7},
keywords = {eLearning Systems, Software Development, Quality, Crowdsourcing},
location = {Jaipur, India},
series = {ISEC '17}
}

@inproceedings{10.1145/2666539.2666569,
author = {Yan, Minzhi and Sun, Hailong and Liu, Xudong},
title = {iTest: testing software with mobile crowdsourcing},
year = {2014},
isbn = {9781450332248},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2666539.2666569},
doi = {10.1145/2666539.2666569},
abstract = {In recent years, a lot of crowdsourcing systems have emerged and lead to many successful crowdsourcing systems like Wiki-pedia, Amazon Mechanical Turk and Waze. In the field of software engineering, crowdtesting has acquired increased interest and adoption, especially among personal developers and smaller companies. In this paper, we present iTest which combines mobile crowdsourcing and software testing together to support the testing of mobile application and web services. iTest is a framework for software developers to submit their software and conveniently get the test results from the crowd testers. Firstly, we analyze the key problems need to be solved in a mobile crowdtesting platform; Secondly, we present the architecture of iTest framework; Thirdly, we introduce the workflow of testing web service in iTest and propose an algorithm for solving the tester selection problem mentioned in Section 2; Then the development kit to support testing mobile application is explained; Finally, we perform two experiments to illustrate that both the way to access network and tester's location influence the performance of web service.},
booktitle = {Proceedings of the 1st International Workshop on Crowd-Based Software Development Methods and Technologies},
pages = {19–24},
numpages = {6},
keywords = {web service, mobile crowdsourcing, mobile application, Software testing},
location = {Hong Kong, China},
series = {CrowdSoft 2014}
}

@inproceedings{10.1109/COASE.2018.8560512,
author = {Fadda, Edoardo and Perboli, Guido and Vallesio, Valerio and Mana, Dario},
title = {Sustainable mobility and user preferences by crowdsourcing data: the Open Agora project},
year = {2018},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/COASE.2018.8560512},
doi = {10.1109/COASE.2018.8560512},
abstract = {One application of network optimization is the study of the policies able to change people habits in transportation mode selection. The main strategy for achieving this objective is to develop a model describing the preferences of the people by considering the characteristics of the transportation modes (such as cost, travel duration) and then to develop policies in order to improve the characteristics of the target transportation mode. These models are called utility models and have a long story. Nevertheless, the data needed for their fitting are difficult to get. One of the main issues in this case is how to collect the data and how to tune a model that can be easily scaled and adapted to different settings. In this paper, we describe the results achieved during the Open Agora project where the utility model is tined with crowdsourcing data coming from mobile phone applications and collected indirectly by the users.},
booktitle = {2018 IEEE 14th International Conference on Automation Science and Engineering (CASE)},
pages = {1243–1248},
numpages = {6},
location = {Munich, Germany}
}

@inproceedings{10.1109/CrowdRE.2015.7367586,
author = {Srivastava, Pratyoush K. and Sharma, Richa},
title = {Crowdsourcing to elicit requirements for MyERP application},
year = {2015},
isbn = {9781509001132},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/CrowdRE.2015.7367586},
doi = {10.1109/CrowdRE.2015.7367586},
abstract = {Crowdsourcing is an emerging method to collect requirements for software systems. Applications seeking global acceptance need to meet the expectations of a wide range of users. Collecting requirements and arriving at consensus with a wide range of users is difficult using traditional method of requirements elicitation. This paper presents crowdsourcing based approach for German medium-size software company MyERP that might help the company to get access to requirements from non-German customers. We present the tasks involved in the proposed solution that would help the company meet the goal of eliciting requirements at a fast pace with non-German customers.},
booktitle = {Proceedings of the 2015 IEEE 1st International Workshop on Crowd-Based Requirements Engineering (CrowdRE)},
pages = {31–35},
numpages = {5},
series = {CROWDRE '15}
}

@inproceedings{10.1145/2556420.2556479,
author = {Pavlick, Ellie and Yan, Rui and Callison-Burch, Chris},
title = {Crowdsourcing for grammatical error correction},
year = {2014},
isbn = {9781450325417},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2556420.2556479},
doi = {10.1145/2556420.2556479},
abstract = {We discuss the problem of grammatical error correction, which has gained attention for its usefulness both in the development of tools for learners of foreign languages and as a component of statistical machine translation systems. We believe the task of suggesting grammar and style corrections in writing is well suited to a crowdsourcing solution but is currently hindered by the difficulty of automatic quality control. In this proposal, we motivate the problem of grammatical error correction and outline the challenges of ensuring quality in a setting where traditional methods of aggregation (e.g. majority vote) fail to produce the desired results. We then propose a design for quality control and present preliminary results indicating the potential of crowd workers to provide a scalable solution.},
booktitle = {Proceedings of the Companion Publication of the 17th ACM Conference on Computer Supported Cooperative Work \&amp; Social Computing},
pages = {209–212},
numpages = {4},
keywords = {postediting, esl, crowdsourcing},
location = {Baltimore, Maryland, USA},
series = {CSCW Companion '14}
}

@inproceedings{10.1145/3308558.3320096,
author = {Aroyo, Lora and Dumitrache, Anca and Inel, Oana and Szl\'{a}vik, Zolt\'{a}n and Timmermans, Benjamin and Welty, Chris},
title = {Crowdsourcing Inclusivity: Dealing with Diversity of Opinions, Perspectives and Ambiguity in Annotated Data},
year = {2019},
isbn = {9781450366755},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3308558.3320096},
doi = {10.1145/3308558.3320096},
abstract = {In this tutorial, we introduce a novel crowdsourcing methodology called CrowdTruth [1, 9]. The central characteristic of CrowdTruth is harnessing the diversity in human interpretation to capture the wide range of opinions and perspectives, and thus provide more reliable, realistic and inclusive real-world annotated data for training and evaluating machine learning components. Unlike other methods, we do not discard dissenting votes, but incorporate them into a richer and more continuous representation of truth. CrowdTruth is a widely used crowdsourcing methodology1 adopted by industrial partners and public organizations such as Google, IBM, New York Times, Cleveland Clinic, Crowdynews, Sound and Vision archive, Rijksmuseum, and in a multitude of domains such as AI, news, medicine, social media, cultural heritage, and social sciences. The goal of this tutorial is to introduce the audience to a novel approach to crowdsourcing that takes advantage of the diversity of opinions and perspectives that is inherent to the Web, as methods that deal with disagreement and diversity in crowdsourcing have become increasingly popular. Creating this more complex notion of truth contributes directly to the larger discussion on how to make the Web more reliable, diverse and inclusive.},
booktitle = {Companion Proceedings of The 2019 World Wide Web Conference},
pages = {1294–1295},
numpages = {2},
keywords = {Perspectives, Medical Text Annotation, Inter-annotator Disagreement, Ground Truth, Diversity, Digital Humanities, Crowdsourcing, Computational Social Sciences, Ambiguity},
location = {San Francisco, USA},
series = {WWW '19}
}

@inproceedings{10.1145/3308558.3313599,
author = {Yang, Jie and Smirnova, Alisa and Yang, Dingqi and Demartini, Gianluca and Lu, Yuan and Cudre-Mauroux, Philippe},
title = {Scalpel-CD: Leveraging Crowdsourcing and Deep Probabilistic Modeling for Debugging Noisy Training Data},
year = {2019},
isbn = {9781450366748},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3308558.3313599},
doi = {10.1145/3308558.3313599},
abstract = {This paper presents Scalpel-CD, a first-of-its-kind system that leverages both human and machine intelligence to debug noisy labels from the training data of machine learning systems. Our system identifies potentially wrong labels using a deep probabilistic model, which is able to infer the latent class of a high-dimensional data instance by exploiting data distributions in the underlying latent feature space. To minimize crowd efforts, it employs a data sampler which selects data instances that would benefit the most from being inspected by the crowd. The manually verified labels are then propagated to similar data instances in the original training data by exploiting the underlying data structure, thus scaling out the contribution from the crowd. Scalpel-CD is designed with a set of algorithmic solutions to automatically search for the optimal configurations for different types of training data, in terms of the underlying data structure, noise ratio, and noise types (random vs. structural). In a real deployment on multiple machine learning tasks, we demonstrate that Scalpel-CD is able to improve label quality by 12.9\% with only 2.8\% instances inspected by the crowd.},
booktitle = {The World Wide Web Conference},
pages = {2158–2168},
numpages = {11},
keywords = {deep probabilistic models, crowdsourcing, Debugging training data},
location = {San Francisco, CA, USA},
series = {WWW '19}
}

@inproceedings{10.1145/2787394.2787397,
author = {Mandalari, Anna Maria and Bagnulo, Marcelo and Lutu, Andra},
title = {Informing Protocol Design Through Crowdsourcing: the Case of Pervasive Encryption},
year = {2015},
isbn = {9781450335393},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2787394.2787397},
doi = {10.1145/2787394.2787397},
abstract = {Middleboxes, such as proxies, firewalls and NATs play an important role in the modern Internet ecosystem. On one hand, they perform advanced functions, e.g. traffic shaping, security or enhancing application performance. On the other hand, they turn the Internet into a hostile ecosystem for innovation, as they limit the deviation from deployed protocols. It is therefore essential, when designing a new protocol, to first understand its interaction with the elements of the path. The emerging area of crowdsourcing solutions can help to shed light on this issue. Such approach allows us to reach large and different sets of users and also different types of devices and networks to perform Internet measurements. In this paper, we show how to make informed protocol design choices by using a crowdsourcing platform. We consider a specific use case, namely the case of pervasive encryption in the modern Internet. Given the latest public disclosures of the NSA global surveillance operations, the issue of privacy in the Internet became of paramount importance. Internet community efforts are thus underway to increase the adoption of encryption. Using a crowdsourcing approach, we perform large-scale TLS measurements to advance our understanding on whether wide adoption of encryption is possible in today's Internet.},
booktitle = {Proceedings of the 2015 ACM SIGCOMM Workshop on Crowdsourcing and Crowdsharing of Big (Internet) Data},
pages = {3–8},
numpages = {6},
keywords = {middleboxes, internet measurements, crowdsourcing, TLS},
location = {London, United Kingdom},
series = {C2B(1)D '15}
}

@inproceedings{10.1109/ACII.2015.7344618,
author = {McDuff, Daniel and el Kaliouby, Rana and Picard, Rosalind W.},
title = {Crowdsourcing facial responses to online videos: Extended abstract},
year = {2015},
isbn = {9781479999538},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/ACII.2015.7344618},
doi = {10.1109/ACII.2015.7344618},
abstract = {Traditional observational research methods required an experimenter's presence in order to record videos of participants, and limited the scalability of data collection to typically less than a few hundred people in a single location. In order to make a significant leap forward in affective expression data collection and the insights based on it, our work has created and validated a novel framework for collecting and analyzing facial responses over the Internet. The first experiment using this framework enabled 3,268 trackable face videos to be collected and analyzed in under two months. Each participant viewed one or more commercials while their facial response was recorded and analyzed. Our data showed significantly different intensity and dynamics patterns of smile responses between subgroups who reported liking the commercials versus those who did not. Since this framework appeared in 2011, we have collected over three million videos of facial responses in over 75 countries using this same methodology, enabling facial analytics to become significantly more accurate and validated across five continents. Many new insights have been discovered based on crowd-sourced facial data, enabling Internet-based measurement of facial responses to become reliable and proven. We are now able to provide large-scale evidence for gender, cultural and age differences in behaviors. Today such methods are used as part of standard practice in industry for copy-testing advertisements and are increasingly used for online media evaluations, distance learning, and mobile applications.},
booktitle = {Proceedings of the 2015 International Conference on Affective Computing and Intelligent Interaction (ACII)},
pages = {512–518},
numpages = {7},
series = {ACII '15}
}

@inproceedings{10.1007/978-3-030-21935-2_9,
author = {Aihara, Kenro and Bin, Piao and Imura, Hajime},
title = {On the Relationship Between Accuracy of Bus Position Estimated by Crowdsourcing and Participation Density},
year = {2019},
isbn = {978-3-030-21934-5},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-21935-2_9},
doi = {10.1007/978-3-030-21935-2_9},
abstract = {The authors proposed a methodology of bus location service by crowdsource. In the conventional bus location service, a GPS receiver or the like for positioning the bus vehicle and a communication line for transmitting the position information are necessary for each bus and it causes expensive cost, whereas in this methodology, the BLE beacon and the application of the service installed in users’ smartphone transmits the beacon signal and the position information of the smartphone using the communication line of users’ smartphone so as to reduce the cost of bus operators.In order to grasp the position of each bus at any time on the server, one of the following is required: (1) At least one service user always exists for each bus, and (2) It is possible to estimate the position with high accuracy where there is no service user in a section.If contributing to the realization of smart city by grasping the situation in the town by crowdsourcing, it is extremely important to clarify the precision with respect to density that may be given as a relation between the number of users who participate in the program for grasping this situation and the size of the town.In this paper, the authors examine and show the relationship between the density of data collection and the accuracy of interpolation for the section where data is missing, using the position information actually obtained using the proposed application.},
booktitle = {Distributed, Ambient and Pervasive Interactions: 7th International Conference, DAPI 2019, Held as Part of the 21st HCI International Conference, HCII 2019, Orlando, FL, USA, July 26–31, 2019, Proceedings},
pages = {101–112},
numpages = {12},
keywords = {Crowdsourcing, Crowdsensing, Smart and hybrid cities, Internet of Things},
location = {Orlando, FL, USA}
}

@inproceedings{10.5555/2969033.2969105,
author = {Jagabathula, Srikanth and Subramanian, Lakshminarayanan and Venkataraman, Ashwin},
title = {Reputation-based Worker filtering in crowdsourcing},
year = {2014},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {In this paper, we study the problem of aggregating noisy labels from crowd workers to infer the underlying true labels of binary tasks. Unlike most prior work which has examined this problem under the random worker paradigm, we consider a much broader class of adversarial workers with no specific assumptions on their labeling strategy. Our key contribution is the design of a computationally efficient reputation algorithm to identify and filter out these adversarial workers in crowd-sourcing systems. Our algorithm uses the concept of optimal semi-matchings in conjunction with worker penalties based on label disagreements, to assign a reputation score for every worker. We provide strong theoretical guarantees for deterministic adversarial strategies as well as the extreme case of sophisticated adversaries where we analyze the worst-case behavior of our algorithm. Finally, we show that our reputation algorithm can significantly improve the accuracy of existing label aggregation algorithms in real-world crowdsourcing datasets.},
booktitle = {Proceedings of the 27th International Conference on Neural Information Processing Systems - Volume 2},
pages = {2492–2500},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'14}
}

@inproceedings{10.5555/2615731.2615808,
author = {Nath, Swaprava and Narayanaswamy, Balakrishnan (Murali)},
title = {Productive output in hierarchical crowdsourcing},
year = {2014},
isbn = {9781450327381},
publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
address = {Richland, SC},
abstract = {Organically grown crowdsourcing networks, which includes production firms and social network-based crowdsourcing applications, tend to have a hierarchical structure. Considering the entire crowdsourcing system as a consolidated organization, a primary goal of a designer is to maximize the net productive output of this hierarchy using reward sharing as an incentive tool. Every individual in a hierarchy has a limited amount of effort that they can split between production and communication. Productive effort yields an agent a direct payoff, while the communication effort of an agent improves the productivity of other agents in her subtree. To understand how the net output of the crowdsourcing network is influenced by these components, we develop a game theoretic model that helps explain how the individuals trade off these two components depending on their position in the hierarchy and their shares of reward. We provide a detailed analysis of the Nash equilibrium efforts and a design recipe of the reward sharing scheme that maximizes the net productive output. Our results show that even under strategic behavior of the agents, it is sometimes possible to achieve the optimal output and also provide bounds on the achievability when this is not the case.},
booktitle = {Proceedings of the 2014 International Conference on Autonomous Agents and Multi-Agent Systems},
pages = {469–476},
numpages = {8},
keywords = {social output, price of anarchy, nash equilibrium, hierarchies, crowdsourcing},
location = {Paris, France},
series = {AAMAS '14}
}

@inproceedings{10.1145/3345035.3345067,
author = {Chi, Aining and Ren, Nan},
title = {Research on the Impact of Task Feedback on the Performance of Creative Crowdsourcing Solvers},
year = {2019},
isbn = {9781450372190},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3345035.3345067},
doi = {10.1145/3345035.3345067},
abstract = {The key factor of crowdsourcing development is the solvers performance. The paper based on the customer perceived value theory to explore the influences of employer's task feedback on the performance of solvers in creative crowdsourcing. Employer's feedback in terms of quality, creativity, transaction price and service has different degrees of influence on the performance of solvers in creative crowdsourcing, while the value perception in terms of time and after-sales commitment has no obvious effect. The feedback of the employer in terms of favorable comment plays an mediating effect in the relationship between service quality and performance of solvers. And the relational behavior between the employer and the solver regulates the above mediating effect. Finally, according to the conclusions of the research, resource allocation suggestions of the strategic for the solvers are proposed.},
booktitle = {Proceedings of the 2019 10th International Conference on E-Business, Management and Economics},
pages = {101–105},
numpages = {5},
keywords = {task feedback, solvers performance, Crowdsourcing},
location = {Beijing, China},
series = {ICEME '19}
}

@inproceedings{10.1007/978-3-030-01391-2_4,
author = {Dong, Zhaoan and Tu, Jianhong and Fan, Ju and Lu, Jiaheng and Du, Xiaoyong and Ling, Tok Wang},
title = {Crowd-Type: A Crowdsourcing-Based Tool for Type Completion in Knowledge Bases},
year = {2018},
isbn = {978-3-030-01390-5},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-01391-2_4},
doi = {10.1007/978-3-030-01391-2_4},
abstract = {Entity type completion in Knowledge Bases (KBs) is an important and challenging problem. In our recent work, we have proposed a hybrid framework which combines the human intelligence of crowdsourcing with automatic algorithms to address the problem. In this demo, we have implemented the framework in a crowdsourcing-based system, named Crowd-Type, for fine-grained type completion in KBs. In particular, Crowd-Type firstly employs automatic algorithms to select the most representative entities and assigns them to human workers, who will verify the types for assigned entities. Then, the system infers and determines the correct types for all entities utilizing both the results of crowdsourcing and machine-based algorithms. Our system gives a vivid demonstration to show how crowdsourcing significantly improves the performance of automatic type completion algorithms.},
booktitle = {Advances in Conceptual Modeling: ER 2018 Workshops Emp-ER, MoBiD, MREBA, QMMQ, SCME, Xi’an, China, October 22-25, 2018, Proceedings},
pages = {17–21},
numpages = {5},
location = {Xi'an, China}
}

@inproceedings{10.1145/2872518.2891113,
author = {Suzuki, Yu and Nakamura, Satoshi},
title = {Assessing the Quality of Wikipedia Editors through Crowdsourcing},
year = {2016},
isbn = {9781450341448},
publisher = {International World Wide Web Conferences Steering Committee},
address = {Republic and Canton of Geneva, CHE},
url = {https://doi.org/10.1145/2872518.2891113},
doi = {10.1145/2872518.2891113},
abstract = {In this paper, we propose a method for assessing the quality of Wikipedia editors. By effectively determining whether the text meaning persists over time, we can determine the actual contribution by editors. This is used in this paper to detect vandal. However, the meaning of text does not always change if a term in the text is added or removed. Therefore, we cannot capture the changes of text meaning automatically, so we cannot detect whether the meaning of text survives or not. To solve this problem, we use crowdsourcing to manually detect changes of text meaning. In our experiment, we confirmed that our proposed method improves the accuracy of detecting vandals by about 5\%.},
booktitle = {Proceedings of the 25th International Conference Companion on World Wide Web},
pages = {1001–1006},
numpages = {6},
keywords = {wikipedia, vandalism, quality, crowdsourcing},
location = {Montr\'{e}al, Qu\'{e}bec, Canada},
series = {WWW '16 Companion}
}

@inproceedings{10.1109/WCNC45663.2020.9120841,
author = {Hao, Lifei and Jia, Bing and Liu, Jingbin and Huang, Baoqi and Li, Wuyungerile},
title = {VCG-QCP: A Reverse Pricing Mechanism Based on VCG and Quality All-pay for Collaborative Crowdsourcing},
year = {2020},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/WCNC45663.2020.9120841},
doi = {10.1109/WCNC45663.2020.9120841},
abstract = {With the rapid development of the Internet and combined with outsourcing, a new paradigm – crowdsourcing which shines brilliantly as a new labor mode. However, the existing pricing strategies for crowdsourcing tasks have several undesirable problems, e.g., no universal pricing model, not meeting the multiple requirements of users, pricing rely too much on decision makers, etc., which bring an unreasonable allocation of task rewards so as to make the pricing results subjective and uncontrollable. Therefore, this paper proposes a reverse pricing mechanism based on VCG and quality all-pay for collaborative crowdsourcing (VCG-QCP). The actual crowdsourcing scenario is considered with VCG mechanism, and the concept of quality all-pay is introduced to evaluate the work quality of workers who might perform the task. Then a general reverse pricing model is established by mathematical modeling, and the pricing algorithm is designed based on this model. Simulations show that the proposed method can achieve higher algorithm efficiency, higher task completion quality, a reasonable balance of benefits between employers and workers, and ensuring the truthfulness of workers’ bidding.},
booktitle = {2020 IEEE Wireless Communications and Networking Conference (WCNC)},
pages = {1–6},
numpages = {6},
location = {Seoul, Korea (South)}
}

@inproceedings{10.1109/ICIP.2015.7351042,
author = {Nicholson, Bryce and Sheng, Victor S. and Zhang, Jing},
title = {Noise correction of image labeling in crowdsourcing},
year = {2015},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICIP.2015.7351042},
doi = {10.1109/ICIP.2015.7351042},
abstract = {We investigate the methods of improving data quality, in terms of label accuracy, in the context of image labeling in crowdsourcing. First, we look at three consensus methods for inferring a ground-truth label from the multiple noisy labels obtained from crowdsourcing, i.e., Majority Voting (MV), Dawid Skene (DS), and KOS. We then apply three noise correction methods to correct labels inferred by these consensus methods, i.e., Polishing Labels (PL), Self-Training Correction (STC), and Cluster Correction (CC). Our experimental results show that the noise correction methods improve the labeling quality significantly.},
booktitle = {2015 IEEE International Conference on Image Processing (ICIP)},
pages = {1458–1462},
numpages = {5},
location = {Quebec City, QC, Canada}
}

@inproceedings{10.1145/3170427.3188667,
author = {Korovina, Olga and Casati, Fabio and Nielek, Radoslaw and Baez, Marcos and Berestneva, Olga},
title = {Investigating Crowdsourcing as a Method to Collect Emotion Labels for Images},
year = {2018},
isbn = {9781450356213},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3170427.3188667},
doi = {10.1145/3170427.3188667},
abstract = {Labeling images is essential towards enabling the search and organization of digital media. This is true for both "factual", objective tags such as time, place and people, as well as for subjective labels, such as the emotion a picture generates. Indeed, the ability to associate emotions to images is one of the key functionality most image analysis services today strive to provide. In this paper we study how emotion labels for images can be crowdsourced and uncover limitations of the approach commonly used to gather training data today, that of harvesting images and tags from social media.},
booktitle = {Extended Abstracts of the 2018 CHI Conference on Human Factors in Computing Systems},
pages = {1–6},
numpages = {6},
keywords = {subjective tasks, image tagging, emotions, crowdsourcing},
location = {Montreal QC, Canada},
series = {CHI EA '18}
}

@inproceedings{10.5555/3020652.3020705,
author = {Lin, Christopher H. and Mausam and Weld, Daniel S.},
title = {Crowdsourcing control: moving beyond multiple choice},
year = {2012},
isbn = {9780974903989},
publisher = {AUAI Press},
address = {Arlington, Virginia, USA},
abstract = {To ensure quality results from crowdsourced tasks, requesters often aggregate worker responses and use one of a plethora of strategies to infer the correct answer from the set of noisy responses. However, all current models assume prior knowledge of all possible outcomes of the task. While not an unreasonable assumption for tasks that can be posited as multiple-choice questions (e.g. n-ary classification), we observe that many tasks do not naturally fit this paradigm, but instead demand a free-response formulation where the outcome space is of infinite size (e.g. audio transcription). We model such tasks with a novel probabilistic graphical model, and design and implement LAZYSUSAN, a decision-theoretic controller that dynamically requests responses as necessary in order to infer answers to these tasks. We also design an EM algorithm to jointly learn the parameters of our model while inferring the correct answers to multiple tasks at a time. Live experiments on Amazon Mechanical Turk demonstrate the superiority of LAZYSUSAN at solving SAT Math questions, eliminating 83.2\% of the error and achieving greater net utility compared to the state-of-the-art strategy, majority-voting. We also show in live experiments that our EM algorithm outperforms majority-voting on a visualization task that we design.},
booktitle = {Proceedings of the Twenty-Eighth Conference on Uncertainty in Artificial Intelligence},
pages = {491–500},
numpages = {10},
location = {Catalina Island, CA},
series = {UAI'12}
}

@inproceedings{10.1145/2800835.2800971,
author = {Cvijikj, Irena Pletikosa and Kadar, Cristina and Ivan, Bogdan and Te, Yiea-Funk},
title = {Towards a crowdsourcing approach for crime prevention},
year = {2015},
isbn = {9781450335751},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2800835.2800971},
doi = {10.1145/2800835.2800971},
abstract = {With the rising level of criminal activities, crime is becoming one of the main problems of modern society. To address this issue, we implement a mobile application for crime prevention. We focus on the usage intention and motivations for content creation and consumption. Our results indicate that people are willing to use the app for acquiring and sharing crime-related information, but not on a daily basis. In addition, participation on the platform was found to be driven by affective and rational motivations, to contribute to the neighborhood safety and in return receive help for maintaining personal safety.},
booktitle = {Adjunct Proceedings of the 2015 ACM International Joint Conference on Pervasive and Ubiquitous Computing and Proceedings of the 2015 ACM International Symposium on Wearable Computers},
pages = {1367–1372},
numpages = {6},
keywords = {public good, participation, motivations, mobile app, crowdsourcing, crime prevention, crime mapping},
location = {Osaka, Japan},
series = {UbiComp/ISWC'15 Adjunct}
}

@inproceedings{10.1145/2858036.2858268,
author = {Doroudi, Shayan and Kamar, Ece and Brunskill, Emma and Horvitz, Eric},
title = {Toward a Learning Science for Complex Crowdsourcing Tasks},
year = {2016},
isbn = {9781450333627},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2858036.2858268},
doi = {10.1145/2858036.2858268},
abstract = {We explore how crowdworkers can be trained to tackle complex crowdsourcing tasks. We are particularly interested in training novice workers to perform well on solving tasks in situations where the space of strategies is large and workers need to discover and try different strategies to be successful. In a first experiment, we perform a comparison of five different training strategies. For complex web search challenges, we show that providing expert examples is an effective form of training, surpassing other forms of training in nearly all measures of interest. However, such training relies on access to domain expertise, which may be expensive or lacking. Therefore, in a second experiment we study the feasibility of training workers in the absence of domain expertise. We show that having workers validate the work of their peer workers can be even more effective than having them review expert examples if we only present solutions filtered by a threshold length. The results suggest that crowdsourced solutions of peer workers may be harnessed in an automated training pipeline.},
booktitle = {Proceedings of the 2016 CHI Conference on Human Factors in Computing Systems},
pages = {2623–2634},
numpages = {12},
keywords = {worker training, worked examples, web search, peer review, education, crowdsourcing},
location = {San Jose, California, USA},
series = {CHI '16}
}

@inproceedings{10.1145/3256341,
author = {Ju, Wendy},
title = {Session details: Crowdsourcing},
year = {2012},
isbn = {9781450310864},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3256341},
doi = {10.1145/3256341},
booktitle = {Proceedings of the ACM 2012 Conference on Computer Supported Cooperative Work},
location = {Seattle, Washington, USA},
series = {CSCW '12}
}

@inproceedings{10.1109/GLOCOM.2016.7842167,
author = {Ying, Xuhang and Roy, Sumit and Poovendran, Radha},
title = {Pricing Mechanism for Quality-Based Radio Mapping via Crowdsourcing},
year = {2016},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/GLOCOM.2016.7842167},
doi = {10.1109/GLOCOM.2016.7842167},
abstract = {White Space (WS) Networking crucially relies on the active monitoring of spatio-temporal spectrum usage (to identify WS opportunities). To achieve this, one way is to gather spectrum data via wide-area sensor deployment and construct better Radio Environment Maps (REMs) with spatial models such as Kriging and Gaussian Process (GP). An economically viable alternative is via incentivized crowdsourcing, i.e., outsourcing sensing tasks to mobile users who have sensorized high-end client devices like tablets or smartphones, and providing proper incentives to compensate for users' sensing costs. In crowdsourced REM, features that impact REM performance and economic cost include user locations and the heterogeneity of user devices, which impact data quality and sensing costs. In this work, we emphasize the use of a hardware noise term in the GP model to account for data quality, and adopt mutual information to quantify sampling performance; we further design a pricing mechanism that allows the platform to maximize its expected utility at each stage and send optimal price offers to users sequentially, with joint consideration of sampling value, data quality and cost. We conduct simulations to evaluate the performance. Simulation results show that our mechanism outperforms two baseline mechanisms, and benefits from more users and less hardware noise (i.e., better data quality).},
booktitle = {2016 IEEE Global Communications Conference (GLOBECOM)},
pages = {1–6},
numpages = {6},
location = {Washington, DC, USA}
}

@inproceedings{10.1145/3251625,
author = {Krishnamurthy, Balachander},
title = {Session details: Uses of Crowdsourcing for Networking},
year = {2015},
isbn = {9781450335393},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3251625},
doi = {10.1145/3251625},
booktitle = {Proceedings of the 2015 ACM SIGCOMM Workshop on Crowdsourcing and Crowdsharing of Big (Internet) Data},
location = {London, United Kingdom},
series = {C2B(1)D '15}
}

@inproceedings{10.5555/3061053.3061159,
author = {Segal, Avi and Gal, Ya'akov and Kamar, Ece and Horvitz, Eric and Bowyer, Alex and Miller, Grant},
title = {Intervention strategies for increasing engagement in crowdsourcing: platform, predictions, and experiments},
year = {2016},
isbn = {9781577357704},
publisher = {AAAI Press},
abstract = {Volunteer-based crowdsourcing depend critically on maintaining the engagement of participants. We explore a methodology for extending engagement in citizen science by combining machine learning with intervention design. We first present a platform for using real-time predictions about forthcoming disengagement to guide interventions. Then we discuss a set of experiments with delivering different messages to users based on the proximity to the predicted time of disengagement. The messages address motivational factors that were found in prior studies to influence users' engagements. We evaluate this approach on Galaxy Zoo, one of the largest citizen science application on the web, where we traced the behavior and contributions of thousands of users who received intervention messages over a period of a few months. We found sensitivity of the amount of user contributions to both the timing and nature of the message. Specifically, we found that a message emphasizing the helpfulness of individual users significantly increased users' contributions when delivered according to predicted times of disengagement, but not when delivered at random times. The influence of the message on users' contributions was more pronounced as additional user data was collected and made available to the classifier.},
booktitle = {Proceedings of the Twenty-Fifth International Joint Conference on Artificial Intelligence},
pages = {3861–3867},
numpages = {7},
location = {New York, New York, USA},
series = {IJCAI'16}
}

@inproceedings{10.1109/COMPSAC.2015.279,
author = {Sakamoto, Mizuki and Nakajima, Tatsuo and Akioka, Sayaka},
title = {Design Strategies for Building Mobile Crowdsourcing Services},
year = {2015},
isbn = {9781467365642},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/COMPSAC.2015.279},
doi = {10.1109/COMPSAC.2015.279},
abstract = {In this paper, we present an overview of three community-based mobile crowd sourcing services that we have developed as case studies. We then extract four lessons learned from our experiences to show that motivating people is an important factor in designing mobile crowd sourcing service. The extracted lessons are essential to successful design strategies for developing future crowd sourcing services.},
booktitle = {Proceedings of the 2015 IEEE 39th Annual Computer Software and Applications Conference - Volume 03},
pages = {234–239},
numpages = {6},
keywords = {Mobile Crowdsourcing, Human and Social Factors},
series = {COMPSAC '15}
}

@inproceedings{10.1109/SAHCN.2017.7964933,
author = {Xu, Jia and Li, Hui and Li, Yanxu and Yang, Dejun and Li, Tao},
title = {Incentivizing the Biased Requesters: Truthful Task Assignment Mechanisms in Crowdsourcing},
year = {2017},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/SAHCN.2017.7964933},
doi = {10.1109/SAHCN.2017.7964933},
abstract = {Crowdsourcing has become an effective tool to utilize human intelligence to perform tasks that are challenging for machines. In the integrated crowdsourcing systems, the requesters are non- monopolistic and may show preferences over the workers. We are the first to design the incentive mechanisms, which consider the issue of stimulating the biased requesters in the competing crowdsourcing market. In this paper, we explore truthful task assignment mechanisms to maximize the total value of accomplished tasks for this new scenario. We present three models of crowdsourcing, which take the preferences of the requesters and the workload constraints of the workers into consideration. We design a task assignment mechanism, which follows the matching approach to solve the Valuation Maximizing Assignment (VMA) problem for each of the three models. Through both rigorous theoretical analyses and extensive simulations, we demonstrate that the proposed assignment mechanisms achieve computational efficiency, workload feasibility, preference (universal) truthfulness and constant approximation.},
booktitle = {2017 14th Annual IEEE International Conference on Sensing, Communication, and Networking (SECON)},
pages = {1–9},
numpages = {9},
location = {San Diego, CA, USA}
}

@inproceedings{10.1145/3358695.3360920,
author = {Mauricio Yagui, Marcela Mayumi and Monsores Passos Maia, Lu\'{\i}s Fernando and Oliveira, Jonice and Vivacqua, Adriana},
title = {A Crowdsourcing Platform for Curating Cultural and Empirical Knowledge. A Study Applied to Botanical Collections},
year = {2019},
isbn = {9781450369886},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3358695.3360920},
doi = {10.1145/3358695.3360920},
abstract = {Empirical and Traditional knowledge is part of the concept of intangible cultural heritage, which represents immaterial cultural heritage items such as facts, stories, and traditions that are part of local culture. However, this knowledge might be lost over time. Encouraging public participation to record empirical knowledge is a way to preserve the culture of a region and make it known to others. This paper presents a web system that aims to record and to preserve empirical knowledge data. To achieve this goal, the tool uses three combined approaches: (i) crowdsourcing calls oriented to content creation by visitors; (ii) collaborative curation among experts for selection, evaluation, and publication of content; and (iii) interconnection between user contributions and data already available in open repositories on the web (in LOD format). The tool was applied in the Botany domain to curate empirical and cultural data about medicinal plants, where experts from the Rio de Janeiro Botanical Garden evaluated it.},
booktitle = {IEEE/WIC/ACM International Conference on Web Intelligence - Companion Volume},
pages = {322–326},
numpages = {5},
keywords = {Linked Open Data, Intangible Cultural Heritage., Cultural Heritage, Crowdsourcing, Content Curation, Collaborative Systems},
location = {Thessaloniki, Greece},
series = {WI '19 Companion}
}

@inproceedings{10.5555/2874493.2874517,
author = {Euzenat, J\'{e}r\^{o}me},
title = {Uncertainty in crowdsourcing ontology matching},
year = {2013},
publisher = {CEUR-WS.org},
address = {Aachen, DEU},
booktitle = {Proceedings of the 8th International Conference on Ontology Matching - Volume 1111},
pages = {221–222},
numpages = {2},
location = {Sydney, Australia},
series = {OM'13}
}

@inproceedings{10.5555/2887007.2887187,
author = {Tran-Thanh, Long and Huynh, Trung Dong and Rosenfeld, Avi and Ramchurn, Sarvapali D. and Jennings, Nicholas R.},
title = {Crowdsourcing complex workflows under budget constraints},
year = {2015},
isbn = {0262511290},
publisher = {AAAI Press},
abstract = {We consider the problem of task allocation in crowdsourcing systems with multiple complex workflows, each of which consists of a set of inter-dependent micro-tasks. We propose Budgeteer, an algorithm to solve this problem under a budget constraint. In particular, our algorithm first calculates an efficient way to allocate budget to each workflow. It then determines the number of inter-dependent micro-tasks and the price to pay for each task within each workflow, given the corresponding budget constraints. We empirically evaluate it on a well-known crowdsourcing-based text correction workflow using Amazon Mechanical Turk, and show that Budgeteer can achieve similar levels of accuracy to current benchmarks, but is on average 45\% cheaper.},
booktitle = {Proceedings of the Twenty-Ninth AAAI Conference on Artificial Intelligence},
pages = {1298–1304},
numpages = {7},
location = {Austin, Texas},
series = {AAAI'15}
}

@inproceedings{10.5555/2887007.2887182,
author = {Jyothi, Preethi and Hasegawa-Johnson, Mark},
title = {Acquiring speech transcriptions using mismatched crowdsourcing},
year = {2015},
isbn = {0262511290},
publisher = {AAAI Press},
abstract = {Transcribed speech is a critical resource for building statistical speech recognition systems. Recent work has looked towards soliciting transcriptions for large speech corpora from native speakers of the language using crowdsourcing techniques. However, native speakers of the target language may not be readily available for crowdsourcing. We examine the following question: can humans unfamiliar with the target language help transcribe? We follow an information-theoretic approach to this problem: (1) We learn the characteristics of a noisy channel that models the transcribers' systematic perception biases. (2) We use an error-correcting code, specifically a repetition code, to encode the inputs to this channel, in conjunction with a maximum-likelihood decoding rule. To demonstrate the feasibility of this approach, we transcribe isolated Hindi words with the help of Mechanical Turk workers unfamiliar with Hindi. We successfully recover Hindi words with an accuracy of over 85\% (and 94\% in a 4-best list) using a 15-fold repetition code. We also estimate the conditional entropy of the input to this channel (Hindi words) given the channel output (transcripts from crowdsourced workers) to be less than 2 bits; this serves as a theoretical estimate of the average number of bits of auxiliary information required for errorless recovery.},
booktitle = {Proceedings of the Twenty-Ninth AAAI Conference on Artificial Intelligence},
pages = {1263–1269},
numpages = {7},
location = {Austin, Texas},
series = {AAAI'15}
}

@inproceedings{10.5555/2655780.2655900,
author = {Rubenstein, Ellen L.},
title = {Crowdsourcing health literacy: the case of an online community},
year = {2013},
isbn = {0877155453},
publisher = {American Society for Information Science},
address = {USA},
abstract = {Drawing on data from 31 semi-structured, in-depth interviews, participant observation, and online archives analysis, this paper examines the health information interactions that participants in an online breast cancer community experienced as they progressed through breast cancer and survivorship. The findings reveal... This research highlights patients' perceptions of information gaps, how patients navigated through their information gaps with the help of the community, and the significance of peer interaction in the comprehension of medical information and medical decision-making.},
booktitle = {Proceedings of the 76th ASIS&amp;T Annual Meeting: Beyond the Cloud: Rethinking Information Boundaries},
articleno = {120},
numpages = {5},
keywords = {online communities, health literacy, health information},
location = {Montreal, Quebec, Canada},
series = {ASIST '13}
}

@inproceedings{10.5555/2832249.2832403,
author = {Chen, Cen and Cheng, Shih-Fen and Lau, Hoong Chuin and Misra, Archan},
title = {Towards city-scale mobile crowdsourcing: task recommendations under trajectory uncertainties},
year = {2015},
isbn = {9781577357384},
publisher = {AAAI Press},
abstract = {In this work, we investigate the problem of largescale mobile crowdsourcing, where workers are financially motivated to perform location-based tasks physically. Unlike current industry practice that relies on workers to manually pick tasks to perform, we automatically make task recommendation based on workers' historical trajectories and desired time budgets. The challenge of predicting workers' trajectories is that it is faced with uncertainties, as a worker does not take same routes every day. In this work, we depart from deterministic modeling and study the stochastic task recommendation problem where each worker is associated with several predicted routine routes with probabilities. We formulate this problem as a stochastic integer linear program whose goal is to maximize the expected total utility achieved by all workers. We further exploit the separable structures of the formulation and apply the Lagrangian relaxation technique to scale up computation. Experiments have been performed over the instances generated using the real Singapore transportation network. The results show that we can find significantly better solutions than the deterministic formulation.},
booktitle = {Proceedings of the 24th International Conference on Artificial Intelligence},
pages = {1113–1119},
numpages = {7},
location = {Buenos Aires, Argentina},
series = {IJCAI'15}
}

@inproceedings{10.1109/GLOCOM.2016.7842248,
author = {Ni, Jianbing and Lin, Xiaodong and Zhang, Kuan and Yu, Yong},
title = {Secure and Deduplicated Spatial Crowdsourcing: A Fog-Based Approach},
year = {2016},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/GLOCOM.2016.7842248},
doi = {10.1109/GLOCOM.2016.7842248},
abstract = {With the proliferation of mobile devices, spatial crowdsourcing is rising as a new paradigm that enables individuals to participate in tasks related to some locations in the physical world. Nevertheless, how to allocate these tasks to proper mobile users and improve communication efficiency are critical in spatial crowdsourcing. In this paper, we propose Fo-DSC, a fog-based deduplicated spatial crowdsourcing framework to achieve precise task allocation and secure data deduplication. Specifically, by integrating fog computing, we design a two-step task allocation mechanism to improve the accuracy of tasks allocation in spatial crowdsourcing. The fog nodes can detect and erase the repeated data in crowdsensing reports without learning any information about the reports. Furthermore, Fo-DSC efficiently records the contributions of mobile users whose data are reduplicated and deleted. As a result, these users do not become discouraged. Finally, we demonstrate that Fo-DSC satisfies the properties of fog-based task allocation and secure data deduplication with low computational and communication overheads.},
booktitle = {2016 IEEE Global Communications Conference (GLOBECOM)},
pages = {1–6},
numpages = {6},
location = {Washington, DC, USA}
}

@inproceedings{10.1145/3011141.3011173,
author = {Gushima, Kota and Sakamoto, Mizuki and Nakajima, Tatsuo},
title = {Community-based crowdsourcing to increase a community's well-being},
year = {2016},
isbn = {9781450348072},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3011141.3011173},
doi = {10.1145/3011141.3011173},
abstract = {The paper proposes a new way to encourage a community to increase the sustainability of their surroundings based on crowdsourcing. The most important characteristic in the approach is to automatically insert tasks that increase the community's well-being. Therefore, the more tasks the community's members perform that make their surroundings sustainable, the more they increase their well-being. The research is based on a microcrowdfunding crowdsourcing infrastructure. A new functionality to automatically add micro-tasks to increase a community's well-being is designed for enhancing the infrastructure. We also describe a preliminary user study to demonstrate the feasibility of our approach.},
booktitle = {Proceedings of the 18th International Conference on Information Integration and Web-Based Applications and Services},
pages = {1–6},
numpages = {6},
keywords = {positive psychology, human well-being, crowdsourcing},
location = {Singapore, Singapore},
series = {iiWAS '16}
}

@inproceedings{10.5555/2095116.2095185,
author = {Chawla, Shuchi and Hartline, Jason D. and Sivan, Balasubramanian},
title = {Optimal crowdsourcing contests},
year = {2012},
publisher = {Society for Industrial and Applied Mathematics},
address = {USA},
abstract = {We study the design and approximation of optimal crowdsourcing contests. Crowdsourcing contests can be modeled as all-pay auctions because entrants must exert effort up-front to enter. Unlike all-pay auctions where a usual design objective would be to maximize revenue, in crowdsourcing contests, the principal only benefits from the submission with the highest quality. We give a theory for optimal crowdsourcing contests that mirrors the theory of optimal auction design: the optimal crowdsourcing contest is a virtual valuation optimizer (the virtual valuation function depends on the distribution of contestant skills and the number of contestants). We also compare crowdsourcing contests with more conventional means of procurement. In this comparison, crowdsourcing contests are relatively disadvantaged because the effort of losing contestants is wasted. Nonetheless, we show that crowdsourcing contests are 2-approximations to conventional methods for a large family of "regular" distributions, and 4-approximations, otherwise.},
booktitle = {Proceedings of the Twenty-Third Annual ACM-SIAM Symposium on Discrete Algorithms},
pages = {856–868},
numpages = {13},
location = {Kyoto, Japan},
series = {SODA '12}
}

@inproceedings{10.1145/3014087.3014102,
author = {Aletdinova, Anna and Kravchenko, Maxim and Bakaev, Maxim},
title = {Crowdsourcing and the effectiveness of C2G interaction in Russia},
year = {2016},
isbn = {9781450348591},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3014087.3014102},
doi = {10.1145/3014087.3014102},
abstract = {E-government is a new form of interaction between the governmental and administrative bodies, as well as with organizations and citizens. Its further development is related to initiative of the society members in promoting their ideas and projects, which may be placed on dedicated e-government Internet portals or with respective commercial services. In our paper we analyze the conceptual foundations of crowdsourcing and its actual development in Russia, mostly in relation to socially important projects. Particularly, we specify the process models for managing different types of crowdsourcing projects and provide historical data on their funding and success rates. We also identify factors that are most significant for crowdsourcing projects to achieve their final goals. Finally, to increase the effectiveness of crowdsourcing within e-government infrastructure, we propose to create the unified system joining several individual platforms and e-receptions, to compensate for certain existing deficiencies in social projects promotion technologies.},
booktitle = {Proceedings of the International Conference on Electronic Governance and Open Society: Challenges in Eurasia},
pages = {202–211},
numpages = {10},
keywords = {projects classification, information society, e-governance effectiveness, crowdsourcing platforms, crowdsourcing development},
location = {St. Petersburg, Russia},
series = {EGOSE '16}
}

@inproceedings{10.1007/978-3-319-91806-8_6,
author = {Spitz, Rejane and Queiroz, Francisco and Pereira, Clorisval and Cardarelli Leite, Leonardo and Ferranti, Marcelo P. and Dam, Peter},
title = {Do You Eat This? Changing Behavior Through Gamification, Crowdsourcing and Civic Engagement},
year = {2018},
isbn = {978-3-319-91805-1},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-319-91806-8_6},
doi = {10.1007/978-3-319-91806-8_6},
abstract = {The current excessive use of artificial additives by the food industry, the side effects of these potentially harmful ingredients and their impact on public health should be more widely acknowledged by consumers and further disclosed and discussed by citizens. Governments should develop stricter regulations on food additives, promote better labeling, apply taxes on miscreant food and conduct tighter industry surveillance. In parallel, broader behavior change towards nutrition habits might also be fostered through social innovation and citizen participation. In this paper, we present the design process for creating Dyet (Do you eat this?), a gamified app devised for collecting data and informing on the presence of such additives in commercially available food products. We argue that information on food ingredients and artificial additives should not only be accessible and legible, but also intelligible and personally meaningful to citizens. Through gameplay, we expect to foster the habit of reading ingredients lists, encouraging users to better inform themselves about what they eat and drink. Our overall goal is to change consumer’s potentially unsafe eating habits by bringing visibility to the excessive intake of artificial additives and on harmful food industry practices, making it possible and easier for everyone to make healthier dietary choices.},
booktitle = {Design, User Experience, and Usability: Users, Contexts and Case Studies: 7th International Conference, DUXU 2018, Held as Part of HCI International 2018, Las Vegas, NV, USA, July 15–20, 2018, Proceedings, Part III},
pages = {67–79},
numpages = {13},
keywords = {Crowdsourcing, Gamification, Interface design},
location = {Las Vegas, NV, USA}
}

@inproceedings{10.1145/2835776.2835797,
author = {Li, Qi and Ma, Fenglong and Gao, Jing and Su, Lu and Quinn, Christopher J.},
title = {Crowdsourcing High Quality Labels with a Tight Budget},
year = {2016},
isbn = {9781450337168},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2835776.2835797},
doi = {10.1145/2835776.2835797},
abstract = {In the past decade, commercial crowdsourcing platforms have revolutionized the ways of classifying and annotating data, especially for large datasets. Obtaining labels for a single instance can be inexpensive, but for large datasets, it is important to allocate budgets wisely. With limited budgets, requesters must trade-off between the quantity of labeled instances and the quality of the final results. Existing budget allocation methods can achieve good quantity but cannot guarantee high quality of individual instances under a tight budget. However, in some scenarios, requesters may be willing to label fewer instances but of higher quality. Moreover, they may have different requirements on quality for different tasks. To address these challenges, we propose a flexible budget allocation framework called Requallo. Requallo allows requesters to set their specific requirements on the labeling quality and maximizes the number of labeled instances that achieve the quality requirement under a tight budget. The budget allocation problem is modeled as a Markov decision process and a sequential labeling policy is produced. The proposed policy greedily searches for the instance to query next as the one that can provide the maximum reward for the goal. The Requallo framework is further extended to consider worker reliability so that the budget can be better allocated. Experiments on two real-world crowdsourcing tasks as well as a simulated task demonstrate that when the budget is tight, the proposed Requallo framework outperforms existing state-of-the-art budget allocation methods from both quantity and quality aspects.},
booktitle = {Proceedings of the Ninth ACM International Conference on Web Search and Data Mining},
pages = {237–246},
numpages = {10},
keywords = {crowdsourcing, budget allocation},
location = {San Francisco, California, USA},
series = {WSDM '16}
}

@inproceedings{10.1007/978-3-642-41338-4_17,
author = {Acosta, Maribel and Zaveri, Amrapali and Simperl, Elena and Kontokostas, Dimitris and Auer, S\"{o}ren and Lehmann, Jens},
title = {Crowdsourcing Linked Data Quality Assessment},
year = {2013},
isbn = {9783642413377},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-642-41338-4_17},
doi = {10.1007/978-3-642-41338-4_17},
abstract = {In this paper we look into the use of crowdsourcing as a means to handle Linked Data quality problems that are challenging to be solved automatically. We analyzed the most common errors encountered in Linked Data sources and classified them according to the extent to which they are likely to be amenable to a specific form of crowdsourcing. Based on this analysis, we implemented a quality assessment methodology for Linked Data that leverages the wisdom of the crowds in different ways: (i) a contest targeting an expert crowd of researchers and Linked Data enthusiasts; complemented by (ii) paid microtasks published on Amazon Mechanical Turk.We empirically evaluated how this methodology could efficiently spot quality issues in DBpedia. We also investigated how the contributions of the two types of crowds could be optimally integrated into Linked Data curation processes. The results show that the two styles of crowdsourcing are complementary and that crowdsourcing-enabled quality assessment is a promising and affordable way to enhance the quality of Linked Data.},
booktitle = {Proceedings of the 12th International Semantic Web Conference - Part II},
pages = {260–276},
numpages = {17},
series = {ISWC '13}
}

@inproceedings{10.5555/2667680.2667681,
author = {Birch, Kate E. and Heffernan, Kayla J.},
title = {Crowdsourcing for clinical research: an evaluation of maturity},
year = {2014},
isbn = {9781921770357},
publisher = {Australian Computer Society, Inc.},
address = {AUS},
abstract = {With the growth of the Internet and individuals using the Internet for person health research, crowdsourcing clinical research has the potential to become a powerful tool in surveilling and monitoring health outcomes. This paper evaluates the maturity of the emerging tool of crowdsourcing clinical research using two carefully selected and adapted evaluation models: Project Management Maturity Model (ProMMM) and National Infrastructure Maturity Model (NIMM). Two models were used in conjunction for the evaluation as ProMMM focuses on a professional's ability to utilise crowdsourcing for clinical research, while NIMM focuses on the maturity of crowdsourcing clinical research itself. To evaluate maturity, the authors reviewed available literature and conducted primary research in the form of interviews at the Melbourne Brain Centre at Royal Melbourne Hospital with Associate Professor Helmut Butzkueven, MS Neurologist and Researcher, and Dr Athina (Tina) Soulis, General Manager of Neuroscience Trials Australia. The tool of crowdsourcing for clinical research and the users and prospective users of the tool were found to be in immaturity. Despite immaturity, the future holds exciting applications for crowdsourcing clinical research with the potential to save costs, time, and recruit wider cohorts into clinical research.},
booktitle = {Proceedings of the Seventh Australasian Workshop on Health Informatics and Knowledge Management - Volume 153},
pages = {3–11},
numpages = {9},
keywords = {maturity, evaluation, crowdsourcing, clinical research},
location = {Auckland, New Zealand},
series = {HIKM '14}
}

@inproceedings{10.1007/978-3-319-91458-9_23,
author = {Chen, Zhao and Cheng, Peng and Zhang, Chen and Chen, Lei},
title = {Effective Solution for Labeling Candidates with a Proper Ration for&nbsp;Efficient Crowdsourcing},
year = {2018},
isbn = {978-3-319-91457-2},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-319-91458-9_23},
doi = {10.1007/978-3-319-91458-9_23},
abstract = {One of the core problems of crowdsourcing research is how to reduce the cost, in other words, how to get better results with a limited budget. To save budget, most researchers concentrate on internal steps of crowdsourcing while in this work we focus on the pre-processing stage: how to select the input for crowds to contribute. A straightforward application of this work is to help budget-limited machine learning researchers to get better balanced training data from crowd labeling. Specifically, we formulate the prior information based input manipulating procedure as the Candidate Selection Problem (CSP) and propose an end-squeezing algorithm for it. Our results show that a considerable cost reduction can be achieved by manipulating the input to the crowd with the help of some additional prior information. We verify the effectiveness and efficiency of these algorithms through extensive experiments.},
booktitle = {Database Systems for Advanced Applications: 23rd International Conference, DASFAA 2018, Gold Coast, QLD, Australia, May 21-24, 2018, Proceedings, Part II},
pages = {386–394},
numpages = {9},
location = {Gold Coast, QLD, Australia}
}

@inproceedings{10.1145/2983323.2983886,
author = {Huang, Chao and Wu, Xian and Wang, Dong},
title = {Crowdsourcing-based Urban Anomaly Prediction System for Smart Cities},
year = {2016},
isbn = {9781450340731},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2983323.2983886},
doi = {10.1145/2983323.2983886},
abstract = {Crowdsourcing has become an emerging data collection paradigm for smart city applications. A new category of crowdsourcing-based urban anomaly reporting systems have been developed to enable pervasive and real-time reporting of anomalies in cities (e.g., noise, illegal use of public facilities, urban infrastructure malfunctions). An interesting challenge in these applications is how to accurately predict an anomaly in a given region of the city before it happens. Prior works have made significant progress in anomaly detection. However, they can only detect anomalies after they happen, which may lead to significant information delay and lack of preparedness to handle the anomalies in an efficient way. In this paper, we develop a Crowdsourcing-based Urban Anomaly Prediction Scheme (CUAPS) to accurately predict the anomalies of a city by exploring both spatial and temporal information embedded in the crowdsourcing data. We evaluated the performance of our scheme and compared it to the state-of-the-art baselines using four real-world datasets collected from 311 service in the city of New York. The results showed that our scheme can predict different categories of anomalies in a city more accurately than the baselines.},
booktitle = {Proceedings of the 25th ACM International on Conference on Information and Knowledge Management},
pages = {1969–1972},
numpages = {4},
keywords = {smart cities, crowdsourcing, anomaly prediction, Bayesian inference},
location = {Indianapolis, Indiana, USA},
series = {CIKM '16}
}

@inproceedings{10.1109/UCC.2014.98,
author = {Hara, Tenshi and Springer, Thomas and Muthmann, Klemens and Schill, Alexander},
title = {Towards a Reusable Infrastructure for Crowdsourcing},
year = {2014},
isbn = {9781479978816},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/UCC.2014.98},
doi = {10.1109/UCC.2014.98},
abstract = {In the course of the last few years crowd sourcing has received growing research focus due to its conception of solving complex tasks with the help of a flexible group of contributors of whom each needs to only contribute a simpler task part. Hence, the crowd can contribute by collecting data from distributed locations, completing map information, or voting on product ideas, et cetera. However, even though it is a necessary conceptual feature, the participation of large numbers of users with heterogeneous devices, generic infrastructures for crowd sourcing can hardly be found. For example, the management of users, mobile devices and contributed data has to be repetitively implemented in new projects. To ease the development of crowd sourcing applications, in this paper we propose a generic platform for simplified crowd sourcing deployment while supporting diverse crowd sourcing scenarios, the ability to handle large numbers of users and the involvement of heterogeneous mobile devices. The focus therein is put on the deployment process. Hence, the evaluation is based on an actual deployment, namely the migration of Cyface, an existing crowd sourcing project build from scratch, into using our proposed infrastructure.},
booktitle = {Proceedings of the 2014 IEEE/ACM 7th International Conference on Utility and Cloud Computing},
pages = {618–623},
numpages = {6},
keywords = {infrastructure, distributed hash table, crowdsourcing, SANE, Cyface},
series = {UCC '14}
}

@inproceedings{10.1145/2660114.2660128,
author = {Nguyen, Nhatvi},
title = {Microworkers Crowdsourcing Approach, Challenges and Solutions},
year = {2014},
isbn = {9781450331289},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2660114.2660128},
doi = {10.1145/2660114.2660128},
abstract = {Founded in May 2009, Microworkers.com is an international Crowdsourcing platform focusing on Microtasks. At present, more than 600,000 users from over 190 countries have already registered to our platform. This extensively diverse workforce is the key to the current success of Microworkers as it gives opportunity to our clients to draw widely varying experiences and knowledge from a large, heterogeneous audience in arriving at innovative solutions. With the explosion of social media, mobile apps and online digital technology, the communication channels on how modern-day workers and tech-savvy consumers have profoundly changed. With that, how businesses communicate with their consumers, and to their employees, have also greatly transformed. While innovation remains the hallmark of staying competitive, the power of crowdsourcing is becoming more widely recognized because of the broad participation that takes place at relatively minimal costs. This brilliant mass collaboration approach allows companies to generate solutions from freelance professionals who get paid only if you utilize their ideas. Crowdsourcing lets any business, of any size and nature, tap into the collective intelligence of global crowds in order to complete business related tasks that a company would normally either perform itself or outsource to a third-party provider. It has become more possible to optimize multimedia systems more rapidly and to address human factors more effectively. Not only it allows businesses to expand the size of their talent pool, it is also a time and resource-efficient method to gain deeper insight into what direct consumers really want.In crowdsourcing platforms, there is perfect meritocracy. Especially in systems like Microworkers; age, gender, race, education, and job history does not matter, as the quality of work is all that counts; and every task is available to Users of every imaginable background. If you are capable of completing the required Microtask, you've got the job. For the past five years, Microworkers have effortlessly given opportunities to countless individuals across the globe whom are either looking for a temporary source of income, supplemental income or in many instances, main source of livelihood. While making opportunities available to eager, talented Workers and at the same time providing cost-effective solutions to job providers, Microworkers creates a win-win structure to anyone who believes can take advantage of its system. Apart from serving as a platform that connects Workers Employers, over time Microworkers Users have formed communities that provide support and assistance to fellow Users. Though having a large diverse workforce is the framework for delivering solutions to our clients, the same pose challenges both on our underlying infrastructure, as well as on providing support. Many other challenges arise in crowdsourcing set ups due to the fact that a community of users (or Microworkers) is a complex and dynamic system highly sensitive to changes in the form and the parameterization of their activities. Microworkers' present approach in dealing with these challenges include identification of optimal crowd members, ensuring clear directions and requirements, designing incentive structures that are not conducive to cheating, among many others.},
booktitle = {Proceedings of the 2014 International ACM Workshop on Crowdsourcing for Multimedia},
pages = {1},
numpages = {1},
keywords = {microworkers, microtask, mass collaboration, crowdtask, crowdsourcing platforms, crowdsourcing, crowdsource},
location = {Orlando, Florida, USA},
series = {CrowdMM '14}
}

@inproceedings{10.1109/CSI-SE.2015.10,
author = {Xie, Tao and Bishop, Judith and Horspool, R. Nigel and Tillmann, Nikolai and Halleux, Jonathan de},
title = {Crowdsourcing Code and Process via Code Hunt},
year = {2015},
isbn = {9781467370400},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/CSI-SE.2015.10},
doi = {10.1109/CSI-SE.2015.10},
abstract = {Crowd sourcing programming relies on active participation. One way to get such participation is through an engaging game. Code Hunt (https://www.codehunt.com/) from Microsoft Research is a web-based serious gaming platform with the potential to be leveraged as a crowd sourcing system. In Code Hunt, players create programs by re-engineering against a changing set of test cases. The game has been played by over 100,000 players in world-wide contests, and to practice coding skills. The vast collected data of code modified by players and the process taken to succeed could be used by others for software construction, teaching, or learning. In this position paper, we discuss these existing crowd sourcing activities in Code Hunt and a future game type for crowd sourcing.},
booktitle = {Proceedings of the 2015 IEEE/ACM 2nd International Workshop on CrowdSourcing in Software Engineering},
pages = {15–16},
numpages = {2},
keywords = {programming contests, educational software engineering, educational gamification, crowdsourcing},
series = {CSI-SE '15}
}

@inproceedings{10.1109/WI-IAT.2014.52,
author = {Ignatov, Dmitry I. and Kaminskaya, Alexandra and Konstantinova, Natalia and Konstantinov, Andrey},
title = {Recommender System for Crowdsourcing Platform Witology},
year = {2014},
isbn = {9781479941438},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/WI-IAT.2014.52},
doi = {10.1109/WI-IAT.2014.52},
abstract = {This paper discusses the recommender models and methods for crowd sourcing platforms. These models are based on modern methods of data analysis of object-attribute data, such as Formal Concept Analysis and biclustering. In particular, the paper is focused on the solution of two tasks - idea and antagonists recommendation - on the example of crowd sourcing platform Witology.},
booktitle = {Proceedings of the 2014 IEEE/WIC/ACM International Joint Conferences on Web Intelligence (WI) and Intelligent Agent Technologies (IAT) - Volume 01},
pages = {327–335},
numpages = {9},
keywords = {recommender systems, crowdsourcing, biclustering, Formal Concept Analysis},
series = {WI-IAT '14}
}

@inproceedings{10.1145/2702123.2702338,
author = {Curmi, Franco and Ferrario, Maria Angela and Whittle, Jon and Mueller, Florian 'Floyd'},
title = {Crowdsourcing Synchronous Spectator Support: (go on, go on, you're the best)n-1},
year = {2015},
isbn = {9781450331456},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2702123.2702338},
doi = {10.1145/2702123.2702338},
abstract = {Many studies have shown that crowd-support, such as cheering during sport events, can have a positive impact on athletes' performance. However, up until recently this support was only possible if the supporters and the athletes were geographically co-located. Can cheering be done remotely and would this be effective? In this paper we investigate the effect and possibilities of live remote cheering on co-located athletes and online supporting crowds that have a weak social tie and no social tie with the athlete. We recruit 140 online spectators and 5 athletes for an ad-hoc 5km road race. Results indicate that crowds socially closer to the athletes are significantly more engaged in the support. The athletes were excited by live remote cheering from friendsourced spectators and cheering from unknown crowdsourced participants indicating that remote friends and outsourced spectators could be an important source of support.},
booktitle = {Proceedings of the 33rd Annual ACM Conference on Human Factors in Computing Systems},
pages = {757–766},
numpages = {10},
keywords = {sports, spectators, spectator support, social networks, human behavior, friendsourcing, crowdsourcing, cheering, broadcast},
location = {Seoul, Republic of Korea},
series = {CHI '15}
}

@inproceedings{10.5555/2999134.2999212,
author = {Liu, Qiang and Peng, Jian and Ihler, Alexander},
title = {Variational inference for crowdsourcing},
year = {2012},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Crowdsourcing has become a popular paradigm for labeling large datasets. However, it has given rise to the computational task of aggregating the crowdsourced labels provided by a collection of unreliable annotators. We approach this problem by transforming it into a standard inference problem in graphical models, and applying approximate variational methods, including belief propagation (BP) and mean field (MF). We show that our BP algorithm generalizes both majority voting and a recent algorithm by Karger et al. [1], while our MF method is closely related to a commonly used EM algorithm. In both cases, we find that the performance of the algorithms critically depends on the choice of a prior distribution on the workers' reliability; by choosing the prior properly, both BP and MF (and EM) perform surprisingly well on both simulated and real-world datasets, competitive with state-of-the-art algorithms based on more complicated modeling assumptions.},
booktitle = {Proceedings of the 25th International Conference on Neural Information Processing Systems - Volume 1},
pages = {692–700},
numpages = {9},
location = {Lake Tahoe, Nevada},
series = {NIPS'12}
}

@inproceedings{10.1145/2370216.2370373,
author = {Tamilin, Andrei and Carreras, Iacopo and Ssebaggala, Emmanuel and Opira, Alfonse and Conci, Nicola},
title = {Context-aware mobile crowdsourcing},
year = {2012},
isbn = {9781450312240},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2370216.2370373},
doi = {10.1145/2370216.2370373},
abstract = {Ubiquity of internet-connected media-and sensor-equipped portable devices has emerged a range of opportunities for direct involvement of citizens into public decision making, leading to a new participatory format of public administration functioning. Intersecting the power of the crowdsourcing problem-solving paradigm by directly relying on human intelligence, with instantaneity and situation-awareness of mobile technologies, one gets a context-aware crowdsourcing approach for problem-solving in the right circumstances with the right people. In this paper, we present a prototype implementation of a context-aware mobile crowdsourcing system that enables the deployment and execution of crowdsourcing campaigns with users carrying mobile devices. The system is designed to maximize conditions for user participation, while minimizing the usage of energy. The paper describes the system architecture, defines an optimized sampling algorithm, and outlines a preliminary experimentation study carried out.},
booktitle = {Proceedings of the 2012 ACM Conference on Ubiquitous Computing},
pages = {717–720},
numpages = {4},
keywords = {mobile crowdsourcing, localization, energy-efficiency, context-aware systems},
location = {Pittsburgh, Pennsylvania},
series = {UbiComp '12}
}

@inproceedings{10.1007/978-3-319-46295-0_49,
author = {Yu, Hao and Wang, Zhongjie and Chi, Xu and Xu, Xiaofei},
title = {Studying Social Collaboration Features and Patterns in Service Crowdsourcing},
year = {2016},
isbn = {978-3-319-46294-3},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-319-46295-0_49},
doi = {10.1007/978-3-319-46295-0_49},
abstract = {Service crowdsourcing follows typical social collaboration processes with stochastic and dynamic characteristics. In this paper, the “bug-fix” social collaboration on GitHub is used as a case scenario of crowdsourcing, and 53,475 issues in 10 OSS projects are collected to conduct an empirical study on features and patterns of service crowdsourcing. Seven collaboration features (CFs) are proposed to delineate social characteristics of crowdsourcing. In terms of these CFs, social collaboration processes are clustered and results show that these features have significant distinguishability. An extended Generalized Sequential Pattern (GSP) algorithm is put forward to identify two types of collaboration patterns called participant-oriented pattern (PP) and role-oriented pattern (RP), and the richness and individualized degree of collaboration patterns in different OSS projects are analyzed and compared.},
booktitle = {Service-Oriented Computing: 14th International Conference, ICSOC 2016, Banff, AB, Canada, October 10-13, 2016, Proceedings},
pages = {697–704},
numpages = {8},
keywords = {Open Source Software (OSS), Collaboration features, Collaboration pattern, Social collaboration process, Service crowdsourcing},
location = {Banff, Canada}
}

@inproceedings{10.1007/978-3-319-07782-6_70,
author = {Ye, Chen and Wang, Hongzhi},
title = {Capture Missing Values Based on Crowdsourcing},
year = {2014},
isbn = {9783319077819},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-319-07782-6_70},
doi = {10.1007/978-3-319-07782-6_70},
abstract = {Due to the unreliable environment in mobile could, attribute values or tuples may be missing or lost. Thus we should capture missing values to make data mining and analysis more accurate. Besides ignoring or setting to default values, many imputation methods have been proposed, but they also have their limitations. This paper proposes a human-machine hybrid workflow to study the missing value filling method with crowdsourcing. First we propose a missing value selection algorithm to select the missing values which are suitable to use crowdsourcing for filling. Then we propose three missing values filling methods according to different attribute types to select answers from crowdsourcing. Experimental results show that our algorithms could improve data quality significantly with low costs.},
booktitle = {Proceedings of the 9th International Conference on Wireless Algorithms, Systems, and Applications - Volume 8491},
pages = {783–792},
numpages = {10},
keywords = {missing values, data cleaning, crowdsourcing},
location = {Harbin, China},
series = {WASA 2014}
}

@inproceedings{10.5555/2820116.2820119,
author = {Xie, Tao and Bishop, Judith and Horspool, R. Nigel and Tillmann, Nikolai and de Halleux, Jonathan},
title = {Crowdsourcing code and process via code hunt},
year = {2015},
publisher = {IEEE Press},
abstract = {Crowdsourcing programming relies on active participation. One way to get such participation is through an engaging game. Code Hunt (https://www.codehunt.com/) from Microsoft Research is a web-based serious gaming platform with the potential to be leveraged as a crowdsourcing system. In Code Hunt, players create programs by re-engineering against a changing set of test cases. The game has been played by over 100,000 players in world-wide contests, and to practice coding skills. The vast collected data of code modified by players and the process taken to succeed could be used by others for software construction, teaching, or learning. In this position paper, we discuss these existing crowdsourcing activities in Code Hunt and a future game type for crowdsourcing.},
booktitle = {Proceedings of the Second International Workshop on CrowdSourcing in Software Engineering},
pages = {15–16},
numpages = {2},
location = {Florence, Italy},
series = {CSI-SE '15}
}

@inproceedings{10.1145/2835776.2835835,
author = {Kazai, Gabriella and Zitouni, Imed},
title = {Quality Management in Crowdsourcing using Gold Judges Behavior},
year = {2016},
isbn = {9781450337168},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2835776.2835835},
doi = {10.1145/2835776.2835835},
abstract = {Crowdsourcing relevance labels has become an accepted practice for the evaluation of IR systems, where the task of constructing a test collection is distributed over large populations of unknown users with widely varied skills and motivations. Typical methods to check and ensure the quality of the crowd's output is to inject work tasks with known answers (gold tasks) on which workers' performance can be measured. However, gold tasks are expensive to create and have limited application. A more recent trend is to monitor the workers' interactions during a task and estimate their work quality based on their behavior. In this paper, we show that without gold behavior signals that reflect trusted interaction patterns, classifiers can perform poorly, especially for complex tasks, which can lead to high quality crowd workers getting blocked while poorly performing workers remain undetected. Through a series of crowdsourcing experiments, we compare the behaviors of trained professional judges and crowd workers and then use the trained judges' behavior signals as gold behavior to train a classifier to detect poorly performing crowd workers. Our experiments show that classification accuracy almost doubles in some tasks with the use of gold behavior data.},
booktitle = {Proceedings of the Ninth ACM International Conference on Web Search and Data Mining},
pages = {267–276},
numpages = {10},
keywords = {measurement, experimentation},
location = {San Francisco, California, USA},
series = {WSDM '16}
}

@inproceedings{10.1145/3152494.3167980,
author = {Mazumdar, Pramit and Patra, Bidyut Kr. and Babu, Korra Sathya},
title = {Handling cold-start scenarios in point-of-interest recommendations through crowdsourcing},
year = {2018},
isbn = {9781450363419},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3152494.3167980},
doi = {10.1145/3152494.3167980},
abstract = {Point-of-Interest (POI) recommender systems are used to suggest places or venues for a target user based on her preferences. The traditional recommender systems utilise the historical data to learn user preferences and then select places that relate to them. Therefore, knowledge of the features of a POI is important along with the preferences of the target users for a traditional recommender system. This technique faces a serious problem when a new POI emerges in a city. A 'new' POI has no historical data and hence a recommender system fails to learn about its features. This results in absence of the 'new' POIs in the recommended list. Such a scenario is popularly known as the POI cold-start scenario. Online social networks such as Yelp, TripAdvisor, Foursquare, etc. that provide POI recommendations as a service mostly face this particular problem. To address this issue, the proposed work gathers content on the cold-start POIs by crowdsourcing other online social networks and subsequently the dominating features at POIs are identified from the collected review and rating data. These features are utilized to address the cold-start problem. Finally, we develop a POI recommender system that can handle the POI cold-start scenario. We experimented on the real-world data provided by Yelp and the results are found to be significantly better than the state-of-art techniques in handling cold-start scenarios.},
booktitle = {Proceedings of the ACM India Joint International Conference on Data Science and Management of Data},
pages = {322–324},
numpages = {3},
keywords = {spam reviews, recommender systems, online social networks},
location = {Goa, India},
series = {CODS-COMAD '18}
}

@inproceedings{10.5220/0005835004820489,
author = {Machado, Leticia and Kroll, Josiane and Prikladnicki, Rafael and Souza, Cleidson R. B. de and Carmel, Erran},
title = {Software Crowdsourcing Challenges in the Brazilian IT Industry},
year = {2016},
isbn = {9789897581878},
publisher = {SCITEPRESS - Science and Technology Publications, Lda},
address = {Setubal, PRT},
url = {https://doi.org/10.5220/0005835004820489},
doi = {10.5220/0005835004820489},
abstract = {Software crowdsourcing has been regarded as a new paradigm for the provision of crowd-labor in software development tasks. Companies around the world adopt this paradigm to identify collective solutions to solve problems, ways to accelerate time-to-market, increase the quality and reduce the software cost. Although this paradigm is a trend in the software engineering area, several challenges are behind software crowdsourcing. In this study, we explore how the software crowdsourcing has been developed in the Brazilian IT industry. We have conducted 20 interviews with Brazilians practitioners in order to identify the main challenges for software crowdsourcing in Brazil. Additionally, we identified and discussed enablers and blockers\^{a} factors, practice implications and directions for future research in the area. Our paper aims to provide an overview of the software crowdsourcing in Brazil and motivation for researchers to better understand challenges faced by the Brazilian IT industry.},
booktitle = {Proceedings of the 18th International Conference on Enterprise Information Systems},
pages = {482–489},
numpages = {8},
keywords = {Software Engineering, Software Development, Software Crowdsourcing, IT Industry., Challenges, Brazil},
location = {Rome, Italy},
series = {ICEIS 2016}
}

@inproceedings{10.1145/2960811.2960815,
author = {Granell, Emilio and Mart\'{\i}nez-Hinarejos, Carlos-D.},
title = {A Multimodal Crowdsourcing Framework for Transcribing Historical Handwritten Documents},
year = {2016},
isbn = {9781450344388},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2960811.2960815},
doi = {10.1145/2960811.2960815},
abstract = {Transcription of handwritten historical documents is one of the main topics in document analysis systems, due to cultural reasons. State-of-the-art handwritten text recognition systems allow to speed up the transcription task. Currently, this automatic transcription is far from perfect, and human expert revision is required in order to obtain the actual transcription. In this context, crowdsourcing emerged as a powerful tool for massive transcription at a relatively low cost, since the supervision effort of professional transcribers may be dramatically reduced. However, current transcription crowdsourcing platforms are mainly limited to the use of non-mobile devices, since the use of keyboards in mobile devices is not friendly enough for most users. This work presents the alternative of using speech dictation of handwritten text lines as transcription source in a crowdsourcing platform. The experiments explore how an initial handwritten text recognition hypothesis can be improved by using the contribution of speech recognition from several speakers, providing as a final result a better hypothesis to be amended by a professional transcriber with less effort.},
booktitle = {Proceedings of the 2016 ACM Symposium on Document Engineering},
pages = {157–163},
numpages = {7},
keywords = {speech recognition, multimodal combination, historical handwritten transcription, crowdsourcing framework},
location = {Vienna, Austria},
series = {DocEng '16}
}

@inproceedings{10.1145/2591062.2591153,
author = {Chen, Zhenyu and Luo, Bin},
title = {Quasi-crowdsourcing testing for educational projects},
year = {2014},
isbn = {9781450327688},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2591062.2591153},
doi = {10.1145/2591062.2591153},
abstract = {The idea of crowdsourcing tasks in software engineering, especially software testing, has gained popularity in recent years. Crowdsourcing testing and educational projects are natural complementary. One of the challenges of crowdsourcing testing is to find a number of qualified workers with low cost. Students in software engineering are suitable candidates for crowdsourcing testing. On the other hand, practical projects play a key role in software engineering education. In order to enhance educational project outcomes and achieve industrial-strength training, we need to provide the opportunity for students to be exposed to commercial software development.  In this paper, we report a preliminary study on crowdsourcing testing for educational projects. We introduce three commercial software products as educational testing projects, which are crowdsourced by our teaching support system. We call this "Quasi-Crowdsourcing Test" (QCT) because the candidate workers are students, who have certain social relations. The investigation results are encouraging and show to be beneficial to both the students and industry in QCT projects.},
booktitle = {Companion Proceedings of the 36th International Conference on Software Engineering},
pages = {272–275},
numpages = {4},
keywords = {system testing, educational projects, Crowdsourcing test},
location = {Hyderabad, India},
series = {ICSE Companion 2014}
}

@inproceedings{10.1007/978-3-030-33702-5_10,
author = {Ye, Bin and Wang, Yan and Orgun, Mehmet and Sheng, Quan Z.},
title = {N2TM: A New Node to Trust Matrix Method for Spam Worker Defense in Crowdsourcing Environments},
year = {2019},
isbn = {978-3-030-33701-8},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-33702-5_10},
doi = {10.1007/978-3-030-33702-5_10},
abstract = {To defend against spam workers in crowdsourcing environments, the existing solutions overlook the fact that a spam worker with guises can easily bypass the defense. To alleviate this problem, in this paper, we propose a Node to Trust Matrix method (N2TM) that represents a worker node in a crowdsourcing network as an un-manipulable Worker Trust Matrix (WTM) for identifying the worker’s identity. In particular, we first present a crowdsourcing trust network consisting of requester nodes, worker nodes, and transaction-based edges. Then, we construct WTMs for workers based on the trust network. A WTM consists of trust indicators measuring the extent to which a worker is trusted by different requesters in different sub-networks. Moreover, we show the un-manipulable property and the usable property of a WTM that are crucial for identifying a worker’s identity. Furthermore, we leverage deep learning techniques to predict a worker’s identity with its WTM as input. Finally, we demonstrate the superior performance of our proposed N2TM in identifying spam workers with extensive experiments.},
booktitle = {Service-Oriented Computing: 17th International Conference, ICSOC 2019, Toulouse, France, October 28–31, 2019, Proceedings},
pages = {119–134},
numpages = {16},
keywords = {Crowdsourcing, Trust, Spam worker identification},
location = {Toulouse, France}
}

@inproceedings{10.1007/978-3-642-41924-9_46,
author = {Lukyanenko, Roman and Parsons, Jeffrey},
title = {Lightweight Conceptual Modeling for Crowdsourcing},
year = {2013},
isbn = {9783642419232},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-642-41924-9_46},
doi = {10.1007/978-3-642-41924-9_46},
abstract = {As more organizations rely on externally-produced information, an important issue is how to develop conceptual models for such data. Considering the limitations of traditional conceptual modeling, we propose a "lightweight" modeling alternative to traditional "class-based" conceptual modeling as typified by the E-R model. We demonstrate the approach using a real-world crowdsourcing project, NLNature.},
booktitle = {Proceedings of the 32nd International Conference on Conceptual Modeling - Volume 8217},
pages = {508–511},
numpages = {4},
keywords = {Ontology, Information Quality, Conceptual Modeling, Cognition},
location = {Hong-Kong, China},
series = {ER 2013}
}

@inproceedings{10.1145/2600428.2609479,
author = {Alonso, Omar and Stone, Maria},
title = {Building a query log via crowdsourcing},
year = {2014},
isbn = {9781450322577},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2600428.2609479},
doi = {10.1145/2600428.2609479},
abstract = {A query log is a key asset in a commercial search engine. Everyday millions of users rely on search engines to find information on the Web by entering a few keywords on a simple search interface. Those queries represent a subset of user behavioral data which is used to mine and discover search patterns for improving the overall end user experience. While queries are very useful, it is not always possible to capture precisely what the user was looking for when the intent is not that clear. We explore a different alternative based on human computation to gather a bit more information from users and show the type of query log that would be possible to construct.},
booktitle = {Proceedings of the 37th International ACM SIGIR Conference on Research \&amp; Development in Information Retrieval},
pages = {939–942},
numpages = {4},
keywords = {user studies, query logs, query annotation, crowdsourcing},
location = {Gold Coast, Queensland, Australia},
series = {SIGIR '14}
}

@inproceedings{10.5555/2892753.2892959,
author = {Aydin, Bahadir Ismail and Yilmaz, Yavuz Selim and Li, Yaliang and Li, Qi and Gao, Jing and Demirbas, Murat},
title = {Crowdsourcing for multiple-choice question answering},
year = {2014},
publisher = {AAAI Press},
abstract = {We leverage crowd wisdom for multiple-choice question answering, and employ lightweight machine learning techniques to improve the aggregation accuracy of crowdsourced answers to these questions. In order to develop more effective aggregation methods and evaluate them empirically, we developed and deployed a crowdsourced system for playing the "Who wants to be a millionaire?" quiz show. Analyzing our data (which consist of more than 200,000 answers), we find that by just going with the most selected answer in the aggregation, we can answer over 90\% of the questions correctly, but the success rate of this technique plunges to 60\% for the later/harder questions in the quiz show. To improve the success rates of these later/harder questions, we investigate novel weighted aggregation schemes for aggregating the answers obtained from the crowd. By using weights optimized for reliability of participants (derived from the participants' confidence), we show that we can pull up the accuracy rate for the harder questions by 15\%, and to overall 95\% average accuracy. Our results provide a good case for the benefits of applying machine learning techniques for building more accurate crowdsourced question answering systems.},
booktitle = {Proceedings of the Twenty-Eighth AAAI Conference on Artificial Intelligence},
pages = {2946–2953},
numpages = {8},
location = {Qu\'{e}bec City, Qu\'{e}bec, Canada},
series = {AAAI'14}
}

@inproceedings{10.1145/3306500.3313980,
author = {Guo, Jie and Wang, JiaWei and Yan, ZhouYao},
title = {Motivation and factors effecting the participation behavior in the urban crowdsourcing logistics: evidence from China},
year = {2019},
isbn = {9781450366021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3306500.3313980},
doi = {10.1145/3306500.3313980},
abstract = {Purpose- The booming development of the new business model of local e-commerce has challenged the distribution capability of traditional urban logistics company in China. Crowdsourcing logistics, as a new terminal logistics distribution mode, provide a new perspective to solve the local logistics bottleneck in E-commerce effectively. In order to apply and develop crowdsourcing logistics better and design an effective crowdsourcing logistics platform, a better understanding of the participation behavior to the crowdsourcing logistics is needed. The purpose of this paper is to use the Unified Theory of Acceptance and Use of Technology (UTAU) as a framework to develop a model to identify the effective factors of the participation behavior to the crowdsourcing logistics.Design/methodology/approach- To test the model, a survey of 296 respondents in China is undertaken. And to analyze the participation behavior to the crowdsourcing logistics from the survey data, the structural equation modeling(SEM) is used. Findings- The results indicated that performance expectancy and social influence positively affect the intention of participation; perceived risk negatively influence the intention of participation; the higher the intention of participation, the more participative behavior of crowdsourcing logistics; and facilitating conditions also an important factor that leads to more participative behavior.Research Limitations/implications-This research is limited by the young adults sample and the website questionnaire platform that might confine the generalizability of the study. Also, additional variables need to be examined in order to better explain crowdsourcing logistic behavior. The result of the research provides insights for company related take-out O2O and logistic to build successful crowdsourcing model, engage young employees who are familiar with network in urban crowdsourcing logistic, and increase involvement in sharing economy.Practical implications- The results of this research can help the management of the urban crowdsourcing logistics companies to understand the participative intention and behavior to their products and services of people, so that they can improve their business model and design an effective and attractive crowdsourcing logistics platform.Originality/value- This study developed the Unified Theory of Acceptance and Use of Technology to better explain the participation behavior in the crowdsourcing logistics. And the paper develops an understanding of how crowdsourcing logistics platform should be improved and design to appeal more people to take part in the new logistics model.},
booktitle = {Proceedings of the 10th International Conference on E-Education, E-Business, E-Management and E-Learning},
pages = {334–341},
numpages = {8},
keywords = {participant behavior, crowdsourcing logistics, UTAUT, SEM},
location = {Tokyo, Japan},
series = {IC4E '19}
}

@inproceedings{10.1145/2702123.2702145,
author = {Cheng, Justin and Teevan, Jaime and Bernstein, Michael S.},
title = {Measuring Crowdsourcing Effort with Error-Time Curves},
year = {2015},
isbn = {9781450331456},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2702123.2702145},
doi = {10.1145/2702123.2702145},
abstract = {Crowdsourcing systems lack effective measures of the effort required to complete each task. Without knowing how much time workers need to execute a task well, requesters struggle to accurately structure and price their work. Objective measures of effort could better help workers identify tasks that are worth their time. We propose a data-driven effort metric, ETA (error-time area), that can be used to determine a task's fair price. It empirically models the relationship between time and error rate by manipulating the time that workers have to complete a task. ETA reports the area under the error-time curve as a continuous metric of worker effort. The curve's 10th percentile is also interpretable as the minimum time most workers require to complete the task without error, which can be used to price the task. We validate the ETA metric on ten common crowdsourcing tasks, including tagging, transcription, and search, and find that ETA closely tracks how workers would rank these tasks by effort. We also demonstrate how ETA allows requesters to rapidly iterate on task designs and measure whether the changes improve worker efficiency. Our findings can facilitate the process of designing, pricing, and allocating crowdsourcing tasks.},
booktitle = {Proceedings of the 33rd Annual ACM Conference on Human Factors in Computing Systems},
pages = {1365–1374},
numpages = {10},
keywords = {task effort, microtasks, crowdsourcing},
location = {Seoul, Republic of Korea},
series = {CHI '15}
}

@inproceedings{10.1145/3253092,
author = {Cox, Landon},
title = {Session details: Crowdsourcing},
year = {2011},
isbn = {9781450306430},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3253092},
doi = {10.1145/3253092},
booktitle = {Proceedings of the 9th International Conference on Mobile Systems, Applications, and Services},
location = {Bethesda, Maryland, USA},
series = {MobiSys '11}
}

@inproceedings{10.1145/2737095.2742931,
author = {Jain, Ayush and Raj, Saswat and Harshit and Misra, Rajiv and Baveja, B. M.},
title = {Road congestion sensing via crowdsourcing and MapReduce},
year = {2015},
isbn = {9781450334754},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2737095.2742931},
doi = {10.1145/2737095.2742931},
abstract = {Road congestion has become a major problem in cities in developing countries resulting in massive delays, wastage of fuel and road accidents. For proper handling it is essential to observe the road congestion patterns. Methods like on-road cameras, etc. require huge investments whereas crowdsourcing methods generate large amount of redundant data. This paper presents a new approach using event sensing to capture relevant crowdsourced data to estimate road traffic congestion and utilizes MapReduce for generating analytics efficiently.},
booktitle = {Proceedings of the 14th International Conference on Information Processing in Sensor Networks},
pages = {356–357},
numpages = {2},
location = {Seattle, Washington},
series = {IPSN '15}
}

@inproceedings{10.1145/2486001.2491719,
author = {Wang, Tianyi and Wang, Gang and Li, Xing and Zheng, Haitao and Zhao, Ben Y.},
title = {Characterizing and detecting malicious crowdsourcing},
year = {2013},
isbn = {9781450320566},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2486001.2491719},
doi = {10.1145/2486001.2491719},
abstract = {Popular Internet services in recent years have shown that remarkable things can be achieved by harnessing the power of the masses. However, crowd-sourcing systems also pose a real challenge to existing security mechanisms deployed to protect Internet services, particularly those tools that identify malicious activity by detecting activities of automated programs such as CAPTCHAs.In this work, we leverage access to two large crowdturfing sites to gather a large corpus of ground-truth data generated by crowdturfing campaigns. We compare and contrast this data with "organic" content generated by normal users to identify unique characteristics and potential signatures for use in real-time detectors. This poster describes first steps taken focused on crowdturfing campaigns targeting the Sina Weibo microblogging system. We describe our methodology, our data (over 290K campaigns, 34K worker accounts, 61 million tweets...), and some initial results.},
booktitle = {Proceedings of the ACM SIGCOMM 2013 Conference on SIGCOMM},
pages = {537–538},
numpages = {2},
keywords = {user behavior, malicious crowdsourcing, crowdturfing},
location = {Hong Kong, China},
series = {SIGCOMM '13}
}

@article{10.1145/2534169.2491719,
author = {Wang, Tianyi and Wang, Gang and Li, Xing and Zheng, Haitao and Zhao, Ben Y.},
title = {Characterizing and detecting malicious crowdsourcing},
year = {2013},
issue_date = {October 2013},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {43},
number = {4},
issn = {0146-4833},
url = {https://doi.org/10.1145/2534169.2491719},
doi = {10.1145/2534169.2491719},
abstract = {Popular Internet services in recent years have shown that remarkable things can be achieved by harnessing the power of the masses. However, crowd-sourcing systems also pose a real challenge to existing security mechanisms deployed to protect Internet services, particularly those tools that identify malicious activity by detecting activities of automated programs such as CAPTCHAs.In this work, we leverage access to two large crowdturfing sites to gather a large corpus of ground-truth data generated by crowdturfing campaigns. We compare and contrast this data with "organic" content generated by normal users to identify unique characteristics and potential signatures for use in real-time detectors. This poster describes first steps taken focused on crowdturfing campaigns targeting the Sina Weibo microblogging system. We describe our methodology, our data (over 290K campaigns, 34K worker accounts, 61 million tweets...), and some initial results.},
journal = {SIGCOMM Comput. Commun. Rev.},
month = aug,
pages = {537–538},
numpages = {2},
keywords = {user behavior, malicious crowdsourcing, crowdturfing}
}

@inproceedings{10.1109/SCC.2015.89,
author = {Balamurugan, Chithralekha and Kunde, Shruti and Gupta, Avantika and Chander, Deepthi and Dasgupta, Koustuv},
title = {Service Assurance Framework for Enterprise Task Crowdsourcing},
year = {2015},
isbn = {9781467372817},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/SCC.2015.89},
doi = {10.1109/SCC.2015.89},
abstract = {Crowd sourcing platforms enable enterprise requesters to leverage the online workforce to process voluminous enterprise tasks on a regular basis. Web services provided by these platforms facilitate requesters to post tasks, retrieve results and incentivize crowd workers. However, service assurance associated with task execution by crowd workers is not provided by these platforms. Owing to the flexible, uncommitted, discretionary working patterns of online crowd workers, service assurance for task execution is considered to be beyond the service assurance offerings of existing crowd sourcing platforms. Enterprises however, require these guarantees to be able to adopt crowd sourcing in a profound manner. In this paper, we propose a Service Assurance Framework as a crowd sourcing platform augmenting service, that provides service assurance for task execution by crowd workers. The framework helps enterprise requesters to identify and engage with workers who possess suitable service assurance attributes. To the best of our knowledge, this work is a first of its kind, in providing service assurance associated with task execution by crowd workers, with respect to enterprise tasks. We implemented the proposed framework and conducted a four-week long, large scale crowd sourcing experiment involving digitization of forms posted by an enterprise requester. Our results validate the efficacy of the proposed service assurance framework for enterprise crowd sourcing and advocate its adoption.},
booktitle = {Proceedings of the 2015 IEEE International Conference on Services Computing},
pages = {616–623},
numpages = {8},
keywords = {Worker Performance, Service Assurance-Guaranteed Crowdsourcing, Service Assurance, Enterprise Task Crowdsourcing},
series = {SCC '15}
}

@inproceedings{10.1145/3249011,
author = {Kittur, Niki},
title = {Session details: Crowdsourcing},
year = {2011},
isbn = {9781450302289},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3249011},
doi = {10.1145/3249011},
booktitle = {Proceedings of the SIGCHI Conference on Human Factors in Computing Systems},
location = {Vancouver, BC, Canada},
series = {CHI '11}
}

@inproceedings{10.1145/2390034.2390036,
author = {Guy, Ido},
title = {Crowdsourcing in the enterprise},
year = {2012},
isbn = {9781450317153},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2390034.2390036},
doi = {10.1145/2390034.2390036},
abstract = {This talk reviews several of the recent studies conducted by the Social Technologies group at IBM Research-Haifa, which demonstrate the use of social analytics tools to extract value of enterprise social media. From recommender systems, through activity stream filtering and analysis, to crowdsourcing games in the enterprise, the voice of the employees can now be heard and utilized better than ever within the newly formed social business.},
booktitle = {Proceedings of the 1st International Workshop on Multimodal Crowd Sensing},
pages = {1–2},
numpages = {2},
keywords = {social technologies, social media, social business, social analytics, enterprise, crowdsourcing, crowd sensing, crowd computing},
location = {Maui, Hawaii, USA},
series = {CrowdSens '12}
}

@inproceedings{10.1145/2642918.2647362,
author = {Hosio, Simo and Goncalves, Jorge and Lehdonvirta, Vili and Ferreira, Denzil and Kostakos, Vassilis},
title = {Situated crowdsourcing using a market model},
year = {2014},
isbn = {9781450330695},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2642918.2647362},
doi = {10.1145/2642918.2647362},
abstract = {Research is increasingly highlighting the potential for situated crowdsourcing to overcome some crucial limitations of online crowdsourcing. However, it remains unclear whether a situated crowdsourcing market can be sustained, and whether worker supply responds to price-setting in such a market. Our work is the first to systematically investigate workers' behaviour and response to economic incentives in a situated crowdsourcing market. We show that the market-based model is a sustainable approach to recruiting workers and obtaining situated crowdsourcing contributions. We also show that the price mechanism is a very effective tool for adjusting the supply of labour in a situated crowdsourcing market. Our work advances the body of work investigating situated crowdsourcing.},
booktitle = {Proceedings of the 27th Annual ACM Symposium on User Interface Software and Technology},
pages = {55–64},
numpages = {10},
keywords = {virtual currency, situated technologies, market, crowdsourcing},
location = {Honolulu, Hawaii, USA},
series = {UIST '14}
}

@inproceedings{10.5555/2540128.2540429,
author = {Han, Jun and Fan, Ju and Zhou, Lizhu},
title = {Crowdsourcing-assisted query structure interpretation},
year = {2013},
isbn = {9781577356332},
publisher = {AAAI Press},
abstract = {Structured Web search incorporating data from structured sources into search engine results has attracted much attention from both academic and industrial communities. To understand user's intent, query structure interpretation is proposed to analyze the structure of queries in a query log and map query terms to the semantically relevant attributes of data sources in a target domain. Existing methods assume all queries should be classified to the target domain, and thus they are limited when interpreting queries from different domains in real query logs. To address the problem, we introduce a human-machine hybrid method by utilizing crowdsourcing platforms. Our method selects a small number of query terms and asks the crowdsourcing workers to interpret them, and then infers the interpretations based on the crowdsourcing results. To improve the performance, we propose an iterative probabilistic inference method based on a similarity graph of query terms, and select the most useful query terms for crowdsourcing by considering their domain-relevance and gained benefit. We evaluate our method on a real query log, and the experimental results show that our method outperforms the state-of-the-art method.},
booktitle = {Proceedings of the Twenty-Third International Joint Conference on Artificial Intelligence},
pages = {2092–2098},
numpages = {7},
location = {Beijing, China},
series = {IJCAI '13}
}

@inproceedings{10.5555/2390665.2390704,
author = {Zeichner, Naomi and Berant, Jonathan and Dagan, Ido},
title = {Crowdsourcing inference-rule evaluation},
year = {2012},
publisher = {Association for Computational Linguistics},
address = {USA},
abstract = {The importance of inference rules to semantic applications has long been recognized and extensive work has been carried out to automatically acquire inference-rule resources. However, evaluating such resources has turned out to be a non-trivial task, slowing progress in the field. In this paper, we suggest a framework for evaluating inference-rule resources. Our framework simplifies a previously proposed "instance-based evaluation" method that involved substantial annotator training, making it suitable for crowdsourcing. We show that our method produces a large amount of annotations with high inter-annotator agreement for a low cost at a short period of time, without requiring training expert annotators.},
booktitle = {Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Short Papers - Volume 2},
pages = {156–160},
numpages = {5},
location = {Jeju Island, Korea},
series = {ACL '12}
}

@inproceedings{10.1145/3244832,
author = {schraefel, mc},
title = {Session details: Crowdsourcing},
year = {2011},
isbn = {9781450307161},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3244832},
doi = {10.1145/3244832},
booktitle = {Proceedings of the 24th Annual ACM Symposium on User Interface Software and Technology},
location = {Santa Barbara, California, USA},
series = {UIST '11}
}

@inproceedings{10.1145/3308560.3317084,
author = {Zequeira Jim\'{e}nez, Rafael and Llagostera, Anna and Naderi, Babak and M\"{o}ller, Sebastian and Berger, Jens},
title = {Intra- and Inter-rater Agreement in a Subjective Speech Quality Assessment Task in Crowdsourcing},
year = {2019},
isbn = {9781450366755},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3308560.3317084},
doi = {10.1145/3308560.3317084},
abstract = {Crowdsourcing is a great tool for conducting subjective user studies with large amounts of users. Collecting reliable annotations about the quality of speech stimuli is challenging. The task itself is of high subjectivity and users in crowdsourcing work without supervision. This work investigates the intra- and inter-listener agreement withing a subjective speech quality assessment task. To this end, a study has been conducted in the laboratory and in crowdsourcing in which listeners were requested to rate speech stimuli with respect to their overall quality. Ratings were collected on a 5-point scale in accordance with the ITU-T Rec. P.800 and P.808, respectively. The speech samples were taken from the database ITU-T Rec. P.501 Annex D, and were presented four times to the listeners. Finally, the crowdsourcing results were contrasted to the ratings collected in the laboratory. Strong and significant Spearman’s correlation was achieved when contrasting the ratings collected in both environments. Our analysis show that while the inter-rater agreement increased the more the listeners conducted the assessment task, the intra-rater reliability remained constant. Our study setup helped to overcome the subjectivity of the task and we found that disagreement can represent a source of information to some extent.},
booktitle = {Companion Proceedings of The 2019 World Wide Web Conference},
pages = {1138–1143},
numpages = {6},
keywords = {subjectivity in crowdsourcing, speech quality assessment, listeners’ agreement, inter-rater reliability, crowdsourcing},
location = {San Francisco, USA},
series = {WWW '19}
}

@inproceedings{10.1145/3157754.3157764,
author = {Barashev, Andrey and Li, Guoxin},
title = {Personal Trait Predicting Work Engagement in Crowdsourcing through Achievement Goals: Mediation Analyses},
year = {2017},
isbn = {9781450353670},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3157754.3157764},
doi = {10.1145/3157754.3157764},
abstract = {The purpose of this research is to investigate how personal traits influence work engagement on crowdsourcing platform through achievement goals. This study uses Reinforcement Sensitivity Theory (RST) in order to explain initial energization of behavior as well as Achievement goal theory's constructs to explain direction of this energy. The indirect effects of Fight-Flight-Freeze system (FFFS) through performance approach and performance avoidance goals on work engagement has been studied.The results of current study show that avoidance based personal trait FFFS indirectly relates to work engagement through performance approach and performance avoidance goals, such as FFFS positively predicts work engagement through performance approach goals and negatively predict work engagement through performance avoidance goals. In addition, this study proves that FFFS positively predict adoption of both performance approach and performance avoidance goals. Hence, promoting adoption of performance approach goals is a good way for improving workers' work engagement.These results are important for crowdsourcing platform managers and could help them to improve work engagement of workers without providing additional reward.},
booktitle = {Proceedings of the 8th International Conference on E-Business, Management and Economics},
pages = {28–32},
numpages = {5},
keywords = {Work Engagement, Personal traits, Motivation, Crowdsourcing, Achievement Goals},
location = {Birmingham, United Kingdom},
series = {ICEME '17}
}

@inproceedings{10.1145/3184558.3191519,
author = {Kandappu, Thivya and Misra, Archan and Koh, Desmond and Tandriansyah, Randy Daratan and Jaiman, Nikita},
title = {A Feasibility Study on Crowdsourcing to Monitor Municipal Resources in Smart Cities},
year = {2018},
isbn = {9781450356404},
publisher = {International World Wide Web Conferences Steering Committee},
address = {Republic and Canton of Geneva, CHE},
url = {https://doi.org/10.1145/3184558.3191519},
doi = {10.1145/3184558.3191519},
abstract = {Active citizenry, whereby citizens actively participate in reporting and addressing challenges in urban service delivery is a strategic goal of smart cities such as Singapore. In spite of the promise, we believe that the success of such large-scale nation-wide crowdsourcing deployments depend on the real-word user preferences and behavioral characteristics of citizens. In this paper, we first present our findings on behavioral preferences and key concerns of citizens regarding smart-city services via an opinion survey conducted with 1300 participants. We then propose a "citizen-controlled" urban services reporting platform where citizens actively report on the status of various municipal resources. We advocate the importance of matching user mobility patterns against task locations to make the platform more efficient (i.e., higher task completion rate and lower detour overhead).},
booktitle = {Companion Proceedings of the The Web Conference 2018},
pages = {919–925},
numpages = {7},
keywords = {citizens, mobile crowdsourcing, municipal monitoring, smart cities},
location = {Lyon, France},
series = {WWW '18}
}

@inproceedings{10.1145/2989238.2989245,
author = {Alelyani, Turki and Yang, Ye},
title = {Software crowdsourcing reliability: an empirical study on developers behavior},
year = {2016},
isbn = {9781450343954},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2989238.2989245},
doi = {10.1145/2989238.2989245},
abstract = {Crowdsourcing has become an emergent paradigm for software production in recent decades. Its open-call format attracts the participation of hundreds of thousands of developers. To ensure the success of software crowdsourcing, we must accurately measure and monitor the reliability of participating crowd workers, which, surprisingly, has rarely been done. To that end, this paper aims to examine the dependability of crowd workers in selecting tasks for software crowdsourcing. Empirical analysis of worker behaviors will investigate the following: (1) workers’ behavior when registering and carrying out the announced tasks; (2) the relationship between rewards and performance; (3) the effects of development type among different groups; and (4) the evolution of workers’ behavior according to the skills they have adopted. This study’s findings include: (1) On average, most reliable crowdsourcing group responds to a task call within 10\% of their allotted time and completes the assigned work in less than 5\% of that time. (2) Crowd workers tend to focus on tasks according to specific ranges of rewards and types of challenges, based on their skill levels. (3) Crowd skills spread evenly across the entire range of groups. In summary, our results can guide future research into crowdsourcing service design and can inform ideas for crowdsourcing strategy conception according to time, reward, development type, and other aspects of crowdsourcing.},
booktitle = {Proceedings of the 2nd International Workshop on Software Analytics},
pages = {36–42},
numpages = {7},
keywords = {workers, software, reliability, TopCoder, Crowdsourcing},
location = {Seattle, WA, USA},
series = {SWAN 2016}
}

@inproceedings{10.1145/3255760,
author = {Sellis, Timos},
title = {Session details: Research session 10: crowdsourcing},
year = {2014},
isbn = {9781450323765},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3255760},
doi = {10.1145/3255760},
booktitle = {Proceedings of the 2014 ACM SIGMOD International Conference on Management of Data},
location = {Snowbird, Utah, USA},
series = {SIGMOD '14}
}

@inproceedings{10.1109/WCNC.2018.8377405,
author = {Enami, Rita and Rajan, Dinesh and Camp, Joseph},
title = {RAIK: Regional analysis with geodata and crowdsourcing to infer key performance indicators},
year = {2018},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/WCNC.2018.8377405},
doi = {10.1109/WCNC.2018.8377405},
abstract = {Key Performance Indicators (KPIs) are important measures of the quality of service in cellular networks. There are multiple efforts by cellular carriers and 5G standardization to leverage the KPIs to minimize drive tests (MDT) and self-organize the network for optimal performance via user feedback. Such an approach accounts for user devices in the field of their operation according to their normal usage and circumvents a number of costs (e.g., manpower, equipment) traditionally covered by the carrier, either directly or through a third party. In this paper, we build a Regional Analysis to Infer KPIs (RAIK) framework to establish a relationship between geographical data and user data using crowdsourced measurements. To do so, we use a neural network and crowdsourced data obtained by user equipment (UE) to predict the KPIs in terms of the reference signal's received power (RSRP) and path loss estimation. Since these KPIs are a function of terrain type, we provide a two-layer coverage map by overlaying a performance layer on a 3-dimensional geographical map. As a result, we can efficiently use crowdsourced data (to not overextend user bandwidth and battery) and infer KPIs in areas where measurements have not or can not be performed. For example, we show that RAIK can use only geographical information to predict the KPIs in areas that lack signal quality data with a negligible mean squared error, a seven-fold reduction in error from state-of-the-art solutions.},
booktitle = {2018 IEEE Wireless Communications and Networking Conference (WCNC)},
pages = {1–6},
numpages = {6},
location = {Barcelona, Spain}
}

@inproceedings{10.1145/2684822.2685308,
author = {Niu, Shuzi and Lan, Yanyan and Guo, Jiafeng and Cheng, Xueqi and Yu, Lei and Long, Guoping},
title = {Listwise Approach for Rank Aggregation in Crowdsourcing},
year = {2015},
isbn = {9781450333177},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2684822.2685308},
doi = {10.1145/2684822.2685308},
abstract = {Inferring a gold-standard ranking over a set of objects, such as documents or images, is a key task to build test collections for various applications like Web search and recommender systems. Crowdsourcing services provide an efficient and inexpensive way to collect judgments via labeling by sets of annotators. We thus study the problem of finding a consensus ranking from crowdsourced judgments. In contrast to conventional rank aggregation methods which minimize the distance between predicted ranking and input judgments from either pointwise or pairwise perspective, we argue that it is critical to consider the distance in a listwise way to emphasize the position importance in ranking. Therefore, we introduce a new listwise approach in this paper, where ranking measure based objective functions are utilized for optimization. In addition, we also incorporate the annotator quality into our model since the reliability of annotators can vary significantly in crowdsourcing. For optimization, we transform the optimization problem to the Linear Sum Assignment Problem, and then solve it by a very efficient algorithm named CrowdAgg guaranteeing the optimal solution. Experimental results on two benchmark data sets from different crowdsourcing tasks show that our algorithm is much more effective, efficient and robust than traditional methods.},
booktitle = {Proceedings of the Eighth ACM International Conference on Web Search and Data Mining},
pages = {253–262},
numpages = {10},
keywords = {rank aggregation, evaluation measures, crowdsourced labeling},
location = {Shanghai, China},
series = {WSDM '15}
}

@inproceedings{10.5555/2615731.2615807,
author = {Xu, Haifeng and Larson, Kate},
title = {Improving the efficiency of crowdsourcing contests},
year = {2014},
isbn = {9781450327381},
publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
address = {Richland, SC},
abstract = {While winner-take-all crowdsourcing contests are wide spread in practice, several researchers have observed that their social welfare can be poor due to effort exerted by contestants who are never rewarded. In this paper we study the problem of efficiency in winner-take-all crowdsourcing contests. Using a discrete choice model to capture contestants' production qualities, we introduce a mechanism which filters out low-expertise contestants, before they are asked to produce a solution. We show that under a set of natural assumptions, such a mechanism has desirable incentive properties, attracts high-quality contestants and can improve social welfare. We also provide insights into the problem of prize setting for such contests.},
booktitle = {Proceedings of the 2014 International Conference on Autonomous Agents and Multi-Agent Systems},
pages = {461–468},
numpages = {8},
keywords = {winner-take-all crowdsourcing, prize setting, mechanims design, incentive compatible},
location = {Paris, France},
series = {AAMAS '14}
}

@inproceedings{10.1007/978-3-662-48616-0_34,
author = {Liang, Tingting and Chen, Liang and Xie, Zhining and Yang, Wei and Wu, Jian},
title = {CASE: A Platform for Crowdsourcing Based API Search},
year = {2015},
isbn = {978-3-662-48615-3},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-662-48616-0_34},
doi = {10.1007/978-3-662-48616-0_34},
abstract = {With the rapid growth of Web APIs on the Internet, searching appropriate APIs is becoming a challenging problem. General API search systems (e.g., ProgrammableWeb) implement API search through simple keywords matching leading to unsatisfactory search results. In this paper, we presents a crowdsourcing based API search engine CASE. Specifically, the API search engine leverages social information, Twitter List, a tool used by individual users to organize accounts that interest them on semantics. Based on the lists information, Latent Semantic Indexing (LSI) model is employed to compute the semantic similarity between the APIs and queries. Furthermore, the popularity of APIs inferred from the lists number is integrated with the semantic similarity to generate the final search result.},
booktitle = {Service-Oriented Computing: 13th International Conference, ICSOC 2015, Goa, India, November 16-19, 2015, Proceedings},
pages = {482–485},
numpages = {4},
keywords = {Latent Semantic Indexing (LSI), ProgrammableWeb (PW), Final Search Results, Twitter Lists, Application Programming Interface (API)},
location = {Goa, India}
}

@inproceedings{10.5555/2891460.2891721,
author = {Lasecki, Walter S.},
title = {Crowdsourcing for deployable intelligent systems},
year = {2013},
publisher = {AAAI Press},
abstract = {My work aims to create a scaffold for deployable intelligent systems using crowdsourcing. Current approaches in artificial intelligence (AI) typically focus on solving a narrow subset of problems in a given space - for example: automatic speech recognition as part of a conversational assistant, machine vision as part of a question answering service for blind people, or planning as part of a home assistive robot. This approach is necessary to scope the solution, but often results in a large number of systems that are rarely deployed in real-world setting, but instead operate in toy domains, or in situations where other parts of the problem are assumed to be solved.The framework I have developed aims to use the crowd to help in two ways: (i) make it possible to use human intelligence to power parts of a system that automated approaches cannot or do not yet handle, and (ii) provide a means of enabling more effective deployable systems by people to provide reliable training data on-demand. This summary begins with a brief review of prior work, then outlines a number of different system that I have developed to demonstrate the capabilities of this framework, and concludes with future work to be completed as part of my thesis.},
booktitle = {Proceedings of the Twenty-Seventh AAAI Conference on Artificial Intelligence},
pages = {1670–1671},
numpages = {2},
location = {Bellevue, Washington},
series = {AAAI'13}
}

@inproceedings{10.1145/2740908.2743972,
author = {Brambilla, Marco and Ceri, Stefano and Mauri, Andrea and Volonterio, Riccardo},
title = {An Explorative Approach for Crowdsourcing Tasks Design},
year = {2015},
isbn = {9781450334730},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2740908.2743972},
doi = {10.1145/2740908.2743972},
abstract = {Crowdsourcing applications are becoming widespread; they cover very different scenarios, including opinion mining, multimedia data annotation, localised information gathering, marketing campaigns, expert response gathering, and so on. The quality of the outcome of these applications depends on different design parameters and constraints, and it is very hard to judge about their combined effects without doing some experiments; on the other hand, there are no experiences or guidelines that tell how to conduct experiments, and thus these are often conducted in an ad-hoc manner, typically through adjustments of an initial strategy that may converge to a parameter setting which is quite different from the best possible one. In this paper we propose a comparative, explorative approach for designing crowdsourcing tasks. The method consists of defining a representative set of execution strategies, then execute them on a small dataset, then collect quality measures for each candidate strategy, and finally decide the strategy to be used with the complete dataset.},
booktitle = {Proceedings of the 24th International Conference on World Wide Web},
pages = {1125–1130},
numpages = {6},
keywords = {social computation, empirical method, crowdsourcing},
location = {Florence, Italy},
series = {WWW '15 Companion}
}

@inproceedings{10.1145/2875913.2875926,
author = {Yan, Minzhi and Sun, Hailong and Liu, Xudong},
title = {Efficient Testing of Web Services with Mobile Crowdsourcing},
year = {2015},
isbn = {9781450336413},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2875913.2875926},
doi = {10.1145/2875913.2875926},
abstract = {Nowadays, online Internet services are pervasive and can be invoked from diverse locations in anytime with multitudinous devices. Conventional testing approaches for online services like Web services are conducted by professional tester or developers and cannot simulate the real world running environment of a service. Fortunately, crowdtesting technology brings us promising hope and has acquired increasing interests and adoption because it can recruit plenty of end users to test services under real world environment with low cost. Meanwhile, improved mobile network techniques make crowdsourcing happen anywhere and anytime. In this paper, we present iTest which combines mobile crowdsourcing and web service testing together to support the performance testing of web services. iTest is a framework for service developers to submit their web services and conveniently get the test results from the crowd testers. Firstly, we analyze the key problems need to be solved in a mobile crowdtesting platform; secondly, the architecture of iTest framework and the workflow in it are presented; Thirdly, we perform experiments to illustrate that both the way to access network and tester's location influence the performance of web service, and formulate the tester selection problem as a Set Cover Problem and propose a greedy algorithm for solving this problem; Next, experimental evaluation of the tester selection algorithm is performed to illustrate its efficiency. Finally, we conclude our work and provide the directions for future work.},
booktitle = {Proceedings of the 7th Asia-Pacific Symposium on Internetware},
pages = {157–165},
numpages = {9},
keywords = {web service, set cover, mobile crowdsourcing, crowd testing, Web service testing},
location = {Wuhan, China},
series = {Internetware '15}
}

@inproceedings{10.1145/2818052.2874356,
author = {Girotto, Victor},
title = {Collective Creativity through a Micro-Tasks Crowdsourcing Approach},
year = {2016},
isbn = {9781450339506},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2818052.2874356},
doi = {10.1145/2818052.2874356},
abstract = {Research and commerce activity has been expanding the potential of micro-task markets. Initially used for simple, disconnected tasks, they have now been able to achieve impressive results in creative domains such as writing and design. The goals for this research are to further explore the possibilities of micro-task markets for performing creative work by defining a set of tasks and processes for such a synergistic creative collaboration, as well as expanding this micro-task approach beyond traditional markets such as Mechanical Turk to skilled and motivated communities.},
booktitle = {Proceedings of the 19th ACM Conference on Computer Supported Cooperative Work and Social Computing Companion},
pages = {143–146},
numpages = {4},
keywords = {crowdsourcing, Creativity, CSCW},
location = {San Francisco, California, USA},
series = {CSCW '16 Companion}
}

@inproceedings{10.1145/2598153.2602248,
author = {Rahmanian, Bahareh and Davis, Joseph G.},
title = {User interface design for crowdsourcing systems},
year = {2014},
isbn = {9781450327756},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2598153.2602248},
doi = {10.1145/2598153.2602248},
abstract = {Harnessing human computation through crowdsourcing offers an alternative approach to solving complex problems, especially those that are relatively easy for humans but difficult for computers. Micro-tasking platforms such as Amazon Mechanical Turk have attracted large, on-demand work force of millions of workers as well as hundreds of thousands of job requesters. Achieving high quality results by putting humans in the loop is one of the main goals of these crowdsourcing systems. We study the effects of different user interface designs on the performance of crowdsourcing systems. Our results indicate that user interface design choices have a significant effect on crowdsourced worker performance.},
booktitle = {Proceedings of the 2014 International Working Conference on Advanced Visual Interfaces},
pages = {405–408},
numpages = {4},
keywords = {user interface, crowdsourcing, cognitive load},
location = {Como, Italy},
series = {AVI '14}
}

@inproceedings{10.1007/978-3-319-26135-5_7,
author = {Thuan, Nguyen Hoang and Antunes, Pedro and Johnstone, David and Duy, Nguyen Huynh},
title = {Establishing a Decision Tool for Business Process Crowdsourcing},
year = {2015},
isbn = {9783319261348},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-319-26135-5_7},
doi = {10.1007/978-3-319-26135-5_7},
abstract = {The integration of crowdsourcing in organisations fosters new managerial and business capabilities, especially regarding flexibility and agility of external human resources. However, a crowdsourcing project involves considering multiple contextual factors and choices and dealing with the novelty of the strategy, which makes managerial decisions difficult. This research addresses the problem by proposing a tool supporting business decision-makers in the establishment of crowdsourcing projects. The proposed tool is based on an extensive review of prior research in crowdsourcing and an ontology that standardises the fundamental crowdsourcing concepts, processes, dependencies, constraints, and managerial decisions. In particular, we discuss the architecture of the proposed tool and present two prototypes, one supporting what-if analysis and the other supporting detailed establishment of crowdsourcing processes.},
booktitle = {Proceedings of the Second International Conference on Future Data and Security Engineering - Volume 9446},
pages = {85–97},
numpages = {13},
keywords = {Ontology, Design science, Decision support system, Crowdsourcing, Business process crowdsourcing},
location = {Ho Chi Minh City, Vietnam},
series = {FDSE 2015}
}

@inproceedings{10.1109/IPDPS.2013.84,
author = {Boutsis, Ioannis and Kalogeraki, Vana},
title = {Crowdsourcing under Real-Time Constraints},
year = {2013},
isbn = {9780769549712},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/IPDPS.2013.84},
doi = {10.1109/IPDPS.2013.84},
abstract = {In the recent years we are experiencing the rapid growth of crowd sourcing systems, in which "human workers" are enlisted to perform tasks more effectively than computers, and get compensated for the work they provide. The common belief is that the wisdom of the "human crowd" can greatly complement many computer tasks which are assigned to machines. A significant challenge facing these systems is determining the most efficient allocation of tasks to workers to achieve successful completion of the tasks under real-time constraints. This paper presents REACT, a crowd sourcing system that seeks to address this challenge and proposes algorithms that aim to stimulate user participation and handle dynamic task assignment and execution in the crowd sourcing system. The goal is to determine the most appropriate workers to assign incoming tasks, in such a way so that the real-time demands are met and high quality results are returned. We empirically evaluate our approach and show that REACT meets the requested real-time demands, achieves good accuracy, is efficient, and improves the amount of successful tasks that meet their deadlines up to 61\% compared to traditional approaches like AMT.},
booktitle = {Proceedings of the 2013 IEEE 27th International Symposium on Parallel and Distributed Processing},
pages = {753–764},
numpages = {12},
keywords = {real-time, distributed systems, crowdsourcing},
series = {IPDPS '13}
}

@inproceedings{10.5555/2772879.2773290,
author = {Dayama, Pankaj and Narayanaswamy, Balakrishnan and Garg, Dinesh and Narahari, Y.},
title = {Truthful Interval Cover Mechanisms for Crowdsourcing Applications},
year = {2015},
isbn = {9781450334136},
publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
address = {Richland, SC},
abstract = {The varying nature of qualities and costs of the crowdworkers makes task allocation a non-trivial problem in almost all crowdsourcing applications. If crowdworkers are strategic about their costs, the problem becomes even more challenging. Interestingly, in several crowdsourcing applications, for example, traffic monitoring, air pollution monitoring, digital epidemiology, smart grids operations, etc., the structure of the tasks in space or time exhibits a natural linear ordering. Motivated by the above observation, we model the problem of task allocation to strategic crowdworkers as an interval cover mechanism design problem. In this mechanism, a planner (or task requester) needs to crowdsource labels for a set of tasks in a cost effective manner and make a high quality inference. We consider two different scenarios in this problem: homogeneous and heterogeneous, based on the qualities of crowdworkers. We show that the task allocation problem is polynomial time solvable in the homogeneous case while it is NP-hard in the heterogeneous case. When the crowdworkers are strategic about their costs, we design truthful mechanisms for both the scenarios. In particular, for the heterogeneous case, we propose a novel approximation algorithm that is monotone, leading to a truthful interval cover mechanism via appropriate payments.},
booktitle = {Proceedings of the 2015 International Conference on Autonomous Agents and Multiagent Systems},
pages = {1091–1099},
numpages = {9},
keywords = {truthful mechanisms, task allocation, mechanism design, crowd sourcing},
location = {Istanbul, Turkey},
series = {AAMAS '15}
}

@inproceedings{10.1109/COMM48946.2020.9141972,
author = {Enache, Florin and Greu, Victor and Ciot\^{\i}rnae, Petric\u{a} and Popescu, Florin},
title = {Model and Algorithms for Optimizing a Human Computing System Oriented to Knowledge Extraction by Use of Crowdsourcing},
year = {2020},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/COMM48946.2020.9141972},
doi = {10.1109/COMM48946.2020.9141972},
abstract = {The paper is addressing the actual context of Data Deluge, where the need and also premises to extract more knowledge are increasing, along with the increase of our expectations about performances. Besides, improving artificial intelligence (AI), by machine learning (ML), deep learning (DL) or cognitive learning (CL) performance/potential, when adding human contributions where necessary, is an important and promising research area. Consequently, our model, algorithms (ALG1; ALG2) and soft programs provide useful new instruments for implementing and optimizing the workflow based on crowdsourcing, when using human potential in a human computing system. We aim to increase AI quality adding multiple human outputs for every AI task and leveraging learning rules to be then extended to larger sets of tasks. This way, such hybrid system could be oriented to more knowledge extraction, by the generalization of images/ captions/labels toward more complex tasks, like providing content essential or question answering. Our instruments include features of ranking workers and tasks profiles, which will support the main original process of knowledge extraction, but also the inference elements, by small amounts of learning data (regarding the workers skills and tasks efficiency) to be transferred to AI/ML/DL/CL, which then could be used for processing larger volumes of similar data. Among the results conclusions is that using progressive optimization, structuring the data/tasks in variable (progressive) sets and potential (skill/number) of workers, is both efficacious and efficient, allowing a flexible control of the system and workflow for matching a diversity of tasks complexity/ difficulty/volume and leveraging knowledge extraction.},
booktitle = {2020 13th International Conference on Communications (COMM)},
pages = {297–302},
numpages = {6},
location = {Bucharest, Romania}
}

@inproceedings{10.1145/3318299.3318314,
author = {Jianwei, Xiang and Shuang, Liu and Han, Xu},
title = {A Monte-Carlo Approach to the Value of Information in Crowdsourcing Quality Control Tasks},
year = {2019},
isbn = {9781450366007},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3318299.3318314},
doi = {10.1145/3318299.3318314},
abstract = {In the process of decision-making, the purpose of computing value of information (VOI) is to guide information collection process under uncertain environment, improve the quality of decision-making, and ultimately achieve the optimal decision. In the field of artificial intelligence, MDP is a basic theoretical model for modeling and planning decision problems, and also a major research area of sequential decision-making. In this paper, we presents a novel framework at a specific type of optimal uncertain sequential decision problems that need achieve the best trade-off between decision qualities and cost. We apply it to quality control in crowdsourcing task. Because of the combinatorial challenge of the state space when calculating the optimal policy of the general Markov decision model, this paper considers a more efficient approximation method: A Monte-Carlo Tree method computing the value of information (BMCT) based on belief states.},
booktitle = {Proceedings of the 2019 11th International Conference on Machine Learning and Computing},
pages = {119–123},
numpages = {5},
keywords = {monte-carlo sampling, crowdsourcing, Value of information, Markov decision process},
location = {Zhuhai, China},
series = {ICMLC '19}
}

@inproceedings{10.1109/GLOCOM.2017.8254430,
author = {Wu, Shuang and Gao, Xiaofeng and Wu, Fan and Chen, Guihai},
title = {A Constant-Factor Approximation for Bounded Task Allocation Problem in Crowdsourcing},
year = {2017},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/GLOCOM.2017.8254430},
doi = {10.1109/GLOCOM.2017.8254430},
abstract = {As the technology advances, there are more and more mobile crowdsensing (MCS) platforms that try to leverage these devices to improve the quality of our life. In this paper, we consider a bounded task allocation problem (BTAP) in MCS platforms that involve the time-sensitive and location-dependent tasks. We ﬁrst formulate the bounded task allocation problem as an integer programing problem and prove its NP-hardness. Then we propose an approximated algorithm to solve the problem with (2+ε)-approximated ratio and show that it is a tight bound. So far as we know, we are the first to give a constant approximated ratio for such task allocation problems. Finally, we make some simulations to show the performance of our scheme comparing with other two benchmarks.},
booktitle = {GLOBECOM 2017 - 2017 IEEE Global Communications Conference},
pages = {1–6},
numpages = {6},
location = {Singapore}
}

@inproceedings{10.1145/2567948.2577371,
author = {Spirin, Nikita and Eslami, Motahhare and Ding, Jie and Jain, Pooja and Bailey, Brian and Karahalios, Karrie},
title = {Searching for design examples with crowdsourcing},
year = {2014},
isbn = {9781450327459},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2567948.2577371},
doi = {10.1145/2567948.2577371},
abstract = {Examples are very important in design, but existing tools for design example search still do not cover many cases. For instance, long tail queries containing subtle and subjective design concepts, like "calm and quiet", "elegant", "dark background with a hint of color to make it less boring", are poorly supported. This is mainly due to the inherent complexity of the task, which so far has been tackled only algorithmically using general image search techniques. We propose a powerful new approach based on crowdsourcing, which complements existing algorithmic approaches and addresses their shortcomings. Out of many explored crowdsourcing configurations we found that (1) a design need should be represented via several query images and (2) AMT crowd workers should assess a query-specific relevance of a candidate example from a pre-built design collection. To test the utility of our approach, we compared it with Google Images in a query-by-example mode. Based on feedback from expert designers, the crowd selects more relevant design examples.},
booktitle = {Proceedings of the 23rd International Conference on World Wide Web},
pages = {381–382},
numpages = {2},
keywords = {query-by-example, design search, crowdsourcing, amturk},
location = {Seoul, Korea},
series = {WWW '14 Companion}
}

@inproceedings{10.1145/2488388.2488489,
author = {Singer, Yaron and Mittal, Manas},
title = {Pricing mechanisms for crowdsourcing markets},
year = {2013},
isbn = {9781450320351},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2488388.2488489},
doi = {10.1145/2488388.2488489},
abstract = {Every day millions of crowdsourcing tasks are performed in exchange for payments. Despite the important role pricing plays in crowdsourcing campaigns and the complexity of the market, most platforms do not provide requesters appropriate tools for effective pricing and allocation of tasks.In this paper, we introduce a framework for designing mechanisms with provable guarantees in crowdsourcing markets. The framework enables automating the process of pricing and allocation of tasks for requesters in complex markets like Amazon's Mechanical Turk where workers arrive in an online fashion and requesters face budget constraints and task completion deadlines. We present constant-competitive incentive compatible mechanisms for maximizing the number of tasks under a budget, and for minimizing payments given a fixed number of tasks to complete. To demonstrate the effectiveness of this framework we created a platform that enables applying pricing mechanisms in markets like Mechanical Turk. The platform allows us to show that the mechanisms we present here work well in practice, as well as to give experimental evidence to workers' strategic behavior in absence of appropriate incentive schemes.},
booktitle = {Proceedings of the 22nd International Conference on World Wide Web},
pages = {1157–1166},
numpages = {10},
keywords = {mechanism design, mechanical turk, mechanical perk, human computation, crowdsourcing},
location = {Rio de Janeiro, Brazil},
series = {WWW '13}
}

@inproceedings{10.1145/2487788.2488125,
author = {Celis, L. Elisa and Dasgupta, Koustuv and Rajan, Vaibhav},
title = {Adaptive crowdsourcing for temporal crowds},
year = {2013},
isbn = {9781450320382},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2487788.2488125},
doi = {10.1145/2487788.2488125},
abstract = {Crowdsourcing is rapidly emerging as a computing paradigm that can employ the collective intelligence of a distributed human population to solve a wide variety of tasks. However, unlike organizational environments where workers have set work hours, known skill sets and performance indicators that can be monitored and controlled, most crowdsourcing platforms leverage the capabilities of fleeting workers who exhibit changing work patterns, expertise, and quality of work. Consequently, platforms exhibit significant variability in terms of performance characteristics (like response time, accuracy, and completion rate). While this variability has been folklore in the crowdsourcing community, we are the first to show data that displays this kind of changing behavior. Notably, these changes are not due to a distribution with high variance; rather, the distribution itself is changing over time.Deciding which platform is most suitable given the requirements of a task is of critical importance in order to optimize performance; further, making the decision(s) adaptively to accommodate the dynamically changing crowd characteristics is a problem that has largely been ignored. In this paper, we address the changing crowds problem and, specifically, propose a multi-armed bandit based framework. We introduce the simple epsilon-smart algorithm that performs robustly. Counterfactual results based on real-life data from two popular crowd platforms demonstrate the efficacy of the proposed approach. Further simulations using a random-walk model for crowd performance demonstrate its scalability and adaptability to more general scenarios.},
booktitle = {Proceedings of the 22nd International Conference on World Wide Web},
pages = {1093–1100},
numpages = {8},
keywords = {temporal behavior, online learning, multi-armed bandit, crowdsourcing},
location = {Rio de Janeiro, Brazil},
series = {WWW '13 Companion}
}

@inproceedings{10.1145/2461121.2461135,
author = {Takagi, Hironobu and Kosugi, Akihiro and Saito, Shin and Teraguchi, Masayoshi},
title = {Crowdsourcing platform for workplace accessibility},
year = {2013},
isbn = {9781450318440},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2461121.2461135},
doi = {10.1145/2461121.2461135},
abstract = {Our modern workplace is filled with information sources such as the Web, videos, documents and images. Each employee is required to learn from these sources to work effectively and to contribute to the company's business. Crowdsourcing services have a great potential to improve workplace accessibility by providing captions for meeting videos, describing key diagrams, and converting scanned materials into text files. However, it is risky to expose confidential materials in a public crowd. Crowdsourcing to employees who have knowledge and expertise may solve the issue of confidentiality, but it is difficult to reach them to access their "niche" spare time for tasks given their busy work schedules. Therefore, we propose a crowdsourcing platform to securely disseminate tasks to employees by inviting them to do microtasks. The basic strategy is akin to Web advertising. The system automatically suggests tasks to employees as a part of intranet webpages and in e-mail clients by considering work contexts, employee interests and expertise, and the security of the materials. We will first discuss the pros and cons of intraorganizational crowdsourcing and then propose the new crowdsourcing platform.},
booktitle = {Proceedings of the 10th International Cross-Disciplinary Conference on Web Accessibility},
articleno = {28},
numpages = {4},
keywords = {workplace, intranet, images, crowdsourcing, accessibility},
location = {Rio de Janeiro, Brazil},
series = {W4A '13}
}

@inproceedings{10.1145/3106426.3106436,
author = {Saberi, Morteza and Hussain, Omar K. and Chang, Elizabeth},
title = {An online statistical quality control framework for performance management in crowdsourcing},
year = {2017},
isbn = {9781450349512},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3106426.3106436},
doi = {10.1145/3106426.3106436},
abstract = {The big data research topic has grown rapidly for the past decade due to the advent of the "data deluge". Recent advancements in the literature leverage human computing power known as crowdsourcing to manage and harness big data for various applications. However, human involvement in the completion of crowdsourcing tasks is an error-prone process that affects the overall performance of the crowd. Thus, controlling the quality of workers is an essential step for crowdsourcing systems, which due to unavailability of ground-truth data for any task at hand becomes increasingly challenging. To propose a solution to this problem, in this study, we propose OSQC (Online Statistical Quality Control Framework) for managing the performance of workers in crowdsourcing. OSQC ascertains the worker's performance by using a statistical model and then leverages the traditional statistical control techniques to decide whether to retain a worker for crowdsourcing or to evict him. We evaluate our proposed framework on a real dataset and demonstrate how OSQC assists crowdsourcing to maintain its accuracy.},
booktitle = {Proceedings of the International Conference on Web Intelligence},
pages = {476–482},
numpages = {7},
keywords = {statistical quality control, multiple choice HIT, crowdsourcing management, crowd workers},
location = {Leipzig, Germany},
series = {WI '17}
}

@inproceedings{10.1145/3041021.3051693,
author = {Tinati, Ramine and Madaan, Aastha and Hall, Wendy},
title = {The Role of Crowdsourcing in the Emerging Internet-Of-Things},
year = {2017},
isbn = {9781450349147},
publisher = {International World Wide Web Conferences Steering Committee},
address = {Republic and Canton of Geneva, CHE},
url = {https://doi.org/10.1145/3041021.3051693},
doi = {10.1145/3041021.3051693},
abstract = {In this position paper we wish to propose and discuss several open research questions associated with the IoT. In particular, we wish to consider how crowdsourcing can be used as a scalable, reliable, and sustainable approach to support various computationally difficult and ambiguous tasks recognised in IoT research. We illustrate our work by examining a number of use cases related to healthcare and smart cities, and finally consider the future development of the IoT eco-system with respect to the socio-technical philosophy and implementation of the Web Observatory.},
booktitle = {Proceedings of the 26th International Conference on World Wide Web Companion},
pages = {1669–1672},
numpages = {4},
keywords = {web observatory, iot, internet-of-things, crowdsourcing},
location = {Perth, Australia},
series = {WWW '17 Companion}
}

@inproceedings{10.1145/2740908.2741994,
author = {Amer-Yahia, Sihem and Basu Roy, Senjuti},
title = {From Complex Object Exploration to Complex Crowdsourcing.},
year = {2015},
isbn = {9781450334730},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2740908.2741994},
doi = {10.1145/2740908.2741994},
abstract = {Forming and exploring complex objects is at the heart of a variety of emerging web applications. Historically, existing work on complex objects has been developed in two separate areas: composite item retrieval and team formation. At the same time, emerging applications that harness the wisdom of crowd workers, such as, document editing by workers, sentence translation by fans (or fan-subbing), innovative design, citizen science or journalism, represent complex crowdsourcing, in which an object may represent a complex task formed by a set of sub-tasks or a team of workers who work together to solve the task. The goal of this tutorial is to bridge the gap between composite item retrieval and team formation and define new research directions for complex crowdsourcing applications},
booktitle = {Proceedings of the 24th International Conference on World Wide Web},
pages = {1531–1532},
numpages = {2},
keywords = {optimization algorithms, complex object exploration, complex crowdsourcing},
location = {Florence, Italy},
series = {WWW '15 Companion}
}

@inproceedings{10.1109/MDM.2013.109,
author = {Chatzimilioudis, Georgios and Zeinalipour-Yazti, Demetrios},
title = {Crowdsourcing for Mobile Data Management},
year = {2013},
isbn = {9780769549736},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/MDM.2013.109},
doi = {10.1109/MDM.2013.109},
abstract = {Crowdsourcing refers to a distributed problemsolving model in which a crowd of undefined size is engaged in the task of solving a complex problem through an open call. This novel problem-solving model found its way into numerous applications on the web for voting, fund-raising, micro-works and wisdom-of-the-crowd scenarios. On the other hand, the shift of desktop users to mobile platforms in the post-PC era, along with the unique multi-sensing capabilities of modern mobile devices are expected to eventually unfold the full potential of Crowdsourcing. Smartphones offer a great platform for extending and diversifying web-based crowdsourcing applications to a larger contributing crowd, making contribution easier and omni-present. This advanced seminar presents the fundamental concepts behind crowdsourcing and its applications to mobile data management. In the first part of the seminar, we will overview the crowdsourcing research landscape from a variety of perspectives, with a particular emphasis on the latest data management trends. In the second and more extended part of the seminar, we will focus on an in-depth coverage of emerging mobile crowdsourcing architectures and systems, through a multi-dimensional taxonomy that will address location, sensing, power, performance, big-data and privacy among others. Furthermore, we will overview a number of in-house crowdsourcing prototypes we have developed and deployed over the last few years. The seminar concludes with challenges, opportunities and new directions in the field.},
booktitle = {Proceedings of the 2013 IEEE 14th International Conference on Mobile Data Management - Volume 02},
pages = {3–4},
numpages = {2},
series = {MDM '13}
}

@inproceedings{10.1145/2212776.2212801,
author = {Hughes, Lucy and Atkinson, Douglas and Berthouze, Nadia and Baurley, Sharon},
title = {Crowdsourcing an emotional wardrobe},
year = {2012},
isbn = {9781450310161},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2212776.2212801},
doi = {10.1145/2212776.2212801},
abstract = {Selecting clothing online requires decision-making about sensorial experiences, but online environments provide only limited sensorial information. Inferences are therefore made on the basis of product pictures and their textual description. This is often unreliable as it is either based on the designer's understanding of the product or deprived of perceptual content due to the difficulty of expressing such experiences. Using a purpose built website that combines and cross references multi-modal descriptive media, this study aims at investigating the possibility of using crowdsourcing mechanisms and multi-modal language to engage consumers in providing enriched descriptions of their tactile experiences of garments.},
booktitle = {CHI '12 Extended Abstracts on Human Factors in Computing Systems},
pages = {231–240},
numpages = {10},
keywords = {design research, crowdsourcing, affective computing},
location = {Austin, Texas, USA},
series = {CHI EA '12}
}

@inproceedings{10.1109/MASS.2015.40,
author = {Miao, Xin and Liu, Kebin and Chen, Lei and Liu, Yunhao},
title = {Quality-Aware Online Task Assignment in Mobile Crowdsourcing},
year = {2015},
isbn = {9781467391016},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/MASS.2015.40},
doi = {10.1109/MASS.2015.40},
abstract = {Mobile crowd sourcing (MCS) has grown to be a powerful computation paradigm to harness human power to solve real-world problems. Many commercial MCS platforms have arisen, enabling various novel applications. As crowd workers can be unreliable, a critical issue of these platforms is quality control. Many task assignment approaches have been proposed to increase the quality of crowd sourced tasks by matching workers and tasks in a bipartite graph. However, they fail to apply to MCS platforms where tasks are bound with locations. This paper considers the quality-aware online task assignment problem with location-based tasks. The goal is to optimize tasks' overall quality by assigning appropriate sets of tasks to workers in an online manner. To solve this problem, we propose a probabilistic quality measurement model and a hitchhiking model to characterize workers' behavior. Then we design a polynomial-time online assignment algorithm and prove that the proposed algorithm approximates the offline optimal solution with a competitive ratio of 10/7. Through extensive simulations, we demonstrate the efficiency and effectiveness of our solution.},
booktitle = {Proceedings of the 2015 IEEE 12th International Conference on Mobile Ad Hoc and Sensor Systems (MASS)},
pages = {127–135},
numpages = {9},
series = {MASS '15}
}

@inproceedings{10.1109/ICSS.2015.14,
author = {Liu, Kaixu and Motta, Gianmario and You, Linlin and Ma, Tianyi},
title = {A Threefold Similarity Analysis of Crowdsourcing Feeds},
year = {2015},
isbn = {9781479999477},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/ICSS.2015.14},
doi = {10.1109/ICSS.2015.14},
abstract = {Crowdsourcing is a valuable social sensing for the smarter city. We present an approach for classifying crowd sourced feeds from a threefold point of view, namely image, text, and geography. The main idea is to extract feeds within a specific geographic range, and then analyze similarity of image color and text semantic. The approach enables to identify feeds that report the same issue, hence filtering redundant information. Based on proved methods and algorithms, such approach has been implemented in a software application, called CITY FEED, that is used by the Municipality of Pavia.},
booktitle = {Proceedings of the 2015 International Conference on Service Science},
pages = {93–98},
numpages = {6},
keywords = {Text similarity analysis, Smart city, Image similarity analysis, Crowdsourcing},
series = {ICSS '15}
}

@inproceedings{10.1145/2806416.2806627,
author = {Zhang, Jing and Sheng, Victor S. and Wu, Jian and Fu, Xiaoqin and Wu, Xindong},
title = {Improving Label Quality in Crowdsourcing Using Noise Correction},
year = {2015},
isbn = {9781450337946},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2806416.2806627},
doi = {10.1145/2806416.2806627},
abstract = {This paper proposes a novel framework that introduces noise correction techniques to further improve label quality after ground truth inference in crowdsourcing. In the framework, an adaptive voting noise correction algorithm (AVNC) is proposed to identify and correct the most likely noises with the help of estimated qualities of labelers provided by the ground truth inference. The experimental results on two real-world datasets show that (1) the framework can improve label quality regardless of inference algorithms, especially under the circumstance that each example has a few noisy labels; and (2) since the algorithm AVNC considers both the number of and the probability of potential noises, it outperforms a baseline noise correction algorithm.},
booktitle = {Proceedings of the 24th ACM International on Conference on Information and Knowledge Management},
pages = {1931–1934},
numpages = {4},
keywords = {noise correction, label quality, inference, crowdsourcing},
location = {Melbourne, Australia},
series = {CIKM '15}
}

@inproceedings{10.1145/3041021.3055128,
author = {Goncalves, Jorge and Feldman, Michael and Hu, Subingqian and Kostakos, Vassilis and Bernstein, Abraham},
title = {Task Routing and Assignment in Crowdsourcing based on Cognitive Abilities},
year = {2017},
isbn = {9781450349147},
publisher = {International World Wide Web Conferences Steering Committee},
address = {Republic and Canton of Geneva, CHE},
url = {https://doi.org/10.1145/3041021.3055128},
doi = {10.1145/3041021.3055128},
abstract = {Appropriate task routing and assignment is an important, but often overlooked, element in crowdsourcing research and practice. In this paper, we explore and evaluate a mechanism that can enable matching crowdsourcing tasks to suitable crowd-workers based on their cognitive abilities. We measure participants' visual and fluency cognitive abilities with the well-established Kit of Factor-Referenced Cognitive Test, and measure crowdsourcing performance with our own set of developed tasks. Our results indicate that participants' cognitive abilities correlate well with their crowdsourcing performance. We also built two predictive models (beta and linear regression) for crowdsourcing task performance based on the performance on cognitive tests as explanatory variables. The model results suggest that it is feasible to predict crowdsourcing performance based on cognitive abilities. Finally, we discuss the benefits and challenges of leveraging workers' cognitive abilities to improve task routing and assignment in crowdsourcing environments.},
booktitle = {Proceedings of the 26th International Conference on World Wide Web Companion},
pages = {1023–1031},
numpages = {9},
keywords = {worker performance, visual tasks, task routing, task assignment, kit of factor-referenced cognitive tests., fluency tasks, crowdsourcing, cognitive abilities},
location = {Perth, Australia},
series = {WWW '17 Companion}
}

@inproceedings{10.1145/2837126.2837181,
author = {Fonteles, Andr\'{e} Sales and Bouveret, Sylvain and Gensel, J\'{e}r\^{o}me},
title = {Heuristics for Task Recommendation in Spatiotemporal Crowdsourcing Systems},
year = {2015},
isbn = {9781450334938},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2837126.2837181},
doi = {10.1145/2837126.2837181},
abstract = {Crowdsourcing systems (CS) are platforms that enable a system or a user to publish tasks in order to be accomplished by others. Typically, a CS is a system where users, called workers, perform tasks using desktop computers. Recently, some CS have appeared with spatiotemporal tasks. Such tasks require a worker to be in a given location within a specific time-window to be accomplished. We propose and study here the usage of five heuristics for solving the NP-hard trajectory recommendation problem (TRP). In a TRP, the system recommends a trajectory to a worker that allows him to accomplish spatiotemporal tasks he has skill and/or affinity with, without exceeding his available time. Our experiments show that some of our heuristics are efficient alternatives for a heavy optimal approach providing trajectories with an average utility of about 60\% of the optimal ones.},
booktitle = {Proceedings of the 13th International Conference on Advances in Mobile Computing and Multimedia},
pages = {1–5},
numpages = {5},
keywords = {task recommendation, task assignment, spatial crowdsourcing, crowdsensing},
location = {Brussels, Belgium},
series = {MoMM 2015}
}

@inproceedings{10.1007/978-3-319-68204-4_24,
author = {Gil, Yolanda and Garijo, Daniel and Ratnakar, Varun and Khider, Deborah and Emile-Geay, Julien and McKay, Nicholas},
title = {A Controlled Crowdsourcing Approach for Practical Ontology Extensions and Metadata Annotations},
year = {2017},
isbn = {978-3-319-68203-7},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-319-68204-4_24},
doi = {10.1007/978-3-319-68204-4_24},
abstract = {Traditional approaches to ontology development have a large lapse between the time when a user using the ontology has found a need to extend it and the time when it does get extended. For scientists, this delay can be weeks or months and can be a significant barrier for adoption. We present a new approach to ontology development and data annotation enabling users to add new metadata properties on the fly as they describe their datasets, creating terms that can be immediately adopted by others and eventually become standardized. This approach combines a traditional, consensus-based approach to ontology development, and a crowdsourced approach where expert users (the crowd) can dynamically add terms as needed to support their work. We have implemented this approach as a socio-technical system that includes: (1) a crowdsourcing platform to support metadata annotation and addition of new terms, (2) a range of social editorial processes to make standardization decisions for those new terms, and (3) a framework for ontology revision and updates to the metadata created with the previous version of the ontology. We present a prototype implementation for the Paleoclimate community, the Linked Earth Framework, currently containing 700 datasets and engaging over 50 active contributors. Users exploit the platform to do science while extending the metadata vocabulary, thereby producing useful and practical metadata.},
booktitle = {The Semantic Web – ISWC 2017: 16th International Semantic Web Conference, Vienna, Austria, October 21-25, 2017, Proceedings, Part II},
pages = {231–246},
numpages = {16},
keywords = {Incremental vocabulary development, Semantic science, Collaborative ontology engineering, Semantic wiki, Crowdsourcing, Metadata},
location = {Vienna, Austria}
}

@inproceedings{10.1007/978-3-319-27974-9_23,
author = {Packham, Sean and Suleman, Hussein},
title = {Crowdsourcing a Text Corpus is not a Game},
year = {2015},
isbn = {9783319279732},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-319-27974-9_23},
doi = {10.1007/978-3-319-27974-9_23},
abstract = {Building language corpora for low resource languages such as South Africa's isiXhosa is challenging because of limited digitized texts. Language corpora are needed for building information retrieval services such as search and translation and to support further online content creation. A novel solution was proposed to source original and relevant multilingual content by crowdsourcing translations via an online competitive game where participants would be paid for their contributions. Four experiments were conducted and the results support the idea that gamification by itself does not yield the widely expected benefits of increased motivation and engagement. We found that people do not volunteer without financial incentives, the form of payment does not matter, they would not continue contributing if the money is taken away and people preferred direct incentives and the possibility of incentives was not as strong a motivator.},
booktitle = {Proceedings of the 17th International Conference on Asia-Pacific Digital Libraries - Volume 9469},
pages = {225–234},
numpages = {10},
keywords = {Translation, Language corpora, Information retrieval, Gamification, Crowdsourcing}
}

@inproceedings{10.1145/2806416.2806451,
author = {Asudeh, Abolfazl and Zhang, Gensheng and Hassan, Naeemul and Li, Chengkai and Zaruba, Gergely V.},
title = {Crowdsourcing Pareto-Optimal Object Finding By Pairwise Comparisons},
year = {2015},
isbn = {9781450337946},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2806416.2806451},
doi = {10.1145/2806416.2806451},
abstract = {This is the first study of crowdsourcing Pareto-optimal object finding over partial orders and by pairwise comparisons, which has applications in public opinion collection, group decision making, and information exploration. Departing from prior studies on crowdsourcing skyline and ranking queries, it considers the case where objects do not have explicit attributes and preference relations on objects are strict partial orders. The partial orders are derived by aggregating crowdsourcers' responses to pairwise comparison questions. The goal is to find all Pareto-optimal objects by the fewest possible questions. It employs an iterative question-selection framework. Guided by the principle of eagerly identifying non-Pareto optimal objects, the framework only chooses candidate questions which must satisfy three conditions. This design is both sufficient and efficient, as it is proven to find a short terminal question sequence. The framework is further steered by two ideas---macro-ordering and micro-ordering. By different micro-ordering heuristics, the framework is instantiated into several algorithms with varying power in pruning questions. Experiment results using both real crowdsourcing marketplace and simulations exhibited not only orders of magnitude reductions in questions when compared with a brute-force approach, but also close-to-optimal performance from the most efficient instantiation.},
booktitle = {Proceedings of the 24th ACM International on Conference on Information and Knowledge Management},
pages = {753–762},
numpages = {10},
keywords = {skyline query, partial order, pareto-optimal object, pairwise comparison, opinion collection, information exploration, human computation, decision making, crowdsourcing},
location = {Melbourne, Australia},
series = {CIKM '15}
}

@inproceedings{10.5555/2964060.2964109,
author = {Hu, Qinmin and Huang, Xiangji},
title = {Bringing Information Retrieval into Crowdsourcing: A Case Study},
year = {2014},
isbn = {9783319060279},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {We propose a novel and economic framework for bringing information retrieval into crowdsourcing. Both crowdsourcing and information retrieval achieve mutual benefits, which result in 1 workers' quality control by using the query-oriented training; 2 cost savings in money and time; and 3 better qualified feedback information. In our case study, the costs of crowdsourcing for 18,260 jobs are as low as $47.25 and as short as 5 hours in total. Furthermore, the experimental results show that information retrieval techniques greatly reduce the workloads of crowdsourcing, which is only 5\% of the original work. At the other hand, crowdsourcing improves the accuracy of the information retrieval system through providing qualified feedback information.},
booktitle = {Proceedings of the 36th European Conference on IR Research on Advances in Information Retrieval - Volume 8416},
pages = {631–637},
numpages = {7},
location = {Amsterdam, The Netherlands},
series = {ECIR 2014}
}

@inproceedings{10.5555/3157382.3157663,
author = {Lahouti, Farshad and Hassibi, Babak},
title = {Fundamental limits of budget-fidelity trade-off in label crowdsourcing},
year = {2016},
isbn = {9781510838819},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Digital crowdsourcing (CS) is a modern approach to perform certain large projects using small contributions of a large crowd. In CS, a taskmaster typically breaks down the project into small batches of tasks and assigns them to so-called workers with imperfect skill levels. The crowdsourcer then collects and analyzes the results for inference and serving the purpose of the project. In this work, the CS problem, as a human-in-the-loop computation problem, is modeled and analyzed in an information theoretic rate-distortion framework. The purpose is to identify the ultimate fidelity that one can achieve by any form of query from the crowd and any decoding (inference) algorithm with a given budget. The results are established by a joint source channel (de)coding scheme, which represent the query scheme and inference, over parallel noisy channels, which model workers with imperfect skill levels. We also present and analyze a query scheme dubbed k-ary incidence coding and study optimized query pricing in this setting.},
booktitle = {Proceedings of the 30th International Conference on Neural Information Processing Systems},
pages = {5065–5073},
numpages = {9},
location = {Barcelona, Spain},
series = {NIPS'16}
}

@inproceedings{10.1145/2598153.2602249,
author = {Kucherbaev, Pavel and Daniel, Florian and Marchese, Maurizio and Casati, Fabio and Reavey, Brian},
title = {Toward effective tasks navigation in crowdsourcing},
year = {2014},
isbn = {9781450327756},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2598153.2602249},
doi = {10.1145/2598153.2602249},
abstract = {Crowdsourcing platforms are changing the way people can work and earn money. The population of workers on crowdsourcing platforms already counts millions and keeps growing. Workers on these platforms face several usability challenges, which we identify in this work by running two surveys on the CrowdFlower platform. Our surveys show that the majority of workers spend more than 25\% of their time on searching tasks to work on. Limitations in the current user interface of the task listing page prevent workers from focusing more on the execution. In this work we present an attempt to design and implement a specific user interface for task listing aimed to help workers spend less time searching for tasks and thus navigate among them more easily.},
booktitle = {Proceedings of the 2014 International Working Conference on Advanced Visual Interfaces},
pages = {401–404},
numpages = {4},
keywords = {user interfaces, search, crowdsourcing},
location = {Como, Italy},
series = {AVI '14}
}

@inproceedings{10.24963/ijcai.2023/333,
author = {Fang, Yili and Shen, Chaojie and Gu, Huamao and Han, Tao and Ding, Xinyi},
title = {TDG4Crowd: test data generation for evaluation of aggregation algorithms in crowdsourcing},
year = {2023},
isbn = {978-1-956792-03-4},
url = {https://doi.org/10.24963/ijcai.2023/333},
doi = {10.24963/ijcai.2023/333},
abstract = {In crowdsourcing, existing efforts mainly use real datasets collected from crowdsourcing as test datasets to evaluate the effectiveness of aggregation algorithms. However, these work ignore the fact that the datasets obtained by crowdsourcing are usually sparse and imbalanced due to limited budget. As a result, applying the same aggregation algorithm on different datasets often show contradicting conclusions. For example, on the RTE dataset, Dawid and Skene model performs significantly better than Majority Voting, while on the LableMe dataset, the experiments give the opposite conclusion. It is challenging to obtain comprehensive and balanced datasets at a low cost. To our best knowledge, little effort have been made to the fair evaluation of aggregation algorithms. To fill in this gap, we propose a novel method named TDG4Crowd that can automatically generate comprehensive and balanced datasets. Using Kullback Leibler divergence and Kolmogorov-Smirnov test, the experiment results show the superior of our method compared with others. Aggregation algorithms also perform more consistently on the synthetic datasets generated using our method.},
booktitle = {Proceedings of the Thirty-Second International Joint Conference on Artificial Intelligence},
articleno = {333},
numpages = {9},
location = {Macao, P.R.China},
series = {IJCAI '23}
}

@inproceedings{10.1145/3148456.3148491,
author = {J\'{u}nior, Paulo Sim\~{o}es and Novais, Renato and Vieira, Vaninha and Pedraza, Laia G. and Mendon\c{c}a, Manoel and Villela, Karina},
title = {Visualization mechanisms for crowdsourcing information in emergency coordination},
year = {2015},
isbn = {9781450353625},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3148456.3148491},
doi = {10.1145/3148456.3148491},
abstract = {To perform emergency coordination, people in a command centre need to process a large amount of data about the incident to make decisions, generally, under time pressure. A main challenge is to quickly obtain contextual information about the situation, which can be obtained from people in the place of the incident, in a crowdsourcing manner. This paper presents our investigation about visualization mechanisms to support command centres on analysing crowdsourcing information regarding emergency situations. As contributions, we highlight: 1) discussion of existing visualization mechanisms and their support on emergency management; 2) prototype of the Emergency Response Toolkit (ERTK), a set of tools to support command centres on using information from the crowd, e.g. in large-scale events; and 3) evaluation of ERTK and its visualization mechanisms with 11 emergency experts, in Brazil, Austria and Spain, collecting feedback to improve information visualization for emergency management.},
booktitle = {Proceedings of the 14th Brazilian Symposium on Human Factors in Computing Systems},
articleno = {35},
numpages = {8},
keywords = {information visualization, emergency management, emergency coordination, crowdsourcing},
location = {Salvador, Brazil},
series = {IHC '15}
}

@inproceedings{10.1145/2872518.2889409,
author = {Basharat, Amna and Arpinar, I. Budak and Rasheed, Khaled},
title = {Leveraging Crowdsourcing for the Thematic Annotation of the Qur'an},
year = {2016},
isbn = {9781450341448},
publisher = {International World Wide Web Conferences Steering Committee},
address = {Republic and Canton of Geneva, CHE},
url = {https://doi.org/10.1145/2872518.2889409},
doi = {10.1145/2872518.2889409},
abstract = {In this paper, we illustrate how we leverage crowdsourcing to create workflows for knowledge engineering in specialized and knowledge intensive domains. We undertake the special case of the Arabic script of the Qur'an, a widely studied manuscript, and attempt to employ crowdsourcing methods for its thematic annotation at the sub-verse level, for which, there is no standardized knowledge model available to date. We demonstrate that our proposed method presents feasibility to achieve reliable annotations in an efficient and scalable manner. The proposed methodology and framework is meant to be generalizable to other knowledge intensive and specialized domains.},
booktitle = {Proceedings of the 25th International Conference Companion on World Wide Web},
pages = {13–14},
numpages = {2},
keywords = {thematic annotation, semantic web, qur'an, ontology, knowledge engineering, disambiguation, crowdsourcing},
location = {Montr\'{e}al, Qu\'{e}bec, Canada},
series = {WWW '16 Companion}
}

@inproceedings{10.1109/ICPP.2015.67,
author = {Sun, Wei and Zhu, Yanmin and Ni, Lionel M. and Li, Bo},
title = {Crowdsourcing Sensing Workloads of Heterogeneous Tasks: A Distributed Fairness-Aware Approach},
year = {2015},
isbn = {9781467375870},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/ICPP.2015.67},
doi = {10.1109/ICPP.2015.67},
abstract = {Crowd sourced sensing over smartphones presents a new paradigm for collecting sensing data over a vast area for real-time monitoring applications. A monitoring application may require different types of sensing data, while under a budget constraint. This paper explores the crucial problem of maximizing the aggregate data utility of heterogeneous sensing tasks while maintaining utility-centric fairness across different tasks under a budget constraint. In particular, we take the redundancy of sensing data into account. This problem is highly challenging given its unique characteristics including the intrinsic trade off between aggregate data utility and fairness, and the large number of smartphones. We propose a fairness-aware distributed approach to solving this problem. To overcome the intractability of the problem, we decompose it to two sub problems of recruiting smartphones under a budget constraint and allocating workloads of sensing tasks. For the first sub problem, we propose an efficient greedy algorithm which has a constant approximation ratio of two. For the second problem, we apply dual based decomposition based on which we design a distributed algorithm for determining the workloads of different tasks on each recruited smartphone. We have implemented our distributed algorithm on a windows-based server and Android-based smartphones. With extensive simulations we demonstrate that our approach achieves high aggregate data utility while maintaining good utility-centric fairness across sensing tasks.},
booktitle = {Proceedings of the 2015 44th International Conference on Parallel Processing (ICPP)},
pages = {580–589},
numpages = {10},
series = {ICPP '15}
}

@inproceedings{10.1109/MDM.2014.69,
author = {Phuttharak, Jurairat and Loke, Seng W.},
title = {Towards Declarative Programming for Mobile Crowdsourcing: P2P Aspects},
year = {2014},
isbn = {9781479957057},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/MDM.2014.69},
doi = {10.1109/MDM.2014.69},
abstract = {Peer-to-Peer technologies have been widely used in networks which manage vast amount of data daily. The proliferation of mobile devices strongly motivates mobile peer-to-peer network (M-P2P) applications, with benefits from network effects. We argue that logic programming for crowd sourcing can be useful in peer-to-peer computing for querying and multicasting tasks shared over peer networks. We introduce a declarative crowd sourcing platform for mobile applications, which combines conventional machine computation and the power of the crowd in social networking, particularly in M-P2P networks. This paper discusses a simple extension of Prolog, which we call Logic Crowd, focusing on enabling goal evaluation over peers in mobile peer networks. Additionally, we demonstrate that logic programming for crowd sourcing can be useful in peer-to-peer computing for querying and P2P style of task sharing over short-range networks. In this paper, we illustrate the potential of our approach via programming idioms, a prototype implementation and scenarios.},
booktitle = {Proceedings of the 2014 IEEE 15th International Conference on Mobile Data Management - Volume 02},
pages = {61–66},
numpages = {6},
keywords = {peer-to-peer network, mobile crowdsourcing, mobile application, declarative programming language},
series = {MDM '14}
}

@inproceedings{10.1007/978-3-319-94277-3_5,
author = {Li, Liangcheng and Bu, Jiajun and Wang, Can and Yu, Zhi and Wang, Wei and Wu, Yue and Gu, Chunbin and Zhou, Qin},
title = {CrowdAE: A Crowdsourcing System with Human Inspection Quality Enhancement for Web Accessibility Evaluation},
year = {2018},
isbn = {978-3-319-94276-6},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-319-94277-3_5},
doi = {10.1007/978-3-319-94277-3_5},
abstract = {Crowdsourcing technology can help manual testing by soliciting the contributions from volunteer evaluators. But crowd evaluators may give inaccurate or invalid evaluation results. This paper proposes an advanced crowdsourcing-based web accessibility evaluation system called CrowdAE by enhancing the crowdsourcing-based manual testing module of the previous version. Through three main process namely learning system, task assignment and task review, we can improve the quality of evaluation results from the crowd. From the comparison on the two years’ evaluation process of Chinese government websites, our CrowdAE outperforms the previous version and improve the accuracy of the evaluation results.},
booktitle = {Computers Helping People with Special Needs: 16th International Conference, ICCHP 2018, Linz, Austria, July 11-13, 2018, Proceedings, Part I},
pages = {27–30},
numpages = {4},
keywords = {Crowdsourcing, Web accessibility evaluation},
location = {Linz, Austria}
}

@inproceedings{10.5555/2936924.2937143,
author = {Wang, Wanyuan and He, Zhanpeng and Shi, Peng and Wu, Weiwei and Jiang, Yichuan},
title = {Truthful Team Formation for Crowdsourcing in Social Networks: (Extended Abstract)},
year = {2016},
isbn = {9781450342391},
publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
address = {Richland, SC},
abstract = {This paper studies complex task crowdsourcing by team formation in social networks (SNs), where the requester wishes to hire a group of socially connected workers that can work together as a team. Previous social team crowdsourcing approaches mainly focus on the algorithmic part for social welfare maximization, however, ignore the strategic behavior of workers. In practical crowdsourcing markets, workers are selfish for maximizing their own profit. Within the traditional researches, these selfish workers can be encouraged to manipulate the crowdsourcing system. This untruthful behavior will discourage other workers from participations and is unprofitable for the requester. Thus, a truthful mechanism, guaranteeing that each worker's profit is optimized by behaving honestly, is essential to the success of a crowdsourcing system. Towards this end, in this paper, we develop two efficient truthful mechanisms for the small-scale and large-scale social team crowdsourcing applications, respectively. The experimental results on a real dataset show that compared to the benchmark optimal mechanism, the proposed mechanisms perform well for various scale applications on social welfare maximization.},
booktitle = {Proceedings of the 2016 International Conference on Autonomous Agents \&amp; Multiagent Systems},
pages = {1327–1328},
numpages = {2},
keywords = {team formation, social networks, mechanism design, crowdsourcing},
location = {Singapore, Singapore},
series = {AAMAS '16}
}

@inproceedings{10.1145/2942358.2942402,
author = {Hu, Qin and Wang, Shengling and Ma, Liran and Cheng, Xiuzhen and Bie, Rongfang},
title = {Solving the crowdsourcing dilemma using the zero-determinant strategy: poster},
year = {2016},
isbn = {9781450341844},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2942358.2942402},
doi = {10.1145/2942358.2942402},
abstract = {As a promising technology, crowdsourcing aims to accomplish a complex task via eliciting services from a large group of workers. However, recent observations indicate that the success of crowdsourcing is being hindered by the malicious behaviors of the workers. In this paper, we analyze the attack problem using an iterated prisoner's dilemma (IPD) game and propose an zero-determinant (ZD) strategy based algorithm. Simulation results demonstrate that the requestor can incentivize the worker to keep on cooperating.},
booktitle = {Proceedings of the 17th ACM International Symposium on Mobile Ad Hoc Networking and Computing},
pages = {373–374},
numpages = {2},
keywords = {zero-determinant strategy, game theory, crowdsourcing},
location = {Paderborn, Germany},
series = {MobiHoc '16}
}

@inproceedings{10.1109/GLOCOM.2017.8254421,
author = {Li, Shuai and Xi, Teng and Tian, Ye and Wang, Wendong},
title = {Inferring Fine-Grained PM2.5 with Bayesian Based Kernel Method for Crowdsourcing System},
year = {2017},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/GLOCOM.2017.8254421},
doi = {10.1109/GLOCOM.2017.8254421},
abstract = {Air pollution seriously affect people's lives, among which PM$_{2.5}$ is especially harmful for humans health. Although many countries have established fixed air quality monitoring stations (AQMS) to monitor air pollution, the costs of constructing and maintaining for AQMS are extremely expensive and the density of AQMS is very low. To acquire fine-grained concentration of PM$_{2.5}$, this paper have proposed a novel Bayesian based kernel method. \%using images and camera information which are easy to get. Our model leverage heterogeneous data which jointly using images information, camera lens information, GPS information and magnetic sensor information. To study the relationship between PM$_{2.5}$ concentration and images information, we have established a crowdsourcing system and have collected photos for consecutive 16 months. The performance of the proposed method has been evaluated thoroughly by real dataset we have collected. The results show that, compared with three baselines, our proposed algorithm can reduce up to 35\% prediction error in average.},
booktitle = {GLOBECOM 2017 - 2017 IEEE Global Communications Conference},
pages = {1–6},
numpages = {6},
location = {Singapore}
}

@inproceedings{10.1145/2736277.2741685,
author = {Difallah, Djellel Eddine and Catasta, Michele and Demartini, Gianluca and Ipeirotis, Panagiotis G. and Cudr\'{e}-Mauroux, Philippe},
title = {The Dynamics of Micro-Task Crowdsourcing: The Case of Amazon MTurk},
year = {2015},
isbn = {9781450334693},
publisher = {International World Wide Web Conferences Steering Committee},
address = {Republic and Canton of Geneva, CHE},
url = {https://doi.org/10.1145/2736277.2741685},
doi = {10.1145/2736277.2741685},
abstract = {Micro-task crowdsourcing is rapidly gaining popularity among research communities and businesses as a means to leverage Human Computation in their daily operations. Unlike any other service, a crowdsourcing platform is in fact a marketplace subject to human factors that affect its performance, both in terms of speed and quality. Indeed, such factors shape the dynamics of the crowdsourcing market. For example, a known behavior of such markets is that increasing the reward of a set of tasks would lead to faster results. However, it is still unclear how different dimensions interact with each other: reward, task type, market competition, requester reputation, etc. In this paper, we adopt a data-driven approach to (A) perform a long-term analysis of a popular micro-task crowdsourcing platform and understand the evolution of its main actors (workers, requesters, and platform). (B) We leverage the main findings of our five year log analysis to propose features used in a predictive model aiming at determining the expected performance of any batch at a specific point in time. We show that the number of tasks left in a batch and how recent the batch is are two key features of the prediction. (C) Finally, we conduct an analysis of the demand (new tasks posted by the requesters) and supply (number of tasks completed by the workforce) and show how they affect task prices on the marketplace.},
booktitle = {Proceedings of the 24th International Conference on World Wide Web},
pages = {238–247},
numpages = {10},
keywords = {trend identification, tracking, human factors, forecasting, experimentation, design, crowdsourcing},
location = {Florence, Italy},
series = {WWW '15}
}

@inproceedings{10.1145/2810188.2810191,
author = {Melenhorst, Mark and Novak, Jasminko and Micheel, Isabel and Larson, Martha and Boeckle, Martin},
title = {Bridging the Utilitarian-Hedonic Divide in Crowdsourcing Applications},
year = {2015},
isbn = {9781450337465},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2810188.2810191},
doi = {10.1145/2810188.2810191},
abstract = {This paper introduces a novel perspective on the gamification of crowdsourcing tasks by conceptualizing it as the introduction of hedonic quality into the solution of utilitarian tasks and into the design of corresponding systems. We demonstrate how such a conceptualization can enable crowdsourcing applications to involve new kinds of crowds in everyday contexts that cannot be reached with existing models. We illustrate its application with the design of TrendRack, a gamified crowdsourcing application in the domain of fashion. We then discuss the results from a first evaluation, suggesting successful engagement of fashion customers in everyday contexts.},
booktitle = {Proceedings of the Fourth International Workshop on Crowdsourcing for Multimedia},
pages = {9–14},
numpages = {6},
keywords = {user study, motivation, mobile application development, hedonic quality, crowdsourcing, GWAPs},
location = {Brisbane, Australia},
series = {CrowdMM '15}
}

@inproceedings{10.1145/2187836.2187970,
author = {Ghosh, Arpita and McAfee, Preston},
title = {Crowdsourcing with endogenous entry},
year = {2012},
isbn = {9781450312295},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2187836.2187970},
doi = {10.1145/2187836.2187970},
abstract = {We investigate the design of mechanisms to incentivize high quality outcomes in crowdsourcing environments with strategic agents, when entry is an endogenous, strategic choice. Modeling endogenous entry in crowdsourcing markets is important because there is a nonzero cost to making a contribution of any quality which can be avoided by not participating, and indeed many sites based on crowdsourced content do not have adequate participation. We use a mechanism with monotone, rank-based, rewards in a model where agents strategically make participation and quality choices to capture a wide variety of crowdsourcing environments, ranging from conventional crowdsourcing contests with monetary rewards such as TopCoder, to crowdsourced content as in online Q&amp;A forums.We begin by explicitly constructing the unique mixed-strategy equilibrium for such monotone rank-order mechanisms, and use the participation probability and distribution of qualities from this construction to address the question of designing incentives for two kinds of rewards that arise in the context of crowdsourcing. We first show that for attention rewards that arise in the crowdsourced content setting, the entire equilibrium distribution and therefore every increasing statistic including the maximum and average quality (accounting for participation), improves when the rewards for every rank but the last are as high as possible. In particular, when the cost of producing the lowest possible quality content is low, the optimal mechanism displays all but the poorest contribution. We next investigate how to allocate rewards in settings where there is a fixed total reward that can be arbitrarily distributed amongst participants, as in crowdsourcing contests. Unlike models with exogenous entry, here the expected number of participants can be increased by subsidizing entry, which could potentially improve the expected value of the best contribution. However, we show that subsidizing entry does not improve the expected quality of the best contribution, although it may improve the expected quality of the average contribution. In fact, we show that free entry is dominated by taxing entry---making all entrants pay a small fee, which is rebated to the winner along with whatever rewards were already assigned, can improve the quality of the best contribution over a winner-take-all contest with no taxes.},
booktitle = {Proceedings of the 21st International Conference on World Wide Web},
pages = {999–1008},
numpages = {10},
keywords = {user generated content (UGC), social computing, mechanism design, game theory, crowdsourcing, contest design},
location = {Lyon, France},
series = {WWW '12}
}

@inproceedings{10.1007/978-3-319-24258-3_8,
author = {Gadiraju, Ujwal and Fetahu, Besnik and Kawase, Ricardo},
title = {Training Workers for Improving Performance in Crowdsourcing Microtasks},
year = {2015},
isbn = {978-3-319-24257-6},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-319-24258-3_8},
doi = {10.1007/978-3-319-24258-3_8},
abstract = {With the advent and growing use of crowdsourcing labor markets for a variety of applications, optimizing the quality of results produced is of prime importance. The quality of the results produced is typically a function of the performance of crowd workers. In this paper, we investigate the notion of treating crowd workers as ‘learners’ in a novel learning environment. This learning context is characterized by a short-lived learning phase and immediate application of learned concepts. We draw motivation from the desire of crowd workers to perform well in order to maintain a good reputation, while attaining monetary rewards successfully. Thus, we delve into training workers in specific microtasks of different types. We exploit (i) implicit training, where workers are provided training when they provide erraneous responses to questions with priorly known answers, and (ii) explicit training, where workers are required to go through a training phase before they attempt to work on the task itself. We evaluated our approach in 4 different types of microtasks with a total of 1200 workers, who were subjected to either one of the proposed training strategies or baseline case of no training. The results show that workers who undergo training depict an improvement in performance upto 5&nbsp;\%, and a reduction in the task completion time upto 41&nbsp;\%. Additionally, crowd training led to the elimination of malicious workers and a costs-benefit gain upto nearly 15&nbsp;\%.},
booktitle = {Design for Teaching and Learning in a Networked World: 10th European Conference on Technology Enhanced Learning, EC-TEL 2015, Toledo, Spain, September 15-18, 2015, Proceedings},
pages = {100–114},
numpages = {15},
keywords = {Performance, Microtask, Learning, Training, Workers, Crowdsourcing},
location = {Toledo, Spain}
}

@inproceedings{10.1145/2948649.2948657,
author = {To, Hien and Geraldes, R\'{u}ben and Shahabi, Cyrus and Kim, Seon Ho and Prendinger, Helmut},
title = {An empirical study of workers' behavior in spatial crowdsourcing},
year = {2016},
isbn = {9781450343091},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2948649.2948657},
doi = {10.1145/2948649.2948657},
abstract = {With the ubiquity of smartphones, spatial crowdsourcing (SC) has emerged as a new paradigm that engages mobile users to perform tasks in the physical world. Thus, various SC techniques have been studied for performance optimization. However, little research has been done to understand workers' behavior in the real world. In this study, we designed and performed two real world SC campaigns utilizing our mobile app, called Genkii, which is a GPS-enabled app for users to report their affective state (e.g., happy, sad). We used Yahoo! Japan Crowdsourcing as the payment platform to reward users for reporting their affective states at different locations and times. We studied the relationship between incentives and participation by analyzing the impact of offering a fixed reward versus an increasing reward scheme. We observed that users tend to stay in a campaign longer when the provided incentives gradually increase over time. We also found that the degree of mobility is correlated with the reported information. For example, users who travel more are observed to be happier than the ones who travel less. Furthermore, analyzing the spatiotemporal information of the reports reveals interesting mobility patterns that are unique to spatial crowdsourcing.},
booktitle = {Proceedings of the Third International ACM SIGMOD Workshop on Managing and Mining Enriched Geo-Spatial Data},
articleno = {8},
numpages = {6},
keywords = {spatial crowdsourcing, mobility, incentives},
location = {San Francisco, California},
series = {GeoRich '16}
}

@inproceedings{10.5220/0005084800540063,
author = {Silva, C\^{a}ndida and Ramos, Isabel},
title = {An Ontology Roadmap for Crowdsourcing Innovation Intermediaries},
year = {2014},
isbn = {9789897580505},
publisher = {SCITEPRESS - Science and Technology Publications, Lda},
address = {Setubal, PRT},
url = {https://doi.org/10.5220/0005084800540063},
doi = {10.5220/0005084800540063},
abstract = {Ontologies have proliferated in the last years, essentially justified by the need of achieving a consensus in the multiple representations of reality inside computers, and therefore the accomplishment of interoperability between machines and systems. Ontologies provide an explicit conceptualization that describes the semantics of the data. Crowdsourcing innovation intermediaries are organizations that mediate the communication and relationship between companies that aspire to solve some problem or to take advantage of any business opportunity with a crowd that is prone to give ideas based on their knowledge, experience and wisdom, taking advantage of web 2.0 tools. Various ontologies have emerged, but at the best of our knowledge, there isn't any ontology that represents the entire process of intermediation of crowdsourcing innovation. In this paper we present an ontology roadmap for developing crowdsourcing innovation ontology of the intermediation process. Over the years, several authors have proposed some distinct methodologies, by different proposals of combining practices, activities, languages, according to the project they were involved in. We start making a literature review on ontology building, and analyse and compare ontologies that propose the development from scratch with the ones that propose reusing other ontologies. We also review enterprise and innovation ontologies known in literature. Finally, are presented the criteria for selecting the methodology and the roadmap for building crowdsourcing innovation intermediary ontology.},
booktitle = {Proceedings of the International Joint Conference on Knowledge Discovery, Knowledge Engineering and Knowledge Management - Volume 3},
pages = {54–63},
numpages = {10},
keywords = {Ontology Enterprise., Ontology Building Methodologies, Innovation Ontology, Crowdsourcing Innovation},
location = {Rome, Italy},
series = {IC3K 2014}
}

@inproceedings{10.1109/ICMeCG.2014.39,
author = {Liu, Nianzu and Chen, Xiao},
title = {Contribution-Based Incentive Design for Mobile Crowdsourcing},
year = {2014},
isbn = {9781479965434},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/ICMeCG.2014.39},
doi = {10.1109/ICMeCG.2014.39},
abstract = {In this paper, we propose COIN as a general pattern for carrying out effective mobile crowd sourcing. As a demonstration, we apply the pattern to design a crowd-based system for realizing smart parking. Compared with existing solutions, our proposal possesses several desirable features and improves the efficiency of crowd sourcing in the process of problem solving.},
booktitle = {Proceedings of the 2014  International Conference on Management of E-Commerce and e-Government},
pages = {151–155},
numpages = {5},
keywords = {Mobile application, Intelligent Transportation, Crowdsourcing},
series = {ICMECG '14}
}

