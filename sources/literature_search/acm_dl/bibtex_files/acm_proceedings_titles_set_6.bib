@inproceedings{10.1145/2948649.2948657,
author = {To, Hien and Geraldes, R\'{u}ben and Shahabi, Cyrus and Kim, Seon Ho and Prendinger, Helmut},
title = {An empirical study of workers' behavior in spatial crowdsourcing},
year = {2016},
isbn = {9781450343091},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2948649.2948657},
doi = {10.1145/2948649.2948657},
abstract = {With the ubiquity of smartphones, spatial crowdsourcing (SC) has emerged as a new paradigm that engages mobile users to perform tasks in the physical world. Thus, various SC techniques have been studied for performance optimization. However, little research has been done to understand workers' behavior in the real world. In this study, we designed and performed two real world SC campaigns utilizing our mobile app, called Genkii, which is a GPS-enabled app for users to report their affective state (e.g., happy, sad). We used Yahoo! Japan Crowdsourcing as the payment platform to reward users for reporting their affective states at different locations and times. We studied the relationship between incentives and participation by analyzing the impact of offering a fixed reward versus an increasing reward scheme. We observed that users tend to stay in a campaign longer when the provided incentives gradually increase over time. We also found that the degree of mobility is correlated with the reported information. For example, users who travel more are observed to be happier than the ones who travel less. Furthermore, analyzing the spatiotemporal information of the reports reveals interesting mobility patterns that are unique to spatial crowdsourcing.},
booktitle = {Proceedings of the Third International ACM SIGMOD Workshop on Managing and Mining Enriched Geo-Spatial Data},
articleno = {8},
numpages = {6},
keywords = {incentives, mobility, spatial crowdsourcing},
location = {San Francisco, California},
series = {GeoRich '16}
}

@inproceedings{10.1109/HICSS.2015.580,
author = {Dissanayake, Indika and Zhang, Jie and Gu, Bin},
title = {Virtual Team Performance in Crowdsourcing Contest: A Social Network Perspective},
year = {2015},
isbn = {9781479973675},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/HICSS.2015.580},
doi = {10.1109/HICSS.2015.580},
abstract = {Social media technologies have made it feasible for organizations to tap "wisdom of the crowd" beyond their own workforce. Many organizations use online crowd sourcing contests to find solutions for their business problems. In these contests, self-organized virtual teams compete for monetary reward. Motivated by this phenomenon, this research investigates how the social network structure of a virtual team impacts its performance in the context of online crowd sourcing contests. Specifically, we empirically assess the impacts of member social-capital, intellectual-capital, and the alignment of these two measures on team performances. Our analysis suggests that the alignment of member social-capital and intellectual-capital has a negative impact on team performances. Our findings have strategic implications to participants of virtual crowd sourcing competitions and to the design of virtual work teams.},
booktitle = {Proceedings of the 2015 48th Hawaii International Conference on System Sciences},
pages = {4894–4897},
numpages = {4},
keywords = {Crowdsourcing, Intellectual capital, Social capital, Social network analysis, Virtual teams},
series = {HICSS '15}
}

@inproceedings{10.5555/3237383.3237900,
author = {Rangi, Anshuka and Franceschetti, Massimo},
title = {Multi-Armed Bandit Algorithms for Crowdsourcing Systems with Online Estimation of Workers' Ability},
year = {2018},
publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
address = {Richland, SC},
abstract = {Crowdsourcing systems have become a valuable solution for various organizations to outsource work on a temporary basis. Quality assurance in these systems remains a key issue due to the distributed setup of the crowdsourcing platforms and the absence of a priori information about the workers. Our work develops a notion of Limited-information Crowdsourcing Systems (LCS), where the task master can assign the work based on some knowledge of the workers' ability acquired over time. The key challenges in this new setup are determining an efficient workers' selection policy and estimating the abilities of the workers. To address the first challenge, we reduce the problem to an arm-limited, budget limited, multi-armed bandit (MAB) set-up, and use the simplified bounded KUBE (B-KUBE) algorithm as a solution. This algorithm has previously only been experimentally evaluated, and we provide provable performance guarantees, showing that it is order optimal, namely the expected regret of B-KUBE is $O(\l{}og(B))$ where B is the total budget of the task master. The second challenge is solved by formalizing the notion of workers' ability mathematically, and proposing a strategy for its estimation. We experimentally evaluate B-KUBE in conjunction with this strategy, showing that it outperforms other state-of- the-art MAB algorithms when applied in the same setting.},
booktitle = {Proceedings of the 17th International Conference on Autonomous Agents and MultiAgent Systems},
pages = {1345–1352},
numpages = {8},
keywords = {bounded knapsack problem., crowdsourcing systems, multi-armed bandits},
location = {Stockholm, Sweden},
series = {AAMAS '18}
}

@inproceedings{10.1007/978-3-319-94277-3_5,
author = {Li, Liangcheng and Bu, Jiajun and Wang, Can and Yu, Zhi and Wang, Wei and Wu, Yue and Gu, Chunbin and Zhou, Qin},
title = {CrowdAE: A Crowdsourcing System with Human Inspection Quality Enhancement for Web Accessibility Evaluation},
year = {2018},
isbn = {978-3-319-94276-6},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-319-94277-3_5},
doi = {10.1007/978-3-319-94277-3_5},
abstract = {Crowdsourcing technology can help manual testing by soliciting the contributions from volunteer evaluators. But crowd evaluators may give inaccurate or invalid evaluation results. This paper proposes an advanced crowdsourcing-based web accessibility evaluation system called CrowdAE by enhancing the crowdsourcing-based manual testing module of the previous version. Through three main process namely learning system, task assignment and task review, we can improve the quality of evaluation results from the crowd. From the comparison on the two years’ evaluation process of Chinese government websites, our CrowdAE outperforms the previous version and improve the accuracy of the evaluation results.},
booktitle = {Computers Helping People with Special Needs: 16th International Conference, ICCHP 2018, Linz, Austria, July 11-13, 2018, Proceedings, Part I},
pages = {27–30},
numpages = {4},
keywords = {Web accessibility evaluation, Crowdsourcing},
location = {Linz, Austria}
}

@inproceedings{10.1109/GLOCOM.2017.8254421,
author = {Li, Shuai and Xi, Teng and Tian, Ye and Wang, Wendong},
title = {Inferring Fine-Grained PM2.5 with Bayesian Based Kernel Method for Crowdsourcing System},
year = {2017},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/GLOCOM.2017.8254421},
doi = {10.1109/GLOCOM.2017.8254421},
abstract = {Air pollution seriously affect people's lives, among which PM$_{2.5}$ is especially harmful for humans health. Although many countries have established fixed air quality monitoring stations (AQMS) to monitor air pollution, the costs of constructing and maintaining for AQMS are extremely expensive and the density of AQMS is very low. To acquire fine-grained concentration of PM$_{2.5}$, this paper have proposed a novel Bayesian based kernel method. \%using images and camera information which are easy to get. Our model leverage heterogeneous data which jointly using images information, camera lens information, GPS information and magnetic sensor information. To study the relationship between PM$_{2.5}$ concentration and images information, we have established a crowdsourcing system and have collected photos for consecutive 16 months. The performance of the proposed method has been evaluated thoroughly by real dataset we have collected. The results show that, compared with three baselines, our proposed algorithm can reduce up to 35\% prediction error in average.},
booktitle = {GLOBECOM 2017 - 2017 IEEE Global Communications Conference},
pages = {1–6},
numpages = {6},
location = {Singapore}
}

@inproceedings{10.1145/2702123.2702296,
author = {Dergousoff, Kristen and Mandryk, Regan L.},
title = {Mobile Gamification for Crowdsourcing Data Collection: Leveraging the Freemium Model},
year = {2015},
isbn = {9781450331456},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2702123.2702296},
doi = {10.1145/2702123.2702296},
abstract = {Classic ways of gathering data on human behaviour are time-consuming, costly and are subject to limited participant pools. Crowdsourcing offers a reduction in operating costs and access to a diverse and large participant pool; however issues arise concerning low worker pay and questions about data quality. Gamification provides a motivation to participate, but also requires the development of specialized, research-question specific games that can be costly to produce. Our solution combines gamification and crowdsourcing in a smartphone-based system that emulates the popular Freemium model of play to motivate voluntary participation through in-game rewards, using a robust framework to study multiple unrelated research questions within the same system. We deployed our game on the Android store and compared it to a gamified laboratory version and a non-gamified laboratory version, and found that players who used the in-game rewards were motivated to do experimental tasks. There was no difference between the systems for performance on a motor task; however, performance on the cognitive task was worse for the crowdsourced game. We discuss options for improving performance on tasks requiring attention.},
booktitle = {Proceedings of the 33rd Annual ACM Conference on Human Factors in Computing Systems},
pages = {1065–1074},
numpages = {10},
keywords = {crowdsourcing, freemium, gamification, psychophysics},
location = {Seoul, Republic of Korea},
series = {CHI '15}
}

@inproceedings{10.1145/2736277.2741685,
author = {Difallah, Djellel Eddine and Catasta, Michele and Demartini, Gianluca and Ipeirotis, Panagiotis G. and Cudr\'{e}-Mauroux, Philippe},
title = {The Dynamics of Micro-Task Crowdsourcing: The Case of Amazon MTurk},
year = {2015},
isbn = {9781450334693},
publisher = {International World Wide Web Conferences Steering Committee},
address = {Republic and Canton of Geneva, CHE},
url = {https://doi.org/10.1145/2736277.2741685},
doi = {10.1145/2736277.2741685},
abstract = {Micro-task crowdsourcing is rapidly gaining popularity among research communities and businesses as a means to leverage Human Computation in their daily operations. Unlike any other service, a crowdsourcing platform is in fact a marketplace subject to human factors that affect its performance, both in terms of speed and quality. Indeed, such factors shape the dynamics of the crowdsourcing market. For example, a known behavior of such markets is that increasing the reward of a set of tasks would lead to faster results. However, it is still unclear how different dimensions interact with each other: reward, task type, market competition, requester reputation, etc. In this paper, we adopt a data-driven approach to (A) perform a long-term analysis of a popular micro-task crowdsourcing platform and understand the evolution of its main actors (workers, requesters, and platform). (B) We leverage the main findings of our five year log analysis to propose features used in a predictive model aiming at determining the expected performance of any batch at a specific point in time. We show that the number of tasks left in a batch and how recent the batch is are two key features of the prediction. (C) Finally, we conduct an analysis of the demand (new tasks posted by the requesters) and supply (number of tasks completed by the workforce) and show how they affect task prices on the marketplace.},
booktitle = {Proceedings of the 24th International Conference on World Wide Web},
pages = {238–247},
numpages = {10},
keywords = {crowdsourcing, design, experimentation, forecasting, human factors, tracking, trend identification},
location = {Florence, Italy},
series = {WWW '15}
}

@inproceedings{10.1109/HICSS.2014.181,
author = {Tajedin, Hamed and Nevo, Dorti},
title = {Value-Adding Intermediaries in Software Crowdsourcing},
year = {2014},
isbn = {9781479925049},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/HICSS.2014.181},
doi = {10.1109/HICSS.2014.181},
abstract = {The information systems (IS) discipline has been fertile ground for research that delineates the role of technology in transforming organizations. Crowd sourcing counts as one such phenomenon, but our empirical understanding of it is nascent at best. This paper presents a preliminary theoretical justification for the emergence of crowd sourcing intermediaries by describing how they add value to this new sourcing arrangement. We report findings of a case study as initial evidence confirming two sets of value-adding activities taking place in a crowd sourcing platform: those at the market (macro) level and those at the transaction (micro) level.},
booktitle = {Proceedings of the 2014 47th Hawaii International Conference on System Sciences},
pages = {1396–1405},
numpages = {10},
keywords = {Crowdsourcing, intermediation, software development},
series = {HICSS '14}
}

@inproceedings{10.1145/2187836.2187970,
author = {Ghosh, Arpita and McAfee, Preston},
title = {Crowdsourcing with endogenous entry},
year = {2012},
isbn = {9781450312295},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2187836.2187970},
doi = {10.1145/2187836.2187970},
abstract = {We investigate the design of mechanisms to incentivize high quality outcomes in crowdsourcing environments with strategic agents, when entry is an endogenous, strategic choice. Modeling endogenous entry in crowdsourcing markets is important because there is a nonzero cost to making a contribution of any quality which can be avoided by not participating, and indeed many sites based on crowdsourced content do not have adequate participation. We use a mechanism with monotone, rank-based, rewards in a model where agents strategically make participation and quality choices to capture a wide variety of crowdsourcing environments, ranging from conventional crowdsourcing contests with monetary rewards such as TopCoder, to crowdsourced content as in online Q&amp;A forums.We begin by explicitly constructing the unique mixed-strategy equilibrium for such monotone rank-order mechanisms, and use the participation probability and distribution of qualities from this construction to address the question of designing incentives for two kinds of rewards that arise in the context of crowdsourcing. We first show that for attention rewards that arise in the crowdsourced content setting, the entire equilibrium distribution and therefore every increasing statistic including the maximum and average quality (accounting for participation), improves when the rewards for every rank but the last are as high as possible. In particular, when the cost of producing the lowest possible quality content is low, the optimal mechanism displays all but the poorest contribution. We next investigate how to allocate rewards in settings where there is a fixed total reward that can be arbitrarily distributed amongst participants, as in crowdsourcing contests. Unlike models with exogenous entry, here the expected number of participants can be increased by subsidizing entry, which could potentially improve the expected value of the best contribution. However, we show that subsidizing entry does not improve the expected quality of the best contribution, although it may improve the expected quality of the average contribution. In fact, we show that free entry is dominated by taxing entry---making all entrants pay a small fee, which is rebated to the winner along with whatever rewards were already assigned, can improve the quality of the best contribution over a winner-take-all contest with no taxes.},
booktitle = {Proceedings of the 21st International Conference on World Wide Web},
pages = {999–1008},
numpages = {10},
keywords = {contest design, crowdsourcing, game theory, mechanism design, social computing, user generated content (UGC)},
location = {Lyon, France},
series = {WWW '12}
}

@inproceedings{10.1109/MDM.2014.69,
author = {Phuttharak, Jurairat and Loke, Seng W.},
title = {Towards Declarative Programming for Mobile Crowdsourcing: P2P Aspects},
year = {2014},
isbn = {9781479957057},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/MDM.2014.69},
doi = {10.1109/MDM.2014.69},
abstract = {Peer-to-Peer technologies have been widely used in networks which manage vast amount of data daily. The proliferation of mobile devices strongly motivates mobile peer-to-peer network (M-P2P) applications, with benefits from network effects. We argue that logic programming for crowd sourcing can be useful in peer-to-peer computing for querying and multicasting tasks shared over peer networks. We introduce a declarative crowd sourcing platform for mobile applications, which combines conventional machine computation and the power of the crowd in social networking, particularly in M-P2P networks. This paper discusses a simple extension of Prolog, which we call Logic Crowd, focusing on enabling goal evaluation over peers in mobile peer networks. Additionally, we demonstrate that logic programming for crowd sourcing can be useful in peer-to-peer computing for querying and P2P style of task sharing over short-range networks. In this paper, we illustrate the potential of our approach via programming idioms, a prototype implementation and scenarios.},
booktitle = {Proceedings of the 2014 IEEE 15th International Conference on Mobile Data Management - Volume 02},
pages = {61–66},
numpages = {6},
keywords = {declarative programming language, mobile application, mobile crowdsourcing, peer-to-peer network},
series = {MDM '14}
}

@inproceedings{10.5555/2002472.2002626,
author = {Zaidan, Omar F. and Callison-Burch, Chris},
title = {Crowdsourcing translation: professional quality from non-professionals},
year = {2011},
isbn = {9781932432879},
publisher = {Association for Computational Linguistics},
address = {USA},
abstract = {Naively collecting translations by crowd-sourcing the task to non-professional translators yields disfluent, low-quality results if no quality control is exercised. We demonstrate a variety of mechanisms that increase the translation quality to near professional levels. Specifically, we solicit redundant translations and edits to them, and automatically select the best output among them. We propose a set of features that model both the translations and the translators, such as country of residence, LM perplexity of the translation, edit rate from the other translations, and (optionally) calibration against professional translators. Using these features to score the collected translations, we are able to discriminate between acceptable and unacceptable translations. We recreate the NIST 2009 Urdu-to-English evaluation set with Mechanical Turk, and quantitatively show that our models are able to select translations within the range of quality that we expect from professional translators. The total cost is more than an order of magnitude lower than professional translation.},
booktitle = {Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies - Volume 1},
pages = {1220–1229},
numpages = {10},
location = {Portland, Oregon},
series = {HLT '11}
}

@inproceedings{10.1109/INFOCOM.2016.7524548,
author = {Pu, Lingjun and Chen, Xu and Xu, Jingdong and Fu, Xiaoming},
title = {Crowdlet: Optimal worker recruitment for self-organized mobile crowdsourcing},
year = {2016},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/INFOCOM.2016.7524548},
doi = {10.1109/INFOCOM.2016.7524548},
abstract = {In this paper, we advocate Crowdlet, a novel self-organized mobile crowdsourcing paradigm, in which a mobile task requester can proactively exploit a massive crowd of encountered mobile workers at real-time for quick and high-quality results. We present a comprehensive system model of Crowdlet that defines task, worker arrival and worker ability models. Further, we introduce a service quality concept to indicate the expected service gain that a requester can enjoy when he recruits an encountered worker, by jointly taking into account worker ability, real-timeness and task reward. Based on the models, we formulate an online worker recruitment problem to maximize the expected sum of service quality. We derive an optimal worker recruitment policy through the dynamic programming principle, and show that it exhibits a nice threshold based structure. We conduct extensive performance evaluation based on real traces, and numerical results demonstrate that our policy can achieve superior performance and improve more than 30\% performance gain over classic policies. Besides, our Android prototype shows that Crowdlet is cost-efficient, requiring less than 7 seconds and 6 Joule in terms of time and energy cost for policy computation in most cases.},
booktitle = {IEEE INFOCOM 2016 - The 35th Annual IEEE International Conference on Computer Communications},
pages = {1–9},
numpages = {9},
location = {San Francisco, CA, USA}
}

@inproceedings{10.1145/2670518.2673866,
author = {Shi, Jinghao and Guan, Zhangyu and Qiao, Chunming and Melodia, Tommaso and Koutsonikolas, Dimitrios and Challen, Geoffrey},
title = {Crowdsourcing Access Network Spectrum Allocation Using Smartphones},
year = {2014},
isbn = {9781450332569},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2670518.2673866},
doi = {10.1145/2670518.2673866},
abstract = {The hundreds of millions of deployed smartphones provide an unprecedented opportunity to collect data to monitor, debug, and continuously adapt wireless networks to improve performance. In contrast with previous mobile devices, such as laptops, smartphones are always on but mostly idle, making them available to perform measurements that help other nearby active devices make better use of available network resources. We present the design of PocketSniffer, a system delivering wireless measurements from smartphones both to network administrators for monitoring and debugging purposes and to algorithms performing realtime network adaptation. By collecting data from smartphones, PocketSniffer supports novel adaptation algorithms designed around common deployment scenarios involving both cooperative and self-interested clients and networks. We present preliminary results from a prototype and discuss challenges to realizing this vision.},
booktitle = {Proceedings of the 13th ACM Workshop on Hot Topics in Networks},
pages = {1–7},
numpages = {7},
keywords = {Smartphones, crowdsourcing, monitoring},
location = {Los Angeles, CA, USA},
series = {HotNets-XIII}
}

@inproceedings{10.1109/ICMeCG.2014.39,
author = {Liu, Nianzu and Chen, Xiao},
title = {Contribution-Based Incentive Design for Mobile Crowdsourcing},
year = {2014},
isbn = {9781479965434},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/ICMeCG.2014.39},
doi = {10.1109/ICMeCG.2014.39},
abstract = {In this paper, we propose COIN as a general pattern for carrying out effective mobile crowd sourcing. As a demonstration, we apply the pattern to design a crowd-based system for realizing smart parking. Compared with existing solutions, our proposal possesses several desirable features and improves the efficiency of crowd sourcing in the process of problem solving.},
booktitle = {Proceedings of the 2014  International Conference on Management of E-Commerce and e-Government},
pages = {151–155},
numpages = {5},
keywords = {Crowdsourcing, Intelligent Transportation, Mobile application},
series = {ICMECG '14}
}

@inproceedings{10.1145/2390034.2390038,
author = {Lukyanenko, Roman and Parsons, Jeffrey},
title = {Conceptual modeling principles for crowdsourcing},
year = {2012},
isbn = {9781450317153},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2390034.2390038},
doi = {10.1145/2390034.2390038},
abstract = {Traditionally, the research and practice of conceptual modeling assumed all relevant information about a domain could be discovered through user-analyst communication. The increasing ubiquity of crowdsourcing challenges a number of long-held propositions about conceptual modeling. Given significant differences in levels of domain expertise among contributors in crowdsourcing projects, it is often impossible to predict all valid conceptualizations of a domain by potential users. Approaching conceptual modeling in crowdsourcing using traditional principles of modeling is highly constraining. This paper explores fundamental conceptual modeling challenges in crowdsourcing domains. We then use theoretical foundations in philosophy (ontology) to offer potential solutions.},
booktitle = {Proceedings of the 1st International Workshop on Multimodal Crowd Sensing},
pages = {3–6},
numpages = {4},
keywords = {conceptual modeling, crowdsourcing, user-generated content},
location = {Maui, Hawaii, USA},
series = {CrowdSens '12}
}

@inproceedings{10.1109/FOCS.2014.36,
author = {Anari, Nima and Goel, Gagan and Nikzad, Afshin},
title = {Mechanism Design for Crowdsourcing: An Optimal 1-1/e Competitive Budget-Feasible Mechanism for Large Markets},
year = {2014},
isbn = {9781479965175},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/FOCS.2014.36},
doi = {10.1109/FOCS.2014.36},
abstract = {In this paper we consider a mechanism design problem in the context of large-scale crowdsourcing markets such as Amazon's Mechanical Turk mturk, ClickWorker clickworker, CrowdFlower crowdflower. In these markets, there is a requester who wants to hire workers to accomplish some tasks. Each worker is assumed to give some utility to the requester on getting hired. Moreover each worker has a minimum cost that he wants to get paid for getting hired. This minimum cost is assumed to be private information of the workers. The question then is--if the requester has a limited budget, how to design a direct revelation mechanism that picks the right set of workers to hire in order to maximize the requester's utility? We note that although the previous work (Singer (2010) chen et al. (2011)) has studied this problem, a crucial difference in which we deviate from earlier work is the notion of large-scale markets that we introduce in our model. Without the large market assumption, it is known that no mechanism can achieve a competitive ratio better than 0.414 and 0.5 for deterministic and randomized mechanisms respectively (while the best known deterministic and randomized mechanisms achieve an approximation ratio of 0.292 and 0.33 respectively). In this paper, we design a budget-feasible mechanism for large markets that achieves a competitive ratio of 1--1/e = 0.63. Our mechanism can be seen as a generalization of an alternate way to look at the proportional share mechanism, which is used in all the previous works so far on this problem. Interestingly, we can also show that our mechanism is optimal by showing that no truthful mechanism can achieve a factor better than 1--1/e, thus, fully resolving this setting. Finally we consider the more general case of submodular utility functions and give new and improved mechanisms for the case when the market is large.},
booktitle = {Proceedings of the 2014 IEEE 55th Annual Symposium on Foundations of Computer Science},
pages = {266–275},
numpages = {10},
keywords = {Budget-feasibility, Crowdsourcing, Large Markets, Truthful Mechanisms},
series = {FOCS '14}
}

@inproceedings{10.1145/3144457.3144499,
author = {Zhao, Ziming and Liu, Fang and Cai, Zhiping and Xiao, Nong},
title = {Edge-based Content-aware Crowdsourcing Approach for Image Sensing in Disaster Environment},
year = {2017},
isbn = {9781450353687},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3144457.3144499},
doi = {10.1145/3144457.3144499},
abstract = {Photos obtained via crowdsourcing can be used in image sensing for disaster management. Due to the weak communication environment after a disaster, it is difficult to transfer the huge amount of crowdsourced photos. To address this problem, we propose COCO, a content-aware crowdsourcing system that leverages edge computing to support real-time image sensing in disaster environment. COCO filters the crowdsourced images at the data source and only uploads the images that contain relevant objects which the application is interested in. We use a machine-learning based computer vision detector to understand the content of images. Considering the resource constraints of mobile devices, we implement the computer vision detector at the edge server which located in the close proximity to data source. As the unstable network bandwidth is normal in disaster environment, we propose an adaptive mechanism to further improve the sensing performance. We have implemented the COCO prototype which is evaluated via a real-world dataset. The experimental results demonstrate the effectiveness of COCO.},
booktitle = {Proceedings of the 14th EAI International Conference on Mobile and Ubiquitous Systems: Computing, Networking and Services},
pages = {225–231},
numpages = {7},
keywords = {Crowdsourcing, Edge Computing, Image Sensing},
location = {Melbourne, VIC, Australia},
series = {MobiQuitous 2017}
}

@inproceedings{10.1145/2942358.2942402,
author = {Hu, Qin and Wang, Shengling and Ma, Liran and Cheng, Xiuzhen and Bie, Rongfang},
title = {Solving the crowdsourcing dilemma using the zero-determinant strategy: poster},
year = {2016},
isbn = {9781450341844},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2942358.2942402},
doi = {10.1145/2942358.2942402},
abstract = {As a promising technology, crowdsourcing aims to accomplish a complex task via eliciting services from a large group of workers. However, recent observations indicate that the success of crowdsourcing is being hindered by the malicious behaviors of the workers. In this paper, we analyze the attack problem using an iterated prisoner's dilemma (IPD) game and propose an zero-determinant (ZD) strategy based algorithm. Simulation results demonstrate that the requestor can incentivize the worker to keep on cooperating.},
booktitle = {Proceedings of the 17th ACM International Symposium on Mobile Ad Hoc Networking and Computing},
pages = {373–374},
numpages = {2},
keywords = {crowdsourcing, game theory, zero-determinant strategy},
location = {Paderborn, Germany},
series = {MobiHoc '16}
}

@inproceedings{10.1145/2702123.2702443,
author = {Gadiraju, Ujwal and Kawase, Ricardo and Dietze, Stefan and Demartini, Gianluca},
title = {Understanding Malicious Behavior in Crowdsourcing Platforms: The Case of Online Surveys},
year = {2015},
isbn = {9781450331456},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2702123.2702443},
doi = {10.1145/2702123.2702443},
abstract = {Crowdsourcing is increasingly being used as a means to tackle problems requiring human intelligence. With the ever-growing worker base that aims to complete microtasks on crowdsourcing platforms in exchange for financial gains, there is a need for stringent mechanisms to prevent exploitation of deployed tasks. Quality control mechanisms need to accommodate a diverse pool of workers, exhibiting a wide range of behavior. A pivotal step towards fraud-proof task design is understanding the behavioral patterns of microtask workers. In this paper, we analyze the prevalent malicious activity on crowdsourcing platforms and study the behavior exhibited by trustworthy and untrustworthy workers, particularly on crowdsourced surveys. Based on our analysis of the typical malicious activity, we define and identify different types of workers in the crowd, propose a method to measure malicious activity, and finally present guidelines for the efficient design of crowdsourced surveys.},
booktitle = {Proceedings of the 33rd Annual ACM Conference on Human Factors in Computing Systems},
pages = {1631–1640},
numpages = {10},
keywords = {crowdsourcing, malicious intent, microtasks, online surveys, user behavior},
location = {Seoul, Republic of Korea},
series = {CHI '15}
}

@inproceedings{10.5555/2729485.2729514,
author = {Hourcade, Juan Pablo and Gehrt, Laura},
title = {Crowdsourcing for delivering research results to patients},
year = {2014},
isbn = {9788968487521},
publisher = {Hanbit Media, Inc.},
address = {Seoul, KOR},
abstract = {Participatory research, where participants in research projects take an active role in research activities is an emerging trend that helps participants understand the long- term impact of research, reduces participant dropout rates, and shapes research for greater value to patients. One way to involve participants in research is to have them participate in making research results available to the general public. To learn about the feasibility of this approach, we conducted a study comparing crowdsourced medication warnings derived from research literature to expert-produced warnings derived from the same literature. Through a survey with 159 respondents, we learned that they mostly favored the expert-produced warnings, although for one set of warnings the respondents preferred the crowdsourced warnings in terms of how comprehensive they were. The study suggests that crowdsourcing techniques combined with expert supervision and input could provide reasonable ways for participants in clinical trials to be more active participants in the studies in which they participate.},
booktitle = {Proceedings of HCI Korea},
pages = {196–202},
numpages = {7},
keywords = {Amazon mechanical turk, clinical, crowdsourcing, older adults, research, translation},
location = {Seoul, Republic of Korea},
series = {HCIK '15}
}

@proceedings{10.5555/3100549,
title = {CSI-SE '17: Proceedings of the 4th International Workshop on CrowdSourcing in Software Engineering},
year = {2017},
isbn = {9781538640418},
publisher = {IEEE Press},
abstract = {It is our pleasure to welcome the reader to the (pre-workshop) proceedings of the 4th International Workshop on CrowdSourcing in Software Engineering (CSI-SE 2017), co-located with the 39th International Conference on Software Engineering (ICSE 2017) held in Buenos Aires, Argentina, during May 20-28, 2017.A number of trends under the broad banner of crowdsourcing are beginning to fundamentally disrupt the way in which software is engineered. Programmers increasingly rely on crowdsourced knowledge and code, as they look to Question \&amp; Answer (Q&amp;A) sites for answers or use code from publicly posted snippets. Programmers play, compete, and learn with the crowd, engaging in programming competitions and puzzles with crowds of programmers. Online IDEs make possible radically new forms of collaboration, allowing developers to synchronously program with crowds of distributed programmers. Programmers' reputation is increasingly visible on Q&amp;A sites and public code repositories, opening new possibilities in how developers find jobs and companies identify talent. Crowds of non-programmers increasingly participate in development, usability testing software or even constructing specifications while playing games. Crowds play an increasing role in shaping software requirements, broadening the software which might be feasibly constructed. Approaches for crowd development seek to microtask software development, dramatically increasing participation in open source by enabling software projects to be built through casual, transient work.},
location = {Buenos Aires, Argentina}
}

@inproceedings{10.5220/0005084800540063,
author = {Silva, C\^{a}ndida and Ramos, Isabel},
title = {An Ontology Roadmap for Crowdsourcing Innovation Intermediaries},
year = {2014},
isbn = {9789897580505},
publisher = {SCITEPRESS - Science and Technology Publications, Lda},
address = {Setubal, PRT},
url = {https://doi.org/10.5220/0005084800540063},
doi = {10.5220/0005084800540063},
abstract = {Ontologies have proliferated in the last years, essentially justified by the need of achieving a consensus in the multiple representations of reality inside computers, and therefore the accomplishment of interoperability between machines and systems. Ontologies provide an explicit conceptualization that describes the semantics of the data. Crowdsourcing innovation intermediaries are organizations that mediate the communication and relationship between companies that aspire to solve some problem or to take advantage of any business opportunity with a crowd that is prone to give ideas based on their knowledge, experience and wisdom, taking advantage of web 2.0 tools. Various ontologies have emerged, but at the best of our knowledge, there isn't any ontology that represents the entire process of intermediation of crowdsourcing innovation. In this paper we present an ontology roadmap for developing crowdsourcing innovation ontology of the intermediation process. Over the years, several authors have proposed some distinct methodologies, by different proposals of combining practices, activities, languages, according to the project they were involved in. We start making a literature review on ontology building, and analyse and compare ontologies that propose the development from scratch with the ones that propose reusing other ontologies. We also review enterprise and innovation ontologies known in literature. Finally, are presented the criteria for selecting the methodology and the roadmap for building crowdsourcing innovation intermediary ontology.},
booktitle = {Proceedings of the International Joint Conference on Knowledge Discovery, Knowledge Engineering and Knowledge Management - Volume 3},
pages = {54–63},
numpages = {10},
keywords = {Crowdsourcing Innovation, Innovation Ontology, Ontology Building Methodologies, Ontology Enterprise.},
location = {Rome, Italy},
series = {IC3K 2014}
}

@inproceedings{10.1109/CSI-SE.2017.1,
author = {Diaz-Mosquera, Juan D. and Sanabria, Pablo and Neyem, Andres and Parra, Denis and Navon, Jaime},
title = {Enriching capstone project-based learning experiences using a crowdsourcing recommender engine},
year = {2017},
isbn = {9781538640418},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/CSI-SE.2017.1},
doi = {10.1109/CSI-SE.2017.1},
abstract = {Capstone project-based learning courses generate a suitable space where students can put into action knowledge specific to an area. In the case of Software Engineering (SE), students must apply knowledge at the level of Analysis, Design, Development, Implementation and Management of Software Projects. There is a large number of supportive resources for SE that one can find on the web, however, information overload ends up saturating the students who wish to find resources more accurate depending on their needs. This is why we propose a crowdsourcing recommender engine as part of an educational software platform. This engine based its recommendations on content from StackExchange posts using the project's profile in which a student is currently working. To generate the project's profile, our engine takes advantage of the information stored by students in the aforementioned platform.Content-based algorithms based on Okapi BM25 and Latent Dirichlet Allocation (LDA) are used to provide suitable recommendations. The evaluation of the engine was held with students from the capstone course in SE of the University Catholic of Chile. Results show that Cosine similarity over traditional bag-of-words TF-IDF content vectors yield interesting results, but they are outperformed by the integration of BM25 with LDA.},
booktitle = {Proceedings of the 4th International Workshop on CrowdSourcing in Software Engineering},
pages = {25–29},
numpages = {5},
keywords = {capstones, crowdsourcing, recommender systems, software engineering},
location = {Buenos Aires, Argentina},
series = {CSI-SE '17}
}

@inproceedings{10.1145/3192714.3192827,
author = {Song, Shuyi and Bu, Jiajun and Wang, Ye and Yu, Zhi and Artmeier, Andreas and Dai, Lianjun and Wang, Can},
title = {Web Accessibility Evaluation in a Crowdsourcing-Based System with Expertise-Based Decision Strategy},
year = {2018},
isbn = {9781450356510},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3192714.3192827},
doi = {10.1145/3192714.3192827},
abstract = {The rising awareness of accessibility increases the demand for Web accessibility evaluation projects to verify the implementation of Web accessibility guidelines and identify accessibility barriers in websites. However, the complexity of accessibility evaluation tasks and the lack of experts limits their scope and reduces their significance. Due to this complexity, they could not directly rely on a technique called crowdsourcing, which made great contributions in many fields by dividing a problem into many tedious micro-tasks and solving tasks in parallel. Addressing this issue, we develop a new crowdsourcing-based Web accessibility evaluation system with two novel decision strategies, golden set strategy and time-based golden set strategy. These strategies enable the generation of task results with high accuracy synthesized from micro-tasks solved by workers with heterogeneous expertise. An accessibility evaluation of 98 websites by 55 workers with varying experience verifies that our system can complete the evaluation in half the time with a 7.2\% improvement on accuracy than the current approach.},
booktitle = {Proceedings of the 15th International Web for All Conference},
articleno = {23},
numpages = {4},
keywords = {Crowdsourcing, Evaluation System, Expertise, Web Accessibility},
location = {Lyon, France},
series = {W4A '18}
}

@inproceedings{10.1109/ICALT.2014.151,
author = {Erdt, Mojisola and Rensing, Christoph},
title = {Evaluating Recommender Algorithms for Learning Using Crowdsourcing},
year = {2014},
isbn = {9781479940387},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/ICALT.2014.151},
doi = {10.1109/ICALT.2014.151},
abstract = {Keeping focused on a certain goal or topic when learning with resources found on the Web is a challenge. Creating a hierarchical learning goal structure with activities and sub-activities can help the learner to keep on track. Moreover, providing useful recommendations to such activities can further support the learner. However, recommendations need to be relevant to the specific goal or activity the learner is currently working on, as well as being novel and diverse to the learner. Such user-centric metrics like novelty and diversity are best measured by asking the users themselves. Nonetheless, conducting user experiments are notoriously time-consuming and access to an adequate amount of users is often very limited. Crowd sourcing offers a means to evaluate TEL recommender algorithms by reaching out to sufficient participants in a shorter time-frame and with less effort. In this paper, a concept for evaluating TEL recommender algorithms using crowd sourcing is presented as well as a repeated proof-of-concept evaluation experiment of a TEL graph-based recommender algorithm AScore that exploits hierarchical activity structures. Results from both experiments support the postulated hypotheses, thereby showing that crowd sourcing can be successfully applied to evaluate TEL recommender algorithms.},
booktitle = {Proceedings of the 2014 IEEE 14th International Conference on Advanced Learning Technologies},
pages = {513–517},
numpages = {5},
keywords = {crowdsourcing, TEL, recommender systems, evaluation methods},
series = {ICALT '14}
}

@inproceedings{10.5555/2863694.2864275,
author = {Qiang, Yang Mao and Lue, Fan Li},
title = {Crowdsourcing Translation Based on the Divide and Conquer},
year = {2015},
isbn = {9781467393935},
publisher = {IEEE Computer Society},
address = {USA},
abstract = {With the rapid development of Internet technology, the network has become the necessary media of every-day life. It is the most convenient, fastest and most effective way to learn about the world through the Internet. Text translation is one of the most important techniques, however the existed human-translation has the shortcomings of high price and long waiting time, and it is difficult to guarantee the quality for machine translation. This paper proposes crowdsourcing-based translation measures using the thought of dividing and conquering. The experiments show that this way can translate text with lower monetary cost, smaller latency and higher quality, which can meet the practical needs.},
booktitle = {Proceedings of the 2015 Sixth International Conference on Intelligent Systems Design and Engineering Applications},
pages = {251–255},
numpages = {5},
series = {ISDEA '15}
}

@inproceedings{10.1145/2740908.2744109,
author = {Difallah, Djellel Eddine and Catasta, Michele and Demartini, Gianluca and Ipeirotis, Panagiotis G. and Cudr\'{e}-Mauroux, Philippe},
title = {The Dynamics of Micro-Task Crowdsourcing: The Case of Amazon MTurk},
year = {2015},
isbn = {9781450334730},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2740908.2744109},
doi = {10.1145/2740908.2744109},
abstract = {Micro-task crowdsourcing is rapidly gaining popularity among research communities and businesses as a means to leverage Human Computation in their daily operations. Unlike any other service, a crowdsourcing platform is in fact a marketplace subject to human factors that affect its performance, both in terms of speed and quality. Indeed, such factors shape the emph{dynamics} of the crowdsourcing market. For example, a known behavior of such markets is that increasing the reward of a set of tasks would lead to faster results. However, it is still unclear how different dimensions interact with each other: reward, task type, market competition, requester reputation, etc.In this paper, we adopt a data-driven approach to (A) perform a long-term analysis of a popular micro-task crowdsourcing platform and understand the evolution of its main actors (workers, requesters, tasks, and platform). (B) We leverage the main findings of our five year log analysis to propose features used in a predictive model aiming at determining the expected performance of any batch at a specific point in time. We show that the number of tasks left in a batch and how recent the batch is are two key features of the prediction. (C) Finally, we conduct an analysis of the demand (new tasks posted by the requesters) and supply (number of tasks completed by the workforce) and show how they affect task prices on the marketplace.},
booktitle = {Proceedings of the 24th International Conference on World Wide Web},
pages = {617},
numpages = {1},
keywords = {crowdsourcing, forecasting, tracking, trend identification},
location = {Florence, Italy},
series = {WWW '15 Companion}
}

@inproceedings{10.1145/2046396.2046421,
author = {Weld, Daniel S. and Mausam and Dai, Peng},
title = {Execution control for crowdsourcing},
year = {2011},
isbn = {9781450310147},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2046396.2046421},
doi = {10.1145/2046396.2046421},
abstract = {Crowdsourcing marketplaces enable a wide range of applications, but constructing any new application is challenging - usually requiring a complex, self-managing workflow in order to guarantee quality results. We report on the CLOWDER project, which uses machine learning to continually refine models of worker performance and task difficulty. We present decision-theoretic optimization techniques that can select the best parameters for a range of workflows. Initial experiments show our optimized workflows are significantly more economical than with manually set parameters.},
booktitle = {Proceedings of the 24th Annual ACM Symposium Adjunct on User Interface Software and Technology},
pages = {57–58},
numpages = {2},
keywords = {crowdsourcing, decision theory, execution control, human computation, pomdp},
location = {Santa Barbara, California, USA},
series = {UIST '11 Adjunct}
}

@inproceedings{10.1145/2517351.2517397,
author = {Faggiani, Adriano and Gregori, Enrico and Lenzini, Luciano and Luconi, Valerio and Vecchio, Alessio},
title = {Network sensing through smartphone-based crowdsourcing},
year = {2013},
isbn = {9781450320276},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2517351.2517397},
doi = {10.1145/2517351.2517397},
abstract = {Portolan is a crowdsourcing system, aimed at monitoring and measuring large-scale networks, that uses smartphones as mobile observation elements. Currently, Portolan is able to collect information about both wired and wireless networks, in particular it is used to obtain the graph of the Internet with unprecedented resolution and to associate performance indexes (received signal strength, maximum throughput) of cellular networks to geographic locations.},
booktitle = {Proceedings of the 11th ACM Conference on Embedded Networked Sensor Systems},
articleno = {31},
numpages = {2},
keywords = {crowdsourcing, network sensing, smartphone},
location = {Roma, Italy},
series = {SenSys '13}
}

@inproceedings{10.1145/2872518.2890087,
author = {Schnitzer, Steffen and Neitzel, Svenja and Schmidt, Sebastian and Rensing, Christoph},
title = {Perceived Task Similarities for Task Recommendation in Crowdsourcing Systems},
year = {2016},
isbn = {9781450341448},
publisher = {International World Wide Web Conferences Steering Committee},
address = {Republic and Canton of Geneva, CHE},
url = {https://doi.org/10.1145/2872518.2890087},
doi = {10.1145/2872518.2890087},
abstract = {Crowdsourcing platforms support the assignment of jobs while relying on the workers' search capabilities. Recommenders can support the workers' decisions to improve quality and outcome for both worker and requester. A precedent study showed, that many workers expect to get tasks recommended, which are similar to previously finished ones. In order to create genuine task recommendation, similarities between tasks have to be identified and analyzed. Therefore, this work provides an empirical study about how workers perceive task similarities. The perceived task similarities may vary between workers with different cultural background and may depend e.g. on the complexity, required action or the requester of the task.},
booktitle = {Proceedings of the 25th International Conference Companion on World Wide Web},
pages = {585–590},
numpages = {6},
keywords = {crowdsourcing, recommender systems, user survey},
location = {Montr\'{e}al, Qu\'{e}bec, Canada},
series = {WWW '16 Companion}
}

@inproceedings{10.1145/2542355.2542388,
author = {Tan, Chek Tien and Rosser, Daniel and Harrold, Natalie},
title = {Crowdsourcing facial expressions using popular gameplay},
year = {2013},
isbn = {9781450326292},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2542355.2542388},
doi = {10.1145/2542355.2542388},
abstract = {Facial expression analysis systems often employ machine learning algorithms that depend a lot on the quality of the face database they are trained on. Unfortunately, generating high quality face databases is a major challenge that is rather time consuming. We have developed BeFaced, a tile-matching casual tablet game to enable massive crowdsourcing of facial expressions for the purpose of such machine learning algorithms. Based on the popular tile-matching gameplay mechanic, players are required to make facial expressions shown on matched tiles in order to clear them and advance in the game. Dynamic difficulty adjustment of the recognition accuracy is employed in the game in order to increase engagement and hence increase the quantity of varied facial expressions obtained. Each facial expression is automatically captured, labelled and sent to our online face database. At a more abstract level, BeFaced investigates a novel method of using popular game mechanics to aid the advancement of computer vision algorithms.},
booktitle = {SIGGRAPH Asia 2013 Technical Briefs},
articleno = {26},
numpages = {4},
keywords = {crowdsourcing, facial expression analysis, serious games},
location = {Hong Kong, Hong Kong},
series = {SA '13}
}

@inproceedings{10.1145/3038912.3052607,
author = {Kim, Seongsoon and Lee, Seongwoon and Park, Donghyeon and Kang, Jaewoo},
title = {Constructing and Evaluating a Novel Crowdsourcing-based Paraphrased Opinion Spam Dataset},
year = {2017},
isbn = {9781450349130},
publisher = {International World Wide Web Conferences Steering Committee},
address = {Republic and Canton of Geneva, CHE},
url = {https://doi.org/10.1145/3038912.3052607},
doi = {10.1145/3038912.3052607},
abstract = {Opinion spam, intentionally written by spammers who do not have actual experience with services or products, has recently become a factor that undermines the credibility of information online. In recent years, studies have attempted to detect opinion spam using machine learning algorithms. However, limitations of gold-standard spam datasets still prove to be a major obstacle in opinion spam research. In this paper, we introduce a novel dataset called Paraphrased OPinion Spam (POPS), which contains a new type of review spam that imitates real human opinions using crowdsourcing. To create such a seemingly truthful review spam dataset, we asked task participants to paraphrase truthful reviews, and include factual information and domain knowledge in their reviews. The classification experiments and semantic analysis results show that our POPS dataset most linguistically and semantically resembles truthful reviews. We believe that our new deceptive opinion spam dataset will help advance opinion spam research.},
booktitle = {Proceedings of the 26th International Conference on World Wide Web},
pages = {827–836},
numpages = {10},
keywords = {crowdsourcing, deceptive opinion spam, paraphrased opinion spam},
location = {Perth, Australia},
series = {WWW '17}
}

@inproceedings{10.1109/BigDataCongress.2015.61,
author = {Tung, Wei-Feng and Jordann, Guillaume},
title = {Crowdsourcing Service Design for Social Enterprise Insight Innovation},
year = {2015},
isbn = {9781467372787},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/BigDataCongress.2015.61},
doi = {10.1109/BigDataCongress.2015.61},
abstract = {Social enterprises (SE) have started up not a few innovative business models or cause-related marketing in modern society, which can discover more and more critical social issues need to solve urgently. In recently years, SEs often use social media of Web 2.0 (e.g., Facebook, Twitter) to seek various opportunities, resources, and supports. Thus, this research is to develop an integrative 'crowd sourcing' as an altruism social media that can connect and leverage external resources to help people undertake some cause-related projects (e.g., Volunteer activity) or start-ups of SE. It can answer what would new SE look like? How could SE work? Is this possible sustainable? A proposed a crowd sourcing platform called 'HIVE' means a kind of corporation concepts. As we know, all bees work in cooperation with each other and settled in a hive. Thus, the various social issues and resources information can be integrated and emerged for innovative business insights to support the developments of social enterprises.},
booktitle = {Proceedings of the 2015 IEEE International Congress on Big Data},
pages = {367–373},
numpages = {7},
keywords = {Collective Intelligence, Crowdsourcing, Hive, SE, Social Enterprises, Start-up},
series = {BIGDATACONGRESS '15}
}

@inproceedings{10.5555/2892753.2892804,
author = {Fang, Meng and Yin, Jie and Tao, Dacheng},
title = {Active learning for crowdsourcing using knowledge transfer},
year = {2014},
publisher = {AAAI Press},
abstract = {This paper studies the active learning problem in crowd-sourcing settings, where multiple imperfect annotators with varying levels of expertise are available for labeling the data in a given task. Annotations collected from these labelers may be noisy and unreliable, and the quality of labeled data needs to be maintained for data mining tasks. Previous solutions have attempted to estimate individual users' reliability based on existing knowledge in each task, but for this to be effective each task requires a large quantity of labeled data to provide accurate estimates. In practice, annotation budgets for a given task are limited, so each instance can be presented to only a few users, each of whom can only label a few examples. To overcome data scarcity we propose a new probabilistic model that transfers knowledge from abundant unlabeled data in auxiliary domains to help estimate labelers' expertise. Based on this model we present a novel active learning algorithm that: a) simultaneously selects the most informative example and b) queries its label from the labeler with the best expertise. Experiments on both text and image datasets demonstrate that our proposed method outperforms other state-of-the-art active learning methods.},
booktitle = {Proceedings of the Twenty-Eighth AAAI Conference on Artificial Intelligence},
pages = {1809–1815},
numpages = {7},
location = {Qu\'{e}bec City, Qu\'{e}bec, Canada},
series = {AAAI'14}
}

@inproceedings{10.1007/978-3-319-49169-1_23,
author = {Granell, Emilio and Mart\'{\i}nez-Hinarejos, Carlos-D.},
title = {Collaborator Effort Optimisation in Multimodal Crowdsourcing for Transcribing Historical Manuscripts},
year = {2016},
isbn = {978-3-319-49168-4},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-319-49169-1_23},
doi = {10.1007/978-3-319-49169-1_23},
abstract = {Crowdsourcing is a powerful tool for massive transcription at a relatively low cost, since the transcription effort is distributed into a set of collaborators, and therefore, supervision effort of professional transcribers may be dramatically reduced. Nevertheless, collaborators are a scarce resource, which makes optimisation very important in order to get the maximum benefit from their efforts. In this work, the optimisation of the work load in the side of collaborators is studied in a multimodal crowdsourcing platform where speech dictation of handwritten text lines is used as transcription source. The experiments explore how this optimisation allows to obtain similar results reducing the number of collaborators and the number of text lines that they have to read.},
booktitle = {Advances in Speech and Language Technologies for Iberian Languages: Third International Conference, IberSPEECH 2016, Lisbon, Portugal, November 23-25, 2016, Proceedings},
pages = {234–244},
numpages = {11},
keywords = {Crowdsourcing framework, Collaborator effort, Speech recognition, Document transcription},
location = {Lisbon, Portugal}
}

@inproceedings{10.1145/2757226.2764775,
author = {Kim, Joy},
title = {Designing Crowdsourcing Techniques Based on Expert Creative Practice},
year = {2015},
isbn = {9781450335980},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2757226.2764775},
doi = {10.1145/2757226.2764775},
abstract = {Current crowdsourcing workflows comprise of discrete tasks that guide the crowd towards predetermined goals. However, this approach is ill-suited towards supporting massively collaborative open-ended creative work, which often involves an exploration of possible end results and revision of creative goals. My dissertation explores how existing expert creative practice can inform new crowdsourcing techniques that allow the crowd to collaborate on complex creative tasks such as writing short stories.},
booktitle = {Proceedings of the 2015 ACM SIGCHI Conference on Creativity and Cognition},
pages = {349–350},
numpages = {2},
keywords = {creativity, crowdsourcing, experts, novices, social computing, storytelling},
location = {Glasgow, United Kingdom},
series = {C&amp;C '15}
}

@inproceedings{10.5555/2999792.2999826,
author = {Liu, Qiang and Steyvers, Mark and Ihler, Alexander},
title = {Scoring workers in crowdsourcing: how many control questions are enough?},
year = {2013},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {We study the problem of estimating continuous quantities, such as prices, probabilities, and point spreads, using a crowdsourcing approach. A challenging aspect of combining the crowd's answers is that workers' reliabilities and biases are usually unknown and highly diverse. Control items with known answers can be used to evaluate workers' performance, and hence improve the combined results on the target items with unknown answers. This raises the problem of how many control items to use when the total number of items each workers can answer is limited: more control items evaluates the workers better, but leaves fewer resources for the target items that are of direct interest, and vice versa. We give theoretical results for this problem under different scenarios, and provide a simple rule of thumb for crowdsourcing practitioners. As a byproduct, we also provide theoretical analysis of the accuracy of different consensus methods.},
booktitle = {Proceedings of the 26th International Conference on Neural Information Processing Systems - Volume 2},
pages = {1914–1922},
numpages = {9},
location = {Lake Tahoe, Nevada},
series = {NIPS'13}
}

@inproceedings{10.1145/2851613.2851933,
author = {Saab, Farah and Elhajj, Imad and Kayssi, Ayman and Chehab, Ali},
title = {A crowdsourcing game-theoretic intrusion detection and rating system},
year = {2016},
isbn = {9781450337397},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2851613.2851933},
doi = {10.1145/2851613.2851933},
abstract = {One of the main concerns for smartphone users is the quality of apps they download. Before installing any app from the market, users first check its rating and reviews. However, these ratings are not computed by experts and most times are not associated with malicious behavior. In this work, we present an IDS/rating system based on a game theoretic model with crowdsourcing. Our results show that, with minor control over the error in categorizing users and the fraction of experts in the crowd, our system provides proper ratings while flagging all malicious apps.},
booktitle = {Proceedings of the 31st Annual ACM Symposium on Applied Computing},
pages = {622–625},
numpages = {4},
keywords = {IDS, app market, app rating, crowdsourcing, game theory},
location = {Pisa, Italy},
series = {SAC '16}
}

@inproceedings{10.5555/2832249.2832277,
author = {Yin, Ming and Chen, Yiling},
title = {Bonus or not? learn to reward in crowdsourcing},
year = {2015},
isbn = {9781577357384},
publisher = {AAAI Press},
abstract = {Recent work has shown that the quality of work produced in a crowdsourcing working session can be influenced by the presence of performance-contingent financial incentives, such as bonuses for exceptional performance, in the session. We take an algorithmic approach to decide when to offer bonuses in a working session to improve the overall utility that a requester derives from the session. Specifically, we propose and train an input-output hidden Markov model to learn the impact of bonuses on work quality and then use this model to dynamically decide whether to offer a bonus on each task in a working session to maximize a requester's utility. Experiments on Amazon Mechanical Turk show that our approach leads to higher utility for the requester than fixed and random bonus schemes do. Simulations on synthesized data sets further demonstrate the robustness of our approach against different worker population and worker behavior in improving requester utility.},
booktitle = {Proceedings of the 24th International Conference on Artificial Intelligence},
pages = {201–207},
numpages = {7},
location = {Buenos Aires, Argentina},
series = {IJCAI'15}
}

@inproceedings{10.1109/ICEBE.2014.25,
author = {Saberi, Morteza and Hussain, Omar K. and Janjua, Naeem Khalid and Chang, Elizabeth},
title = {In-house Crowdsourcing-Based Entity Resolution: Dealing with Common Names},
year = {2014},
isbn = {9781479965632},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/ICEBE.2014.25},
doi = {10.1109/ICEBE.2014.25},
abstract = {Entity Resolution (ER) is one of the techniques used to disambiguate the various manifestations of same object to improve search results in databases. Recently, Crowd sourcing has been utilized to improve entity resolution to gain positive impact when searching for particular information in a database. In this paper, we consider the domain of Customer Relationship Management (CRM) and utilize the approach of Crowd sourcing to enrich the process of achieving ER. Specifically our focus is to identify the right customer that has been manifested in various ways under a common name in a database using In-house Crowd sourcing-based Entity Resolution approach (ICER). The ICER takes the list of possible duplicates into consideration (which are pre-determined) and identifies the pair of record that has the maximum impact in achieving ER. Then, this pair is crowd sourced to Customer Service Representatives (CSRs) to have their input (labeling). ICER incorporates the principles of Human Intelligence Task (HIT) that aims to keep the questions asked to the CSR to a minimum. Two ICER approaches are proposed in this study based on probabilistic (modified approach of Whang et al) and active learning schemas. The applicability of the proposed ICER approaches and comparison of their results have been highlighted by using an example.},
booktitle = {Proceedings of the 2014 IEEE 11th International Conference on E-Business Engineering},
pages = {83–88},
numpages = {6},
keywords = {Common personal names, Crowd Entity resolution, Customer recognition, Customer service representative},
series = {ICEBE '14}
}

@inproceedings{10.1145/2362456.2362479,
author = {Sabou, Marta and Bontcheva, Kalina and Scharl, Arno},
title = {Crowdsourcing research opportunities: lessons from natural language processing},
year = {2012},
isbn = {9781450312424},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2362456.2362479},
doi = {10.1145/2362456.2362479},
abstract = {Although the field has led to promising early results, the use of crowdsourcing as an integral part of science projects is still regarded with skepticism by some, largely due to a lack of awareness of the opportunities and implications of utilizing these new techniques. We address this lack of awareness, firstly by highlighting the positive impacts that crowdsourcing has had on Natural Language Processing research. Secondly, we discuss the challenges of more complex methodologies, quality control, and the necessity to deal with ethical issues. We conclude with future trends and opportunities of crowdsourcing for science, including its potential for disseminating results, making science more accessible, and enriching educational programs.},
booktitle = {Proceedings of the 12th International Conference on Knowledge Management and Knowledge Technologies},
articleno = {17},
numpages = {8},
keywords = {crowdsourcing, games with a purpose, natural language processing, resource acquisition},
location = {Graz, Austria},
series = {i-KNOW '12}
}

@inproceedings{10.1145/2505515.2507858,
author = {Cheng, Yu and Chen, Zhengzhang and Wang, Jiang and Agrawal, Ankit and Choudhary, Alok},
title = {Bootstrapping active name disambiguation with crowdsourcing},
year = {2013},
isbn = {9781450322638},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2505515.2507858},
doi = {10.1145/2505515.2507858},
abstract = {Name disambiguation is a challenging and important problem in many domains, such as digital libraries, social media management and people search systems. Traditional methods, based on direct assignment using supervised machine learning techniques, seem to be the most effective, but their performances are highly dependent on the amount of training data, while large data annotation can be expensive and time-consuming requiring hours of manual inspection by a domain expert. To efficiently acquire labeled data, we propose a bootstrapping algorithm for the name disambiguation task based on active learning and crowdsourced labeling. We show that the proposed method can leverage the advantages of exploration and exploitation by combining two strategies, thereby improving the overall quality of the training data at minimal expense. The experimental results on two datasets DBLP and ArnetMiner demonstrate the superiority of our framework over existing methods.},
booktitle = {Proceedings of the 22nd ACM International Conference on Information \&amp; Knowledge Management},
pages = {1213–1216},
numpages = {4},
keywords = {active learning, bootstrapping, crowdsourcing, name disambiguation},
location = {San Francisco, California, USA},
series = {CIKM '13}
}

@inproceedings{10.1109/SAHCN.2015.7338353,
author = {Fox, Andrew and Kumar, B.V.K. Vijaya and Chen, Jinzhu and Bai, Fan},
title = {Crowdsourcing undersampled vehicular sensor data for pothole detection},
year = {2015},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/SAHCN.2015.7338353},
doi = {10.1109/SAHCN.2015.7338353},
abstract = {The increased availability of embedded vehicle sensors allows for the detection of road features such as potholes. Despite being a promising approach, current vehicle embedded sensors operate at low frequencies and undersample sensor signals, thus degrading detection accuracy. One emerging solution is to crowdsource such undersampled sensor data from multiple vehicles to increase the detection accuracy. Aggregating sensor data from multiple vehicles, nonetheless, is a challenging task given the heterogeneity among vehicles, asynchronous sensor operation, GPS error, and sensor noise. Additionally, there may be bandwidth restrictions in vehicular networks which limit the amount of data available for aggregation. We investigate these issues by focusing on the problem of pothole detection. To quantify the detection accuracies and effects of real-world limitations, we design and evaluate three crowdsourcing pothole detection schemes involving vehicles and the Cloud. We also address the issue of lack of extensive model training data by demonstrating that a detection model applicable to real-world systems can be derived using simulated data. We validate our pothole detection methods using 38.1 km of real-world data collected from driving on roads in Warren, Michigan.},
booktitle = {2015 12th Annual IEEE International Conference on Sensing, Communication, and Networking (SECON)},
pages = {515–523},
numpages = {9},
location = {Seattle, WA, USA}
}

@inproceedings{10.1145/2674396.2674466,
author = {Boutsis, Ioannis and Tomaras, Dimitrios and Kalogeraki, Vana},
title = {Towards real-time emergency response using crowdsourcing},
year = {2014},
isbn = {9781450327466},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2674396.2674466},
doi = {10.1145/2674396.2674466},
abstract = {Crowdsourcing has emerged as an attractive paradigm in recent years for information collection for disaster response, which utilizes data received from the human crowd, to provide critical information collection and dissemination during emergency situations and visualize this data to generate emergency maps for the human crowd. In this paper we investigate the use of crowdsourcing mechanisms for real-time emergency response and describe our approach for developing a crowdsourcing tool that can be effectively used to formulate questions and seek answers from the human crowd using a MapReduce programming model, and integrate this information into a novel spatiotemporal data structure and create a visual emergency map. Our experimental evaluation shows that our approach is practical, efficient and can be used for applications with real-time demands.},
booktitle = {Proceedings of the 7th International Conference on PErvasive Technologies Related to Assistive Environments},
articleno = {62},
numpages = {4},
keywords = {crowdsourcing, distributed sensor systems, emergency response, spatiotemporal data},
location = {Rhodes, Greece},
series = {PETRA '14}
}

@inproceedings{10.1109/UCC.2013.99,
author = {Springer, Thomas and Hara, Tenshi and Schill, Alexander},
title = {On Providing Crowdsourcing as a Service},
year = {2013},
isbn = {9780769551524},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/UCC.2013.99},
doi = {10.1109/UCC.2013.99},
abstract = {In this paper we present an approach to separate submission capturing and processing from basic crowd sourcing functionality to create reusable crowd sourcing services which are more flexible and secure.},
booktitle = {Proceedings of the 2013 IEEE/ACM 6th International Conference on Utility and Cloud Computing},
pages = {510–511},
numpages = {2},
keywords = {crowdsourcing platform, implicit and explicit crowdsourcing, proxy approach},
series = {UCC '13}
}

@inproceedings{10.1109/iThings/CPSCom.2011.128,
author = {Yuen, Man-Ching and King, Irwin and Leung, Kwong-Sak},
title = {Task Matching in Crowdsourcing},
year = {2011},
isbn = {9780769545806},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/iThings/CPSCom.2011.128},
doi = {10.1109/iThings/CPSCom.2011.128},
abstract = {Crowd sourcing is evolving as a distributed problem-solving and business production model in recent years. In crowd sourcing paradigm, tasks are distributed to networked people to complete such that a company s production cost can be greatly reduced. A crowd sourcing process involves operations of both requesters and workers. A requester submits a task request, a worker selects and completes a task, and the requester only pays the worker for the successful completion of the task. Obviously, it is not efficient that the amount of time spent on selecting a task is comparable with that spent on working on a task, but the monetary reward of a task is just a small amount. Literature mainly focused on exploring what type of tasks can be deployed to the crowd and analyzing the performance of crowd sourcing platforms. However, no existing work investigates on how to support workers to select tasks on crowd sourcing platforms easily and effectively. In this paper, we propose a novel idea on task matching in crowd sourcing to motivate workers to keep on working on crowd sourcing platforms in long run. The idea utilizes the past task preference and performance of a worker to produce a list of available tasks in the order of best matching with the worker during his task selection stage. It aims to increase the efficiency of task completion. We present some preliminary experimental results in case studies. Finally, we address the possible challenges and discuss the future directions.},
booktitle = {Proceedings of the 2011 International Conference on Internet of Things and 4th International Conference on Cyber, Physical and Social Computing},
pages = {409–412},
numpages = {4},
keywords = {crowdsourcing, task matching algorithm, task model},
series = {ITHINGSCPSCOM '11}
}

@inproceedings{10.1109/HASE.2016.20,
author = {Awwad, Tarek and Bennani, Nadia and Brunie, Lionel and Coquil, David and Kosch, Harald and Rehn-Sonigo, Veronika},
title = {Task Characterization for an Effective Worker Targeting in Crowdsourcing},
year = {2016},
isbn = {9781467399135},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/HASE.2016.20},
doi = {10.1109/HASE.2016.20},
abstract = {In the last decade, crowdsourcing (CS) has emergedas a very promising approach for obtaining services, feedbackor data from a large number of people connected through theInternet, in a short time and at a reasonable cost. CS has beenused in a large range of contexts, thus proving its versatility. However, the quality of the services or data provided by theworkers (the "crowd") is not guaranteed, and therefore mustbe verified. This verification usually results in additional timeand cost. We propose a novel approach of quality control incrowdsourcing that reduces, and in some cases eliminates, thisoverhead. Our approach uses a learning technique to characterizeand cluster tasks, and selects, within the available crowd, the mostreliable group of workers for a given type of tasks.},
booktitle = {Proceedings of the 2016 IEEE 17th International Symposium on High Assurance Systems Engineering (HASE)},
pages = {63–64},
numpages = {2},
series = {HASE '16}
}

@inproceedings{10.1007/978-3-642-39320-4_7,
author = {Martin, Ursula and Pease, Alison},
title = {Mathematical practice, crowdsourcing, and social machines},
year = {2013},
isbn = {9783642393198},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-642-39320-4_7},
doi = {10.1007/978-3-642-39320-4_7},
abstract = {The highest level of mathematics has traditionally been seen as a solitary endeavour, to produce a proof for review and acceptance by research peers. Mathematics is now at a remarkable inflexion point, with new technology radically extending the power and limits of individuals. Crowdsourcing pulls together diverse experts to solve problems; symbolic computation tackles huge routine calculations; and computers check proofs too long and complicated for humans to comprehend.The Study of Mathematical Practice is an emerging interdisciplinary field which draws on philosophy and social science to understand how mathematics is produced. Online mathematical activity provides a novel and rich source of data for empirical investigation of mathematical practice - for example the community question-answering system mathoverflow contains around 40,000 mathematical conversations, and polymath collaborations provide transcripts of the process of discovering proofs. Our preliminary investigations have demonstrated the importance of "soft" aspects such as analogy and creativity, alongside deduction and proof, in the production of mathematics, and have given us new ways to think about the roles of people and machines in creating new mathematical knowledge. We discuss further investigation of these resources and what it might reveal.Crowdsourced mathematical activity is an example of a "social machine", a new paradigm, identified by Berners-Lee, for viewing a combination of people and computers as a single problem-solving entity, and the subject of major international research endeavours. We outline a future research agenda for mathematics social machines, a combination of people, computers, and mathematical archives to create and apply mathematics, with the potential to change the way people do mathematics, and to transform the reach, pace, and impact of mathematics research.},
booktitle = {Proceedings of the 2013 International Conference on Intelligent Computer Mathematics},
pages = {98–119},
numpages = {22},
location = {Bath, UK},
series = {CICM'13}
}

@inproceedings{10.5555/2772879.2772901,
author = {Davami, Erfan and Sukthankar, Gita},
title = {Improving the Performance of Mobile Phone Crowdsourcing Applications},
year = {2015},
isbn = {9781450334136},
publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
address = {Richland, SC},
abstract = {Mobile phone crowdsourcing is a powerful tool for many types of distributed sensing problems. However, a central issue with this type of system is that it relies on user contributed data, which may be sparse or erroneous. This paper describes our experiences developing a mobile phone crowdsourcing app, Kpark, for monitoring parking availability on a university campus. Our system combines multiple trust- based data fusion techniques to improve the quality of user submitted parking reports and is currently being used by over 1500 students.},
booktitle = {Proceedings of the 2015 International Conference on Autonomous Agents and Multiagent Systems},
pages = {145–153},
numpages = {9},
keywords = {mobile phone crowdsourcing, trust-based fusion},
location = {Istanbul, Turkey},
series = {AAMAS '15}
}

@inproceedings{10.1145/2745844.2745871,
author = {Lee, Donghyeon and Kim, Joonyoung and Lee, Hyunmin and Jung, Kyomin},
title = {Reliable Multiple-choice Iterative Algorithm for Crowdsourcing Systems},
year = {2015},
isbn = {9781450334860},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2745844.2745871},
doi = {10.1145/2745844.2745871},
abstract = {The appearance of web-based crowdsourcing systems gives a promising solution to exploiting the wisdom of crowds efficiently in a short time with a relatively low budget. Despite their efficiency, crowdsourcing systems have an inherent problem in that responses from workers can be unreliable since workers are low-paid and have low responsibility. Although simple majority voting can be a solution, various research studies have sought to aggregate noisy responses to obtain greater reliability in results through effective techniques such as Expectation-Maximization (EM) based algorithms. While EM-based algorithms get the limelight in crowdsourcing systems due to their useful inference techniques, Karger et al. made a significant breakthrough by proposing a novel iterative algorithm based on the idea of low-rank matrix approximations and the message passing technique. They showed that the performance of their iterative algorithm is order-optimal, which outperforms majority voting and EM-based algorithms. However, their algorithm is not always applicable in practice since it can only be applied to binary-choice questions. Recently, they devised an inference algorithm for multi-class labeling, which splits each task into a bunch of binary-choice questions and exploits their existing algorithm. However, it has difficulty in combining into real crowdsourcing systems since it overexploits redundancy in that each split question should be queried in multiple times to obtain reliable results.In this paper, we design an iterative algorithm to infer true answers for multiple-choice questions, which can be directly applied to real crowdsourcing systems. Our algorithm can also be applicable to short-answer questions as well. We analyze the performance of our algorithm, and prove that the error bound decays exponentially. Through extensive experiments, we verify that our algorithm outperforms majority voting and EM-based algorithm in accuracy.},
booktitle = {Proceedings of the 2015 ACM SIGMETRICS International Conference on Measurement and Modeling of Computer Systems},
pages = {205–216},
numpages = {12},
keywords = {crowdsourcing, iterative learning, multiple-choice, resource allocation},
location = {Portland, Oregon, USA},
series = {SIGMETRICS '15}
}

@article{10.1145/2796314.2745871,
author = {Lee, Donghyeon and Kim, Joonyoung and Lee, Hyunmin and Jung, Kyomin},
title = {Reliable Multiple-choice Iterative Algorithm for Crowdsourcing Systems},
year = {2015},
issue_date = {June 2015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {43},
number = {1},
issn = {0163-5999},
url = {https://doi.org/10.1145/2796314.2745871},
doi = {10.1145/2796314.2745871},
abstract = {The appearance of web-based crowdsourcing systems gives a promising solution to exploiting the wisdom of crowds efficiently in a short time with a relatively low budget. Despite their efficiency, crowdsourcing systems have an inherent problem in that responses from workers can be unreliable since workers are low-paid and have low responsibility. Although simple majority voting can be a solution, various research studies have sought to aggregate noisy responses to obtain greater reliability in results through effective techniques such as Expectation-Maximization (EM) based algorithms. While EM-based algorithms get the limelight in crowdsourcing systems due to their useful inference techniques, Karger et al. made a significant breakthrough by proposing a novel iterative algorithm based on the idea of low-rank matrix approximations and the message passing technique. They showed that the performance of their iterative algorithm is order-optimal, which outperforms majority voting and EM-based algorithms. However, their algorithm is not always applicable in practice since it can only be applied to binary-choice questions. Recently, they devised an inference algorithm for multi-class labeling, which splits each task into a bunch of binary-choice questions and exploits their existing algorithm. However, it has difficulty in combining into real crowdsourcing systems since it overexploits redundancy in that each split question should be queried in multiple times to obtain reliable results.In this paper, we design an iterative algorithm to infer true answers for multiple-choice questions, which can be directly applied to real crowdsourcing systems. Our algorithm can also be applicable to short-answer questions as well. We analyze the performance of our algorithm, and prove that the error bound decays exponentially. Through extensive experiments, we verify that our algorithm outperforms majority voting and EM-based algorithm in accuracy.},
journal = {SIGMETRICS Perform. Eval. Rev.},
month = jun,
pages = {205–216},
numpages = {12},
keywords = {crowdsourcing, iterative learning, multiple-choice, resource allocation}
}

@inproceedings{10.1109/CBI.2014.16,
author = {Lofi, Christoph and Maarry, Kinda El},
title = {Design Patterns for Hybrid Algorithmic-Crowdsourcing Workflows},
year = {2014},
isbn = {9781479957798},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/CBI.2014.16},
doi = {10.1109/CBI.2014.16},
abstract = {Crowd sourcing has shown to be a powerful technique for over-coming many challenges in data and information processing where current state-of-the-art algorithms are still struggling. This is especially true for workflows that transparently combine algorithmic heuristics and dynamically crowd sourced tasks that are performed by human workers, and which promise to solve even more complex tasks effectively and efficiently. But still, such hybrid crowd sourcing workflows can be difficult to approach, and they are often designed in an ad-hoc fashion. Therefore, in this paper, we extensively investigate such crowd sourcing workflows as described in the literature, and abstract generic design patterns, which codify commonly recurring challenges and their best-practice solutions. Each design pattern is described and discussed with a special focus on its requirements, constraints, and effects on the overall workflow. We illustrate the practicality of these patterns by providing real-world application examples where such patterns can or have been applied. Furthermore, we show-case how the individual design patterns can be extended and combined to support more complex workflows. Our design patterns provide an extensive overview of the hybrid crowd sourcing workflows' design space, and allow for a more efficient modeling, analysis, and documentation of such workflows.},
booktitle = {Proceedings of the 2014 IEEE 16th Conference on Business Informatics - Volume 01},
pages = {1–8},
numpages = {8},
keywords = {information processing, crowdsourcing, workflows, design patterns},
series = {CBI '14}
}

@inproceedings{10.1145/2660168.2660186,
author = {Saitis, Charalampos and Hankinson, Andrew and Fujinaga, Ichiro},
title = {Correcting Large-Scale OMR Data with Crowdsourcing},
year = {2014},
isbn = {9781450330022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2660168.2660186},
doi = {10.1145/2660168.2660186},
abstract = {This paper discusses several technical challenges in using crowdsourcing for distributed correction interfaces. The specific scenario under investigation involves the implementation of a crowd-sourced adaptive optical music recognition system (Single Interface for Music Score Searching and Analysis project). We envisage the distribution of correction tasks beyond a single workstation to potentially thousands of users around the globe. This will have the effect of producing human-checked transcriptions, as well as significant quantities of human-provided ground-truth data, which may be re-integrated into an adaptive recognition process, allowing an OMR system to "learn" from its mistakes. Drawing from existing crowdsourcing approaches and user interfaces in music (e.g., Bodleian Libraries) and non-music (e.g., CAPTCHAs) applications, this project aims to develop a scientific understanding of what makes crowdsourcing work, how to entice, engage and reward contributors, and how to evaluate their reliability. While results will be considered based on the specific needs of SIMSSA, such knowledge can be useful to a variety of musicological investigations that involve labour-intensive methods.},
booktitle = {Proceedings of the 1st International Workshop on Digital Libraries for Musicology},
pages = {1–3},
numpages = {3},
keywords = {Optical music recognition, crowdsourcing, music notation, music score searching, web applications},
location = {London, United Kingdom},
series = {DLfM '14}
}

@inproceedings{10.1145/2962132.2962134,
author = {Conboy, Kieran and Cullina, Eoin and Morgan, Lorraine},
title = {A Crowdsourcing Practices Framework for Public Scientific Research Funding Agencies},
year = {2016},
isbn = {9781450344814},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2962132.2962134},
doi = {10.1145/2962132.2962134},
booktitle = {Proceedings of the 12th International Symposium on Open Collaboration Companion},
articleno = {2},
numpages = {4},
keywords = {Crowdsourcing, framework, practices},
location = {Berlin, Germany},
series = {OpenSym '16}
}

@inproceedings{10.1145/2593728.2593731,
author = {Stol, Klaas-Jan and Fitzgerald, Brian},
title = {Researching crowdsourcing software development: perspectives and concerns},
year = {2014},
isbn = {9781450328579},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2593728.2593731},
doi = {10.1145/2593728.2593731},
abstract = {Crowdsourcing is an emerging form of `outsourcing’ software development. While there has been considerable research in the area of crowdsourcing in general, very little research has focused specifically on how crowdsourcing works in a software development context, and as far as we know, there have been no published studies of crowdsourcing software development from a customer perspective. Based on a review of the literature, we identified a number of key concerns related to crowdsourcing that are of particular importance in a software development context. Furthermore, we observed a number of recurring key stakeholders, or actors, each of whom has a unique perspective on crowdsourcing. This paper presents a research framework that consists of the various combinations of stakeholders and key concerns. The framework can be used to guide future research on the use of crowdsourcing as a `sourcing’ strategy, as well as a means to review and synthesize research findings so as to be able to compare studies on crowdsourcing in a software development context.},
booktitle = {Proceedings of the 1st International Workshop on CrowdSourcing in Software Engineering},
pages = {7–10},
numpages = {4},
keywords = {crowdsourcing software development, research framework},
location = {Hyderabad, India},
series = {CSI-SE 2014}
}

@inproceedings{10.1145/2506364.2506366,
author = {Figuerola Salas, \'{O}scar and Adzic, Velibor and Shah, Akash and Kalva, Hari},
title = {Assessing internet video quality using crowdsourcing},
year = {2013},
isbn = {9781450323963},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2506364.2506366},
doi = {10.1145/2506364.2506366},
abstract = {In this paper, we present a subjective video quality evaluation system that has been integrated with different crowdsourcing platforms. We try to evaluate the feasibility of replacing the time consuming and expensive traditional tests with a faster and less expensive crowdsourcing alternative. CrowdFlower and Amazon's Mechanical Turk were used as the crowdsourcing platforms to collect data. The data was compared with the formal subjective tests conducted by MPEG as part of the video standardization process, as well as with previous results from a study we ran at the university level. High quality compressed videos with known Mean Opinion Score (MOS) are used as references instead of the original lossless videos in order to overcome intrinsic bandwidth limitations. The bitrates chosen for the experiment were selected targeting Internet use, since this is the environment in which users were going to be evaluating the videos. Evaluations showed that the results are consistent with formal subjective evaluation scores, and can be reproduced across different crowds with low variability, which makes this type of test setting very promising.},
booktitle = {Proceedings of the 2nd ACM International Workshop on Crowdsourcing for Multimedia},
pages = {23–28},
numpages = {6},
keywords = {crowdsourcing, internet video quality, mean opinion score, mos, quality assessment, subjective quality},
location = {Barcelona, Spain},
series = {CrowdMM '13}
}

@inproceedings{10.1109/WI-IAT.2015.81,
author = {Ashikawa, Masayuki and Kawamura, Takahiro and Ohsuga, Akihiko},
title = {Deployment of Private Crowdsourcing System with Quality Control Methods},
year = {2015},
isbn = {9781467396189},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/WI-IAT.2015.81},
doi = {10.1109/WI-IAT.2015.81},
abstract = {Current crowdsourcing platforms such as Amazon Mechanical Turk provide an attractive solution for processing of high-volume tasks at low cost. However, problems of quality control remain a major concern. In the present work, we developed a private crowdsourcing system(PCSS) running in a intranetwork, that allow us to devise for quality control methods. For quality control, we introduce four worker selection methods: preprocessing filtering, real-time filtering, post-processing filtering, and guess processing filtering. In addition to a basic approach involving initial training or the use of gold standard data, these methods include a novel approach, utilizing collaborative filtering techniques. Furthermore, we collected a large amount of vocabulary data for natural language processing, such as voice recognition and text to speech using PCSS. The quality control methods increased accuracy by 32.4\% in collecting vocabulary task. Then, we got 138 thousand vocabulary data. We found that PCSS is a practical system to collect data, and used for three years since 2011.},
booktitle = {Proceedings of the 2015 IEEE / WIC / ACM International Conference on Web Intelligence and Intelligent Agent Technology (WI-IAT) - Volume 01},
pages = {9–16},
numpages = {8},
series = {WI-IAT '15}
}

@inproceedings{10.1145/2600428.2609577,
author = {Zhang, Yinglong and Zhang, Jin and Lease, Matthew and Gwizdka, Jacek},
title = {Multidimensional relevance modeling via psychometrics and crowdsourcing},
year = {2014},
isbn = {9781450322577},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2600428.2609577},
doi = {10.1145/2600428.2609577},
abstract = {While many multidimensional models of relevance have been posited, prior studies have been largely exploratory rather than confirmatory. Lacking a methodological framework to quantify the relationships among factors or measure model fit to observed data, many past models could not be empirically tested or falsified. To enable more positivist experimentation, Xu and Chen [77] proposed a psychometric framework for multidimensional relevance modeling. However, we show their framework exhibits several methodological limitations which could call into question the validity of findings drawn from it. In this work, we identify and address these limitations, scale their methodology via crowdsourcing, and describe quality control methods from psychometrics which stand to benefit crowdsourcing IR studies in general. Methodology we describe for relevance judging is expected to benefit both human-centered and systems-centered IR.},
booktitle = {Proceedings of the 37th International ACM SIGIR Conference on Research \&amp; Development in Information Retrieval},
pages = {435–444},
numpages = {10},
keywords = {crowdsourcing, psychometrics, relevance judgment},
location = {Gold Coast, Queensland, Australia},
series = {SIGIR '14}
}

@inproceedings{10.1145/3248609,
author = {Goncalves, Jorge and Hosio, Simo and Vukovic, Maja and Konomi, Shin'ichi and Lee, Uichin},
title = {Session details: (WMSC) second workshop on mobile and situated crowdsourcing},
year = {2016},
isbn = {9781450344623},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3248609},
doi = {10.1145/3248609},
booktitle = {Proceedings of the 2016 ACM International Joint Conference on Pervasive and Ubiquitous Computing: Adjunct},
location = {Heidelberg, Germany},
series = {UbiComp '16}
}

@inproceedings{10.1145/3058555.3058586,
author = {Guo, Anhong and Bigham, Jeffrey P.},
title = {Making Real-World Interfaces Accessible Through Crowdsourcing, Computer Vision, and Fabrication},
year = {2017},
isbn = {9781450349000},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3058555.3058586},
doi = {10.1145/3058555.3058586},
abstract = {The world is full of physical interfaces that are inaccessible to blind people, from microwaves and information kiosks to thermostats and checkout terminals. Blind people cannot independently use such devices without at least first learning their layout, and usually only after labeling them with sighted assistance. To address this problem, we introduce VizLens---a robust and interactive screen reader for interfaces in the real world. VizLens users take a picture of an interface they would like to use, it is interpreted quickly and robustly by multiple crowd workers in parallel, and then computer vision is able to give interactive feedback and guidance to users to help them use the interface in real time. Built on top of VizLens, we developed automatically generating tactile overlays to physical interfaces to provide blind people with a permanent static solution. We introduce Facade---a crowdsourced fabrication pipeline to help blind people independently make physical interfaces accessible by adding a 3D printed augmentation of tactile buttons overlaying the original panel.},
booktitle = {Proceedings of the 14th International Web for All Conference},
articleno = {29},
numpages = {2},
keywords = {3D printing, Non-visual interfaces, accessibility, blind, computer vision, crowdsourcing, fabrication, mobile, visually impaired},
location = {Perth, Western Australia, Australia},
series = {W4A '17}
}

@inproceedings{10.1145/3058555.3058573,
author = {Li, Liangcheng and Wang, Can and Song, Shuyi and Yu, Zhi and Zhou, Fenqin and Bu, Jiajun},
title = {A task assignment strategy for crowdsourcing-based web accessibility evaluation system},
year = {2017},
isbn = {9781450349000},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3058555.3058573},
doi = {10.1145/3058555.3058573},
abstract = {Web accessibility evaluation aims to find the interactive barrier for people with disabilities in accessing the contents on the Web. As some of the checkpoints require human inspection for conformance evaluation, evaluating a website will usually incur an expensive cost. To address this issue, crowdsourcing-based system is used in web accessibility evaluation to elicit contributions from volunteer participants. However, some of accessibility evaluation tasks are complicated and require a certain level of expertise in evaluation. This makes the task assignment in crowdsourcing a challenging problem in that poor evaluation accuracy will be resulted when complicated tasks are assigned to inexperienced participants. To address this issue, we propose in this paper a novel task assignment strategy called Evaluator-Decision-Based Assignment (EDBA) to better leverage the participation and expertise of the volunteers. Using evaluators' historical evaluation records and experts' review, we train a minimum cost model via machine learning methods to obtain an optimal task assignment map. Experiments on Chinese Web Accessibility Evaluation System show that our method achieves high accuracy in website accessibility evaluation. Meanwhile, the balanced assignments from EDBA also enable both novices and old hands effective participation in accessibility evaluation.},
booktitle = {Proceedings of the 14th International Web for All Conference},
articleno = {18},
numpages = {4},
keywords = {Crowdsourcing System, Task Assignment, Web Accessibility Evaluation},
location = {Perth, Western Australia, Australia},
series = {W4A '17}
}

@inproceedings{10.1145/3184558.3191545,
author = {Zequeira Jim\'{e}nez, Rafael and Fern\'{a}ndez Gallardo, Laura and M\"{o}ller, Sebastian},
title = {Outliers Detection vs. Control Questions to Ensure Reliable Results in Crowdsourcing.: A Speech Quality Assessment Case Study},
year = {2018},
isbn = {9781450356404},
publisher = {International World Wide Web Conferences Steering Committee},
address = {Republic and Canton of Geneva, CHE},
url = {https://doi.org/10.1145/3184558.3191545},
doi = {10.1145/3184558.3191545},
abstract = {Crowdsourcing provides an exceptional opportunity for the rapid collection of human input for data acquisition and labelling. This approach have been adopted in multiple domains and researchers are now able to reach a demographically diverse audience at low cost. However, it remains the question of whether the results are still valid and reliable. Previous work have introduced different mechanisms to ensure data reliability in crowdsourcing. This work examines to which extend, "trapping question" or "outliers detection" assure reliable results to the detriment of, overloading task content with stimuli that are not of interest for the researcher, or by discarding data points that might be the true opinion of a worker. To this end, a speech quality assessment study have been conducted in a web crowdsourcing platform, following the ITU-T Rec. P.800. Workers assessed the speech stimuli of the database 501 from the ITU-T Rec. P.863. We examine results' validity in terms of correlations to previous ratings collected in laboratory. Our outcomes shows that neither of the techniques under investigation improve results accuracy by itself, but a combination of both. Our goal is to provide empirical guidance for designing experiments in crowdsourcing while ensuring data reliability.},
booktitle = {Companion Proceedings of the The Web Conference 2018},
pages = {1127–1130},
numpages = {4},
keywords = {crowdsourcing, data validity, gold-strandard questions, outliers detection, speech quality assessment, trapping questions, users reliability},
location = {Lyon, France},
series = {WWW '18}
}

@inproceedings{10.1145/2615569.2615644,
author = {Oosterman, Jasper and Nottamkandath, Archana and Dijkshoorn, Chris and Bozzon, Alessandro and Houben, Geert-Jan and Aroyo, Lora},
title = {Crowdsourcing knowledge-intensive tasks in cultural heritage},
year = {2014},
isbn = {9781450326223},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2615569.2615644},
doi = {10.1145/2615569.2615644},
abstract = {Large datasets such as Cultural Heritage collections require detailed annotations when digitised and made available online. Annotating different aspects of such collections requires a variety of knowledge and expertise which is not always possessed by the collection curators. Artwork annotation is an example of a knowledge intensive image annotation task, i.e. a task that demands annotators to have domain-specific knowledge in order to be successfully completed.This paper describes the results of a study aimed at investigating the applicability of crowdsourcing techniques to knowledge intensive image annotation tasks. We observed a clear relationship between the annotation difficulty of an image, in terms of number of items to identify and annotate, and the performance of the recruited workers.},
booktitle = {Proceedings of the 2014 ACM Conference on Web Science},
pages = {267–268},
numpages = {2},
keywords = {crowdsourcing, cultural heritage, knowledge intensive tasks},
location = {Bloomington, Indiana, USA},
series = {WebSci '14}
}

@inproceedings{10.1109/HICSS.2013.143,
author = {Pedersen, Jay and Kocsis, David and Tripathi, Abhishek and Tarrell, Alvin and Weerakoon, Aruna and Tahmasbi, Nargess and Xiong, Jie and Deng, Wei and Oh, Onook and de Vreede, Gert-Jan},
title = {Conceptual Foundations of Crowdsourcing: A Review of IS Research},
year = {2013},
isbn = {9780769548920},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/HICSS.2013.143},
doi = {10.1109/HICSS.2013.143},
abstract = {Crowd sourcing is a collaboration model enabled by people-centric web technologies to solve individual, organizational, and societal problems using a dynamically formed crowd of people who respond to an open call for participation. We report on a literature survey of crowd sourcing research, focusing on top journals and conferences in the Information Systems (IS) field. To our knowledge, ours is the first effort of this type in the IS discipline. Contributions include providing a synopsis of crowd sourcing research to date, a common definition for crowd sourcing, and a conceptual model for guiding future studies of crowd sourcing. We show how existing IS literature applies to the elements of that conceptual model: Problem, People (Problem Owner, Individual, and Crowd), Governance, Process, Technology, and Outcome. We close with suggestions for future research.},
booktitle = {Proceedings of the 2013 46th Hawaii International Conference on System Sciences},
pages = {579–588},
numpages = {10},
series = {HICSS '13}
}

@inproceedings{10.1109/IIKI.2015.60,
author = {Liu, Ziwei and Niu, Xiaoguang and Wei, Chuanbo and Huang, Zhen and Wu, Yunlong and Li, Hui},
title = {A Data-centric Cooperative Sensing Scheme in Crowdsourcing Systems},
year = {2015},
isbn = {9781467386371},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/IIKI.2015.60},
doi = {10.1109/IIKI.2015.60},
abstract = {In a densely deployed crowdsourcing system, data observed at neighboring participants often exhibit strong spatial correlation. Exploiting this property, one may put a portion of participants into low power sleep mode without compromising the quality of sensing or the connectivity of the network. In this work, two fundamental scheduling questions are considered: (a) how to select a maximum number of participants to be put into sleep mode so that the overall sensing data integrity is maintained above a given threshold, and (b) how to divide the participants into two "shifts" such that participants at different shifts will perform sensing alternatively while sensing data integrity is being maximized. For question (a), we propose a novel data centric approach to explicitly exploit data correlation among participants. We formulate this subset selection problem as a constrained optimization problem and propose an efficient polynomial time algorithm. For question (b), we formulate this set partitioning problem as a constrained mini-max optimization problem. We validate these algorithms using the New Library of Wuhan University data set and observe very satisfactory results.},
booktitle = {Proceedings of the 2015 International Conference on Identification, Information, and Knowledge in the Internet of Things (IIKI)},
pages = {250–253},
numpages = {4},
series = {IIKI '15}
}

@inproceedings{10.1109/BigData.Congress.2013.43,
author = {Roopa, T. and Iyer, Anantharaman Narayana and Rangaswamy, Shanta},
title = {CroTIS-Crowdsourcing Based Traffic Information System},
year = {2013},
isbn = {9780769550060},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/BigData.Congress.2013.43},
doi = {10.1109/BigData.Congress.2013.43},
abstract = {Road Traffic congestion is a perennial problem across the globe. The nature of the problems associated with vehicular traffic is not uniform across the globe. In the recent past, several solutions were proposed to improve traffic management. However most of these solutions cater to the traffic scenarios in the developed countries where traffic is lane based and the issues being encountered is quite different from that of developing countries. The existing solutions cannot be readily applied to the traffic situation of developing countries. In this paper, we propose an architecture which addresses the challenges of real time traffic management of non-lane based and chaotic traffic of developing countries. The big data collected for traffic analysis can be used to provide predictive analysis and patterns to enable the Traffic Department to manage the traffic better.},
booktitle = {Proceedings of the 2013 IEEE International Congress on Big Data},
pages = {271–277},
numpages = {7},
keywords = {Big data, Crowdsourcing, Intelligent Traffic systems, Smartphone},
series = {BIGDATACONGRESS '13}
}

@inproceedings{10.5555/2760440.2760728,
author = {Mahalingam, Usha and Manju, P. Leela},
title = {A Crowdsourcing Framework for Toll Plazas: Optimizing Delay at Toll Plazas},
year = {2014},
isbn = {9781479970025},
publisher = {IEEE Computer Society},
address = {USA},
abstract = {The aim of this paper is to propose an ideal and time saving method for vehicles to pass through toll plazas. Choosing the best tollbooth to move out of the toll plaza quickly, has become a tough task and is error prone. The proposed system uses crowdsourcing to maintain waiting time for each tollbooth, as a result travelers are provided with the best tollbooth in a toll plaza. This scheme reduces the waiting time in the queue, as a result, people can drive through toll plazas swiftly. Travelers passing the toll plaza act as the crowd that cooperate in choosing the best lane and help maintain the waiting time of each tollbooth and adjust queue parameters.},
booktitle = {Proceedings of the 2014 3rd International Conference on Eco-Friendly Computing and Communication Systems},
pages = {19–22},
numpages = {4},
keywords = {crowdsourcing, optimization, quality of experience, smart transport, sustainable growth, toll plaza, waiting time},
series = {ICECCS '14}
}

@inproceedings{10.1145/2970276.2970334,
author = {Peng, Xin and Gu, Jingxiao and Tan, Tian Huat and Sun, Jun and Yu, Yijun and Nuseibeh, Bashar and Zhao, Wenyun},
title = {CrowdService: serving the individuals through mobile crowdsourcing and service composition},
year = {2016},
isbn = {9781450338455},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2970276.2970334},
doi = {10.1145/2970276.2970334},
abstract = {Some user needs in real life can only be accomplished by leveraging the intelligence and labor of other people via crowdsourcing tasks. For example, one may want to confirm the validity of the description of a secondhand laptop by asking someone else to inspect the laptop on site. To integrate these crowdsourcing tasks into user applications, it is required that crowd intelligence and labor be provided as easily accessible services (e.g., Web services), which can be called crowd services. In this paper, we develop a framework named CROWDSERVICE which supplies crowd intelligence and labor as publicly accessible crowd services via mobile crowdsourcing. We implement the proposed framework on the Android platform and evaluate the usability of the framework with a user study.},
booktitle = {Proceedings of the 31st IEEE/ACM International Conference on Automated Software Engineering},
pages = {214–219},
numpages = {6},
keywords = {mobile crowdsourcing, reliability, service composition},
location = {Singapore, Singapore},
series = {ASE '16}
}

@inproceedings{10.1145/2346676.2346685,
author = {Xie, Yusheng and Cheng, Yu and Honbo, Daniel and Zhang, Kunpeng and Agrawal, Ankit and Choudhary, Alok},
title = {Crowdsourcing recommendations from social sentiment},
year = {2012},
isbn = {9781450315432},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2346676.2346685},
doi = {10.1145/2346676.2346685},
abstract = {In this paper, we investigate an innovative recommendation system by incorporating relevant social opinion and sentiment information. Our recommendation system, a powerful application of social sentiment analysis, differs from many existing models, which investigate the situation where the social network itself is structured to work with the product ranking and is specially built inside an e-commerce website. In contrast, our proposed system focuses on constructing and inferring product recommendations from external social network services (SNS) such as Facebook. In our system, we process product features in a finite-dimensional polynomial linear space. Additional components of our proposed system include an asymmetric similarity measurement and an asymmetric advantage measurement. We also show that our definitions for the two measurements include specific properties that reduce the computational overhead in the experiments. An important aspect of our modeling is to incorporate user-generated high-level semantic sentiment in the analysis. We apply our models to real time data and observe promising results for not only product recommendation but also job recommendation.},
booktitle = {Proceedings of the First International Workshop on Issues of Sentiment Discovery and Opinion Mining},
articleno = {9},
numpages = {8},
keywords = {business intelligence, crowdsourcing},
location = {Beijing, China},
series = {WISDOM '12}
}

@inproceedings{10.1145/2685553.2699324,
author = {Huang, Yi-Ching},
title = {Designing a Micro-Volunteering Platform for Situated Crowdsourcing},
year = {2015},
isbn = {9781450329460},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2685553.2699324},
doi = {10.1145/2685553.2699324},
abstract = {Situated crowdsourcing has emerged to overcome the limitations of online and mobile crowdsourcing to allow people to perform a task by embedding an interface in a physical space. However, crowdsourcing for non-profits is a challenge in situated crowdsourcing platform. My dissertation investigates whether micro-volunteering can be applied successfully to a situated crowdsourcing platform for contributing problem-solving efforts with high-quality results.},
booktitle = {Proceedings of the 18th ACM Conference Companion on Computer Supported Cooperative Work \&amp; Social Computing},
pages = {73–76},
numpages = {4},
keywords = {micro-tasking, micro-volunteering, situated crowdsourcing},
location = {Vancouver, BC, Canada},
series = {CSCW'15 Companion}
}

@inproceedings{10.4108/icst.collaboratecom.2012.250499,
author = {Allahbakhsh, M. and Ignjatovic, A. and Benatallah, B. and Beheshti, S. and Bertino, E. and Foo, N.},
title = {Reputation management in crowdsourcing systems},
year = {2012},
isbn = {9781467327404},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.4108/icst.collaboratecom.2012.250499},
doi = {10.4108/icst.collaboratecom.2012.250499},
abstract = {Worker selection is a significant and challenging issue in crowdsourcing systems. Such selection is usually based on an assessment of the reputation of the individual workers participating in such systems. However, assessing the credibility and adequacy of such calculated reputation is a real challenge. In this paper, we propose a reputation management model which leverages the values of the tasks completed, the credibility of the evaluators of the results of the tasks and time of evaluation of the results of these tasks in order to calculate more dependable quality metrics for workers and evaluators. The model has been implemented and experimentally validated.},
booktitle = {Proceedings of the 2012 8th International Conference on Collaborative Computing: Networking, Applications and Worksharing (CollaborateCom 2012)},
pages = {664–671},
numpages = {8},
series = {COLLABORATECOM '12}
}

@inproceedings{10.1007/978-3-642-31095-9_30,
author = {Khazankin, Roman and Schall, Daniel and Dustdar, Schahram},
title = {Predicting qos in scheduled crowdsourcing},
year = {2012},
isbn = {9783642310942},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-642-31095-9_30},
doi = {10.1007/978-3-642-31095-9_30},
abstract = {Crowdsourcing has emerged as a new paradigm for outsourcing simple for humans yet hard to automate tasks to an undefined network of people. Crowdsourcing platforms like Amazon Mechanical Turk provide scalability and flexibility for customers that need to get manifold similar independent jobs done. However, such platforms do not provide certain guarantees for their services regarding the expected job quality and the time of processing, although such guarantees are advantageous from the perspective of Business Process Management. In this paper, we consider an alternative architecture of a crowdsourcing platform, where the workers are assigned to tasks by the platform according to their availability and skills. We propose the technique for estimating accomplishable guarantees and negotiating Service Level Agreements in such an environment.},
booktitle = {Proceedings of the 24th International Conference on Advanced Information Systems Engineering},
pages = {460–472},
numpages = {13},
keywords = {QoS, SLA, crowdsourcing, negotiation, scheduling},
location = {Gda\'{n}sk, Poland},
series = {CAiSE'12}
}

@inproceedings{10.1109/ICDMW.2015.134,
author = {Liang, Tingting and Chen, Liang and Ying, Haochao and Zheng, Zibin and Wu, Jian},
title = {Crowdsourcing Based API Search via Leveraging Twitter Lists Information},
year = {2015},
isbn = {9781467384933},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/ICDMW.2015.134},
doi = {10.1109/ICDMW.2015.134},
abstract = {With the rapid growth of open APIs on the Internet, searching appropriate APIs for a given query becomes a challenging problem. General API search systems, such as ProgrammableWeb, usually can not provide satisfactory results of API search due to the simple keywords matching between queries and API information offered by providers (e.g. name and description). In this paper, we propose a crowdsourcing based search approach named CrowdAPS to effectively find the appropriate APIs. Specifically, CrowdAPS leverages Twitter lists, which is a tool used by individual users to organize accounts that interest them on semantics. List meta-data, including list name and description, is generated from collective intelligence and can be used by Latent Semantic Indexing (LSI) model to acquire semantic similarity between APIs and queries. Furthermore, CrowdAPS exploits list number to infer the popularity of APIs. The final search result relies on the integration of semantic similarity and popularity. Comprehensive experiment based on real-world datasets crawled from ProgrammableWeb and Twitter demonstrates the effectiveness of CrowdAPS.},
booktitle = {Proceedings of the 2015 IEEE International Conference on Data Mining Workshop (ICDMW)},
pages = {1540–1547},
numpages = {8},
series = {ICDMW '15}
}

@inproceedings{10.1109/IRI.2016.15,
author = {Jarrett, Julian and Blake, M. Brian},
title = {Reusable Meta-Models for Crowdsourcing Driven Elastic Systems (Invited Paper)},
year = {2016},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/IRI.2016.15},
doi = {10.1109/IRI.2016.15},
abstract = {Elastic systems utilize both human and machine working units to accomplish tasks that are eligible for crowdsourcing. The quality in the results of work completed by either type of computing unit is tantamount on the characteristics they bear. In this paper we draw parallels from our previous work into looking at the suitability of working units in completing viable tasks in crowdsourcing. We seek to understand characteristics for modeling tasks and workers within these types of systems. Based on our experiments and lessons learned in related literature, we propose a dynamic worker-task information meta-model with a corresponding operational workflow model that can be used in a variety of problem domains involving crowdsourced tasks to provide support in making this decision.},
booktitle = {2016 IEEE 17th International Conference on Information Reuse and Integration (IRI)},
pages = {50–57},
numpages = {8},
location = {Pittsburgh, PA, USA}
}

@inproceedings{10.1109/CCNC.2018.8319237,
author = {Liu, Zhan and Shabani, Shaban and Balet, Nicole Glassey and Sokhn, Maria and Cretton, Fabian},
title = {How to motivate participation and improve quality of crowdsourcing when building accessibility maps},
year = {2018},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/CCNC.2018.8319237},
doi = {10.1109/CCNC.2018.8319237},
abstract = {Crowdsourcing, as one of the most promising techniques for distributed problem-solving, requires sustained human involvement. Therefore, it also brings new challenges to data management, fundamentally data input and its quality. In this paper, we looked at various forms of user motivations and quality control of crowdsourcing when building accessibility maps mobile applications. We discuss how motivations could be used to contribute to our accessibility maps scenarios, and how data can be improved for two types of participants: individual participants and organization participants. We identified three useful techniques for improving data quality: qualification-based, reputation-based, and aggregation-based. In addition, based on our own mobile application (named WEMAP), we evaluated our approaches through focus group discussions and in-depth interviews.},
booktitle = {2018 15th IEEE Annual Consumer Communications \&amp; Networking Conference (CCNC)},
pages = {1–6},
numpages = {6},
location = {Las Vegas, NV, USA}
}

@inproceedings{10.5555/2021773.2021817,
author = {Sakamoto, Yasuaki and Tanaka, Yuko and Yu, Lixiu and Nickerson, Jeffrey V.},
title = {The crowdsourcing design space},
year = {2011},
isbn = {9783642218514},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {Crowdsourcing is a new kind of organizational structure, one that is conducive to large amounts of short parallel work: thousands of individuals may work for several minutes on tasks, their outputs aggregated into a useful product or service. The dimensions of this new organizational form are described. Areas for future research are identified, focusing on open-ended tasks and the coordination structures that might foster collective creativity.},
booktitle = {Proceedings of the 6th International Conference on Foundations of Augmented Cognition: Directing the Future of Adaptive Systems},
pages = {346–355},
numpages = {10},
keywords = {collective creativity, crowdsourcing, distributed cognition, human computation, organizational design, peer production},
location = {Orlando, FL},
series = {FAC'11}
}

@inproceedings{10.1109/UCC.2014.95,
author = {Smith, Ross and Kilty, Lori Ada},
title = {Crowdsourcing and Gamification of Enterprise Meeting Software Quality},
year = {2014},
isbn = {9781479978816},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/UCC.2014.95},
doi = {10.1109/UCC.2014.95},
abstract = {Microsoft has a 10+ year history with the application of gamification techniques to help crowd source quality related efforts. While this paper includes a brief review of previously published material about past success stories and lessons learned, the focus is on an autumn 2014 project designed to encourage employees to respond to enterprise online meeting software quality survey requests. The product team and IT department have come together to help improve meetings and improve the lives of people around the world. "Meeting Needs" is a proposed experiment that allows you to help us improve your Lync online meeting. At the same time, "Meeting Needs" builds in a charitable feature where participants are able to raise funds on behalf of a set of humanitarian agencies. Enterprise meeting quality is difficult to measure and there are many factors that make up a good meeting. Since the number of employees who typically reply to survey requests is generally low (often &lt; 30\%) there is a unique opportunity to create a feedback loop that will ensure that high quality and a higher rate of survey completion through the use of game mechanics and charitable giving elements.},
booktitle = {Proceedings of the 2014 IEEE/ACM 7th International Conference on Utility and Cloud Computing},
pages = {611–613},
numpages = {3},
keywords = {Enterprise software, Gamification, Productivity Games},
series = {UCC '14}
}

@inproceedings{10.1109/SOSE.2014.63,
author = {Chu, Chih-Han and Wan, Menghsi and Yang, Yufan and Gao, Jerry and Deng, Lei},
title = {Building On-demand Marketing SaaS for Crowdsourcing},
year = {2014},
isbn = {9781479936168},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/SOSE.2014.63},
doi = {10.1109/SOSE.2014.63},
abstract = {The concept of crowdsourcing has been introduced for years and greatly accepted by companies and enterprises. As opposed to current marketing systems, this paper designs a new marketing system by merging the idea of crowdsourcing into marketing information system (MkIS). Our purpose is to develop a cloud-based MkIS which serves as a platform and provides info publishing/tracking via various media and crowdsourcing features. To ensure accessibility and cost-efficiency, the product will be deployed as Software as a Service (SaaS), which will be capable of supporting large number of customers. The implementation of this idea is to construct a prototype system which targets enterprise and crowdsourcing service providers. The outcome has two deliverables. One is application server, which hosts the service API for web clients. Another is database server, which stores and replicates application data. The work serves as one of the first attempts for an information system flexible enough to adapt marketing, crowdsourcing, and other business workflows. Moreover, it shows the potential of the form of MkIS. This paper also presents the design and implementation of the system.},
booktitle = {Proceedings of the 2014 IEEE 8th International Symposium on Service Oriented System Engineering},
pages = {430–438},
numpages = {9},
keywords = {MkIS, SaaS, crowdsourcing, marketing information system, software as a service, workflow},
series = {SOSE '14}
}

@inproceedings{10.1145/2442657.2442661,
author = {Yuen, Man-Ching and King, Irwin and Leung, Kwong-Sak},
title = {Task recommendation in crowdsourcing systems},
year = {2012},
isbn = {9781450315579},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2442657.2442661},
doi = {10.1145/2442657.2442661},
abstract = {In crowdsourcing systems, tasks are distributed to networked people to complete such that a company's production cost can be greatly reduced. Obviously, it is not efficient that the amount of time for a worker spent on selecting a task is comparable with that spent on working on a task, but the monetary reward of a task is just a small amount. The available worker history makes it possible to mine workers' preference on tasks and to provide favorite recommendations. Our exploratory study on the survey results collected from Amazon Mechanical Turk (MTurk) shows that workers' histories can reflect workers' preferences on tasks in crowdsourcing systems. Task recommendation can help workers to find their right tasks faster as well as help requesters to receive good quality output quicker. However, previously proposed classification based task recommendation approach only considers worker performance history, but does not explore worker task searching history. In our paper, we propose a task recommendation framework for task preference modeling and preference-based task recommendation, aiming to recommend tasks to workers who are likely to prefer to work on and provide output that accepted by requesters. We consider both worker performance history and worker task searching history to reflect workers' task preference more accurately. To the best of our knowledge, we are the first to use matrix factorization for task recommendation in crowdsourcing systems.},
booktitle = {Proceedings of the First International Workshop on Crowdsourcing and Data Mining},
pages = {22–26},
numpages = {5},
keywords = {crowdsourcing, matrix factorization, task recommendation},
location = {Beijing, China},
series = {CrowdKDD '12}
}

@inproceedings{10.1109/IECON.2019.8927134,
author = {Nie, Yan and Xu, Kun and Chen, Haoyan and Peng, Lei},
title = {Crowd-parking: A New Idea of Parking Guidance Based on Crowdsourcing of Parking Location Information from Automobiles},
year = {2019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/IECON.2019.8927134},
doi = {10.1109/IECON.2019.8927134},
abstract = {City-wide parking guidance system (CPGS) is the emerging infrastructure of intelligent transportation systems in China. The most difficulty in CPGS development is the lack of parking lots data. Without these data, the traditional well-designed parking guidance algorithms will fail. Meanwhile, the real-time location information of automobiles is increasing and can be accessed easily thanks to the popularity of smartphone and smart vehicular devices. In this paper, we propose a new idea of guiding automobiles to proper parking lots by using the data collected from the vehicular Global Positioning System (GPS) device. Then we build a spatial-temporal classifier based on convolutional neural network (CNN) with long-short term memory (LSTM) to learn the parking experience from the automobiles uploading the parking locations. Finally, we use the classifier to recommend the proper parking lots nearby to vehicles, considering the related driving context. The experiment shows the proposed method can guide automobiles to the target parking lots accurately even if we do not know the real-time data of the target parking lots. Moreover, the classifier shows the obvious spatial characteristics and preference for the first time when guiding vehicles. This method is a kind of crowdsourcing and encourages automobiles to share their parking experience, to help park easier for themselves eventually. So, we think the method, crowd-parking, is very novel and competitive in the current state of endless business and technique negotiation among parking lots.},
booktitle = {IECON 2019 - 45th Annual Conference of the IEEE Industrial Electronics Society},
pages = {2779–2784},
numpages = {6},
location = {Lisbon, Portugal}
}

@inproceedings{10.1145/3244827,
author = {Kumara, Soundar},
title = {Session details: Crowdsourcing landscape},
year = {2011},
isbn = {9781450309271},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3244827},
doi = {10.1145/3244827},
booktitle = {Proceedings of the 2nd International Workshop on Ubiquitous Crowdsouring},
location = {Beijing, China},
series = {UbiCrowd '11}
}

@inproceedings{10.1145/2685553.2702671,
author = {LaLone, Nicolas and Tapia, Andrea and MacDonald, Elizabeth and Case, Nathan and Hall, Michelle and Clayton, Jessica and Heavner, Matthew J.},
title = {Harnessing Twitter and Crowdsourcing to Augment Aurora Forecasting},
year = {2015},
isbn = {9781450329460},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2685553.2702671},
doi = {10.1145/2685553.2702671},
abstract = {The aurora borealis and aurora australis are beautiful space weather driven events whose sighting is typically based on luck given that forecasting is not spatially or temporally precise. To help increase the accuracy and timeliness of auroral forecasting, we have designed a multi-faceted system called Aurorasaurus. This system allows crisis management specialists to test reactions to rare event notifications, space weather scientists to get direct sighting information of auroras (complete with pictures), and science education researchers to evaluate the impact of educational materials about the aurora and the physics surrounding this unique phenomenon. Through manual tweet verification and directly reported aurora borealis or aurora australis sightings, everyday users help make space weather and aurora forecasting more accurate.},
booktitle = {Proceedings of the 18th ACM Conference Companion on Computer Supported Cooperative Work \&amp; Social Computing},
pages = {9–12},
numpages = {4},
keywords = {aurora borealis, crisis, disaster, early warning systems, social media, twitter},
location = {Vancouver, BC, Canada},
series = {CSCW'15 Companion}
}

@inproceedings{10.1109/ASEW.2015.22,
author = {Saremi, Razieh Lotfalian and Yang, Ye},
title = {Empirical Analysis on Parallel Tasks in Crowdsourcing Software Development},
year = {2015},
isbn = {9781467397759},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/ASEW.2015.22},
doi = {10.1109/ASEW.2015.22},
abstract = {Crowdsourcing has become a popular option for rapid acquisition, with reported benefits such as shortened schedule due to mass parallel development, innovative solutions based on the "wisdom of crowds", and reduced cost due to the pre-pricing and bidding effects. However, most of existing studies on software crowdsourcing are focusing on individual task level, providing limited insights on the practice as well as outcomes at overall project level. To develop better understanding of crowdsourcing-based software projects, this paper reports an empirical study on analyzing four largest projects on Topcoder platform that intensively leverage crowdsourcing throughout the product implementation, testing, and assembly phases. The analysis results conclude that: (1) crowdsourcing task scheduling follows typical patterns including prototyping, component development, bug hunt, and assembly and coding (2) budget phase distribution patterns does not following traditional patterns, and uploading task rate is not representing same budget rate associated with them as about 75\% of uploaded tasks would price under 67\% of total project budget, (3) Higher degree of parallelism would lead to higher demand for competing on tasks and shorter planning schedule to complete the project consequently better resource allocation.},
booktitle = {Proceedings of the 2015 30th IEEE/ACM International Conference on Automated Software Engineering Workshop (ASEW)},
pages = {28–34},
numpages = {7},
series = {ASEW '15}
}

@inproceedings{10.1145/2806416.2806460,
author = {Davtyan, Martin and Eickhoff, Carsten and Hofmann, Thomas},
title = {Exploiting Document Content for Efficient Aggregation of Crowdsourcing Votes},
year = {2015},
isbn = {9781450337946},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2806416.2806460},
doi = {10.1145/2806416.2806460},
abstract = {The use of crowdsourcing for document relevance assessment has been found to be a viable alternative to corpus annotation by highly trained experts. The question of quality control is a recurring challenge that is often addressed by aggregating multiple individual assessments of the same topic-document pair from independent workers. In the past, such aggregation schemes have been weighted or filtered by estimates of worker reliability based on a multitude of behavioral features. In this paper, we propose an alternative approach by relying on document information. Inspired by the clustering hypothesis of information retrieval, we assume textually similar documents to show similar degrees of relevance towards a given topic. Following up on this intuition, we propagate crowd-generated relevance judgments to similar documents, effectively smoothing the distribution of relevance labels across the similarity space.Our experiments are based on TREC Crowdsourcing Track data and show that even simple aggregation methods utilizing document similarity information significantly improve over majority voting in terms of accuracy as well as cost efficiency.},
booktitle = {Proceedings of the 24th ACM International on Conference on Information and Knowledge Management},
pages = {783–790},
numpages = {8},
keywords = {clustering hypothesis, crowdsourcing, relevance assessment},
location = {Melbourne, Australia},
series = {CIKM '15}
}

@inproceedings{10.5555/2018966.2018975,
author = {Rumshisky, Anna},
title = {Crowdsourcing word sense definition},
year = {2011},
isbn = {9781932432930},
publisher = {Association for Computational Linguistics},
address = {USA},
abstract = {In this paper, we propose a crowdsourcing methodology for a single-step construction of both an empirically-derived sense inventory and the corresponding sense-annotated corpus. The methodology taps the intuitions of non-expert native speakers to create an expert-quality resource, and natively lends itself to supplementing such a resource with additional information about the structure and reliability of the produced sense inventories. The resulting resource will provide several ways to empirically measure distances between related word senses, and will explicitly address the question of fuzzy boundaries between them.},
booktitle = {Proceedings of the 5th Linguistic Annotation Workshop},
pages = {74–81},
numpages = {8},
location = {Portland, Oregon},
series = {LAW V '11}
}

@inproceedings{10.1109/SMC.2019.8914041,
author = {Murata, Shingo and Yanagida, Hikaru and Katahira, Kentaro and Suzuki, Shinsuke and Ogata, Tetsuya and Yamashita, Yuichi},
title = {Large-scale Data Collection for Goal-directed Drawing Task with Self-report Psychiatric Symptom Questionnaires via Crowdsourcing},
year = {2019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/SMC.2019.8914041},
doi = {10.1109/SMC.2019.8914041},
abstract = {Drawing is a representative human cognitive ability and may mirror cognitive characteristics including those associated with psychiatric symptoms. Therefore, analysis of drawing data collected from various populations such as healthy people and psychiatric patients may be beneficial for better understanding human cognition. However, collecting such large-scale data about the relationship between drawing and cognitive/personality traits offline—in a laboratory—is a difficult issue. To overcome this issue, we devised a novel experimental paradigm involving a goal-directed drawing task conducted online—on the eb—with participants recruited via a crowdsourcing platform. With the assistance of 1155 participants with differing levels of psychiatric symptoms, we collected a total of 194,040 trajectory data and answers to seven different self-report psychiatric symptom questionnaires comprising 181 items. We visualized the collected trajectory data and performed an exploratory factor analysis on the correlation matrix of the psychiatric symptom questionnaire items. Our results suggest that there were associations between psychiatric symptoms represented by specific psychiatric factors and atypical behavior observed while performing the goal-directed drawing task. This indicates the efficacy of a dimensional approach to large-scale online experiments with respect to clinical psychiatry.},
booktitle = {2019 IEEE International Conference on Systems, Man and Cybernetics (SMC)},
pages = {3859–3865},
numpages = {7},
location = {Bari, Italy}
}

@inproceedings{10.1145/3250113,
author = {Parikh, Tapan},
title = {Session details: Papers: crowdsourcing: people power},
year = {2013},
isbn = {9781450318990},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3250113},
doi = {10.1145/3250113},
booktitle = {Proceedings of the SIGCHI Conference on Human Factors in Computing Systems},
location = {Paris, France},
series = {CHI '13}
}

@inproceedings{10.1145/2348543.2348567,
author = {Yang, Dejun and Xue, Guoliang and Fang, Xi and Tang, Jian},
title = {Crowdsourcing to smartphones: incentive mechanism design for mobile phone sensing},
year = {2012},
isbn = {9781450311595},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2348543.2348567},
doi = {10.1145/2348543.2348567},
abstract = {Mobile phone sensing is a new paradigm which takes advantage of the pervasive smartphones to collect and analyze data beyond the scale of what was previously possible. In a mobile phone sensing system, the platform recruits smartphone users to provide sensing service. Existing mobile phone sensing applications and systems lack good incentive mechanisms that can attract more user participation. To address this issue, we design incentive mechanisms for mobile phone sensing. We consider two system models: the platform-centric model where the platform provides a reward shared by participating users, and the user-centric model where users have more control over the payment they will receive. For the platform-centric model, we design an incentive mechanism using a Stackelberg game, where the platform is the leader while the users are the followers. We show how to compute the unique Stackelberg Equilibrium, at which the utility of the platform is maximized, and none of the users can improve its utility by unilaterally deviating from its current strategy. For the user-centric model, we design an auction-based incentive mechanism, which is computationally efficient, individually rational, profitable, and truthful. Through extensive simulations, we evaluate the performance and validate the theoretical properties of our incentive mechanisms.},
booktitle = {Proceedings of the 18th Annual International Conference on Mobile Computing and Networking},
pages = {173–184},
numpages = {12},
keywords = {crowdsourcing, incentive mechanism design, mobile phone sensing},
location = {Istanbul, Turkey},
series = {Mobicom '12}
}

@inproceedings{10.5555/2540128.2540538,
author = {Le Bras, Ronan and Bernstein, Richard and Gomes, Carla P. and Selman, Bart and Van Dover, R. Bruce},
title = {Crowdsourcing backdoor identification for combinatorial optimization},
year = {2013},
isbn = {9781577356332},
publisher = {AAAI Press},
abstract = {We will show how human computation insights can be key to identifying so-called backdoor variables in combinatorial optimization problems. Backdoor variables can be used to obtain dramatic speedups in combinatorial search. Our approach leverages the complementary strength of human input, based on a visual identification of problem structure, crowdsourcing, and the power of combinatorial solvers to exploit complex constraints. We describe our work in the context of the domain of materials discovery. The motivation for considering the materials discovery domain comes from the fact that new materials can provide solutions for key challenges in sustainability, e.g., in energy, new catalysts for more efficient fuel cell technology.},
booktitle = {Proceedings of the Twenty-Third International Joint Conference on Artificial Intelligence},
pages = {2840–2847},
numpages = {8},
location = {Beijing, China},
series = {IJCAI '13}
}

@inproceedings{10.1145/2566486.2567989,
author = {Venanzi, Matteo and Guiver, John and Kazai, Gabriella and Kohli, Pushmeet and Shokouhi, Milad},
title = {Community-based bayesian aggregation models for crowdsourcing},
year = {2014},
isbn = {9781450327442},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2566486.2567989},
doi = {10.1145/2566486.2567989},
abstract = {This paper addresses the problem of extracting accurate labels from crowdsourced datasets, a key challenge in crowdsourcing. Prior work has focused on modeling the reliability of individual workers, for instance, by way of confusion matrices, and using these latent traits to estimate the true labels more accurately. However, this strategy becomes ineffective when there are too few labels per worker to reliably estimate their quality. To mitigate this issue, we propose a novel community-based Bayesian label aggregation model, CommunityBCC, which assumes that crowd workers conform to a few different types, where each type represents a group of workers with similar confusion matrices. We assume that each worker belongs to a certain community, where the worker's confusion matrix is similar to (a perturbation of) the community's confusion matrix. Our model can then learn a set of key latent features: (i) the confusion matrix of each community, (ii) the community membership of each user, and (iii) the aggregated label of each item. We compare the performance of our model against established aggregation methods on a number of large-scale, real-world crowdsourcing datasets. Our experimental results show that our CommunityBCC model consistently outperforms state-of-the-art label aggregation methods, requiring, on average, 50\% less data to pass the 90\% accuracy mark.},
booktitle = {Proceedings of the 23rd International Conference on World Wide Web},
pages = {155–164},
numpages = {10},
keywords = {bayesian inference, community detection, crowdsourcing},
location = {Seoul, Korea},
series = {WWW '14}
}

@inproceedings{10.1007/978-3-319-64248-2_14,
author = {Sakamoto, Mizuki and Gushima, Kota and Alexandrova, Todorka and Nakajima, Tatsuo},
title = {Designing Human Behavior Through Social Influence in Mobile Crowdsourcing with Micro-communities},
year = {2017},
isbn = {978-3-319-64247-5},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-319-64248-2_14},
doi = {10.1007/978-3-319-64248-2_14},
abstract = {This paper proposes a new mobile social media infrastructure for motivating collective people to participate in flourishing our society. For motivating them, designing human behavior through social influence within communities is one of the most important issues to make their lifestyle better, but it is not easy to promote the entire collective people’s activities towards achieving a common goal. Our proposal is to use a layered approach where an entire community consists of many micro-communities; if each independent community is encouraged to contribute to its society, the entire community will be finally motivated to achieve a flourishing society. Our approach adopts a virtual currency and a crowdfunding concept to encourage members in a micro-community; we analyze the effect of social influence on human behavior from the experiment-based analysis. The analysis shows that the proposed approach might work well within a community of well-known people. We finally suggest a possible solution to overcome potential limitations as a future direction.},
booktitle = {Electronic Government and the Information Systems Perspective: 6th International Conference, EGOVIS 2017, Lyon, France, August 28-31, 2017, Proceedings},
pages = {189–205},
numpages = {17},
keywords = {Human behavior, Social influence, Mobile crowdsourcing, Micro-level crowdfunding},
location = {Lyon, France}
}

@inproceedings{10.1145/2465529.2465761,
author = {Karger, David R. and Oh, Sewoong and Shah, Devavrat},
title = {Efficient crowdsourcing for multi-class labeling},
year = {2013},
isbn = {9781450319003},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2465529.2465761},
doi = {10.1145/2465529.2465761},
abstract = {Crowdsourcing systems like Amazon's Mechanical Turk have emerged as an effective large-scale human-powered platform for performing tasks in domains such as image classification, data entry, recommendation, and proofreading. Since workers are low-paid (a few cents per task) and tasks performed are monotonous, the answers obtained are noisy and hence unreliable. To obtain reliable estimates, it is essential to utilize appropriate inference algorithms (e.g. Majority voting) coupled with structured redundancy through task assignment. Our goal is to obtain the best possible trade-off between reliability and redundancy. In this paper, we consider a general probabilistic model for noisy observations for crowd-sourcing systems and pose the problem of minimizing the total price (i.e. redundancy) that must be paid to achieve a target overall reliability. Concretely, we show that it is possible to obtain an answer to each task correctly with probability 1-ε as long as the redundancy per task is O((K/q) log (K/ε)), where each task can have any of the $K$ distinct answers equally likely, q is the crowd-quality parameter that is defined through a probabilistic model. Further, effectively this is the best possible redundancy-accuracy trade-off any system design can achieve. Such a single-parameter crisp characterization of the (order-)optimal trade-off between redundancy and reliability has various useful operational consequences. Further, we analyze the robustness of our approach in the presence of adversarial workers and provide a bound on their influence on the redundancy-accuracy trade-off.Unlike recent prior work [GKM11, KOS11, KOS11], our result applies to non-binary (i.e. K&gt;2) tasks. In effect, we utilize algorithms for binary tasks (with inhomogeneous error model unlike that in [GKM11, KOS11, KOS11]) as key subroutine to obtain answers for K-ary tasks. Technically, the algorithm is based on low-rank approximation of weighted adjacency matrix for a random regular bipartite graph, weighted according to the answers provided by the workers.},
booktitle = {Proceedings of the ACM SIGMETRICS/International Conference on Measurement and Modeling of Computer Systems},
pages = {81–92},
numpages = {12},
keywords = {crowdsourcing, human computation, low-rank matrix, random graphs},
location = {Pittsburgh, PA, USA},
series = {SIGMETRICS '13}
}

@article{10.1145/2494232.2465761,
author = {Karger, David R. and Oh, Sewoong and Shah, Devavrat},
title = {Efficient crowdsourcing for multi-class labeling},
year = {2013},
issue_date = {June 2013},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {41},
number = {1},
issn = {0163-5999},
url = {https://doi.org/10.1145/2494232.2465761},
doi = {10.1145/2494232.2465761},
abstract = {Crowdsourcing systems like Amazon's Mechanical Turk have emerged as an effective large-scale human-powered platform for performing tasks in domains such as image classification, data entry, recommendation, and proofreading. Since workers are low-paid (a few cents per task) and tasks performed are monotonous, the answers obtained are noisy and hence unreliable. To obtain reliable estimates, it is essential to utilize appropriate inference algorithms (e.g. Majority voting) coupled with structured redundancy through task assignment. Our goal is to obtain the best possible trade-off between reliability and redundancy. In this paper, we consider a general probabilistic model for noisy observations for crowd-sourcing systems and pose the problem of minimizing the total price (i.e. redundancy) that must be paid to achieve a target overall reliability. Concretely, we show that it is possible to obtain an answer to each task correctly with probability 1-ε as long as the redundancy per task is O((K/q) log (K/ε)), where each task can have any of the $K$ distinct answers equally likely, q is the crowd-quality parameter that is defined through a probabilistic model. Further, effectively this is the best possible redundancy-accuracy trade-off any system design can achieve. Such a single-parameter crisp characterization of the (order-)optimal trade-off between redundancy and reliability has various useful operational consequences. Further, we analyze the robustness of our approach in the presence of adversarial workers and provide a bound on their influence on the redundancy-accuracy trade-off.Unlike recent prior work [GKM11, KOS11, KOS11], our result applies to non-binary (i.e. K&gt;2) tasks. In effect, we utilize algorithms for binary tasks (with inhomogeneous error model unlike that in [GKM11, KOS11, KOS11]) as key subroutine to obtain answers for K-ary tasks. Technically, the algorithm is based on low-rank approximation of weighted adjacency matrix for a random regular bipartite graph, weighted according to the answers provided by the workers.},
journal = {SIGMETRICS Perform. Eval. Rev.},
month = jun,
pages = {81–92},
numpages = {12},
keywords = {crowdsourcing, human computation, low-rank matrix, random graphs}
}

@inproceedings{10.1109/SOSE.2015.53,
author = {Aldhahri, Eman and Shandilya, Vivek and Shiva, Sajjan},
title = {Towards an Effective Crowdsourcing Recommendation System: A Survey of the State-of-the-Art},
year = {2015},
isbn = {9781479983568},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/SOSE.2015.53},
doi = {10.1109/SOSE.2015.53},
abstract = {Crowdsourcing is an approach where requesters can call for workers with different capabilities to process a task for monetary reward. With the vast amount of tasks posted every day, satisfying workers, requesters, and service providers--who are the stakeholders of any crowdsourcing system--is critical to its success. To achieve this, the system should address three objectives: (1) match the worker with a suitable task that fits the worker's interests and skills, and raise the worker's rewards; (2) give requesters more qualified solutions with lower cost and time; and (3) raise the accepted tasks rate which will raise the aggregated commissions accordingly. For these objectives, we present a critical study of the state-of-the-art in recommendation systems that are ubiquitous among crowdsourcing and other online systems to highlight the potential of the best approaches which could be applied in a crowdsourcing system, and highlight the shortcomings in the existing crowdsourcing recommendation systems that should be addressed.},
booktitle = {Proceedings of the 2015 IEEE Symposium on Service-Oriented System Engineering},
pages = {372–377},
numpages = {6},
keywords = {Crowdsourcing, Survey, recommendation, task matching},
series = {SOSE '15}
}

@inproceedings{10.1145/3025453.3026032,
author = {Huang, Yun and Huang, Yifeng and Xue, Na and Bigham, Jeffrey P.},
title = {Leveraging Complementary Contributions of Different Workers for Efficient Crowdsourcing of Video Captions},
year = {2017},
isbn = {9781450346559},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3025453.3026032},
doi = {10.1145/3025453.3026032},
abstract = {Hearing-impaired people and non-native speakers rely on captions for access to video content, yet most videos remain uncaptioned or have machine-generated captions with high error rates. In this paper, we present the design, implementation and evaluation of BandCaption, a system that combines automatic speech recognition with input from crowd workers to provide a cost-efficient captioning solution for accessible online videos. We consider four stakeholder groups as our source of crowd workers: (i) individuals with hearing impairments, (ii) second-language speakers with low proficiency, (iii) second-language speakers with high proficiency, and (iv) native speakers. Each group has different abilities and incentives, which our workflow leverages. Our findings show that BandCaption enables crowd workers who have different needs and strengths to accomplish micro-tasks and make complementary contributions. Based on our results, we outline opportunities for future research and provide design suggestions to deliver cost-efficient captioning solutions.},
booktitle = {Proceedings of the 2017 CHI Conference on Human Factors in Computing Systems},
pages = {4617–4626},
numpages = {10},
keywords = {complementary contributions, crowdsourcing, video caption},
location = {Denver, Colorado, USA},
series = {CHI '17}
}

@inproceedings{10.5555/2936924.2937172,
author = {Bhat, Satyanath and Padmanabhan, Divya and Jain, Shweta and Narahari, Yadati},
title = {A Truthful Mechanism with Biparameter Learning for Online Crowdsourcing: (Extended Abstract)},
year = {2016},
isbn = {9781450342391},
publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
address = {Richland, SC},
booktitle = {Proceedings of the 2016 International Conference on Autonomous Agents \&amp; Multiagent Systems},
pages = {1385–1386},
numpages = {2},
keywords = {mechanism design, strategic agents},
location = {Singapore, Singapore},
series = {AAMAS '16}
}

@inproceedings{10.1145/2987386.2987413,
author = {Alabduljabbar, Reham and Al-Dossari, Hmood},
title = {A Task Ontology-based Model for Quality Control in Crowdsourcing Systems},
year = {2016},
isbn = {9781450344555},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2987386.2987413},
doi = {10.1145/2987386.2987413},
abstract = {In the era of big data, a vast amount of data is created every day. Crowdsourcing systems have recently gained significance as an interesting practice in managing and performing big data operations. Crowdsourcing has facilitated the process of performing tasks that cannot be adequately solved by machines including image labeling, transcriptions, data validation and sentiment analysis. However, quality control remains one of the biggest challenges for crowdsourcing. Current crowdsourcing systems use the same quality control mechanism for evaluating different types of tasks. In this paper, we argue that quality mechanisms vary by task type. We propose a task ontology-based model to identify the most appropriate quality mechanism for a given task. The proposed model has been enriched by a reputation system to collect requesters' feedback on quality mechanisms. Accordingly, the reputation of each mechanism can be established and used for mapping between tasks and mechanisms. Description of the model's framework, algorithms, and its components' interaction are presented.},
booktitle = {Proceedings of the International Conference on Research in Adaptive and Convergent Systems},
pages = {22–28},
numpages = {7},
keywords = {Big Data, Crowd Computing, Crowdsourcing, HITs, Human Computation, MTurk, Ontology, Quality Control, Reputation},
location = {Odense, Denmark},
series = {RACS '16}
}

@inproceedings{10.1109/HotWeb.2015.9,
author = {Jarrett, Julian and Silva, Larissa Ferreira da and Mello, Laerte and Andere, Sadallo and Cruz, Gustavo and Blake, M. Brian},
title = {Self-Generating a Labor Force for Crowdsourcing: Is Worker Confidence a Predictor of Quality?},
year = {2015},
isbn = {9781467396882},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/HotWeb.2015.9},
doi = {10.1109/HotWeb.2015.9},
abstract = {When leveraging the crowd to perform complex tasks, it is imperative to identify the most effective worker for a particular job. Demographic profiles provided by workers, skill self-assessments by workers, and past performance as captured by employers all represent viable data points available within labor markets. Employers often question the validity of a worker's self-assessment of skills and expertise level when selecting workers in context of other information. More specifically, employers would like to answer the question, "Is worker confidence a predictor of quality?" In this paper, we discuss the state-of-the-art in recommending crowd workers based on assessment information. A major contribution of our work is an architecture, platform, and push/pull process for categorizing and recommending workers based on available self-assessment information. We present a study exploring the validity of skills input by workers in light of their actual performance and other metrics captured by employers. A further contribution of this approach is the extrapolation of a body of workers to describe the nature of the community more broadly. Through experimentation, within the language-processing domain, we demonstrate a new capability of deriving trends that might help future employers to select appropriate workers.},
booktitle = {Proceedings of the 2015 Third IEEE Workshop on Hot Topics in Web Systems and Technologies (HotWeb)},
pages = {85–90},
numpages = {6},
series = {HOTWEB '15}
}

@inproceedings{10.1145/3406865.3418572,
author = {Qiu, Sihang and Gadiraju, Ujwal and Bozzon, Alessandro},
title = {TickTalkTurk: Conversational Crowdsourcing Made Easy},
year = {2020},
isbn = {9781450380591},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3406865.3418572},
doi = {10.1145/3406865.3418572},
abstract = {This demo presents TickTalkTurk, a tool that can assist task requesters in quickly deploying crowdsourcing tasks in a customizable conversational worker interface. The conversational worker interface can convey task instructions, deploy microtasks, and gather worker input in a dialogue-based workflow. The interface is implemented as a Web-based application, which makes it compatible with popular crowdsourcing platforms. The tool we developed is demonstrated through two microtask crowdsourcing examples with different task types. Results reveal that our conversational worker interface is capable of better engaging workers and analyzing workers performance.},
booktitle = {Companion Publication of the 2020 Conference on Computer Supported Cooperative Work and Social Computing},
pages = {53–57},
numpages = {5},
keywords = {chatbot, conversational agent, conversational interface, microtask crowdsourcing},
location = {Virtual Event, USA},
series = {CSCW '20 Companion}
}

@inproceedings{10.5555/2857070.2857138,
author = {Erez, Eden S. and Zhitomirsky-Geffet, Maayan and Bar-Ilan, Judit},
title = {Subjective vs. objective evaluation of ontological statements with crowdsourcing},
year = {2015},
isbn = {087715547X},
publisher = {American Society for Information Science},
address = {USA},
abstract = {In this paper we propose and test a methodology for evaluation of statements of a multi-viewpoint ontology by crowdsourcing. The task for the workers was to assess each of the given statement as true statements, controversial viewpoint statement or error. Typically, in crowdsourcing experiments the workers are asked for their personal opinions on the given subject. However, in our case their ability to objectively assess others' opinions is examined as well. We conducted two large-scale crowdsourcing experiments with about 750 ontological statements originating from diverse single-viewpoint ontologies. Our results show substantially higher accuracy in evaluation for the objective assessment approach compared to the experiment based on personal opinions.},
booktitle = {Proceedings of the 78th ASIS&amp;T Annual Meeting: Information Science with Impact: Research in and for the Community},
articleno = {68},
numpages = {4},
keywords = {crowdsourcing, multi-viewpoint ontology, ontology statement classification},
location = {St. Louis, Missouri},
series = {ASIST '15}
}

@inproceedings{10.1109/SOSE.2014.79,
author = {Hu, Zhenghui and Wu, Wenjun},
title = {A Game Theoretic Model of Software Crowdsourcing},
year = {2014},
isbn = {9781479936168},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/SOSE.2014.79},
doi = {10.1109/SOSE.2014.79},
abstract = {With the rapid development of crowdsourcing all over the world, crowdsourcing for software engineering begins to attract more and more considerable attentions from software developers, coders and researchers. And many successful online platforms such as TopCoder has demonstrated crowdsourcing's capability and potential for supporting various software development activities. In order to study the competitive behaviours for software crowdsourcing, we apply the famous game theory to model the 2-player algorithm challenges on TopCoder. And as all the participants are aware of other players' information and they make decisions almost simultaneously, this article adopts the theory of static games with complete information. Through Nash equilibria computing, we find that the value of successfully challenging probability can be used to deduce specific competitive decisions of coders in the algorithm challenge stage. Specifically, if a coder's probability to make a successful challenge exceeds some certain value, then he will always choose to challenge. The paper provides a new research perspective for software engineering crowdsourcing, and empirical research will be done in the next step of work.},
booktitle = {Proceedings of the 2014 IEEE 8th International Symposium on Service Oriented System Engineering},
pages = {446–453},
numpages = {8},
series = {SOSE '14}
}

@inproceedings{10.1145/2493432.2493481,
author = {Goncalves, Jorge and Ferreira, Denzil and Hosio, Simo and Liu, Yong and Rogstadius, Jakob and Kukka, Hannu and Kostakos, Vassilis},
title = {Crowdsourcing on the spot: altruistic use of public displays, feasibility, performance, and behaviours},
year = {2013},
isbn = {9781450317702},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2493432.2493481},
doi = {10.1145/2493432.2493481},
abstract = {This study is the first attempt to investigate altruistic use of interactive public displays in natural usage settings as a crowdsourcing mechanism. We test a non-paid crowdsourcing service on public displays with eight different motivation settings and analyse users' behavioural patterns and crowdsourcing performance (e.g., accuracy, time spent, tasks completed). The results show that altruistic use, such as for crowdsourcing, is feasible on public displays, and through the controlled use of motivational design and validation check mechanisms, performance can be improved. The results shed insights on three research challenges in the field: i) how does crowdsourcing performance on public displays compare to that of online crowdsourcing, ii) how to improve the quality of feedback collected from public displays which tends to be noisy, and iii) identify users' behavioural patterns towards crowdsourcing on public displays in natural usage settings.},
booktitle = {Proceedings of the 2013 ACM International Joint Conference on Pervasive and Ubiquitous Computing},
pages = {753–762},
numpages = {10},
keywords = {altruism, crowdsourcing, motivation, public displays},
location = {Zurich, Switzerland},
series = {UbiComp '13}
}

@inproceedings{10.1145/2254129.2254133,
author = {Simperl, Elena},
title = {Crowdsourcing semantic data management: challenges and opportunities},
year = {2012},
isbn = {9781450309158},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2254129.2254133},
doi = {10.1145/2254129.2254133},
abstract = {Linked Data refers to a set of guidelines and best practices for publishing and accessing structured data on the Web. It builds upon established Web technologies, in particular HTTP and URIs, extended with Semantic Web representation formats and protocols such as RDF, RDFS, OWL and SPARQL, by which data from different sources can be shared, interconnected and used beyond the application scenarios for which it was originally created. RDF is a central building block of the Linked Data technology stack. It is a graph-based data model based on the idea of making statements about (information and non-information) resources on the Web in terms of triples of the form subject predicate object. The object of any RDF triple may be used in the subject position in other triples, leading to a directed, labeled graph typically referred to as an 'RDF graph'. Both nodes and edges in such graphs are identified via URIs; nodes represent Web resources, while edges stand for attributes of such resources or properties connecting them. Schema information can be expressed using languages such as RDFS and OWL, by which resources can be typed as classes described in terms of domain-specific attributes, properties and constraints. RDF graphs can be natively queried using the query language SPARQL. A SPARQL query is composed of graph patterns and can be stored as RDF triples together with any RDF domain model using SPIN to facilitate the definition of constraints and inference rules in ontologies.},
booktitle = {Proceedings of the 2nd International Conference on Web Intelligence, Mining and Semantics},
articleno = {1},
numpages = {3},
location = {Craiova, Romania},
series = {WIMS '12}
}

@inproceedings{10.1145/2534142.2534151,
author = {Yen, Yu-Chuan and Chu, Cing-Yu and Yeh, Su-Ling and Chu, Hao-Hua and Huang, Polly},
title = {Lab experiment vs. crowdsourcing: a comparative user study on Skype call quality},
year = {2013},
isbn = {9781450324519},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2534142.2534151},
doi = {10.1145/2534142.2534151},
abstract = {To deliver voice over the Internet in a cost-effective way, it is essential to quantify the quality of user experience (i.e., QoE) of a voice service at various provisioning levels. Conducting user studies is an inevitable step facilitating quantitative studies of QoE. The two experimental methods -- lab experiment vs. crowdsourcing via Amazon Mechanical Turk [1] -- are compared in this study. We find that, for the study of Skype call quality, the crowdsourcing approach stands out in terms of efficiency and user diversity, which in turn strengthens the robustness and the depth of the analysis.},
booktitle = {Proceedings of the 9th Asian Internet Engineering Conference},
pages = {65–72},
numpages = {8},
keywords = {QoE, Skype, VoIP, mechanical turk, rate control},
location = {Chiang Mai, Thailand},
series = {AINTEC '13}
}

@inproceedings{10.5555/2486788.2486963,
author = {Mao, Ke and Yang, Ye and Li, Mingshu and Harman, Mark},
title = {Pricing crowdsourcing-based software development tasks},
year = {2013},
isbn = {9781467330763},
publisher = {IEEE Press},
abstract = {Many organisations have turned to crowdsource their software development projects. This raises important pricing questions, a problem that has not previously been addressed for the emerging crowdsourcing development paradigm. We address this problem by introducing 16 cost drivers for crowdsourced development activities and evaluate 12 predictive pricing models using 4 popular performance measures. We evaluate our predictive models on TopCoder, the largest current crowdsourcing platform for software development. We analyse all 5,910 software development tasks (for which partial data is available), using these to extract our proposed cost drivers. We evaluate our predictive models using the 490 completed projects (for which full details are available). Our results provide evidence to support our primary finding that useful prediction quality is achievable (Pred(30)&gt;0.8). We also show that simple actionable advice can be extracted from our models to assist the 430,000 developers who are members of the TopCoder software development market.},
booktitle = {Proceedings of the 2013 International Conference on Software Engineering},
pages = {1205–1208},
numpages = {4},
location = {San Francisco, CA, USA},
series = {ICSE '13}
}

@inproceedings{10.1145/2661829.2661918,
author = {Nguyen, Dong and Trieschnigg, Dolf and Theune, Mari\"{e}t},
title = {Using Crowdsourcing to Investigate Perception of Narrative Similarity},
year = {2014},
isbn = {9781450325981},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2661829.2661918},
doi = {10.1145/2661829.2661918},
abstract = {For many applications measuring the similarity between documents is essential. However, little is known about how users perceive similarity between documents. This paper presents the first large-scale empirical study that investigates perception of narrative similarity using crowdsourcing. As a dataset we use a large collection of Dutch folk narratives. We study the perception of narrative similarity by both experts and non-experts by analyzing their similarity ratings and motivations for these ratings. While experts focus mostly on the plot, characters and themes of narratives, non-experts also pay attention to dimensions such as genre and style. Our results show that a more nuanced view is needed of narrative similarity than captured by story types, a concept used by scholars to group similar folk narratives. We also evaluate to what extent unsupervised and supervised models correspond with how humans perceive narrative similarity.},
booktitle = {Proceedings of the 23rd ACM International Conference on Conference on Information and Knowledge Management},
pages = {321–330},
numpages = {10},
keywords = {crowdsourcing, folktales, narratives, similarity},
location = {Shanghai, China},
series = {CIKM '14}
}

@inproceedings{10.1145/1979742.1979803,
author = {Wu, Shao-Yu and Thawonmas, Ruck and Chen, Kuan-Ta},
title = {Video summarization via crowdsourcing},
year = {2011},
isbn = {9781450302685},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1979742.1979803},
doi = {10.1145/1979742.1979803},
abstract = {Although video summarization has been studied extensively, existing schemes are neither lightweight nor generalizable to all types of video content. To generate accurate abstractions of all types of video, we propose a framework called Click2SMRY, which leverages the wisdom of the crowd to generate video summaries with a low workload for workers. The framework is lightweight because workers only need to click a dedicated key when they feel that the video being played is reaching a highlight. One unique feature of the framework is that it can generate different abstraction levels of video summaries according to viewers' preferences in real time. The results of experiments conducted to evaluate the framework demonstrate that it can generate satisfactory summaries for different types of video clips.},
booktitle = {CHI '11 Extended Abstracts on Human Factors in Computing Systems},
pages = {1531–1536},
numpages = {6},
keywords = {crowdsourcing, human computation, video skimming, video summarization},
location = {Vancouver, BC, Canada},
series = {CHI EA '11}
}

@inproceedings{10.1145/2470654.2470684,
author = {Komarov, Steven and Reinecke, Katharina and Gajos, Krzysztof Z.},
title = {Crowdsourcing performance evaluations of user interfaces},
year = {2013},
isbn = {9781450318990},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2470654.2470684},
doi = {10.1145/2470654.2470684},
abstract = {Online labor markets, such as Amazon's Mechanical Turk (MTurk), provide an attractive platform for conducting human subjects experiments because the relative ease of recruitment, low cost, and a diverse pool of potential participants enable larger-scale experimentation and faster experimental revision cycle compared to lab-based settings. However, because the experimenter gives up the direct control over the participants' environments and behavior, concerns about the quality of the data collected in online settings are pervasive. In this paper, we investigate the feasibility of conducting online performance evaluations of user interfaces with anonymous, unsupervised, paid participants recruited via MTurk. We implemented three performance experiments to re-evaluate three previously well-studied user interface designs. We conducted each experiment both in lab and online with participants recruited via MTurk. The analysis of our results did not yield any evidence of significant or substantial differences in the data collected in the two settings: All statistically significant differences detected in lab were also present on MTurk and the effect sizes were similar. In addition, there were no significant differences between the two settings in the raw task completion times, error rates, consistency, or the rates of utilization of the novel interaction mechanisms introduced in the experiments. These results suggest that MTurk may be a productive setting for conducting performance evaluations of user interfaces providing a complementary approach to existing methodologies.},
booktitle = {Proceedings of the SIGCHI Conference on Human Factors in Computing Systems},
pages = {207–216},
numpages = {10},
keywords = {crowdsourcing, mechanical turk, user interface evaluation},
location = {Paris, France},
series = {CHI '13}
}

@inproceedings{10.1145/2145204.2145354,
author = {Kulkarni, Anand and Can, Matthew and Hartmann, Bj\"{o}rn},
title = {Collaboratively crowdsourcing workflows with turkomatic},
year = {2012},
isbn = {9781450310864},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2145204.2145354},
doi = {10.1145/2145204.2145354},
abstract = {Preparing complex jobs for crowdsourcing marketplaces requires careful attention to workflow design, the process of decomposing jobs into multiple tasks, which are solved by multiple workers. Can the crowd help design such workflows? This paper presents Turkomatic, a tool that recruits crowd workers to aid requesters in planning and solving complex jobs. While workers decompose and solve tasks, requesters can view the status of worker-designed workflows in real time; intervene to change tasks and solutions; and request new solutions to subtasks from the crowd. These features lower the threshold for crowd employers to request complex work. During two evaluations, we found that allowing the crowd to plan without requester supervision is partially successful, but that requester intervention during workflow planning and execution improves quality substantially. We argue that Turkomatic's collaborative approach can be more successful than the conventional workflow design process and discuss implications for the design of collaborative crowd planning systems.},
booktitle = {Proceedings of the ACM 2012 Conference on Computer Supported Cooperative Work},
pages = {1003–1012},
numpages = {10},
keywords = {crowdsourcing, task decomposition, workflows},
location = {Seattle, Washington, USA},
series = {CSCW '12}
}

@inproceedings{10.1109/MASS.2015.76,
author = {Armengol, Patrick and Tobkes, Rachelle and Akkaya, Kemal and Ciftler, Bekir S. and Guvenc, Ismail},
title = {Efficient Privacy-Preserving Fingerprint-Based Indoor Localization Using Crowdsourcing},
year = {2015},
isbn = {9781467391016},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/MASS.2015.76},
doi = {10.1109/MASS.2015.76},
abstract = {Indoor localization has been widely studied due to the inability of GPS to function indoors. Numerous approaches have been proposed in the past and a number of these approaches are currently being used commercially. However, little attention was paid to the privacy of the users especially in the commercial products. Malicious individuals can determine a client's daily habits and activities by simply analyzing their WiFi signals and tracking information. In this paper, we implemented a privacy-preserving indoor localization scheme that is based on a fingerprinting approach to analyze the performance issues in terms of accuracy, complexity, scalability and privacy. We developed an Android app and collected a large number of data on the third floor of the FIU Engineering Center. The analysis of data provided excellent opportunities for performance improvement which have been incorporated to the privacy-preserving localization scheme.},
booktitle = {Proceedings of the 2015 IEEE 12th International Conference on Mobile Ad Hoc and Sensor Systems (MASS)},
pages = {549–554},
numpages = {6},
series = {MASS '15}
}

@inproceedings{10.1145/2213836.2213951,
author = {Van Pelt, Chris and Sorokin, Alex},
title = {Designing a scalable crowdsourcing platform},
year = {2012},
isbn = {9781450312479},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2213836.2213951},
doi = {10.1145/2213836.2213951},
abstract = {Computers are extremely efficient at crawling, storing and processing huge volumes of structured data. They are great at exploiting link structures to generate valuable knowledge. Yet there are plenty of data processing tasks that are difficult today. Labeling sentiment, moderating images, and mining structured content from the web are still too hard for computers. Automated techniques can get us a long way in some of those, but human inteligence is required when an accurate decision is ultimately important. In many cases that decision is easy for people and can be made quickly - in a few seconds to few minutes.By creating millions of simple online tasks we create a distributed computing machine. By shipping the tasks to millions of contributers around the globe, we make this human computer available 24/7 to make important decisions about your data. In this talk, I will describe our approach to designing CrowdFlower - a scalable crowdsourcing platform - as it evolved over the last 4 years.We think about crowdsourcing in terms of Quality, Cost and Speed. They are the ultimate design objectives of a human computer. Unfortunately, we can't have all 3. A general price-constrained task requiring 99.9\% accuracy and 10 minute turnaround is not possible today. I will discuss design decisions behind CrowdFlower that allow us to pursue any two of these objectives.I will briefly present examples of common crowdsourced tasks and tools built into the platform to make the design of complex tasks easy, tools such as CrowdFlower Markup Language(CML).Quality control is the single most important challenge in Crowdsourcing. To enable an unidentified crowd of people to produce meaningful work, we must be certain that we can filter out bad contributors and produce high quality output. Initially we only used consensus. As the diversity and size of our crowd grew, so did the number of people attempting fraud. CrowdFlower developed "Gold standard" to block attempts of fraud. The use of gold allowed us to train contributors for the details of specific domains. By defining expected responses for a subset of the work and providing explanations of why a given response was expected, we are able distribute tasks to an ever-expanding anonymous workforce without sacrificing quality.},
booktitle = {Proceedings of the 2012 ACM SIGMOD International Conference on Management of Data},
pages = {765–766},
numpages = {2},
keywords = {crowdsourcing, data, workforce},
location = {Scottsdale, Arizona, USA},
series = {SIGMOD '12}
}

@inproceedings{10.5555/2878453.2878517,
author = {Hanika, Florian and Wohlgenannt, Gerhard and Sabou, Marta},
title = {The uComp protege plugin for crowdsourcing ontology validation},
year = {2014},
publisher = {CEUR-WS.org},
address = {Aachen, DEU},
abstract = {The validation of ontologies using domain experts is expensive. Crowdsourcing has been shown a viable alternative for many knowledge acquisition tasks. We present a Prot\'{e}g\'{e} plugin and a workflow for outsourcing a number of ontology validation tasks to Games with a Purpose and paid micro-task crowdsourcing.},
booktitle = {Proceedings of the 2014 International Conference on Posters \&amp; Demonstrations Track - Volume 1272},
pages = {253–256},
numpages = {4},
keywords = {crowdsourcing, human computation, ontology engineering, prot\'{e}g\'{e} plugin},
location = {Riva del Garda, Italy},
series = {ISWC-PD'14}
}

@inproceedings{10.5555/2888116.2888384,
author = {Bigham, Jeffrey P.},
title = {What's hot in crowdsourcing and human computation},
year = {2015},
isbn = {0262511290},
publisher = {AAAI Press},
abstract = {The focus of HCOMP 2014 was the crowd worker. While crowdsourcing is motivated by the promise of leveraging people's intelligence and diverse skillsets in computational processes, the human aspects of this workforce are all too often overlooked. Instead, workers are frequently viewed as interchangeable components that can be statistically managed to eek out reasonable outputs. We are quickly moving past and rejecting these notions, and beginning to understand that it is sometimes the very abstractions that we introduce to make human computation feasible, e.g., abstracting humans behind APIs or isolating workers from others in order to ensure independent input, that can lead to the problems that we then set about trying to solve, e.g., poor or inconsistent quality work. Creating a brighter future for crowd work will require new socio-technical systems that not only decompose tasks, recruit and coordinate workers, and make sense of results, but also find interesting tasks for people to contribute to, structure tasks so that workers learn from them as they go, and eventually automate mundane parts of work. Research in artificial intelligence will be vital for achieving this future.},
booktitle = {Proceedings of the Twenty-Ninth AAAI Conference on Artificial Intelligence},
pages = {4318–4319},
numpages = {2},
location = {Austin, Texas},
series = {AAAI'15}
}

@inproceedings{10.1109/DySPAN.2015.7343932,
author = {Ying, Xuhang and Roy, Sumit and Poovendran, Radha},
title = {Incentivizing crowdsourcing for radio environment mapping with statistical interpolation},
year = {2015},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/DySPAN.2015.7343932},
doi = {10.1109/DySPAN.2015.7343932},
abstract = {White Space Networking crucially relies on the active monitoring of spectrum usage (to identify white space opportunities) in both space and time. One way to achieve this is wide-area deployment of spectrum sensors to gather spatio-temporal spectrum data, and use them to construct better Radio Environment Maps (REMs) via suitable statistical interpolation techniques (i.e., Kriging). Cost of such large-scale sensor deployment can be reduced via crowdsourcing, i.e., outsourcing the sensing task to mobile users equipped with sensorized high-end client devices (e.g., tablets or smartphones), and success of such crowdsourced sensing presumes some incentive mechanisms to attract user participation. In this work, we present an incentivized crowdsourcing system architecture that (periodically) acquires spectrum data from users, so as to optimize the resulting radio environment map (i.e., minimizing the average prediction-error variance) for a given data acquisition budget. First, we introduce an auction-based incentive mechanism that is computationally efficient, individually rational and truthful, and prove that the total payment of the proposed mechanism is a monotonically increasing function of the cardinality of the winner set. Then we propose a budget-feasible version and through extensive simulations, we evaluate the performance of proposed mechanisms for comparison to a baseline to demonstrate its significantly superior performance in crowdsourced radio mapping.},
booktitle = {2015 IEEE International Symposium on Dynamic Spectrum Access Networks (DySPAN)},
pages = {365–374},
numpages = {10},
location = {Stockholm, Sweden}
}

@inproceedings{10.1145/2187836.2187969,
author = {Venetis, Petros and Garcia-Molina, Hector and Huang, Kerui and Polyzotis, Neoklis},
title = {Max algorithms in crowdsourcing environments},
year = {2012},
isbn = {9781450312295},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2187836.2187969},
doi = {10.1145/2187836.2187969},
abstract = {Our work investigates the problem of retrieving the maximum item from a set in crowdsourcing environments. We first develop parameterized families of max algorithms, that take as input a set of items and output an item from the set that is believed to be the maximum. Such max algorithms could, for instance, select the best Facebook profile that matches a given person or the best photo that describes a given restaurant. Then, we propose strategies that select appropriate max algorithm parameters. Our framework supports various human error and cost models and we consider many of them for our experiments. We evaluate under many metrics, both analytically and via simulations, the tradeoff between three quantities: (1) quality, (2) monetary cost, and (3) execution time. Also, we provide insights on the effectiveness of the strategies in selecting appropriate max algorithm parameters and guidelines for choosing max algorithms and strategies for each application.},
booktitle = {Proceedings of the 21st International Conference on World Wide Web},
pages = {989–998},
numpages = {10},
keywords = {crowdsourcing, human computation, max algorithms, plurality voting, vote aggregation, worker models},
location = {Lyon, France},
series = {WWW '12}
}

@inproceedings{10.1007/978-3-319-49004-5_44,
author = {ul Hassan, Umair and Zaveri, Amrapali and Marx, Edgard and Curry, Edward and Lehmann, Jens},
title = {ACRyLIQ: Leveraging DBpedia for Adaptive Crowdsourcing in Linked Data Quality Assessment},
year = {2016},
isbn = {978-3-319-49003-8},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-319-49004-5_44},
doi = {10.1007/978-3-319-49004-5_44},
abstract = {Crowdsourcing has emerged as a powerful paradigm for quality assessment and improvement of Linked Data. A major challenge of employing crowdsourcing, for quality assessment in Linked Data, is the cold-start problem: how to estimate the reliability of crowd workers and assign the most reliable workers to tasks? We address this challenge by proposing a novel approach for generating test questions from DBpedia based on the topics associated with quality assessment tasks. These test questions are used to estimate the reliability of the new workers. Subsequently, the tasks are dynamically assigned to reliable workers to help improve the accuracy of collected responses. Our proposed approach, ACRyLIQ, is evaluated using workers hired from Amazon Mechanical Turk, on two real-world Linked Data datasets. We validate the proposed approach in terms of accuracy and compare it against the baseline approach of reliability estimate using gold-standard task. The results demonstrate that our proposed approach achieves high accuracy without using gold-standard task.},
booktitle = {Knowledge Engineering and Knowledge Management: 20th International Conference, EKAW 2016, Bologna, Italy, November 19-23, 2016, Proceedings},
pages = {681–696},
numpages = {16},
keywords = {Link Data, Task Assignment, Test Question, Overhead Cost, Assignment Algorithm},
location = {Bologna, Italy}
}

@inproceedings{10.1109/PIMRC.2015.7343555,
author = {Zhang, Bo and Liu, Chi Harold and Ren, Ziyu and Ma, Jian and Wang, Wendong},
title = {Crowdsourcing energy-efficient participants to ensure quality-of-information},
year = {2015},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/PIMRC.2015.7343555},
doi = {10.1109/PIMRC.2015.7343555},
abstract = {Crowdsourcing systems, by using smart devices like smartphones and iPad, have been widely used in various domains, but are currently facing new challenges. On one hand, different tasks offer different amount of incentive budgets in multitask systems, thus those tasks pay more should be satisfied preferentially. On the other hand, even if there are not enough budget for a sensing task, the system should also try to provide as much sensory data as possible to obtain potential clients. Furthermore, it is challenging to select an “optimal” set of participants as data contributors due to the above two points. In this paper, first, we introduce a metric to describe how the collected sensory data can quantify task's the multi-dimensional data requirements, in terms of data distribution in spatiotemporal domains. Second, we propose a “task priority model” based on incentive budget, to explicitly quantify the relationship between the incentive budget usage and task priority. Then, we present a quality of information (QoI) aware participant selection approach as a suboptimal solution to the defined optimization problem. Finally, we compare our proposed scheme with existing methods via extensive simulations based on the real movement traces of ordinary citizens in Beijing. Extensive simulation results well justify the effectiveness and robustness of our approach.},
booktitle = {2015 IEEE 26th Annual International Symposium on Personal, Indoor, and Mobile Radio Communications (PIMRC)},
pages = {1606–1610},
numpages = {5},
location = {Hong Kong, China}
}

@inproceedings{10.1145/3079628.3079702,
author = {Lowe, Ryan and Steichen, Ben},
title = {Multilingual Search User Behaviors -- Exploring Multilingual Querying and Result Selection Through Crowdsourcing},
year = {2017},
isbn = {9781450346351},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3079628.3079702},
doi = {10.1145/3079628.3079702},
abstract = {The unprecedented increase in online search user diversity across the globe has led to new challenges for search engine providers. In particular, among these challenges is the need to better support individuals who are proficient in multiple languages. To investigate this particular user characteristic, this paper presents an analysis of multilingual search user behaviors through a series of large-scale studies using crowdsourcing. Results show that multilingual users make significant use of each of their languages when searching, and that there are significant differences in behaviors between querying and result selection. In addition, results show that language use strongly depends on a number of task factors and individual user characteristics. These results are discussed in terms of building novel adaptive multilingual search solutions that better support and adapt to users who have multiple language abilities.},
booktitle = {Proceedings of the 25th Conference on User Modeling, Adaptation and Personalization},
pages = {303–307},
numpages = {5},
keywords = {crowd-sourcing, multilingual search, multilingual user characteristics, personalized search, user study},
location = {Bratislava, Slovakia},
series = {UMAP '17}
}

@inproceedings{10.1145/2757757.2757758,
author = {Huo, Zhiqiang and Shu, Lei and Zhou, Zhangbing and Chen, Yuanfang and Li, Kailiang and Zeng, Junlin},
title = {Data Collection Middleware for Crowdsourcing-based Industrial Sensing Intelligence},
year = {2015},
isbn = {9781450335140},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2757757.2757758},
doi = {10.1145/2757757.2757758},
abstract = {In this paper, crowdsourcing-based industrial sensing intelligence (CISI) is proposed as a collaborative approach for large-scale monitoring in modern industrial plants, targeting at improved productivity and increased workplace safety. The proposed approach focuses on middleware, which considers both application and industry-grade requirements. Through embedding crowdsourcing knowledge at different levels and supporting QoS services, systems based on CISI can perform effective work assignment and flexible configuration of wireless sensor networks (WSNs). This paper presents a middleware that addresses these characteristics, which is an extension of GSN, our earlier work on middleware for rapid deployment and integration of heterogeneous sensor networks. Wireless sensor devices and wearable equipment are employed as modeling tools for the middleware implementation.},
booktitle = {Proceedings of the ACM International Workshop on Mobility and MiddleWare Management in HetNets},
pages = {3–8},
numpages = {6},
keywords = {cisi, middleware, wearable equipment, wsns},
location = {Hangzhou, China},
series = {MobiMWareHN '15}
}

@inproceedings{10.1109/HICSS.2015.646,
author = {Dissanayake, Indika and Zhang, Jie and Yuan, Feirong and Wang, Jingguo},
title = {Peer-recognition and Performance in Online Crowdsourcing Communities},
year = {2015},
isbn = {9781479973675},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/HICSS.2015.646},
doi = {10.1109/HICSS.2015.646},
abstract = {Recent advances in information technology bring significant changes to the nature of social interactions and information exchange. Physical face-to-face communications are slowly replaced by online virtual communities. Motivated by this phenomenon, this research investigates how voluntary community involvement and self-disclosure behavior impact a member's peer-recognition and task performance within a virtual crowd sourcing competition community. We collected secondary data from the discussion forums of a specialized crowd sourcing platform that focuses on data analytics projects. Our results reveal that a member's community involvement improves both the peer-recognition of the member in the community and his/her performance ranking. Our findings have strategic implications to participants of virtual crowd sourcing communities and other professional online communities.},
booktitle = {Proceedings of the 2015 48th Hawaii International Conference on System Sciences},
pages = {4262–4265},
numpages = {4},
series = {HICSS '15}
}

@inproceedings{10.1109/INFOCOM.2016.7524615,
author = {Chatterjee, Avhishek and Borokhovich, Michael and Varshney, Lav R. and Vishwanath, Sriram},
title = {Efficient and flexible crowdsourcing of specialized tasks with precedence constraints},
year = {2016},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/INFOCOM.2016.7524615},
doi = {10.1109/INFOCOM.2016.7524615},
abstract = {Many companies now use crowdsourcing to leverage external (as well as internal) crowds to perform specialized work, and so methods of improving efficiency are critical. Tasks in crowdsourcing systems with specialized work have multiple steps and each step requires multiple skills. Steps may have different flexibilities in terms of obtaining service from one or multiple agents, due to varying levels of dependency among parts of steps. Steps of a task may have precedence constraints among them. Moreover, there are variations in loads of different types of tasks requiring different skill-sets and availabilities of different types of agents with different skill-sets. Considering these constraints together necessitates the design of novel schemes to allocate steps to agents. In addition, large crowdsourcing systems require allocation schemes that are simple, fast, decentralized and offer customers (task requesters) the freedom to choose agents. In this work we study the performance limits of such crowdsourcing systems and propose efficient allocation schemes that provably meet the performance limits under these additional requirements. We demonstrate our algorithms on data from a crowdsourcing platform run by a non-profit company and show significant improvements over current practice.},
booktitle = {IEEE INFOCOM 2016 - The 35th Annual IEEE International Conference on Computer Communications},
pages = {1–9},
numpages = {9},
location = {San Francisco, CA, USA}
}

@inproceedings{10.1145/1880071.1880139,
author = {Wiggins, Andrea},
title = {Crowdsourcing science: organizing virtual participation in knowledge production},
year = {2010},
isbn = {9781450303873},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1880071.1880139},
doi = {10.1145/1880071.1880139},
abstract = {Citizen science is a form of research collaboration that involves the public in scientific research to address real-world problems. Virtual citizen science projects, entirely mediated by information and communication technologies (ICTs), are often considered a form of crowdsourcing applied to science. The use of ICTs to support citizen science has already yielded significant impacts on the scale and scope of participation and research, but there is little guidance to help projects choose and implement appropriate technologies to support research and participation goals. This dissertation study employs a comparative case study methodology to examine how virtuality and technology shape processes of organizing and participation in citizen science, and how these processes influence scientific outcomes. The goal of the study is to conceptualize virtual participation by examining the relationship between ICT and practice in order to inform design and management of cyberinfrastructure for citizen science.},
booktitle = {Proceedings of the 2010 ACM International Conference on Supporting Group Work},
pages = {337–338},
numpages = {2},
keywords = {citizen science, organizing, participation, virtuality},
location = {Sanibel Island, Florida, USA},
series = {GROUP '10}
}

@inproceedings{10.5555/2772879.2773387,
author = {Yu, Han and Miao, Chunyan and Shen, Zhiqi and Leung, Cyril},
title = {Quality and Budget Aware Task Allocation for Spatial Crowdsourcing},
year = {2015},
isbn = {9781450334136},
publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
address = {Richland, SC},
abstract = {A major research challenge for spatial crowdsourcing is to improve the expected quality of the results. However, existing research in this field mostly focuses on achieving this objective in volunteer-based spatial crowdsourcing. In this paper, we introduce the budget limitations into the above problem and consider realistic cases where workers are paid unequally based on their trustworthiness. We propose a novel quality and budget aware spatial task allocation approach which jointly considers the workers' reputation and proximity to the task locations to maximize the expected quality of the results while staying within a limited budget.},
booktitle = {Proceedings of the 2015 International Conference on Autonomous Agents and Multiagent Systems},
pages = {1689–1690},
numpages = {2},
keywords = {reputation, spatial crowdsourcing, task allocation, trust},
location = {Istanbul, Turkey},
series = {AAMAS '15}
}

@inproceedings{10.1007/978-3-319-91563-0_9,
author = {Suzuki, Rikuya and Sakaguchi, Tetsuo and Matsubara, Masaki and Kitagawa, Hiroyuki and Morishima, Atsuyuki},
title = {CrowdSheet: An Easy-To-Use One-Stop Tool for Writing and Executing Complex Crowdsourcing},
year = {2018},
isbn = {978-3-319-91562-3},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-319-91563-0_9},
doi = {10.1007/978-3-319-91563-0_9},
abstract = {Developing crowdsourcing applications with dataflows among tasks requires requesters to submit tasks to crowdsourcing services, obtain results, write programs to process the results, and often repeat this process. This paper proposes CrowdSheet, an application that provides a spreadsheet interface to easily write and execute such complex crowdsourcing applications. We prove that a natural extension to existing spreadsheets, with only two types of new spreadsheet functions, allows us to write a fairly wide range of real-world applications. Our experimental results indicate that many spreadsheet users can easily write complex crowdsourcing applications with CrowdSheet.},
booktitle = {Advanced Information Systems Engineering: 30th International Conference, CAiSE 2018, Tallinn, Estonia, June 11-15, 2018, Proceedings},
pages = {137–153},
numpages = {17},
keywords = {Rapid development, Complex crowdsourcing, Expressive power analysis},
location = {Tallinn, Estonia}
}

@inproceedings{10.1145/3051457.3053973,
author = {Whitehill, Jacob and Seltzer, Margo},
title = {A Crowdsourcing Approach to Collecting Tutorial Videos -- Toward Personalized Learning-at-Scale},
year = {2017},
isbn = {9781450344500},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3051457.3053973},
doi = {10.1145/3051457.3053973},
abstract = {We investigated the feasibility of crowdsourcing full- fledged tutorial videos from ordinary people on the Web on how to solve math problems related to logarithms. This kind of approach (a form of learnersourcing [9, 11]) to efficiently collecting tutorial videos and other learning resources could be useful for realizing personalized learning-at-scale, whereby students receive specific learning resources -- drawn from a large and diverse set -- that are tailored to their individual and time-varying needs. Results of our study, in which we collected 399 videos from 66 unique "teachers" on Mechanical Turk, suggest that (1) approximately 100 videos -- over 80\% of which are mathematically fully correct -- can be crowdsourced per week for $5/video; (2) the average learning gains (posttest minus pretest score) associated with watching the videos was stat. sig. higher than for a control video (0.105 versus 0.045); and (3) the average learning gains (0.1416) from watching the best tested crowdsourced videos was comparable to the learning gains (0.1506) from watching a popular Khan Academy video on logarithms.},
booktitle = {Proceedings of the Fourth (2017) ACM Conference on Learning @ Scale},
pages = {157–160},
numpages = {4},
keywords = {crowdsourcing, personalized learning},
location = {Cambridge, Massachusetts, USA},
series = {L@S '17}
}

@inproceedings{10.1609/aaai.v37i13.27045,
author = {Xu, Ruoyu and Li, Gaoxiang and Jin, Wei and Chen, Austin and Sheng, Victor S.},
title = {ACCD: an adaptive clustering-based collusion detector in crowdsourcing (student abstract)},
year = {2023},
isbn = {978-1-57735-880-0},
publisher = {AAAI Press},
url = {https://doi.org/10.1609/aaai.v37i13.27045},
doi = {10.1609/aaai.v37i13.27045},
abstract = {Crowdsourcing is a popular method for crowd workers to collaborate on tasks. However, workers coordinate and share answers during the crowdsourcing process. The term for this is "collusion". Copies from others and repeated submissions are detrimental to the quality of the assignments. The majority of the existing research on collusion detection is limited to ground truth problems (e.g., labeling tasks) and requires a predetermined threshold to be established in advance. In this paper, we aim to detect collusion behavior of workers in an adaptive way, and propose an Adaptive Clustering Based Collusion Detection approach (ACCD) for a broad range of task types and data types solved via crowdsourcing (e.g., continuous rating with or without distributions). Extensive experiments on both real-world and synthetic datasets show the superiority of ACCD over state-of-the-art approaches.},
booktitle = {Proceedings of the Thirty-Seventh AAAI Conference on Artificial Intelligence and Thirty-Fifth Conference on Innovative Applications of Artificial Intelligence and Thirteenth Symposium on Educational Advances in Artificial Intelligence},
articleno = {1976},
numpages = {2},
series = {AAAI'23/IAAI'23/EAAI'23}
}

@inproceedings{10.1145/2145204.2145382,
author = {Ambati, Vamshi and Vogel, Stephan and Carbonell, Jaime},
title = {Collaborative workflow for crowdsourcing translation},
year = {2012},
isbn = {9781450310864},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2145204.2145382},
doi = {10.1145/2145204.2145382},
abstract = {In this paper we explore the challenges in crowdsourcing the task of translation over the web in which remotely located translators work on providing translations independent of each other. We then propose a collaborative workflow for crowdsourcing translation to address some of these challenges. In our pipeline model, the translators are working in phases where output from earlier phases can be enhanced in the subsequent phases. We also highlight some of the novel contributions of the pipeline model like assistive translation and translation synthesis that can leverage monolingual and bilingual speakers alike. We evaluate our approach by eliciting translations for both a minority-to-majority language pair and a minority-to-minority language pair. We observe that in both scenarios, our workflow produces better quality translations in a cost-effective manner, when compared to the traditional crowdsourcing workflow.},
booktitle = {Proceedings of the ACM 2012 Conference on Computer Supported Cooperative Work},
pages = {1191–1194},
numpages = {4},
keywords = {amazon mechanical turk, collaborative workflow, crowdsourcing, language translation},
location = {Seattle, Washington, USA},
series = {CSCW '12}
}

@inproceedings{10.1109/CGAMES.2011.6000339,
author = {van 't Woud, J. S. S. and Sandberg, J. A. C. and Wielinga, B. J.},
title = {The Mars crowdsourcing experiment: Is crowdsourcing in the form of a serious game applicable for annotation in a semantically-rich research domain?},
year = {2011},
isbn = {9781457714511},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/CGAMES.2011.6000339},
doi = {10.1109/CGAMES.2011.6000339},
abstract = {This study investigates crowdsourcing using a serious game concerning the annotation of semantically rich features of Mars. Photographic data is used transmitted from the Mars Reconnaissance Orbiter recording the surface. A computer game, called Cerberus, was developed allowing players to tag features of the Mars surface. The game investigated in four different conditions what the effects were of different help levels to support knowledge transfer and different levels of game features to provide the players with a stimulating game experience. The performance of the participating players was measured in terms of precision and motivation. Precision reflects the quality of the work done and motivation is represented by the amount of work done by the players. The four game conditions varied in an implicit and an explicit level of help and poor and rich game features. The game condition with the explicit help function combined with the rich game experience showed significantly more motivation among the players than the game condition with the implicit help function combined with the poor gaming experience. Precision did not show any significant difference between the game conditions, but was high enough to generate Mars maps exposing aeolian processes, surface layering, river meanders and other concepts. Apparently the players were capable of acquiring deeper concepts about Mars's geology and the results were of such a high quality that they could be used as input for and to reinforce scientific research.},
booktitle = {Proceedings of the 2011 16th International Conference on Computer Games},
pages = {201–208},
numpages = {8},
keywords = {surface layering, semantically rich features, river meanders, photographic data, motivation, knowledge transfer, computer game, aeolian processes, Mars's geology, Mars reconnaissance orbiter, Mars crowdsourcing experiment, Cerberus},
series = {CGAMES '11}
}

@inproceedings{10.1109/HICSS.2013.537,
author = {Simula, Henri},
title = {The Rise and Fall of Crowdsourcing?},
year = {2013},
isbn = {9780769548920},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/HICSS.2013.537},
doi = {10.1109/HICSS.2013.537},
abstract = {Crowd sourcing has been discussed both in academic and managerial articles in recent years. Despite some critical voices, the overall attitude towards crowd sourcing has been quite positive in extant literature. In this paper we want to address potential drawbacks and issue that create shadows on top of crowd sourcing. The overall purpose of this paper is to discuss the reasons why crowd sourcing initiatives may not always live up to the expectations placed upon them. Despite some seemingly successful case examples, not every crowd sourcing initiative has taken off. While some of the barriers are case or industry specific, there are also certain overall reasons hindering crowd sourcing from reaching de facto modus operandi, especially in the innovation creation context. This paper is intentionally written through a critical lens by design and hopefully provides a constructive balance for those with an overly positive approach towards crowd sourcing.},
booktitle = {Proceedings of the 2013 46th Hawaii International Conference on System Sciences},
pages = {2783–2791},
numpages = {9},
keywords = {problems, issues, future directions, crowdsourcing},
series = {HICSS '13}
}

@inproceedings{10.1109/DASC.2011.60,
author = {Afridi, Ahmad Hassan},
title = {Crowdsourcing in Mobile: A Three Stage Context Based Process},
year = {2011},
isbn = {9780769546124},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/DASC.2011.60},
doi = {10.1109/DASC.2011.60},
abstract = {Crowdsourcing is a new paradigm that has influenced the businesses, emergency management, collaboration and online interaction. The term crowd source means tapping the power of crowd. Research in crowd sourcing involves issues such as quality of data, incentives, working modal and contracts. This research deals with the issues of uncertainty and quality of social mobile computing systems. It suggests the relationship between context and crowd sourcing activity. This understanding can enhance the future and existing applications in a way that it can optimize systems response, adapt the components and decrease the uncertainty.},
booktitle = {Proceedings of the 2011 IEEE Ninth International Conference on Dependable, Autonomic and Secure Computing},
pages = {242–245},
numpages = {4},
keywords = {Mobile Context Aware Systems, Crowdsourcing},
series = {DASC '11}
}

@inproceedings{10.1007/978-3-319-25639-9_20,
author = {Morbidoni, Christian and Piccioli, Alessio},
title = {Curating a Document Collection via Crowdsourcing with Pundit 2.0},
year = {2015},
isbn = {978-3-319-25638-2},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-319-25639-9_20},
doi = {10.1007/978-3-319-25639-9_20},
abstract = {Pundit 2.0 is a semantic web annotation system that supports users in creating structured data on top of web pages. Annotations in Pundit are RDF triples that users build starting from web page elements, as text or images. Annotations can be made public and developers can access and combine them into RDF knowledge graphs, while authorship of each triple is always retrievable. In this demo we showcase Pundit 2.0 and demonstrate how it can be used to enhance a digital library, by providing a data crowdsourcing platform. Pundit enables users to annotate different kind of entities and to contribute to the collaborative creation of a knowledge graph. This, in turn, refines in real-time the exploration functionalities of the library’s faceted search, providing an immediate added value out of the annotation effort. Ad-hoc configurations can be used to drive specific visualisations, like the timeline-map shown in this demo.},
booktitle = {The Semantic Web: ESWC 2015 Satellite Events: ESWC 2015 Satellite Events, Portoro\v{z}, Slovenia, May 31 – June 4, 2015, Revised Selected Papers},
pages = {102–106},
numpages = {5},
keywords = {Pundit, Digital humanities, Faceted browsing, Linked data, Semantic annotation},
location = {Portoro\v{z}, Slovenia}
}

@inproceedings{10.1145/2660114.2660118,
author = {Redi, Judith and Povoa, Isabel},
title = {Crowdsourcing for Rating Image Aesthetic Appeal: Better a Paid or a Volunteer Crowd?},
year = {2014},
isbn = {9781450331289},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2660114.2660118},
doi = {10.1145/2660114.2660118},
abstract = {Crowdsourcing has the potential to become a preferred tool to study image aesthetic appeal preferences of users. Nevertheless, some reliability issues still exist, partially due to the sometimes doubtful commitment of paid workers to perform the rating task properly. In this paper we compare the reliability in scoring image aesthetic appeal of both a paid and a volunteer crowd. We recruit our volunteers through Facebook and our paid users via Microworkers. We conclude that, whereas volunteer participants are more likely to leave the rating task unfinished, when they complete it they do so more reliably than paid users.},
booktitle = {Proceedings of the 2014 International ACM Workshop on Crowdsourcing for Multimedia},
pages = {25–30},
numpages = {6},
keywords = {qoe, photography, crowdsourcing, aesthetics},
location = {Orlando, Florida, USA},
series = {CrowdMM '14}
}

@inproceedings{10.1145/2818052.2869074,
author = {Saito, Susumu and Kobayashi, Tetsunori and Nakano, Teppei},
title = {Towards a Framework for Collaborative Video Surveillance System Using Crowdsourcing},
year = {2016},
isbn = {9781450339506},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2818052.2869074},
doi = {10.1145/2818052.2869074},
abstract = {This paper proposes a new framework for video surveillance systems for crime prevention. The main purpose of this framework is to help provide reasonable and stable solutions for automated video surveillance systems in a collaborative way. This framework is characterized by a verification process using crowdsourcing after the image analysis process: automated image analyzer detects as many suspicious events as possible followed by filtering process using human intelligence, to achieve both high re-call and high precision rates. Here we describe the basic mechanisms for collaboration between camera devices, data stores, image analyzers and surveillance crowds.},
booktitle = {Proceedings of the 19th ACM Conference on Computer Supported Cooperative Work and Social Computing Companion},
pages = {393–396},
numpages = {4},
keywords = {framework, crowdsourcing, Collaborative video surveillance},
location = {San Francisco, California, USA},
series = {CSCW '16 Companion}
}

@inproceedings{10.1145/2837689.2837705,
author = {Keles, Ilkcan and Saltenis, Simonas and Jensen, Christian S.},
title = {Synthesis of partial rankings of points of interest using crowdsourcing},
year = {2015},
isbn = {9781450339377},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2837689.2837705},
doi = {10.1145/2837689.2837705},
abstract = {The web is increasingly being accessed from mobile devices, and studies suggest that a large fraction of keyword-based search engine queries have local intent, meaning that users are interested in local content and that the underlying ranking function should take into account both relevance to the query keywords and the query location. A key challenge in being able to make progress on the design of ranking functions is to be able to assess the quality of the results returned by ranking functions. We propose a model that synthesizes a ranking of points of interest from answers to crowdsourced pairwise relevance questions. To evaluate the model, we propose an innovative methodology that enables evaluation of the quality of synthesized rankings in a simulated setting. We report on an experimental evaluation based on the methodology that shows that the proposed model produces promising results in pertinent settings and that it is capable of outperforming an approach based on majority voting.},
booktitle = {Proceedings of the 9th Workshop on Geographic Information Retrieval},
articleno = {15},
numpages = {10},
keywords = {ranking, points of interest, pairwise relevance, crowdsourcing},
location = {Paris, France},
series = {GIR '15}
}

@inproceedings{10.1145/1868914.1868976,
author = {Wightman, Doug},
title = {Crowdsourcing human-based computation},
year = {2010},
isbn = {9781605589343},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1868914.1868976},
doi = {10.1145/1868914.1868976},
abstract = {Thousands of websites have been created to crowdsource tasks. In this paper, systems that crowdsource human-based computations are organized into four distinct classes using two factors: the users' motivation for completing the task (direct or indirect) and whether task completion is competitive. These classes are described and compared. Considerations and selection criteria for systems designers are presented. This investigation also identified several opportunities for further research. For example, existing systems might benefit from the integration of methods for transforming complex tasks into many simple tasks.},
booktitle = {Proceedings of the 6th Nordic Conference on Human-Computer Interaction: Extending Boundaries},
pages = {551–560},
numpages = {10},
keywords = {human-based computation, distributed knowledge acquisition, crowdsourcing},
location = {Reykjavik, Iceland},
series = {NordiCHI '10}
}

@inproceedings{10.1145/2506364.2506369,
author = {Tavares, Gon\c{c}alo and Mour\~{a}o, Andr\'{e} and Magalhaes, Jo\~{a}o},
title = {Crowdsourcing for affective-interaction in computer games},
year = {2013},
isbn = {9781450323963},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2506364.2506369},
doi = {10.1145/2506364.2506369},
abstract = {Affective-interaction in computer games is a novel area with several new challenges, such as detecting players' facial expressions (e.g., happy, sad, surprise) in a robust manner. In this paper we describe a crowdsourcing effort for creating the ground-truth of a large-scale dataset of images capturing users playing a computer game. The computer game is designed to elicit a particular facial expressions and the game will score the player according to the detected expression. For designing the crowdsourcing task, some of the examined variables include: reward, tagging limits, golden questions, workers' location. In the end, we designed a large tagging job to maximize workers agreement. Each image with a facial expressions is tagged with one of the following expressions labels: happy, anger, disgust, contempt, sad, fear, surprise, and neutral. The dataset included over 40,000 images, the workers' judgments, the game's detected facial expression and what facial expression the player should be performing.},
booktitle = {Proceedings of the 2nd ACM International Workshop on Crowdsourcing for Multimedia},
pages = {7–12},
numpages = {6},
keywords = {facial expressions, crowdsourcing, affective-interaction},
location = {Barcelona, Spain},
series = {CrowdMM '13}
}

@inproceedings{10.1145/2740908.2743052,
author = {Zubiaga, Arkaitz and Liakata, Maria and Procter, Rob and Bontcheva, Kalina and Tolmie, Peter},
title = {Crowdsourcing the Annotation of Rumourous Conversations in Social Media},
year = {2015},
isbn = {9781450334730},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2740908.2743052},
doi = {10.1145/2740908.2743052},
abstract = {Social media are frequently rife with rumours, and the study of rumour conversational aspects can provide valuable knowledge about how rumours evolve over time and are discussed by others who support or deny them. In this work, we present a new annotation scheme for capturing rumour-bearing conversational threads, as well as the crowdsourcing methodology used to create high quality, human annotated datasets of rumourous conversations from social media. The rumour annotation scheme is validated through comparison between crowdsourced and reference annotations. We also found that only a third of the tweets in rumourous conversations contribute towards determining the veracity of rumours, which reinforces the need for developing methods to extract the relevant pieces of information automatically.},
booktitle = {Proceedings of the 24th International Conference on World Wide Web},
pages = {347–353},
numpages = {7},
keywords = {social media, rumours, rumors, crowdsourcing, annotation},
location = {Florence, Italy},
series = {WWW '15 Companion}
}

@inproceedings{10.1145/3206129.3239431,
author = {Roy, Debopriyo},
title = {A Model for Language Learning with Crowdsourcing and Social Network Analysis for Community Decision-Making},
year = {2018},
isbn = {9781450365253},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3206129.3239431},
doi = {10.1145/3206129.3239431},
abstract = {Based on the fundamentals of content and language integrated learning (CLIL), this article provides a holistic overview of how different technological applications such as Google Maps, social network analysis (SNA), and in general, crowdsourcing of spatial and location-specific data could help identify poverty, and local socio-economic and lifestyle-oriented problems, and trigger a discussion about community decision making. Such use of technology could potentially help make a convincing case for the type of poverty; including exact issues in the area, proximity to resource hubs, lack of basic facilities, and employment and so on. The primary focus in this article is on how content language integrated learning (CLIL) combines content areas such as mechanism and technology for poverty identification and analysis on the way to learning the target language. Use of such technological applications in a foreign language-learning course for policy decision-making and community engagement is rather unique. We need for students to have a rich experience with different combinations of text-graphics-video modality including hands-on engagement, and language acquisition is expected to happen as a result. Technical communication could be an important focus for such courses with report writing, feasibility and recommendation studies, email communication, writing commentaries, chats, text captions, interviewing etc. With such use of technology and documentation, the aim is to empower students in revitalization efforts in the community.},
booktitle = {Proceedings of the 2nd International Conference on Education and Multimedia Technology},
pages = {14–19},
numpages = {6},
keywords = {social networks, poverty, language, design, crowdsourcing, Maps},
location = {Okinawa, Japan},
series = {ICEMT '18}
}

@inproceedings{10.1109/SERVICES-I.2009.56,
author = {Vukovic, Maja},
title = {Crowdsourcing for Enterprises},
year = {2009},
isbn = {9780769537085},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/SERVICES-I.2009.56},
doi = {10.1109/SERVICES-I.2009.56},
abstract = {Crowdsourcing is emerging as the new on-line distributed problem solving and production model in which networked people collaborate to complete a task.Enterprises are increasingly employing crowdsourcing to access scalable workforce on-line. In parallel, cloud computing has emerged as a new paradigm for delivering computational services, which seamlessly interweave physical and digital worlds through a common infrastructure.This paper presents a sample crowdsourcing scenario in software development domain to derive the requirements for delivering a general-purpose crowdsourcing service in the Cloud. It proposes taxonomy for categorization of crowdsourcing platforms, and evaluates a number of existing systems against the set of identified features. Finally, the paper outlines a research agenda for enhancing crowdsourcing capabilities, with focus on virtual team building and task-based service provisioning, whose lack has been a barrier to the realization of a peer-production model that engages providers from around the world.},
booktitle = {Proceedings of the 2009 Congress on Services - I},
pages = {686–692},
numpages = {7},
keywords = {people services, crowdsourcing, cloud},
series = {SERVICES '09}
}

@inproceedings{10.1145/2386958.2386960,
author = {Chen, Xiao and Santos-Neto, Elizeu and Ripeanu, Matei},
title = {Crowdsourcing for on-street smart parking},
year = {2012},
isbn = {9781450316255},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2386958.2386960},
doi = {10.1145/2386958.2386960},
abstract = {Crowdsourcing has inspired a variety of novel mobile applications. However, identifying common practices across different applications is still challenging. In this paper, we use smart parking as a case study to investigate features of crowdsourcing that may apply to other mobile applications. Based on this we derive principles for efficiently harnessing crowdsourcing. We draw three key guidelines: First, we suggest that that the organizer can play an important role in coordinating participants', a key factor to successful crowdsourcing experience. Second, we suggest that the expected participation rate is a key factor when designing the crowdsourcing system: a system with a lower expected participation rate will place a higher burden in individual participants (e.g., through more complex interfaces that aim to improve the accuracy of the collected data). Finally, we suggest that not only above certain threshold of contributors, a crowdsourcing-based system is resilient to freeriding but, surprisingly, that including freeriders (i.e., actors that do not participate in system effort but share its benefits in terms of coordination) benefits the entire system.},
booktitle = {Proceedings of the Second ACM International Symposium on Design and Analysis of Intelligent Vehicular Networks and Applications},
pages = {1–8},
numpages = {8},
keywords = {smart parking, mobile crowdsourcing, collaborative sensing},
location = {Paphos, Cyprus},
series = {DIVANet '12}
}

@inproceedings{10.5555/2986459.2986660,
author = {Wauthier, Fabian L. and Jordan, Michael I.},
title = {Bayesian Bias Mitigation for Crowdsourcing},
year = {2011},
isbn = {9781618395993},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Biased labelers are a systemic problem in crowdsourcing, and a comprehensive toolbox for handling their responses is still being developed. A typical crowdsourcing application can be divided into three steps: data collection, data curation, and learning. At present these steps are often treated separately. We present Bayesian Bias Mitigation for Crowdsourcing (BBMC), a Bayesian model to unify all three. Most data curation methods account for the effects of labeler bias by modeling all labels as coming from a single latent truth. Our model captures the sources of bias by describing labelers as influenced by shared random effects. This approach can account for more complex bias patterns that arise in ambiguous or hard labeling tasks and allows us to merge data curation and learning into a single computation. Active learning integrates data collection with learning, but is commonly considered infeasible with Gibbs sampling inference. We propose a general approximation strategy for Markov chains to efficiently quantify the effect of a perturbation on the stationary distribution and specialize this approach to active learning. Experiments show BBMC to outperform many common heuristics.},
booktitle = {Proceedings of the 24th International Conference on Neural Information Processing Systems},
pages = {1800–1808},
numpages = {9},
location = {Granada, Spain},
series = {NIPS'11}
}

@inproceedings{10.5555/2887007.2887181,
author = {Chandra, Praphul and Narahari, Yadati and Mandal, Debmalya and Dey, Prasenjit},
title = {Novel mechanisms for online crowdsourcing with unreliable, strategic agents},
year = {2015},
isbn = {0262511290},
publisher = {AAAI Press},
abstract = {Motivated by current day crowdsourcing platforms and emergence of online labor markets, this work addresses the problem of task allocation and payment decisions when unreliable and strategic workers arrive over time to work on tasks which must be completed within a deadline. We consider the following scenario: a requester has a set of tasks that must be completed before a deadline; agents (aka crowd workers) arrive over time and it is required to make sequential decisions regarding task allocation and pricing. Agents may have different costs for providing service and these costs are private information of the agents. We assume that agents are not strategic about their arrival times but could be strategic about their costs of service. In addition, agents could be unreliable in the sense of not being able to complete the assigned tasks within the allocated time; these tasks must then be reallocated to other agents to ensure on-time completion of the set of tasks by the deadline. For this setting, we propose two mechanisms: a DPM (Dynamic Price Mechanism) and an ABM (Auction Based Mechanism). Both mechanisms are dominant strategy incentive compatible, budget feasible, and also satisfy ex-post individual rationality for agents who complete the allocated tasks. These mechanisms can be implemented in current day crowdsourcing platforms with minimal changes to the current interaction model.},
booktitle = {Proceedings of the Twenty-Ninth AAAI Conference on Artificial Intelligence},
pages = {1256–1262},
numpages = {7},
location = {Austin, Texas},
series = {AAAI'15}
}

@inproceedings{10.1007/978-3-642-39209-2_26,
author = {Nakatsu, Robbie and Grossman, Elissa},
title = {Designing effective user interfaces for crowdsourcing: an exploratory study},
year = {2013},
isbn = {9783642392085},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-642-39209-2_26},
doi = {10.1007/978-3-642-39209-2_26},
abstract = {We investigate characteristics of the technology platform for different types of crowdsourcing initatives, as characterized by their task type--specifically we classify crowdsourcing applications by task structure, task interdependence, and task commitment. The method employed is to examine best practices of well-known crowdsourcing applications, investigating their user interface features, and characteristics that make them successful examples of crowdsourcing. Among the best practices uncovered were the following: easy searching for information; adaptive user interfaces that learned from the crowd; easy-to-use mobile interfaces; the ability to vote ideas up or down; credentialing; and creating sticky user interfaces that engaged the user. Finally, we consider issues for further study and investigation.},
booktitle = {Proceedings of the 15th International Conference on Human Interface and the Management of Information: Information and Interaction Design - Volume Part I},
pages = {221–229},
numpages = {9},
keywords = {wisdom of the crowds, user interface design, open source design, online problem-solving platforms, distributed knowledge gathering, crowdsourcing},
location = {Las Vegas, NV},
series = {HCI International'13}
}

@inproceedings{10.1145/2556288.2557241,
author = {Kong, Nicholas and Hearst, Marti A. and Agrawala, Maneesh},
title = {Extracting references between text and charts via crowdsourcing},
year = {2014},
isbn = {9781450324731},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2556288.2557241},
doi = {10.1145/2556288.2557241},
abstract = {News articles, reports, blog posts and academic papers often include graphical charts that serve to visually reinforce arguments presented in the text. To help readers better understand the relation between the text and the chart, we present a crowdsourcing pipeline to extract the references between them. Specifically, we give crowd workers paragraph-chart pairs and ask them to select text phrases as well as the corresponding visual marks in the chart. We then apply automated clustering and merging techniques to unify the references generated by multiple workers into a single set. Comparing the crowdsourced references to a set of gold standard references using a distance measure based on the F1 score, we find that the average distance between the raw set of references produced by a single worker and the gold standard is 0.54 (out of a max of 1.0). When we apply clustering and merging techniques the average distance between the unified set of references and the gold standard reduces to 0.39; an improvement of 27\%. We conclude with an interactive document viewing application that uses the extracted references; readers can select phrases in the text and the system highlights the related marks in the chart.},
booktitle = {Proceedings of the SIGCHI Conference on Human Factors in Computing Systems},
pages = {31–40},
numpages = {10},
keywords = {visualization, interactive documents, crowdsourcing},
location = {Toronto, Ontario, Canada},
series = {CHI '14}
}

@inproceedings{10.1145/3277139.3277177,
author = {Zhang, Xi-zheng and Zhang, Leshao and Luo, Wen},
title = {A task assignment model and its application for crowdsourcing project factored multi-objective and risks},
year = {2018},
isbn = {9781450364867},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3277139.3277177},
doi = {10.1145/3277139.3277177},
abstract = {Assigning tasks to workers for a crowdsourcing project is challenging and needs to consider worker capacity, project duration, total cost and some operational constraints. We formulate the problem as a tri-objective optimisation model, which minimises project duration while simultaneously achieving lower total cost and fewer workers under the considering the capacity risk of contractor. A heuristic algorithm is developed to obtain Pareto efficient solutions. we used the fuzzy AHP(analytic hierarchy process) to confirm the priority among the time cost and the number of selected workers in the process of collaborative manufacturing task allocation. Finally, we designed the heuristic algorithm to solve the multi- objectives model and verified the validity superiority of the algorithm through a specific example.},
booktitle = {Proceedings of the 1st International Conference on Information Management and Management Science},
pages = {174–179},
numpages = {6},
keywords = {task assignment, multi-objective, heuristic algorithm, crowdsourcing},
location = {Chengdu, China},
series = {IMMS '18}
}

@inproceedings{10.1145/3127404.3127408,
author = {Fang, Yili and Chen, Pengpeng and Sun, Kai and Sun, Hailong},
title = {A Decision Tree Based Quality Control Framework for Multi-phase Tasks in Crowdsourcing},
year = {2017},
isbn = {9781450353526},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3127404.3127408},
doi = {10.1145/3127404.3127408},
abstract = {In crowdsourcing, there exists an important category of tasks that comprise an ordered sequence of subtasks, which we refer to as Multi-phase Tasks (MPTs) - e.g. travel planning, translation and micro-writing. Existing result inference methods are ineffective for processing MPTs. The constrained relationships among phase-level subtasks of MPT cannot be ignored for two reasons. First, it is ineffective to conduct a MPT without phase-processing, e.g. for travel planning, recommending a complete route of travel planning, and using existing methods to infer the final result generated by an individual worker can hardly meet various requirements due to the lack of flexibility. Second, although a MPT consists of a set of phase-level subtasks, it is unsuitable to simply split a MPT into subtasks and use top-k methods to recommend final results; because this will not only increase costs but also lose the constrained relationships among the phases. Thus it calls for a new approach to handle MPTs. This research first introduces the concept of MPT to identify these special tasks. Second, a decision tree based framework is provided to control task generation and final result combination in the crowdsourcing cooperative workflow for MPTs. Third, a probabilistic graphical model is proposed to characterize the subtasks of each MPT phase and a maximum likelihood based method is designed for result inference. Finally, extensive experiments were conducted based on real-world travel planning tasks and experimental results demonstrate the superiority of this approach in comparison with the state-of-the-art methods.},
booktitle = {Proceedings of the 12th Chinese Conference on Computer Supported Cooperative Work and Social Computing},
pages = {10–17},
numpages = {8},
keywords = {result inference, quality control, planning, multiphase tasks, Crowdsourcing},
location = {Chongqing, China},
series = {ChineseCSCW '17}
}

@inproceedings{10.1145/3019612.3019923,
author = {Faisal, Mohammad Imran},
title = {Predicting the quality of contests on crowdsourcing-based software development platforms: student research abstract},
year = {2017},
isbn = {9781450344869},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3019612.3019923},
doi = {10.1145/3019612.3019923},
abstract = {As an emerging and promising approach, crowdsourcing-based software development becomes popular in many domains due to the participation of talented pool of developers in the contests, and to promote the ability of requesters (or customers) to choose the 'wining' solution with respect to their desire quality levels. However, due to lack of a central mechanism for team formation, continuity in the developer's work on consecutive tasks and risk of noise in submissions of a contest, requesters of a domain have quality concerns to adopt a crowdsourcing-based software development platform. In order to address this concern, we proposed a measure Quality of Contest (QoC) to analyze and predict the quality of a crowdsourcing-based platform through historical information on its completed tasks. We evaluate the capacity of QoC as assessor to predict the quality. Subsequently, we implement a crawler to mine the information of completed development tasks from the TopCoder platform of Tech Platform Inc (TPI) to empirically investigate the proposed measures. The promising results of QoC measure suggest the applicability of the proposed measure across the other crowdsourcing-based platforms.},
booktitle = {Proceedings of the Symposium on Applied Computing},
pages = {1305–1306},
numpages = {2},
keywords = {topcoder, regression, quality, measure, development tasks, crowdsource},
location = {Marrakech, Morocco},
series = {SAC '17}
}

@inproceedings{10.1145/2595188.2595199,
author = {Forn\'{e}s, Alicia and Llad\'{o}s, Josep and Mas, Joan and Pujades, Joana Maria and Cabr\'{e}, Anna},
title = {A bimodal crowdsourcing platform for demographic historical manuscripts},
year = {2014},
isbn = {9781450325882},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2595188.2595199},
doi = {10.1145/2595188.2595199},
abstract = {In this paper we present a crowdsourcing web-based application for extracting information from demographic handwritten document images. The proposed application integrates two points of view: the semantic information for demographic research, and the ground-truthing for document analysis research. Concretely, the application has the contents view, where the information is recorded into forms, and the labeling view, with the word labels for evaluating document analysis techniques. The crowdsourcing architecture allows to accelerate the information extraction (many users can work simultaneously), validate the information, and easily provide feedback to the users. We finally show how the proposed application can be extended to other kind of demographic historical manuscripts.},
booktitle = {Proceedings of the First International Conference on Digital Access to Textual Cultural Heritage},
pages = {103–108},
numpages = {6},
keywords = {historical documents, ground-truth generation, document image analysis, crowdsourcing},
location = {Madrid, Spain},
series = {DATeCH '14}
}

@inproceedings{10.1145/2393347.2393400,
author = {Xu, Qianqian and Huang, Qingming and Yao, Yuan},
title = {Online crowdsourcing subjective image quality assessment},
year = {2012},
isbn = {9781450310895},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2393347.2393400},
doi = {10.1145/2393347.2393400},
abstract = {Recently, HodgeRank on random graphs has been proposed as an effective framework for multimedia quality assessment problem based on paired comparison method. With the random design on large graphs, it is particularly suitable for large scale crowdsourcing experiments on Internet. However, to make it more practical toward this purpose, it is necessary to develop online algorithms to deal with sequential or streaming data. In this paper, we propose an online rating scheme based on HodgeRank on random graphs, to assess image quality when assessors and image pairs enter the system in a sequential way in a crowdsourceable scenario. The scheme is shown in both theory and experiments to be effective by exhibiting similar performance to batch learning under the Erd\"{o}s-R\'{e}nyi random graph model for sampling. It enables us to derive global rating and monitor intrinsic inconsistency in the real time. We demonstrate the effectiveness of the proposed framework on LIVE and IVC databases.},
booktitle = {Proceedings of the 20th ACM International Conference on Multimedia},
pages = {359–368},
numpages = {10},
keywords = {triangular curl, topology evolution, subjective image quality assessment, random graphs, persistent homology, paired comparison, online, hodgerank, crowdsourcing},
location = {Nara, Japan},
series = {MM '12}
}

@inproceedings{10.1145/2390803.2390820,
author = {Nghiem, Thi Phuong and Carlier, Axel and Morin, Geraldine and Charvillat, Vincent},
title = {Enhancing online 3D products through crowdsourcing},
year = {2012},
isbn = {9781450315890},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2390803.2390820},
doi = {10.1145/2390803.2390820},
abstract = {In this paper, we propose to build semantic links between a product's textual description and its corresponding 3D visualization. These links help gathering knowledge about a product and ease browsing its 3D model. Our goal is to support the common behavior that when reading a textual information of a product, users naturally imagine how it looks like in real life. We generate the association between a textual description and a 3D feature from crowdsourcing. A user study of 82 people assesses the usefulness of the association for subsequent users, both for correctness and efficiency. Users are asked to perform the identification of features on 3D models; from the traces, associations leading to recommended views are derived. This information (recommended view) is proposed to subsequent users for performing the same task. Whereas the associations could be simply given by an expert, crowdsourcing offers advantages: we have inexpensive experts in the crowd as well as a natural access to users' (eg. customers') preferences and opinions.},
booktitle = {Proceedings of the ACM Multimedia 2012 Workshop on Crowdsourcing for Multimedia},
pages = {47–52},
numpages = {6},
keywords = {web3d technologies, web semantics, crowdsourcing},
location = {Nara, Japan},
series = {CrowdMM '12}
}

@inproceedings{10.1007/978-3-319-23063-4_23,
author = {Tranquillini, Stefano and Daniel, Florian and Kucherbaev, Pavel and Casati, Fabio},
title = {BPMN Task Instance Streaming for Efficient Micro-task Crowdsourcing Processes},
year = {2015},
isbn = {9783319230627},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-319-23063-4_23},
doi = {10.1007/978-3-319-23063-4_23},
abstract = {The Business Process Model and Notation BPMN is a standard for modeling and executing business processes with human or machine tasks. The semantics of tasks is usually discrete: a task has exactly one start event and one end event; for multi-instance tasks, all instances must complete before an end event is emitted. We propose a new task type and streaming connector for crowdsourcing able to run hundreds or thousands of micro-task instances in parallel. The two constructs provide for task streaming semantics that is new to BPMN, enable the modeling and efficient enactment of complex crowdsourcing scenarios, and are applicable also beyond the special case of crowdsourcing. We implement the necessary design and runtime support on top of CrowdFlower, demonstrate the viability of the approach via a case study, and report on a set of runtime performance experiments.},
booktitle = {Proceedings of the 13th International Conference on Business Process Management - Volume 9253},
pages = {333–349},
numpages = {17},
keywords = {Task instance streaming, Crowdsourcing processes, BPMN}
}

@inproceedings{10.1109/IMIS.2014.49,
author = {Barr\'{o}n, Jos\'{e} Pablo G\'{o}mez and Manso, Miguel \'{A}ngel and Alcarria, Ram\'{o}n and G\'{o}mez, Rufino P\'{e}rez},
title = {A Mobile Crowdsourcing Platform for Urban Infrastructure Maintenance},
year = {2014},
isbn = {9781479943319},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/IMIS.2014.49},
doi = {10.1109/IMIS.2014.49},
abstract = {Geospatial mobile applications interact with local and remote hardware to connect various data sources. These applications are currently evolving to include crowd sourcing functionalities, for citizen participation. In this paper we propose the identification, collection and information exchange of smart objects their geographic information, facilitating connectivity and communication between citizens and local organizations, and building interoperable services with data from sensors, smart physical objects and social media. We propose to identify spatial patterns, report object locations to identify problems and improve maintenance strategies of a city. As a validation of this approach, we develop a mobile mapping and data hub platform to visualize, monitor, and assist urban maintenance planning that enables better interaction between citizens of Smart Cities.},
booktitle = {Proceedings of the 2014 Eighth International Conference on Innovative Mobile and Internet Services in Ubiquitous Computing},
pages = {358–363},
numpages = {6},
keywords = {social media, geographical characterization, geo-information, crowdsourcing, NFC},
series = {IMIS '14}
}

@inproceedings{10.1109/ICDCS.2014.9,
author = {Boutsis, Ioannis and Kalogeraki, Vana},
title = {On Task Assignment for Real-Time Reliable Crowdsourcing},
year = {2014},
isbn = {9781479951697},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/ICDCS.2014.9},
doi = {10.1109/ICDCS.2014.9},
abstract = {With the rapid growth of mobile smartphone users, several commercial mobile companies have exploited crowd sourcing as an effective approach to collect and analyze data, to improve their services. In a crowd sourcing system, "human workers" are enlisted to perform small tasks, that are difficult to be automated, in return for some monetary compensation. This paper presents our crowd sourcing system that seeks to address the challenge of determining the most efficient allocation of tasks to the human crowd. The goal of our algorithm is to efficiently determine the most appropriate set of workers to assign to each incoming task, so that the real-time demands are met and high quality results are returned. We empirically evaluate our approach and show that our system effectively meets the requested demands, has low overhead and can improve the number of tasks processed under the defined constraints over 71\% compared to traditional approaches.},
booktitle = {Proceedings of the 2014 IEEE 34th International Conference on Distributed Computing Systems},
pages = {1–10},
numpages = {10},
keywords = {distributed systems, crowdsourcing, real-time},
series = {ICDCS '14}
}

@inproceedings{10.1109/SOLI.2016.7551657,
author = {Meht\"{a}l\"{a}, Joanna and Kauranen, Ilkka and Karjalainen, Jesse and Nyberg, Timo},
title = {A crowdsourcing model for new idea development in the fashion industry},
year = {2016},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/SOLI.2016.7551657},
doi = {10.1109/SOLI.2016.7551657},
abstract = {Crowdsourcing is a powerful tool for new product development; companies create online platforms where customers can contribute their own ideas to design tasks, often motivated by either a monetary prize or gaining social status. The phenomenon can help companies gain significant competitive advantage due to e.g. reduced R&amp;D costs and increased customer satisfaction, but to be successful, crowdsourcing needs to be implemented efficiently. The crowdsourcing platform and the practices related to it need to be built to enable effective data management and evaluation. Fashion is a very promising area for crowdsourcing due to its rapidly evolving nature. Styles can change overnight, which means that brands need to rely more and more on the knowledge of their customers to be able to provide them with the newest trends. This article first provides an overview on current crowdsourcing practices and crowdsourcing platforms used by companies in general, and then discusses special requirements that the fashion industry sets for crowdsourcing practices and crowdsourcing platforms. At the end, the article presents a new model of crowdsourcing processes that are especially suitable for the fashion industry in the design phase of new fashion garments.},
booktitle = {2016 IEEE International Conference on Service Operations and Logistics, and Informatics (SOLI)},
pages = {29–36},
numpages = {8},
location = {Beijing, China}
}

@inproceedings{10.1145/3578503.3583622,
author = {Marshall, Catherine C. and Goguladinne, Partha S.R. and Maheshwari, Mudit and Sathe, Apoorva and Shipman, Frank M.},
title = {Who Broke Amazon Mechanical Turk? An Analysis of Crowdsourcing Data Quality over Time},
year = {2023},
isbn = {9798400700897},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3578503.3583622},
doi = {10.1145/3578503.3583622},
abstract = {We present the results of a survey fielded in June of 2022 as a lens to examine recent data reliability issues on Amazon Mechanical Turk. We contrast bad data from this survey with bad data from the same survey fielded among US workers in October 2013, April 2018, and February 2019. Application of an established data cleaning scheme reveals that unusable data has risen from a little over 2\% in 2013 to almost 90\% in 2022. Through symptomatic diagnosis, we attribute the data reliability drop not to an increase in bad faith work, but rather to a continuum of English proficiency levels. A qualitative analysis of workers’ responses to open-ended questions allows us to distinguish between low fluency workers, ultra-low fluency workers, satisficers, and bad faith workers. We go on to show the effects of the new low fluency work on Likert scale data and on the study's qualitative results. Attention checks are shown to be much less effective than they once were at identifying survey responses that should be discarded.},
booktitle = {Proceedings of the 15th ACM Web Science Conference 2023},
pages = {335–345},
numpages = {11},
keywords = {Crowdsourcing, Mechanical Turk, data cleaning, data quality},
location = {Austin, TX, USA},
series = {WebSci '23}
}

@proceedings{10.1145/2810188,
title = {CrowdMM '15: Proceedings of the Fourth International Workshop on Crowdsourcing for Multimedia},
year = {2015},
isbn = {9781450337465},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {Crowdsourcing has the potential to address key challenges in multimedia research. Multimedia evaluation, annotation, retrieval and creation can be obtained at a low time and monetary cost from the contribution of large crowds and by leveraging human computation. In fact, the applicative frontiers of this potential are yet to be discovered. And yet, challenges already arise as to how to cautiously exploit it. The crowd, as a users' (workers) community, is a complex and dynamic system highly sensitive to changes in the form and the parameterization of their activities. Issues concerning motivation, reliability, and engagement are being more and more often documented, and need to be addressed.Since 2012, the International ACM Workshop on Crowdsourcing for Multimedia (CrowdMM) has welcomed new insights on the effective deployment of crowdsourcing towards boosting Multimedia research. On its fourth year, CrowdMM15 focuses on contributions addressing the key challenges that still hinder widespread adoption of crowdsourcing paradigms in the multimedia research community: remote monitoring of the user behavior, effective test design, controlling noise and quality in the results, designing incentive structures that do not breed cheating, and effective ways of keeping the user (the crowd!) in the loop to boost multimedia applications.The call for papers attracted a good number of international submissions, two of which short papers. Of these, three were accepted as oral presentations and four as posters. All papers received at least three double blind reviews, and 3.5 reviews on average.CrowdMM15 also proudly features the keynote talk of Prof. Shih-Fu Chang (Columbia University), addressing Crowdsourcing in video event detection, sentiment analysis and user intent modelling. Furthermore, for the second year this year CrowdMM proposes the Crowdkeynote: a crowd-sourced keynote, during which all members of the CrowdMM community give their view on the future and the Challenges that Crowdsourcing has still ahead. The slides of the Crowdkeynote can be found at https://goo.gl/Xlur2E.},
location = {Brisbane, Australia}
}

@inproceedings{10.1145/2487294.2487332,
author = {Tajedin, Hamed and Nevo, Dorit},
title = {Determinants of success in crowdsourcing software development},
year = {2013},
isbn = {9781450319751},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2487294.2487332},
doi = {10.1145/2487294.2487332},
abstract = {With the advent of digitization, recent years have witnessed a surge toward collective undertaking of production process different from traditional ways of organizing. In this vein, crowdsourcing has lent itself into a successful emerging mode of organizing and firms are increasingly using it in their value creation activities. However, despite popularity in practice, crowdsourcing has received little attention from IS scholars. Specifically, what the determinants of success in this model are remains an unexplored area of research that we strive to address in this paper. We focus on software development via crowdsourcing and drawing on studies from IS success, OSS and software development, we build a model of success that has three determinants: the characteristics of the project, the composition of the crowd and the relationship among key players. Finally, we describe our research methodology and conclude with potential contributions of our work.},
booktitle = {Proceedings of the 2013 Annual Conference on Computers and People Research},
pages = {173–178},
numpages = {6},
keywords = {software development, outsourcing, opensource software, crowdsourcing},
location = {Cincinnati, Ohio, USA},
series = {SIGMIS-CPR '13}
}

@inproceedings{10.1007/978-3-642-34630-9_20,
author = {Hardas, Manas S. and Purvis, Lisa},
title = {Bayesian vote weighting in crowdsourcing systems},
year = {2012},
isbn = {9783642346293},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-642-34630-9_20},
doi = {10.1007/978-3-642-34630-9_20},
abstract = {In social collaborative crowdsourcing platforms, the votes which people give on the content generated by others is a very important component of the system which seeks to find the best content through collaborative action. In a crowdsourced innovation platform, people vote on innovations/ideas generated by others which enables the system to synthesize the view of the crowd about an idea. However, in many such systems gaming or vote spamming as it is commonly known is prevalent. In this paper we present a Bayesian mechanism for weighting the actual vote given by a user to compute an effective vote which incorporates the voters history of voting and also what the crowd is thinking about the value of the innovation. The model results into some interesting insights about social voting systems and new avenues for gamification.},
booktitle = {Proceedings of the 4th International Conference on Computational Collective Intelligence: Technologies and Applications - Volume Part I},
pages = {194–203},
numpages = {10},
location = {Ho Chi Minh City, Vietnam},
series = {ICCCI'12}
}

@inproceedings{10.1109/ACII.2013.18,
author = {Morris, Robert R. and Dontcheva, Mira and Finkelstein, Adam and Gerber, Elizabeth},
title = {Affect and Creative Performance on Crowdsourcing Platforms},
year = {2013},
isbn = {9780769550480},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/ACII.2013.18},
doi = {10.1109/ACII.2013.18},
abstract = {Performance on crowd sourcing platforms varies greatly, especially for tasks requiring significant cognitive effort or creative insight. Researchers have proposed several techniques to address these problems, yet few have considered the role of affect, despite the well-established link between positive affect and creative performance. In this paper, we examine two affective techniques to boost creativity on crowd sourcing platforms - affective priming and affective pre-screening. Across three experiments, we find divergent results, depending on which technique is used. We find that not all happy crowd workers are alike. Those that are primed to feel happy exhibit enhanced creative performance, whereas those that merely report feeling happy exhibit impaired creative performance. We examine these findings in light of preexisting research on creativity, affect, and mood saliency. Lastly, we show how our findings have implications not only for crowd sourcing platforms, but also for other human-computer interaction scenarios that involve affect and creative performance.},
booktitle = {Proceedings of the 2013 Humaine Association Conference on Affective Computing and Intelligent Interaction},
pages = {67–72},
numpages = {6},
keywords = {crowdsourcing, creativity, affective self-report, affective priming, affective computing},
series = {ACII '13}
}

@inproceedings{10.1145/2820783.2820831,
author = {Deng, Dingxiong and Shahabi, Cyrus and Zhu, Linhong},
title = {Task matching and scheduling for multiple workers in spatial crowdsourcing},
year = {2015},
isbn = {9781450339674},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2820783.2820831},
doi = {10.1145/2820783.2820831},
abstract = {A new platform, termed spatial crowdsourcing, is emerging which enables a requester to commission workers to physically travel to some specified locations to perform a set of spatial tasks (i.e., tasks related to a geographical location and time). The current approach is to formulate spatial crowdsourcing as a matching problem between tasks and workers; hence the primary objective of the existing solutions is to maximize the number of matched tasks. Our goal is to solve the spatial crowdsourcing problem in the presence of multiple workers where we optimize for both travel cost and the number of completed tasks, while taking the tasks' expiration times into consideration. The challenge is that the solution should be a mixture of task-matching and task-scheduling, which are fundamentally different. In this paper, we show that a baseline approach that performs a task-matching first, and subsequently schedules the tasks assigned per worker in a following phase, does not perform well. Hence, we add a third phase in which we iterate back to the matching phase to improve the assignment per the output of the scheduling phase, and thus further improves the quality of matching and scheduling. Even though this 3-phase approach generates high quality results, it is very slow and does not scale. Hence, to scale our algorithm to large number of workers and tasks, we propose a Bisection-based framework which recursively divides all the workers and tasks into different partitions such that assignment and scheduling can be performed locally in a much smaller and promising space. Our experiments show that this approach is three orders of magnitude faster than the 3-phase approach while it only sacrifices 4\% of the results' quality.},
booktitle = {Proceedings of the 23rd SIGSPATIAL International Conference on Advances in Geographic Information Systems},
articleno = {21},
numpages = {10},
keywords = {task matching and scheduling, spatial crowdsourcing, scalability},
location = {Seattle, Washington},
series = {SIGSPATIAL '15}
}

@inproceedings{10.1145/1935826.1935831,
author = {Alonso, Omar and Lease, Matthew},
title = {Crowdsourcing 101: putting the WSDM of crowds to work for you},
year = {2011},
isbn = {9781450304931},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1935826.1935831},
doi = {10.1145/1935826.1935831},
abstract = {Crowdsourcing has emerged in recent years as an exciting new avenue for leveraging the tremendous potential and resources of today's digitally-connected, diverse, distributed workforce. Generally speaking, crowdsourcing describes outsourcing of tasks to a large group of people instead of assigning such tasks to an in-house employee or contractor. Crowdsourcing platforms such as Amazon Mechanical Turk and CrowdFlower have gained particular attention as active online market places for reaching and tapping into this glut of a still largely under-utilized workforce.Crowdsourcing offers intriguing new opportunities for accomplishing different kinds of tasks or achieving broader participation than previously possible, as well as completing standard tasks more accurately in less time and at lower cost. Unlocking the potential of crowdsourcing in practice, however, requires a tri-partite understanding of principles, platforms, and best practices. This tutorial will introduce the opportunities and challenges of crowdsourcing while discussing the three issues above. This will provide attendees with a basic foundation to begin applying crowdsourcing in the context of their own particular tasks.},
booktitle = {Proceedings of the Fourth ACM International Conference on Web Search and Data Mining},
pages = {1–2},
numpages = {2},
keywords = {human computation, crowdsourcing},
location = {Hong Kong, China},
series = {WSDM '11}
}

@inproceedings{10.5555/3172077.3172302,
author = {Wang, Wei and Guo, Xiang-Yu and Li, Shao-Yuan and Jiang, Yuan and Zhou, Zhi-Hua},
title = {Obtaining high-quality label by distinguishing between easy and hard items in crowdsourcing},
year = {2017},
isbn = {9780999241103},
publisher = {AAAI Press},
abstract = {Crowdsourcing systems make it possible to hire voluntary workers to label large-scale data by offering them small monetary payments. Usually, the taskmaster requires to collect high-quality labels, while the quality of labels obtained from the crowd may not satisfy this requirement. In this paper, we study the problem of obtaining high-quality labels from the crowd and present an approach of learning the difficulty of items in crowdsourcing, in which we construct a small training set of items with estimated difficulty and then learn a model to predict the difficulty of future items. With the predicted difficulty, we can distinguish between  easy and  hard  items to obtain high-quality labels. For  easy  items, the quality of their labels inferred from the crowd could be high enough to satisfy the requirement; while for  hard  items, the crowd could not provide high-quality labels, it is better to choose a more knowledgable crowd or employ specialized workers to label them. The experimental results demonstrate that the proposed approach by learning to distinguish between  easy  and  hard  items can significantly improve the label quality.},
booktitle = {Proceedings of the 26th International Joint Conference on Artificial Intelligence},
pages = {2964–2970},
numpages = {7},
location = {Melbourne, Australia},
series = {IJCAI'17}
}

@inproceedings{10.1145/2800835.2800967,
author = {Konomi, Shin'ichi and Sasao, Tomoyo},
title = {The use of colocation and flow networks in mobile crowdsourcing},
year = {2015},
isbn = {9781450335751},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2800835.2800967},
doi = {10.1145/2800835.2800967},
abstract = {Requesting relevant tasks to mobile crowds is extremely difficult without considering their movements. We propose an approach to geo-cast crowdsourcing tasks based on networks of human flows, and show that our approach can achieve higher geographical relevance than simple proximity-based approaches.},
booktitle = {Adjunct Proceedings of the 2015 ACM International Joint Conference on Pervasive and Ubiquitous Computing and Proceedings of the 2015 ACM International Symposium on Wearable Computers},
pages = {1343–1348},
numpages = {6},
keywords = {situated crowdsourcing, mobility, mobile crowdsourcing, complex networks, community detection},
location = {Osaka, Japan},
series = {UbiComp/ISWC'15 Adjunct}
}

@inproceedings{10.1109/CVPR.2013.81,
author = {Deng, Jia and Krause, Jonathan and Fei-Fei, Li},
title = {Fine-Grained Crowdsourcing for Fine-Grained Recognition},
year = {2013},
isbn = {9780769549897},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/CVPR.2013.81},
doi = {10.1109/CVPR.2013.81},
abstract = {Fine-grained recognition concerns categorization at sub-ordinate levels, where the distinction between object classes is highly local. Compared to basic level recognition, fine-grained categorization can be more challenging as there are in general less data and fewer discriminative features. This necessitates the use of stronger prior for feature selection. In this work, we include humans in the loop to help computers select discriminative features. We introduce a novel online game called "Bubbles" that reveals discriminative features humans use. The player's goal is to identify the category of a heavily blurred image. During the game, the player can choose to reveal full details of circular regions ("bubbles"), with a certain penalty. With proper setup the game generates discriminative bubbles with assured quality. We next propose the "Bubble Bank" algorithm that uses the human selected bubbles to improve machine recognition performance. Experiments demonstrate that our approach yields large improvements over the previous state of the art on challenging benchmarks.},
booktitle = {Proceedings of the 2013 IEEE Conference on Computer Vision and Pattern Recognition},
pages = {580–587},
numpages = {8},
series = {CVPR '13}
}

@inproceedings{10.1007/978-3-662-45960-7_19,
author = {Poblet, Marta and Garc\'{\i}a-Cuesta, Esteban and Casanovas, Pompeu},
title = {Crowdsourcing Tools for Disaster Management: A Review of Platforms and Methods},
year = {2013},
isbn = {9783662459591},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-662-45960-7_19},
doi = {10.1007/978-3-662-45960-7_19},
abstract = {Recent advances on information technologies and communications, coupled with the advent of the social media applications have fuelled a new landscape of emergency and disaster response systems by enabling affected citizens to generate georeferenced real time information on critical events. The identification and analysis of such events is not straightforward and the application of crowdsourcing methods or automatic tools is needed for that purpose. Whereas crowdsourcing makes emphasis on the resources of people to produce, aggregate, or filter original data, automatic tools make use of information retrieval techniques to analyze publicly available information. This paper reviews a set of online tools and platforms implemented in recent years which are currently being applied in the area of emergency management and proposes a taxonomy for its categorization.},
booktitle = {Revised Selected Papers of the AICOL 2013 International Workshops on AI Approaches to the Complexity of Legal Systems - Volume 8929},
pages = {261–274},
numpages = {14},
keywords = {platforms, mobile apps, micro-tasking, emergency management, disaster management, crowdsourcing, crowdsensing}
}

@inproceedings{10.1109/WAINA.2014.28,
author = {Sharifi, Mahdi and Manaf, Azizah Abdul and Memariani, Ali and Movahednejad, Homa and Sarkan, Haslina Md and Dastjerdi, Amir Vahid},
title = {Multi-criteria Consensus-Based Service Selection Using Crowdsourcing},
year = {2014},
isbn = {9781479926534},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/WAINA.2014.28},
doi = {10.1109/WAINA.2014.28},
abstract = {Different evaluator entities, either human agents (e.g., experts) or software agents (e.g., monitoring services), are involved to assess QoS parameters of candidate services, which leads to diversity in service assessments. This diversity makes the service selection a challenging task, especially when numerous qualities of service criteria and range of providers are considered. To address this problem, this study as first step presents a consensus-based service assessment methodology that utilizes consensus theory to evaluate the service behavior for single QoS criteria using the power of crowd sourcing. For this purpose, trust level metrics are introduced to measure the strength of a consensus based on the trustworthiness levels of crowd members. The peers converged to the most trustworthy evaluation. Next, the fuzzy inference engine was used to aggregate each obtained assessed QoS value based on user preferences because we address multiple QoS criteria in real life scenarios.},
booktitle = {Proceedings of the 2014 28th International Conference on Advanced Information Networking and Applications Workshops},
pages = {114–120},
numpages = {7},
keywords = {Web service, Consensus, Trust, Service selection, Fuzzy aggregation},
series = {WAINA '14}
}

@inproceedings{10.1145/2600057.2602880,
author = {Ho, Chien-Ju and Slivkins, Aleksandrs and Vaughan, Jennifer Wortman},
title = {Adaptive contract design for crowdsourcing markets: bandit algorithms for repeated principal-agent problems},
year = {2014},
isbn = {9781450325653},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2600057.2602880},
doi = {10.1145/2600057.2602880},
abstract = {Crowdsourcing markets have emerged as a popular platform for matching available workers with tasks to complete. The payment for a particular task is typically set by the task's requester, and may be adjusted based on the quality of the completed work, for example, through the use of 'bonus' payments. In this paper, we study the requester's problem of dynamically adjusting quality-contingent payments for tasks. We consider a multi-round version of the well-known principal-agent model, whereby in each round a worker makes a strategic choice of the effort level which is not directly observable by the requester. In particular, our formulation significantly generalizes the budget-free online task pricing problems studied in prior work.We treat this problem as a multi-armed bandit problem, with each 'arm' representing a potential contract. To cope with the large (and in fact, infinite) number of arms, we propose a new algorithm, AgnosticZooming, which discretizes the contract space into a finite number of regions, effectively treating each region as a single arm. This discretization is adaptively refined, so that more promising regions of the contract space are eventually discretized more finely. We provide a full analysis of this algorithm, showing that it achieves regret sublinear in the time horizon and substantially improves over non-adaptive discretization (which is the only competing approach in the literature).},
booktitle = {Proceedings of the Fifteenth ACM Conference on Economics and Computation},
pages = {359–376},
numpages = {18},
keywords = {regret, principal-agent, multi-armed bandits, dynamic pricing, crowdsourcing},
location = {Palo Alto, California, USA},
series = {EC '14}
}

@inproceedings{10.1145/2487575.2487600,
author = {Baba, Yukino and Kashima, Hisashi},
title = {Statistical quality estimation for general crowdsourcing tasks},
year = {2013},
isbn = {9781450321747},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2487575.2487600},
doi = {10.1145/2487575.2487600},
abstract = {One of the biggest challenges for requesters and platform providers of crowdsourcing is quality control, which is to expect high-quality results from crowd workers who are neither necessarily very capable nor motivated. A common approach to tackle this problem is to introduce redundancy, that is, to request multiple workers to work on the same tasks. For simple multiple-choice tasks, several statistical methods to aggregate the multiple answers have been proposed. However, these methods cannot always be applied to more general tasks with unstructured response formats such as article writing, program coding, and logo designing, which occupy the majority on most crowdsourcing marketplaces. In this paper, we propose an unsupervised statistical quality estimation method for such general crowdsourcing tasks. Our method is based on the two-stage procedure; multiple workers are first requested to work on the same tasks in the creation stage, and then another set of workers review and grade each artifact in the review stage. We model the ability of each author and the bias of each reviewer, and propose a two-stage probabilistic generative model using the graded response model in the item response theory. Experiments using several general crowdsourcing tasks show that our method outperforms popular vote aggregation methods, which implies that our method can deliver high quality results with lower costs.},
booktitle = {Proceedings of the 19th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {554–562},
numpages = {9},
keywords = {quality control, human computation, crowdsourcing},
location = {Chicago, Illinois, USA},
series = {KDD '13}
}

@inproceedings{10.1109/SOSE.2015.48,
author = {Fan, Yue and Sun, Hailong and Zhu, Yanmin and Liu, Xudong and Yuan, Ji},
title = {A Truthful Online Auction for Tempo-spatial Crowdsourcing Tasks},
year = {2015},
isbn = {9781479983568},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/SOSE.2015.48},
doi = {10.1109/SOSE.2015.48},
abstract = {Mobile crowdsourcing is an emerging paradigm which utilizes the distributed smartphones to monitor diverse phenomena about human activities and surrounding environment, enabling a large number of mobile crowdsourcing applications. For those applications to collect sufficient data, motivating smartphone users to be interested in mobile crowdsourcing campaign becomes very significant. Most of the incentive mechanisms assume that tasks are static in mobile crowdsourcing systems. Even for those studies that take the uncertain arrival of tasks into consideration, they always ignore the important geographic location information of the tasks. In the paper, we propose a near-optimal online incentive mechanism based on a more realistic scenario in which crowdsourcing tasks and users both arrive dynamically with tempo-spatial constraints. Through adequate simulations and rigorous theoretical analysis, the online mechanism is proved to satisfy the properties of truthfulness, computational efficiency, individual rationality, and achieve high social welfare and low total payment.},
booktitle = {Proceedings of the 2015 IEEE Symposium on Service-Oriented System Engineering},
pages = {332–338},
numpages = {7},
series = {SOSE '15}
}

@inproceedings{10.5555/3304652.3304764,
author = {Salisbury, Elliot and Kamar, Ece and Morris, Meredith Ringel},
title = {Evaluating and complementing vision-to-language technology for people who are blind with conversational crowdsourcing},
year = {2018},
isbn = {9780999241127},
publisher = {AAAI Press},
abstract = {We study how real-time crowdsourcing can be used both for evaluating the value provided by existing automated approaches and for enabling workflows that provide scalable and useful alt text to blind users. We show that the shortcomings of existing AI image captioning systems frequently hinder a user's understanding of an image they cannot see to a degree that even clarifying conversations with sighted assistants cannot correct. Based on analysis of clarifying conversations collected from our studies, we design experiences that can effectively assist users in a scalable way without the need for real-time interaction. Our results provide lessons and guidelines that the designers of future AI captioning systems can use to improve labeling of social media imagery for blind users.},
booktitle = {Proceedings of the 27th International Joint Conference on Artificial Intelligence},
pages = {5349–5353},
numpages = {5},
location = {Stockholm, Sweden},
series = {IJCAI'18}
}

@inproceedings{10.1109/eScience.2015.63,
author = {Martinez-Ortiz, Carlos and Aroyo, Lora and Inel, Oana and Champilomatis, Stavros and Dumitrache, Anca and Timmermans, Benjamin},
title = {Provenance-driven Representation of Crowdsourcing Data for Efficient Data Analysis},
year = {2015},
isbn = {9781467393256},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/eScience.2015.63},
doi = {10.1109/eScience.2015.63},
abstract = {Crowdsourcing has proved to be a feasible way of harnessing human computation for solving complex problems. However, crowdsourcing frequently faces various challenges: data handling, task reusability, and platform selection. Domain scientists rely on eScientists to find solutions for these challenges. CrowdTruth is a framework that builds on existing crowdsourcing platforms and provides an enhanced way to manage crowdsourcing tasks across platforms, offering solutions to commonly faced challenges. Provenance modeling proves means for documenting and examining scientific workflows. CrowdTruth keeps a provenance trace of the data flow through the framework, thus allowing to trace how data was transformed and by whom to reach its final state. In this way, eScientists have a tool to determine the impact that crowdsourcing has on enhancing their data.},
booktitle = {Proceedings of the 2015 IEEE 11th International Conference on E-Science},
pages = {300–303},
numpages = {4},
keywords = {provenance, Crowdsourcing},
series = {E-SCIENCE '15}
}

@inproceedings{10.1145/2442882.2442909,
author = {Ei Chew, Han and Sort, Borort and Haddawy, Peter},
title = {Building a crowdsourcing community: how online social learning helps in poverty reduction},
year = {2013},
isbn = {9781450318563},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2442882.2442909},
doi = {10.1145/2442882.2442909},
abstract = {In this paper, we describe the design and use of a knowledge sharing network that has recently been deployed for agricultural extension work in the Lao People's Democratic Republic (Lao PDR). The system, Poverty Reduction and Agricultural Management -- Knowledge Sharing Network (PRAM-KSN), was built using a collaborative design process that involved both experts and ministerial agricultural extension workers who are also the current users of this web-based platform. This paper also discusses the relevance of the PRAM-KSN for agricultural extension work, how the principles of crowdsourcing apply to the system, and how social learning occurs for the benefit of agricultural extension work. Suggestions for impact assessment of the PRAM-KSN at different time-frames are offered.},
booktitle = {Proceedings of the 3rd ACM Symposium on Computing for Development},
articleno = {21},
numpages = {2},
keywords = {social cognitive theory, poverty reduction, participatory design, evaluation, crowdsourcing},
location = {Bangalore, India},
series = {ACM DEV '13}
}

@inproceedings{10.1145/2501105.2501113,
author = {Di Salvo, R. and Giordano, D. and Kavasidis, I.},
title = {A crowdsourcing approach to support video annotation},
year = {2013},
isbn = {9781450321693},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2501105.2501113},
doi = {10.1145/2501105.2501113},
abstract = {In this paper we present an innovative approach to support efficient large scale video annotation by exploiting the crowdsourcing. In particular, we collect big noisy annotations by an on-line Flash game which aims at taking photos of objects appearing through the game levels. The data gathered (suitably processed) from the game is then used to drive image segmentation approaches, namely the Region Growing and Grab Cut, which allow us to derive meaningful annotations. A comparison against hand-labeled ground truth data showed that the proposed approach constitutes a valid alternative to the existing video annotation approaches and allow a reliable and fast collection of large scale ground truth data for performance evaluation in computer vision.},
booktitle = {Proceedings of the International Workshop on Video and Image Ground Truth in Computer Vision Applications},
articleno = {8},
numpages = {6},
keywords = {seed positioning, online game, image segmentation, ground truth generation},
location = {St. Petersburg, Russia},
series = {VIGTA '13}
}

@inproceedings{10.1145/2818052.2869098,
author = {Wasik, Szymon and Antczak, Maciej and Badura, Jan and Laskowski, Artur and Sternal, Tomasz},
title = {Optil.io: Cloud Based Platform For Solving Optimization Problems Using Crowdsourcing Approach},
year = {2016},
isbn = {9781450339506},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2818052.2869098},
doi = {10.1145/2818052.2869098},
abstract = {The main objective of the presented research is to design a platform for continuous evaluation of optimization algorithms using crowdsourcing technique. The resulting platform, called Optil.io, runs in a cloud using platform as a service model and allows researchers from all over the world to collaboratively solve computational problems. This is the approach that has been already proved to be very successful for data mining problems by web services such as Kaggle. During our project we adapted this concept for solving computational problems that require implementation of software. To achieve this we designed the on-line judge system that receives algorithmic solutions in a form of source code from the crowd of programmers, compiles it, executes in a homogeneous run-time environment and objectively evaluates using the set of test cases. It was verified during internal experiments at the Poznan University of Technology and it is now ready to be presented to wider audience.},
booktitle = {Proceedings of the 19th ACM Conference on Computer Supported Cooperative Work and Social Computing Companion},
pages = {433–436},
numpages = {4},
keywords = {platform as a service, optimization, operational research, on-line judge, crowdsourcing, cloud computing, algorithms},
location = {San Francisco, California, USA},
series = {CSCW '16 Companion}
}

@proceedings{10.1145/2897659,
title = {CSI-SE '16: Proceedings of the 3rd International Workshop on CrowdSourcing in Software Engineering},
year = {2016},
isbn = {9781450341585},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {It is our pleasure to welcome the reader to the (pre-workshop) proceedings of the 3rd International Workshop on CrowdSourcing in Software Engineering (CSI-SE 2016), co-located with the 38th International Conference on Software Engineering (ICSE 2016) held in Austin, TX, USA on May 16, 2016.A number of trends under the broad banner of crowdsourcing are beginning to fundamentally disrupt the way in which software is engineered. Programmers increasingly rely on crowdsourced knowledge and code, as they look at Q&amp;A websites for answers or use code from publicly posted snippets. Programmers play, compete, and learn with the crowd, engaging in programming competitions and puzzles with crowds of programmers. Online IDEs make radically new forms of collaboration possible, allowing developers to synchronously program with crowds of distributed programmers. Programmer reputation is increasingly visible on Q&amp;A sites and public code repositories, opening new possibilities in how developers find jobs and companies identify talent. Crowds of non-programmers increasingly participate in development, usability testing software or even constructing specifications while playing games. Developers can take feedback from a crowd of users to guide further evolution of their applications. Crowdfunding democratizes choices about which software is built, broadening the software which might be feasibly constructed. Approaches for crowd development seek to microtask software development, dramatically increasing participation in open source by enabling software projects to be built through casual, transient work.},
location = {Austin, Texas}
}

@inproceedings{10.1007/978-3-030-67835-7_18,
author = {Nguyen, Dang-Hieu and Nguyen-Tai, Tan-Loc and Nguyen, Minh-Tam and Nguyen, Thanh-Binh and Dao, Minh-Son},
title = {MNR-Air: An Economic and Dynamic Crowdsourcing Mechanism to Collect Personal Lifelog and Surrounding Environment Dataset. A Case Study in Ho Chi Minh City, Vietnam},
year = {2021},
isbn = {978-3-030-67834-0},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-67835-7_18},
doi = {10.1007/978-3-030-67835-7_18},
abstract = {This paper introduces an economical and dynamic crowdsourcing mechanism to collect personal lifelog associated environment datasets, namely MNR-Air. This mechanism’s significant advantage is to use personal sensor boxes that can be carried on citizens (and their vehicles) to collect data. The MNR-HCM dataset is also introduced in this paper as the output of MNR-Air and collected in Ho Chi Minh City, Vietnam. The MNR-HCM dataset contains weather data, air pollution data, GPS data, lifelog images, and citizens’ cognition of urban nature on a personal scale. We also introduce AQI-T-RM, an application that can help people plan their travel to avoid as much air pollution as possible while still saving time on travel. Besides, we discuss how useful MNR-Air is when contributing to the open data science community and other communities that benefit citizens living in urban areas.},
booktitle = {MultiMedia Modeling: 27th International Conference, MMM 2021, Prague, Czech Republic, June 22–24, 2021, Proceedings, Part II},
pages = {206–217},
numpages = {12},
keywords = {Crowdsourcing, Smart navigation, Sensor data, Sensors, AQI, PM2.5, Particulate matter},
location = {Prague, Czech Republic}
}

@inproceedings{10.5555/3054117.3054120,
author = {Xiang, Qikun and Nevat, Ido and Zhang, Pengfei and Zhang, Jie},
title = {Collusion-resistant spatial phenomena crowdsourcing via mixture of Gaussian processes regression},
year = {2016},
publisher = {CEUR-WS.org},
address = {Aachen, DEU},
abstract = {With the rapid development of mobile devices, spatial location-based crowdsourcing applications have attracted much attention. These applications also introduce new security risks due to untrustworthy data sources. In the context of crowdsourcing applications for spatial interpolation (i.e. spatial regression) using crowdsourced data, the results can be seriously affected if malicious data sources initiate a colluding (collaborate) attacks which purposely alter some of the measurements. To combat this serious detrimental effect, and to mitigate such attacks, we develop a robust version via a Gaussian Process mixture model and develop a computationally efficient algorithm which utilises a Markov chain Monte Carlo (MCMC)-based methodology to produce an accurate predictive inference in the presence of collusion attacks. The algorithm is fully Bayesian and produces posterior predictive distribution for any point-of-interest in the input space. It also assesses the trustworthiness of each worker, i.e. the probability of each worker being honest (trustworthy). Simulation results demonstrate the accuracy of this algorithm.},
booktitle = {Proceedings of the 18th International Conference on Trust in Agent Societies - Volume 1578},
pages = {30–41},
numpages = {12},
location = {Singapore, Singapore},
series = {TRUST'16}
}

@inproceedings{10.1145/2463728.2463819,
author = {Goodspeed, Robert and Spanring, Christian and Reardon, Timothy},
title = {Crowdsourcing as data sharing: a regional web-based real estate development database},
year = {2012},
isbn = {9781450312004},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2463728.2463819},
doi = {10.1145/2463728.2463819},
abstract = {The paper describes a web-based database of residential and commercial real estate development projects, created by a regional urban planning agency in Metropolitan Boston. In Phase I, now complete, the tool is used to facilitate inter-agency information sharing, demonstrating the ability of web-base data collection tools to increase information sharing between government agencies by reducing transaction costs. Phase II, now under development, will expand the functionality of the website to allow the general public to contribute information to the database, as well as view and download its contents. The project is unusual in its integration of a crowdsourcing paradigm with traditional methods of spatial information sharing. The project demonstrates the potential for technology to facilitate data sharing in favorable contexts where interorganizational relationships and sharing norms exist.},
booktitle = {Proceedings of the 6th International Conference on Theory and Practice of Electronic Governance},
pages = {460–463},
numpages = {4},
keywords = {web 2.0, urban development, spatial data sharing, crowdsourcing},
location = {Albany, New York, USA},
series = {ICEGOV '12}
}

@inproceedings{10.1109/ASWEC.2014.11,
author = {Xiao, Lu and Paik, Hye-Young},
title = {Supporting Complex Work in Crowdsourcing Platforms: A View from Service-Oriented Computing},
year = {2014},
isbn = {9781479931491},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/ASWEC.2014.11},
doi = {10.1109/ASWEC.2014.11},
abstract = {Crowd sourcing is changing the way people work and solve problems from "in-house working" to "public out-sourcing". Most online crowd sourcing platforms perform two main functions: (i) allowing users to advertise their tasks and (ii) helping them find candidate workers. However, they do not support crowd sourcing of complex work consisting of interdependent tasks. Those tasks require not only having simple task/worker pairs, but also coordinating multiple workers together for completion of the crowd work. In this paper, we propose a conceptual framework to bring the coordination support into current online crowd sourcing platforms. In our framework, each crowd worker is modeled as a service that can be self-described, dynamically discovered and assembled into the complex crowd work, meanwhile, we define a workflow-based schema to structure and represent the complex crowd work. Then the crowd sourcing performance is initiated and managed by our coordination protocol.},
booktitle = {Proceedings of the 2014 23rd Australian Software Engineering Conference},
pages = {11–14},
numpages = {4},
series = {ASWEC '14}
}

@inproceedings{10.1145/2872427.2883070,
author = {Mavridis, Panagiotis and Gross-Amblard, David and Mikl\'{o}s, Zolt\'{a}n},
title = {Using Hierarchical Skills for Optimized Task Assignment in Knowledge-Intensive Crowdsourcing},
year = {2016},
isbn = {9781450341431},
publisher = {International World Wide Web Conferences Steering Committee},
address = {Republic and Canton of Geneva, CHE},
url = {https://doi.org/10.1145/2872427.2883070},
doi = {10.1145/2872427.2883070},
abstract = {Besides the simple human intelligence tasks such as image labeling, crowdsourcing platforms propose more and more tasks that require very specific skills, especially in participative science projects. In this context, there is a need to reason about the required skills for a task and the set of available skills in the crowd, in order to increase the resulting quality. Most of the existing solutions rely on unstructured tags to model skills (vector of skills). In this paper we propose to finely model tasks and participants using a skill tree, that is a taxonomy of skills equipped with a similarity distance within skills. This model of skills enables to map participants to tasks in a way that exploits the natural hierarchy among the skills. We illustrate the effectiveness of our model and algorithms through extensive experimentation with synthetic and real data sets.},
booktitle = {Proceedings of the 25th International Conference on World Wide Web},
pages = {843–853},
numpages = {11},
keywords = {task mapping, skill modeling, crowdsourcing},
location = {Montr\'{e}al, Qu\'{e}bec, Canada},
series = {WWW '16}
}

@inproceedings{10.1007/978-3-319-21410-8_43,
author = {Tiwari, Sunita and Kaushik, Saroj},
title = {Crowdsourcing Based Fuzzy Information Enrichment of Tourist Spot Recommender Systems},
year = {2015},
isbn = {9783319214092},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-319-21410-8_43},
doi = {10.1007/978-3-319-21410-8_43},
abstract = {Tourist Spot Recommender Systems TSRS help users to find the interesting locations/spots in vicinity based on their preferences. Enriching the list of recommended spots with contextual information such as right time to visit, weather conditions, traffic condition, right mode of transport, crowdedness, security alerts etc. may further add value to the systems. This paper proposes the concept of information enrichment for a tourist spot recommender system. Proposed system works in collaboration with a Tourist Spot Recommender System, takes the list of spots to be recommended to the current user and collects the current contextual information for those spots. A new score/rank is computed for each spot to be recommender based on the recommender's rank and current context and sent back to the user. Contextual information may be collected by several techniques such as sensors, collaborative tagging folksonomy, crowdsourcing etc. This paper proposes an approach for information enrichment using just in time location aware crowdsourcing. Location aware crowdsourcing is used to get current contextual information about a spot from the crowd currently available at that spot. Most of the contextual parameters such as traffic conditions, weather conditions, crowdedness etc. are fuzzy in nature and therefore, fuzzy inference is proposed to compute a new score/rank, with each recommended spot. The proposed system may be used with any spot recommender system, however, in this work a personalized tourist spot recommender system is considered as a case for study and evaluation. A prototype system has been implemented and is evaluated by 104 real users.},
booktitle = {Proceedings, Part IV, of the 15th International Conference on Computational Science and Its Applications -- ICCSA 2015 - Volume 9158},
pages = {559–574},
numpages = {16},
keywords = {Tourism, Recommender systems, Information enrichment, Fuzzy inference, Crowdsourcing}
}

@inproceedings{10.1145/2567948.2578841,
author = {Auer, S\"{o}ren and Kontokostas, Dimitris},
title = {Towards web intelligence through the crowdsourcing of semantics},
year = {2014},
isbn = {9781450327459},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2567948.2578841},
doi = {10.1145/2567948.2578841},
abstract = {A key success factor for the Web as a whole was and is its participatory nature. We discuss strategies for engaging human-intelligence to make the Web more semantic.},
booktitle = {Proceedings of the 23rd International Conference on World Wide Web},
pages = {991–992},
numpages = {2},
keywords = {semantic web, crowdsourcing},
location = {Seoul, Korea},
series = {WWW '14 Companion}
}

@inproceedings{10.5555/2900728.2900735,
author = {Ho, Chien-Ju and Vaughan, Jennifer Wortman},
title = {Online task assignment in crowdsourcing markets},
year = {2012},
publisher = {AAAI Press},
abstract = {We explore the problem of assigning heterogeneous tasks to workers with different, unknown skill sets in crowdsourcing markets such as Amazon Mechanical Turk. We first formalize the online task assignment problem, in which a requester has a fixed set of tasks and a budget that specifies how many times he would like each task completed. Workers arrive one at a time (with the same worker potentially arriving multiple times), and must be assigned to a task upon arrival. The goal is to allocate workers to tasks in a way that maximizes the total benefit that the requester obtains from the completed work. Inspired by recent research on the online adwords problem, we present a two-phase exploration-exploitation assignment algorithm and prove that it is competitive with respect to the optimal offline algorithm which has access to the unknown skill levels of each worker. We empirically evaluate this algorithm using data collected on Mechanical Turk and show that it performs better than random assignment or greedy algorithms. To our knowledge, this is the first work to extend the online primal-dual technique used in the online adwords problem to a scenario with unknown parameters, and the first to offer an empirical validation of an online primal-dual algorithm.},
booktitle = {Proceedings of the Twenty-Sixth AAAI Conference on Artificial Intelligence},
pages = {45–51},
numpages = {7},
location = {Toronto, Ontario, Canada},
series = {AAAI'12}
}

@inproceedings{10.5555/2343896.2343988,
author = {Kamar, Ece and Horvitz, Eric},
title = {Incentives for truthful reporting in crowdsourcing},
year = {2012},
isbn = {0981738133},
publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
address = {Richland, SC},
abstract = {A challenge with the programmatic access of human talent via crowdsourcing platforms is the specification of incentives and the checking of the quality of contributions. Methodologies for checking quality include providing a payment if the work is approved by the task owner and hiring additional workers to evaluate contributors' work. Both of these approaches place a burden on people and on the organizations commissioning tasks, and may be susceptible to manipulation by workers and task owners. Moreover, neither a task owner nor the task market may know the task well enough to be able to evaluate worker reports. Methodologies for incentivizing workers without external quality checking include rewards based on agreement with a peer worker or with the final output of the system. These approaches are vulnerable to strategic manipulations by workers. Recent experiments on Mechanical Turk have demonstrated the negative influence of manipulations by workers and task owners on crowdsourcing systems [3]. We address this central challenge by introducing incentive mechanisms that promote truthful reporting in crowdsourcing and discourage manipulation by workers and task owners without introducing additional overhead.},
booktitle = {Proceedings of the 11th International Conference on Autonomous Agents and Multiagent Systems - Volume 3},
pages = {1329–1330},
numpages = {2},
keywords = {peer-prediction rules, crowdsourcing systems},
location = {Valencia, Spain},
series = {AAMAS '12}
}

@inproceedings{10.1007/978-3-319-47099-3_14,
author = {An, Jian and Wu, Ruobiao and Xiang, Lele and Gui, Xiaolin and Peng, Zhenlong},
title = {FCM: A Fine-Grained Crowdsourcing Model Based on Ontology in Crowd-Sensing},
year = {2016},
isbn = {978-3-319-47098-6},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-319-47099-3_14},
doi = {10.1007/978-3-319-47099-3_14},
abstract = {Crowd sensing between users with smart mobile devices is a new trend of development in Internet. In order to recommend the suitable service providers for crowd sensing requests, this paper presents a Fine-grained Crowdsourcing Model (FCM) based on Ontology theory that helps users to select appropriate service providers. First, the characteristic properties which extracted from the service request will be compared with the service provider based on ontology triple. Second, recommendation index of each service provider is calculated through similarity analysis and cluster analysis. Finally, the service decision tree is proposed to predict and recommend appropriate candidate users to participate in crowd sensing service. Experimental results show that this method provides more accurate recommendation than present recommendation systems and consumes less time to find the service provider through clustering algorithm.},
booktitle = {Network and Parallel Computing: 13th IFIP WG 10.3 International Conference, NPC 2016, Xi'an, China, October 28-29, 2016, Proceedings},
pages = {172–179},
numpages = {8},
keywords = {Ontology Theory, Service Recommendation, Service Type, Service Request, Service Provider},
location = {Xi'an, China}
}

@inproceedings{10.1145/2506364.2506368,
author = {Redi, Judith Alice and Ho\ss{}feld, Tobias and Korshunov, Pavel and Mazza, Filippo and Povoa, Isabel and Keimel, Christian},
title = {Crowdsourcing-based multimedia subjective evaluations: a case study on image recognizability and aesthetic appeal},
year = {2013},
isbn = {9781450323963},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2506364.2506368},
doi = {10.1145/2506364.2506368},
abstract = {Research on Quality of Experience (QoE) heavily relies on subjective evaluations of media. An important aspect of QoE concerns modeling and quantifying the subjective notions of 'beauty' (aesthetic appeal) and 'something well-known' (content recognizability), which are both subject to cultural and social effects. Crowdsourcing, which allows employing people worldwide to perform short and simple tasks via online platforms, can be a great tool for performing subjective studies in a time and cost-effective way. On the other hand, the crowdsourcing environment does not allow for the degree of experimental control which is necessary to guarantee reliable subjective data. To validate the use of crowdsourcing for QoE assessments, in this paper, we evaluate aesthetic appeal and recognizability of images using the Microworkers crowdsourcing platform and compare the outcomes with more conventional evaluations conducted in a controlled lab environment. We find high correlation between crowdsourcing and lab scores for recognizability but not for aesthetic appeal, indicating that crowdsourcing can be used for QoE subjective assessments as long as the workers' tasks are designed with extreme care to avoid misinterpretations.},
booktitle = {Proceedings of the 2nd ACM International Workshop on Crowdsourcing for Multimedia},
pages = {29–34},
numpages = {6},
keywords = {subjective evaluations, qoe, crowdsourcing, aesthetics},
location = {Barcelona, Spain},
series = {CrowdMM '13}
}

@inproceedings{10.1145/2557500.2557512,
author = {Park, Sunghyun and Shoemark, Philippa and Morency, Louis-Philippe},
title = {Toward crowdsourcing micro-level behavior annotations: the challenges of interface, training, and generalization},
year = {2014},
isbn = {9781450321846},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2557500.2557512},
doi = {10.1145/2557500.2557512},
abstract = {Research that involves human behavior analysis usually requires laborious and costly efforts for obtaining micro-level behavior annotations on a large video corpus. With the emerging paradigm of crowdsourcing however, these efforts can be considerably reduced. We first present OCTAB (Online Crowdsourcing Tool for Annotations of Behaviors), a web-based annotation tool that allows precise and convenient behavior annotations in videos, directly portable to popular crowdsourcing platforms. As part of OCTAB, we introduce a training module with specialized visualizations. The training module's design was inspired by an observational study of local experienced coders, and it enables an iterative procedure for effectively training crowd workers online. Finally, we present an extensive set of experiments that evaluates the feasibility of our crowdsourcing approach for obtaining micro-level behavior annotations in videos, showing the reliability improvement in annotation accuracy when properly training online crowd workers. We also show the generalization of our training approach to a new independent video corpus.},
booktitle = {Proceedings of the 19th International Conference on Intelligent User Interfaces},
pages = {37–46},
numpages = {10},
keywords = {training crowd workers, micro-level annotations, inter-rater reliability, crowdsourcing, behavior annotations},
location = {Haifa, Israel},
series = {IUI '14}
}

@inproceedings{10.1109/SEAA.2015.33,
author = {Kilamo, Terhi and Rahikkala, Jurka and Mikkonen, Tommi},
title = {Spicing Up Open Source Development with a Touch of Crowdsourcing},
year = {2015},
isbn = {9781467375856},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/SEAA.2015.33},
doi = {10.1109/SEAA.2015.33},
abstract = {Leveraging the work and innovation of third party developers has risen as a viable business model for software companies. Most obviously, open source software has become an opportune ecosystem for creating innovative products with minimum number of paid developers. Then, having a company core where most of the development is done in-house by developers employed by the company can lead to a situation where the community contributions are not smoothly integrated into the code base of the open source product. Similarly, during the last decade, the use of the specialized workforce available online--so-called crowd sourcing--has received a lot of attention. While tapping into the unknown group of experts differs from the open source community-driven approach, they share certain similarities as well. In this paper, we present results of an initial study on how adopting and utilizing elements from crowd sourcing can help to boost community contributions in company lead development of an open source software product. We further discuss how such activity can be supported by an in-house development model where all contributions whether done by the developers of the company or community participants enter a common, automated integration pipeline.},
booktitle = {Proceedings of the 2015 41st Euromicro Conference on Software Engineering and Advanced Applications},
pages = {390–397},
numpages = {8},
keywords = {software ecosystems, open source software, crowdsourcing},
series = {SEAA '15}
}

@inproceedings{10.1109/CCNC.2019.8651850,
author = {Blanc, Nicolas and Liu, Zhan and Ertz, Olivier and Rojas, Diego and Sandoz, Romain and Sokhn, Maria and Ingensand, Jens and Loubier, Jean-Christophe},
title = {Building a Crowdsourcing based Disabled Pedestrian Level of Service routing application using Computer Vision and Machine Learning},
year = {2019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/CCNC.2019.8651850},
doi = {10.1109/CCNC.2019.8651850},
abstract = {The availability of global and scalable tools to assess disabled pedestrian level of service (DPLoS) is a real need, yet still a challenge in today’s world. This is due to the lack of tools that can ease the measurement of a level of service adapted to disabled people, and also to the limitation concerns about the availability of information regarding the existing level of service, especially in real time. This paper describes preliminary results to progress on those needs. It also includes a design for a navigation tool that can help a disabled person move around a city by suggesting the most adapted routes according to the person’s disabilities. The main topics are how to use advanced computer vision technologies, and how to benefit from the prevalence of handheld devices. Our approach intends to show how crowdsourcing techniques can improve data quality by gathering and combining up-to-date data with valuable field observations.},
booktitle = {2019 16th IEEE Annual Consumer Communications \&amp; Networking Conference (CCNC)},
pages = {1–5},
numpages = {5},
location = {Las Vegas, NV, USA}
}

@inproceedings{10.1109/FIE.2017.8190648,
author = {de Deus, William Sim\~{a}o and Machado, Heydi Miura and Barros, Renata Marques and Fabri, Jos\'{e} Augusto and L'Erario, Alexandre},
title = {Enhancing collaboration among undergraduates in informatics: A teaching and learning process based on crowdsourcing},
year = {2017},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/FIE.2017.8190648},
doi = {10.1109/FIE.2017.8190648},
abstract = {Crowdsourcing (CS) is a development model in which small activities are carried out through the collaboration of participants. Many organizations are developing their products through CS to parallel activities, reduce costs, and employ specialists. These factors have boosted the area of informatics, creating a new paradigm for the development of software. However, despite the widespread application of CS in informatics, the literature on teaching and learning CS for undergraduates is still very incipient, and the informatics courses aimed to maximize only the teaching of traditional development processes (Agile, distributed, etc). With this in mind, this study was developed to present the following contributions: (i) a personalized process of teaching and learning CS to the undergraduates in computing courses, and (ii) demonstrates the configuration that a classroom and/or computer lab should possess to generate a CS teaching and learning environment. To accomplishment this study, experimental trials were conducted with undergraduates in different courses in informatics. In conducting the trials, classrooms and computer labs at a university were set up simulating barriers found in CS. As results, all undergraduates have accomplished and reached the ultimate goal through CS, and the process of teaching and learning allowed for the enhancement of several factors, such as teamwork, integration, and support.},
booktitle = {2017 IEEE Frontiers in Education Conference (FIE)},
pages = {1–8},
numpages = {8},
location = {Indianapolis, IN, USA}
}

@inproceedings{10.1145/2487788.2488079,
author = {Singh, Priyanka and Shadbolt, Nigel},
title = {Linked data in crowdsourcing purposive social network},
year = {2013},
isbn = {9781450320382},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2487788.2488079},
doi = {10.1145/2487788.2488079},
abstract = {Internet is an easy medium for people to collaborate and crowdsourcing is an efficient feature of social web where people with common interest and expertise come together to solve specific problems by collective thinking and create a community. It can also be used to filter out important information from large data, remove spams, and gamification techniques are used to reward the users for their contribution and keep a sustainable environment for the growth of the community. Semantic web technologies can be used to structure the community data so it can be combined, decentralized and be used across platform. Using such tools knowledge can be enhanced and easily discovered and merged together. This paper discusses the concept of a purposive social network where people with similar interest and varied expertise come together, use crowdsourcing technique to solve a common problem and build tools for common purpose. The StackOverflow website is chosen to study the purposive network, different network ties and roles of user is studied. Linked Data is used for name disambiguation of keywords and topics for easier search and discovery of experts in a field and provide useful information that is otherwise unavailable in the website.},
booktitle = {Proceedings of the 22nd International Conference on World Wide Web},
pages = {913–918},
numpages = {6},
keywords = {social media, social machine, q&amp;a, name entity disambiguation, linked data, crowdsourcing},
location = {Rio de Janeiro, Brazil},
series = {WWW '13 Companion}
}

@inproceedings{10.1145/3136014.3136033,
author = {Brambilla, Marco and Cabot, Jordi and C\'{a}novas Izquierdo, Javier Luis and Mauri, Andrea},
title = {Better call the crowd: using crowdsourcing to shape the notation of domain-specific languages},
year = {2017},
isbn = {9781450355254},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3136014.3136033},
doi = {10.1145/3136014.3136033},
abstract = {Crowdsourcing has emerged as a novel paradigm where humans are employed to perform computational tasks. In the context of Domain-Specific Modeling Language (DSML) development, where the involvement of end-users is crucial to assure that the resulting language satisfies their needs, crowdsourcing tasks could be defined to assist in the language definition process. By relying on the crowd, it is possible to show an early version of the language to a wider spectrum of users, thus increasing the validation scope and eventually promoting its acceptance and adoption. We propose a systematic method for creating crowdsourcing campaigns aimed at refining the graphical notation of DSMLs. The method defines a set of steps to identify, create and order the questions for the crowd. As a result, developers are provided with a set of notation choices that best fit end-users' needs. We also report on an experiment validating the approach.},
booktitle = {Proceedings of the 10th ACM SIGPLAN International Conference on Software Language Engineering},
pages = {129–138},
numpages = {10},
keywords = {model-driven development, domain-specific languages, crowdsourcing},
location = {Vancouver, BC, Canada},
series = {SLE 2017}
}

@inproceedings{10.1145/2316936.2316940,
author = {Aparicio, Manuela and Costa, Carlos J. and Braga, Andrew Simoes},
title = {Proposing a system to support crowdsourcing},
year = {2012},
isbn = {9781450315258},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2316936.2316940},
doi = {10.1145/2316936.2316940},
abstract = {In this paper, a conceptual framework is proposed, supported in the literature review, derived by identifying the main concepts related to crowdsourcing, as well as ways of improving group participation. We also propose a software solution that may be used to support the crowdsourcing process. This software solution is inspired by the conceptual framework.},
booktitle = {Proceedings of the Workshop on Open Source and Design of Communication},
pages = {13–17},
numpages = {5},
keywords = {web based systems, web application, crowdsourcing, collective intelligence, collaborative systems},
location = {Lisboa, Portugal},
series = {OSDOC '12}
}

@inproceedings{10.1109/HICSS.2015.196,
author = {Jackson, Corey Brian and \O{}sterlund, Carsten and Mugar, Gabriel and Hassman, Katie DeVries and Crowston, Kevin},
title = {Motivations for Sustained Participation in Crowdsourcing: Case Studies of Citizen Science on the Role of Talk},
year = {2015},
isbn = {9781479973675},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/HICSS.2015.196},
doi = {10.1109/HICSS.2015.196},
abstract = {The paper explores the motivations of volunteers in a large crowd sourcing project and contributes to our understanding of the motivational factors that lead to deeper engagement beyond initial participation. Drawing on the theory of legitimate peripheral participation (LPP) and the literature on motivation in crowd sourcing, we analyze interview and trace data from a large citizen science project. The analyses identify ways in which the technical features of the projects may serve as motivational factors leading participants towards sustained participation. The results suggest volunteers first engage in activities to support knowledge acquisition and later share knowledge with other volunteers and finally increase participation in Talk through a punctuated process of role discovery.},
booktitle = {Proceedings of the 2015 48th Hawaii International Conference on System Sciences},
pages = {1624–1634},
numpages = {11},
keywords = {Motivation, Legitimate peripheral participation, Crowdsourcing, Citizen Science},
series = {HICSS '15}
}

@inproceedings{10.1145/2645791.2645807,
author = {Souliotis, Nikos and Tsadimas, Anargyros and Nikolaidou, Mara},
title = {Real-time information about public transport's position using crowdsourcing},
year = {2014},
isbn = {9781450328975},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2645791.2645807},
doi = {10.1145/2645791.2645807},
abstract = {Nowadays there is a multitude of mobile and tablet applications being developed in order to facilitate or disrupt every day tasks. Many of these are location based. A technique to serve in providing information and content is crowdsourcing. This technique is based on the public contributing information or resources giving them the opportunity to become both service providers and recipients at the same time.Taking into account the above and after observing passengers using the public transport system, we came to the conclusion that it would be useful to be able to determine which transport medium (i.e which bus line out of a number running concurrently) is nearer at any given moment. This information allows for better decision making and choice of transportation.For this we propose the development of an application to show the position of a selected transport vehicle. The position will be calculated based on geo-tracking provided by passengers boarded on a vehicle. This will allow for real time information to the application users in order to be able to determine their optimal route.},
booktitle = {Proceedings of the 18th Panhellenic Conference on Informatics},
pages = {1–6},
numpages = {6},
keywords = {Transportations, Mobile Application},
location = {Athens, Greece},
series = {PCI '14}
}

@inproceedings{10.1145/2669557.2669561,
author = {Abdul-Rahman, Alfie and Proctor, Karl J. and Duffy, Brian and Chen, Min},
title = {Repeated measures design in crowdsourcing-based experiments for visualization},
year = {2014},
isbn = {9781450332095},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2669557.2669561},
doi = {10.1145/2669557.2669561},
abstract = {Crowdsourcing platforms, such as Amazon's Mechanical Turk (MTurk), are providing visualization researchers with a new avenue for conducting empirical studies. While such platforms offer several advantages over lab-based studies, they also feature some "unknown" or "uncontrolled" variables, which could potentially introduce serious confounding effects in the resultant measurement data. In this paper, we present our experience of using repeated measures in three empirical studies using MTurk. Each study presented participants with a set of stimuli, each featuring a condition of an independent variable. Participants were exposed to stimuli repeatedly in a pseudo-random order through four trials and their responses were measured digitally. Only a small portion of the participants were able to perform with absolute consistency for all stimuli throughout each experiment. This suggests that a repeated measures design is highly desirable (if not essential) when designing empirical studies for crowdsourcing platforms. Additionally, the majority of participants performed their tasks with reasonable consistency when all stimuli in an experiment are considered collectively. In other words, to most participants, inconsistency occurred occasionally. This suggests that crowdsourcing remains a valid experimental environment, provided that one can integrate the means to observe and alleviate the potential confounding effects of "unknown" or "uncontrolled" variables in the design of the experiment.},
booktitle = {Proceedings of the Fifth Workshop on Beyond Time and Errors: Novel Evaluation Methods for Visualization},
pages = {95–102},
numpages = {8},
keywords = {mechanical turk, empirical studies},
location = {Paris, France},
series = {BELIV '14}
}

@inproceedings{10.5555/3367032.3367230,
author = {Ko, Ching-Yun and Lin, Rui and Li, Shu and Wong, Ngai},
title = {MiSC: mixed strategies crowdsourcing},
year = {2019},
isbn = {9780999241141},
publisher = {AAAI Press},
abstract = {Popular crowdsourcing techniques mostly focus on evaluating workers' labeling quality before adjusting their weights during label aggregation. Recently, another cohort of models regard crowdsourced annotations as incomplete tensors and recover unfilled labels by tensor completion. However, mixed strategies of the two methodologies have never been comprehensively investigated, leaving them as rather independent approaches. In this work, we propose MiSC (Mixed Strategies Crowdsourcing), a versatile framework integrating arbitrary conventional crowdsourcing and tensor completion techniques. In particular, we propose a novel iterative Tucker label aggregation algorithm that outperforms state-of-the-art methods in extensive experiments.},
booktitle = {Proceedings of the 28th International Joint Conference on Artificial Intelligence},
pages = {1394–1400},
numpages = {7},
location = {Macao, China},
series = {IJCAI'19}
}

@inproceedings{10.1145/2461466.2461508,
author = {Nguyen-Dinh, Long-Van and Waldburger, C\'{e}dric and Roggen, Daniel and Tr\"{o}ster, Gerhard},
title = {Tagging human activities in video by crowdsourcing},
year = {2013},
isbn = {9781450320337},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2461466.2461508},
doi = {10.1145/2461466.2461508},
abstract = {Activity annotation in videos is necessary to create a training dataset for most of activity recognition systems. This is a very time consuming and repetitive task. Crowdsourcing gains popularity to distribute annotation tasks to a large pool of taggers. We present for the first time an approach to achieve good quality for activity annotation in videos through crowdsourcing on the AmazonMechanical Turk platform (AMT). Taggers must annotate the start, end boundaries and the label of all occurrences of activities in videos. Two strategies to detect non-serious taggers according to temporal annotated results are presented. Individual filtering checks the consistence in the answers of each tagger with the characteristic of dataset to identify and remove nonserious taggers. Collaborative filtering checks the agreement in annotations among taggers. The filtering techniques detect and remove non-serious taggers and finally, the majority voting applied to AMT temporal tags to generate one final AMT activity annotation set. We conduct the experiments to get activity annotation from AMT on a subset of two rich datasets frequently used in activity recognition. The results show that our proposed filtering strategies can increase the accuracy by up to 40\%. The final annotation set is of comparable quality of the annotation of experts with high accuracy (76\% to 92\%).},
booktitle = {Proceedings of the 3rd ACM Conference on International Conference on Multimedia Retrieval},
pages = {263–270},
numpages = {8},
keywords = {video annotation, human computation, crowdsourcing, amazon mechanical turk, activity recognition},
location = {Dallas, Texas, USA},
series = {ICMR '13}
}

@inproceedings{10.1145/2800835.2800969,
author = {Minoda, Yuki and Ohama, Iku and Muramoto, Eiichi},
title = {A machine learning approach for lighting perception analysis via crowdsourcing},
year = {2015},
isbn = {9781450335751},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2800835.2800969},
doi = {10.1145/2800835.2800969},
abstract = {In this paper, a new analytical scheme designed for crowd sourced subjective lighting evaluation system is proposed. Participants are gathered through crowdsourcing and evaluations are done based on an online system that shows CG (Computer Graphics) on a display. Data about preferable space brightness which is collected by the system is analyzed by machine learning techniques, the Bradley-Terry Mixture (BTM) model and the logistic regression. The results show that applying machine learning techniques enable us to extract important relationships between lighting preference and participants' attributes such as age.},
booktitle = {Adjunct Proceedings of the 2015 ACM International Joint Conference on Pervasive and Ubiquitous Computing and Proceedings of the 2015 ACM International Symposium on Wearable Computers},
pages = {1355–1360},
numpages = {6},
keywords = {machine learning, lighting perception, human factor, crowdsourcing},
location = {Osaka, Japan},
series = {UbiComp/ISWC'15 Adjunct}
}

@inproceedings{10.1109/BIGCOMP.2016.7425956,
author = {Jae-ho Shin and Gyoung-Don Joo and Chulyun Kim},
title = {XPath based crawling method with crowdsourcing for targeted online market places},
year = {2016},
isbn = {9781467387965},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/BIGCOMP.2016.7425956},
doi = {10.1109/BIGCOMP.2016.7425956},
abstract = {An increasing number of online market places have emerged as online shopping becomes more popular for a couple of decades. During that time, technologies to construct web sites have been evolved as well and, currently, AJAX is a representative technique to construct dynamic web pages. Crawling is a basic tool to collect information in the internet, and traditional crawling techniques randomly choose and follow links represented by the anchor tag in order to navigate the Word-Wide-Web. However, when a traditional crawler is applied for gathering information from a targeted up-to-date online market place, there are some critical problems. The first issue is that there are too many links, among which only few are enough to navigate all web pages in the site. The second issue is that most links are given by JavaScript but not by the anchor tags, which cannot be followed by the traditional web crawlers. Therefore, to overcome these issues, we suggest a webpage crawling method which can extract only necessary and sufficient links by adopting crowdsourcing approach and can follow JavaScript links by using a navigating information represented by XPaths.},
booktitle = {Proceedings of the 2016 International Conference on Big Data and Smart Computing (BigComp)},
pages = {395–397},
numpages = {3},
series = {BIGCOMP '16}
}

@inproceedings{10.1145/3250552,
author = {Poole, Erika},
title = {Session details: Crowdsourcing \&amp; peer production II},
year = {2012},
isbn = {9781450310154},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3250552},
doi = {10.1145/3250552},
booktitle = {Proceedings of the SIGCHI Conference on Human Factors in Computing Systems},
location = {Austin, Texas, USA},
series = {CHI '12}
}

@inproceedings{10.1145/2968219.2968586,
author = {Luo, Chu and Kuutila, Miikka and Klakegg, Simon and Ferreira, Denzil and Flores, Huber and Goncalves, Jorge and Kostakos, Vassilis and M\"{a}ntyl\"{a}, Mika},
title = {How to validate mobile crowdsourcing design? leveraging data integration in prototype testing},
year = {2016},
isbn = {9781450344623},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2968219.2968586},
doi = {10.1145/2968219.2968586},
abstract = {Mobile crowdsourcing applications often run in dynamic environments. Due to limited time and budget, developers of mobile crowdsourcing applications sometimes cannot completely test their prototypes in real world situations. We describe a data integration technique for developers to validate their design in prototype testing. Our approach constructs the intended context by combining real-time, historical and simulated data. With correct context-aware design, mobile crowdsourcing applications presenting crowdsourcing questions in relevant context to users are likely to obtain high response quality.},
booktitle = {Proceedings of the 2016 ACM International Joint Conference on Pervasive and Ubiquitous Computing: Adjunct},
pages = {1448–1453},
numpages = {6},
keywords = {ubiquitous computing, software testing and debugging, smartphones, mobile crowdsourcing, ambient intelligence},
location = {Heidelberg, Germany},
series = {UbiComp '16}
}

@inproceedings{10.1109/CCNC.2018.8319312,
author = {Mangiatordi, Andrea and Lazzari, Marco},
title = {Combined use of artificial intelligence and crowdsourcing to provide alternative content for images on websites},
year = {2018},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/CCNC.2018.8319312},
doi = {10.1109/CCNC.2018.8319312},
abstract = {Web accessibility can be efficiently evaluated in both automated and manual ways, but fixing accessibility issues on live websites is still a major challenge, as it requires attention, dedication and knowledge of Assistive Technology requirements. A website revision system is thus proposed, relying on Artificial Intelligence to produce alternative text, and on crowdsourcing to correct it, for images or other generic types of resources. This solution is integrated in a web application called Farfalla, which also offers easy integration of accessibility options in websites. A major advantage offered by this approach is the independence from a specific browser or operating system: the proposed architecture is highly portable and platform-agnostic. The basic idea that will be put forward here is that Artificial Intelligence can be used to generate basic information about images in websites, and crowdsourcing would serve as a way for refining that information. As the development of the project is still in progress, the most important implementation issues are analyzed: they include context-aware alternatives, localization, scalable deployment and input validation.},
booktitle = {2018 15th IEEE Annual Consumer Communications \&amp; Networking Conference (CCNC)},
pages = {1–6},
numpages = {6},
location = {Las Vegas, NV, USA}
}

@inproceedings{10.1007/978-3-319-40216-1_22,
author = {Cucari, Giovanni and Leotta, Francesco and Mecella, Massimo and Vassos, Stavros},
title = {Collecting Human Habit Datasets for Smart Spaces Through Gamification and Crowdsourcing},
year = {2015},
isbn = {9783319402154},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-319-40216-1_22},
doi = {10.1007/978-3-319-40216-1_22},
abstract = {A lot of research in the last years has focused on smart spaces, covering aspects related to ambient intelligence, activity monitoring and mining, etc. All these efforts require datasets to be used for experimental purposes and as benchmarks for novel techniques. Such datasets are today difficult to obtain as, on the one hand, building smart facilities is expensive, requiring considerable costs for maintenance and extension, and, on the other hand, freely available datasets are scarce, not continuously updated and contain a limited set of sensors, thus not allowing the evaluation of algorithms that require the availability of specific categories of sensors. To this aim, we have built a prototype smart virtual environment producing sensor logs on the basis of activities performed by users as if they were really acting in a physical smart space.},
booktitle = {Revised Selected Papers of the 4th International Conference on Games and Learning Alliance - Volume 9599},
pages = {208–217},
numpages = {10},
location = {Rome, Italy},
series = {GALA 2015}
}

@inproceedings{10.1109/MASS.2014.66,
author = {Luo, Tie and Kanhere, Salil S. and Tan, Hwee-Pink},
title = {Optimal Prizes for All-Pay Contests in Heterogeneous Crowdsourcing},
year = {2014},
isbn = {9781479960361},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/MASS.2014.66},
doi = {10.1109/MASS.2014.66},
abstract = {Incentive is key to the success of crowd sourcing which heavily depends on the level of user participation. This paper designs an incentive mechanism to motivate a heterogeneous crowd of users to actively participate in crowd sourcing campaigns. We cast the problem in a new, asymmetric all-pay contest model with incomplete information, where an arbitrary n of users exert irrevocable effort to compete for a prize tuple. The prize tuple is an array of prize functions as opposed to a single constant prize typically used by conventional contests. We design an optimal contest that (a) induces the maximum profit -- total user effort minus the prize payout -- for the crowdsourcer, and (b) ensures users to strictly have incentive to participate. In stark contrast to intuition and prior related work, our mechanism induces an equilibrium in which heterogeneous users behave independently of one another as if they were in a homogeneous setting. This newly discovered property, which we coin as strategy autonomy (SA), is of practical significance: it (a) reduces computational and storage complexity by n-fold for each user, (b) increases the crowdsourcer's revenue by counteracting an effort reservation effect existing in asymmetric contests, and (c) neutralizes the (almost universal) law of diminishing marginal returns (DMR). Through an extensive numerical case study, we demonstrate and scrutinize the superior profitability of our mechanism, as well as draw insights into the SA property.},
booktitle = {Proceedings of the 2014 IEEE 11th International Conference on Mobile Ad Hoc and Sensor Systems},
pages = {136–144},
numpages = {9},
keywords = {strategy autonomy, participatory sensing, network economics, asymmetric contest, all-pay auction, Incentive mechanism},
series = {MASS '14}
}

@inproceedings{10.1007/978-3-319-91125-0_14,
author = {Aihara, Kenro and Bin, Piao and Imura, Hajime and Takasu, Atsuhiro and Tanaka, Yuzuru},
title = {Collecting Bus Locations by Users: A Crowdsourcing Model to Estimate Operation Status of Bus Transit Service},
year = {2018},
isbn = {978-3-319-91124-3},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-319-91125-0_14},
doi = {10.1007/978-3-319-91125-0_14},
abstract = {This paper describes a crowdsourcing model to collect bus locations from onboard passengers.Bus location service, or realtime bus tracking service, is getting more and more general these days. Some models are proposed to build such services. One approach is facilitated every vehicle has a function to position itself with location sensor, such as GPS receiver, and transmits its own location with time to the server. Another is an environmental approach that bus detectors are deployed along the route to detect ids of nearby buses and transmit to the server. These models are well-established and practical. However, it is not easy to install such services especially for small operators because costs on devices and data transmission are relatively high.This paper proposes that a sustainable model even for small operators to provide bus locations to passengers. The key idea of the proposal is that collecting bus locations is not by bus operators but by onboard passengers. To collect them, a smartphone application of bus tracker is provided to public. The application shows current locations of buses in operation on bus transit services, while it detects nearby buses around users and transmits bus ids with time and location of detection to the service platform. That is, locations of buses are collected by users.},
booktitle = {Distributed, Ambient and Pervasive Interactions: Understanding Humans: 6th International Conference, DAPI 2018, Held as Part of HCI International 2018, Las Vegas, NV, USA, July 15–20, 2018, Proceedings, Part I},
pages = {171–180},
numpages = {10},
keywords = {Crowdsourcing, Crowdsensing, Smart and hybrid cities, Internet of Things},
location = {Las Vegas, NV, USA}
}

@inproceedings{10.1145/2207676.2207709,
author = {Willett, Wesley and Heer, Jeffrey and Agrawala, Maneesh},
title = {Strategies for crowdsourcing social data analysis},
year = {2012},
isbn = {9781450310154},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2207676.2207709},
doi = {10.1145/2207676.2207709},
abstract = {Web-based social data analysis tools that rely on public discussion to produce hypotheses or explanations of the patterns and trends in data, rarely yield high-quality results in practice. Crowdsourcing offers an alternative approach in which an analyst pays workers to generate such explanations. Yet, asking workers with varying skills, backgrounds and motivations to simply "Explain why a chart is interesting" can result in irrelevant, unclear or speculative explanations of variable quality. To address these problems, we contribute seven strategies for improving the quality and diversity of worker-generated explanations. Our experiments show that using (S1) feature-oriented prompts, providing (S2) good examples, and including (S3) reference gathering, (S4) chart reading, and (S5) annotation subtasks increases the quality of responses by 28\% for US workers and 196\% for non-US workers. Feature-oriented prompts improve explanation quality by 69\% to 236\% depending on the prompt. We also show that (S6) pre-annotating charts can focus workers' attention on relevant details, and demonstrate that (S7) generating explanations iteratively increases explanation diversity without increasing worker attrition. We used our techniques to generate 910 explanations for 16 datasets, and found that 63\% were of high quality. These results demonstrate that paid crowd workers can reliably generate diverse, high-quality explanations that support the analysis of specific datasets.},
booktitle = {Proceedings of the SIGCHI Conference on Human Factors in Computing Systems},
pages = {227–236},
numpages = {10},
keywords = {social data analysis, information visualization, crowdsourcing},
location = {Austin, Texas, USA},
series = {CHI '12}
}

@inproceedings{10.1007/978-3-319-48496-9_70,
author = {Inzerillo, Laura and Santagati, Cettina},
title = {Crowdsourcing Cultural Heritage: From 3D Modeling to the Engagement of Young Generations},
year = {2016},
isbn = {978-3-319-48495-2},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-319-48496-9_70},
doi = {10.1007/978-3-319-48496-9_70},
abstract = {Monitoring, digitizing and archiving museum artworks represent an important socio-cultural accomplishment and an overcoming in digital preservation today. Cultural heritage is constantly under threat of terrorist attacks and natural disaster. The high costs related to documentation task have prevented a constantly and massive survey activity. The low cost 3D image based acquisition and elaboration techniques of an object, allow to carry out a 3D photorealistic model in a short time. Therefore, a lot of museum adopted these techniques for the artworks archiving. Crowdsourcing activities can significantly speed up survey and elaboration procedures. If, on the one hand, these initiatives can have a positive impact, on the other hand involve the online user with a marginal role. In this paper we demonstrate how it is appropriate thinking the museum visitor as “museum operator/maker” of the digital model overstepping the outcomes achieved so far.},
booktitle = {Digital Heritage. Progress in Cultural Heritage: Documentation, Preservation, and Protection: 6th International Conference, EuroMed 2016, Nicosia, Cyprus, October 31 – November 5, 2016, Proceedings, Part I},
pages = {869–879},
numpages = {11},
keywords = {Crowdsourcing, Museum collections, Structure from Motion (SfM), 3D modeling, Cultural heritage},
location = {Nicosia, Cyprus}
}

@inproceedings{10.1145/2787394.2790462,
author = {Clark, David and Texeira, Renata and Mellia, Macro},
title = {What has Worked and What Won't Work in Crowdsourcing},
year = {2015},
isbn = {9781450335393},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2787394.2790462},
doi = {10.1145/2787394.2790462},
abstract = {An interactive discussion around the questions/issues raised in the CFP including: Are there successful crowdsourcing ideas that have not come out today? Are there specific projects that you think the community should know more about? How should we enlist vantage points in the right locations? How do these platforms differ from/extend human-entered crowdsourcing systems? What kinds of experiments are technically and ethically viable? What is the right programming interface for the experimenter? Given the limited control we have on these platforms, what is the right experimental model? Could we build a federation of platforms and how would that work?},
booktitle = {Proceedings of the 2015 ACM SIGCOMM Workshop on Crowdsourcing and Crowdsharing of Big (Internet) Data},
pages = {51},
numpages = {1},
location = {London, United Kingdom},
series = {C2B(1)D '15}
}

@inproceedings{10.1145/2647868.2647871,
author = {Redi, Judith A. and Lux, Mathias},
title = {CrowdMM14 - 2014 International ACM Workshop on Crowdsourcing for Multimedia},
year = {2014},
isbn = {9781450330633},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2647868.2647871},
doi = {10.1145/2647868.2647871},
abstract = {The power of crowds, leveraging a large number of human contributors and the capabilities of human computation, has enormous potential to address key challenges in the area of multimedia research. This power is, however, of difficult exploitation: challenges arise from the fact that a community of users or workers is a complex and dynamic system highly sensitive to changes in the form and the parameterization of their activities. Since 2012, the International ACM Workshop on Crowdsourcing for Multimedia emph{CrowdMM} has been the venue for collecting new insights on the effective deployment of crowdsourcing towards boosting Multimedia research. In its third edition, CrowdMM14 especially focuses on contributions that propose solutions for the key challenges that face widespread adoption of crowdsourcing paradigms in the multimedia research community. These include: identification of optimal crowd members (e.g., user expertise, worker reliability), providing effective explanations (i.e., good task design), controlling noise and quality in the results, designing incentive structures that do not breed cheating, adversarial environments, gathering necessary background information about crowd members without violating privacy, controlling descriptions of task.},
booktitle = {Proceedings of the 22nd ACM International Conference on Multimedia},
pages = {1247–1248},
numpages = {2},
keywords = {multimedia evaluation, multimedia annotation, human factors, human computation, crowdsourcing},
location = {Orlando, Florida, USA},
series = {MM '14}
}

@inproceedings{10.1007/978-3-642-41347-6_8,
author = {Vreede, Triparna and Nguyen, Cuong and Vreede, Gert-Jan and Boughzala, Imed and Oh, Onook and Reiter-Palmon, Roni},
title = {A Theoretical Model of User Engagement in Crowdsourcing},
year = {2013},
isbn = {9783642413469},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-642-41347-6_8},
doi = {10.1007/978-3-642-41347-6_8},
abstract = {Social media technology has enabled virtual collaborative environments where people actively interact, share knowledge, coordinate activities, solve problems, co-create value, and innovate. Organizations have begun to leverage approaches and technologies to involve numerous people from outside their boundaries to perform organizational tasks. Despite the success and popularity of this 'crowdsourcing' phenomenon, there appears to be a distinct gap in the literature regarding the empirical evaluation of the factors involved in a crowdsourcing user experience. This paper aims to fill this void by proposing a theoretical model of the antecedents and their relationships for crowdsourcing user engagement. It is defined as the quality of effort online users devote to collaboration activities that contribute directly to desired outcomes. Drawing from research in psychology and IS, we identify three critical elements that precede crowdsourcing user engagement: personal interest in topic, goal clarity, and motivation to contribute. This paper examines the theoretical basis of these variables of interest in detail, derives a causal model of their interrelationships, and identifies future plans for model testing.},
booktitle = {Proceedings of the 19th International Conference on Collaboration and Technology - Volume 8224},
pages = {94–109},
numpages = {16},
keywords = {social media, open collaboration, motivation, engagement, Crowdsourcing}
}

@inproceedings{10.1007/978-3-319-15168-7_50,
author = {Novak, Jasminko and Bozzon, Alessandro and Fraternali, Piero and Daras, Petros and Chrons, Otto and Nardi, Bonnie and Jaimes, Alejandro},
title = {SoHuman 2014 – 3rd International Workshop on Social Media in Crowdsourcing and Human Computation - Introduction: Theme: Socially-Aware Crowdsourcing – The Value of the Human Touch},
year = {2015},
isbn = {978-3-319-15167-0},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-319-15168-7_50},
doi = {10.1007/978-3-319-15168-7_50},
abstract = {This workshop aims at bringing together researchers and practitioners from different disciplines to explore the challenges and opportunities of novel approaches to collective intelligence, crowdsourcing and human computation that address social aspects as a core element of their design principles, implementations or scientific investigation.},
booktitle = {Social Informatics: SocInfo 2014 International Workshops, Barcelona, Spain, November 11, 2014, Revised Selected Papers},
pages = {417–420},
numpages = {4},
keywords = {Socio-technical systems, HCI, Multimedia information retrieval, AI, Collaborative systems, Social media, Collective intelligence, Human computation, Crowdsourcing},
location = {Barcelona, Spain}
}

@inproceedings{10.1109/CSI-SE.2015.16,
author = {Zhao, Mengyao and Hoek, Andr\'{e} van der},
title = {A Brief Perspective on Microtask Crowdsourcing Workflows for Interface Design},
year = {2015},
isbn = {9781467370400},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/CSI-SE.2015.16},
doi = {10.1109/CSI-SE.2015.16},
abstract = {User interface design, as a crucial part of software design, is complex. Current micro task crowd sourcing workflows do not support its complexity well. The difficulty particularly relates to the process to decompose an interface design task into micro tasks. In order to make micro task crowd sourcing more supportive for interface design, we need a workflow that can help task owners to break down interface design tasks more easily. This paper briefly describes three experiments that help inform various aspects of workflow design for interface design through micro task crowd sourcing.},
booktitle = {Proceedings of the 2015 IEEE/ACM 2nd International Workshop on CrowdSourcing in Software Engineering},
pages = {45–46},
numpages = {2},
keywords = {interface design, crowdsourcing, complex work},
series = {CSI-SE '15}
}

@inproceedings{10.5555/1920331.1920416,
author = {Giudice, Katherine Del},
title = {Crowdsourcing credibility: the impact of audience feedback on web page credibility},
year = {2010},
publisher = {American Society for Information Science},
address = {USA},
abstract = {Social feedback in the form of audience ratings, community tags, recommendations, and text comments is becoming increasingly commonplace on the Web. Prior research has uncovered a number of Web site features that can impact its perceived credibility. However, to date research has not investigated whether social feedback on a Web page can influence the perceived credibility of the information on the page or increase or decrease the likelihood that an individual will subsequently use the information contained within it. This paper describes a study investigating whether one type of social feedback, audience ratings, can influence perceptions of credibility. The results of the study suggest that the type of audience feedback, positive, mixed, or negative, can influence perceptions of credibility while the size of the audience giving feedback does not. Also, audience feedback does not appear to increase the likelihood of use of the information on a web page.},
booktitle = {Proceedings of the 73rd ASIS&amp;T Annual Meeting on Navigating Streams in an Information Ecosystem - Volume 47},
articleno = {59},
numpages = {9},
keywords = {social information use, credibility, Web},
location = {Pittsburgh, Pennsylvania},
series = {ASIS&amp;T '10}
}

@inproceedings{10.5555/2893873.2893955,
author = {Sina, Sigal and Rosenfeld, Avi and Kraus, Sarit},
title = {Generating content for scenario-based serious-games using crowdsourcing},
year = {2014},
publisher = {AAAI Press},
abstract = {Scenario-based serious-games have become an important tool for teaching new skills and capabilities. An important factor in the development of such systems is reducing the time and cost overheads in manually creating content for these scenarios. To address this challenge, we present Scenario-Gen, an automatic method for generating content about everyday activities through combining computer science techniques with the crowd. ScenarioGen uses the crowd in three different ways: to capture a database of scenarios of everyday activities, to generate a database of likely replacements for specific events within that scenario, and to evaluate the resulting scenarios. We evaluated ScenarioGen in 6 different content domains and found that it was consistently rated as coherent and consistent as the originally captured content. We also compared ScenarioGen's content to that created by traditional planning techniques. We found that both methods were equally effective in generating coherent and consistent scenarios, yet ScenarioGen's content was found to be more varied and easier to create.},
booktitle = {Proceedings of the Twenty-Eighth AAAI Conference on Artificial Intelligence},
pages = {522–529},
numpages = {8},
location = {Qu\'{e}bec City, Qu\'{e}bec, Canada},
series = {AAAI'14}
}

@inproceedings{10.1145/2502081.2502083,
author = {Xu, Qianqian and Xiong, Jiechao and Huang, Qingming and Yao, Yuan},
title = {Robust evaluation for quality of experience in crowdsourcing},
year = {2013},
isbn = {9781450324045},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2502081.2502083},
doi = {10.1145/2502081.2502083},
abstract = {Strategies exploiting crowdsourcing are increasingly being applied in the area of Quality of Experience (QoE) for multimedia. They enable researchers to conduct experiments with a more diverse set of participants and at a lower economic cost than conventional laboratory studies. However, a major challenge for crowdsourcing tests is the detection and control of outliers, which may arise due to different test conditions, human errors or abnormal variations in context. For this purpose, it is desired to develop a robust evaluation methodology to deal with crowdsourceable data, which are possibly incomplete, imbalanced, and distributed on a graph. In this paper, we propose a robust rating scheme based on robust regression and Hodge Decomposition on graphs, to assess QoE using crowdsourcing. The scheme shows that the removal of outliers in crowdsourcing experiments would be helpful for purifying data and could provide us with more reliable results. The effectiveness of the proposed scheme is further confirmed by experimental studies on both simulated examples and real-world data.},
booktitle = {Proceedings of the 21st ACM International Conference on Multimedia},
pages = {43–52},
numpages = {10},
keywords = {robust evaluation, random graph, quality of experience (QoE), paired comparison, outlier detection, lasso, hodge decomposition, crowdsourcing},
location = {Barcelona, Spain},
series = {MM '13}
}

@inproceedings{10.1109/ITNG.2015.78,
author = {Brito, Jailson and Vieira, Vaninha and Duran, Adolfo},
title = {Towards a Framework for Gamification Design on Crowdsourcing Systems: The G.A.M.E. Approach},
year = {2015},
isbn = {9781479988280},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/ITNG.2015.78},
doi = {10.1109/ITNG.2015.78},
abstract = {Crowd sourcing systems call a crowd of users to collaborate on solving real-life problems. One key issue for the success of such systems is to guarantee users' participation. A strategy that has been used to promote user participation is the use of game design techniques, since games have successful strategies to grant enjoyable user experience. However, most gamification methods and guidelines are too generic, do not emphasize the collaboration aspects and focus on introducing rewarding elements into the application, instead of designing player-centric applications. Reward-based design is dangerous, specially for collaborative systems, because it may put points gathering as the primary purpose of the application instead of the collaboration goal. This paper presents G.A.M.E., a conceptual framework to guide the design of gamification in crowd sourcing-based systems. The framework provides a flexible step-by-step guideline that combines knowledge from software engineering, collaborative systems, game design and interaction design. To evaluate our proposal, we instantiated G.A.M.E. Into two applications in the domain of public transportation. The influence of gamification in those applications were evaluated through controlled navigation tests in a crowd sourcing usability testing platform. Our findings showed us that gamification improved user interfaces of collaboration activities by 16\% on usability and were more trustworthy in 80\% the cases.},
booktitle = {Proceedings of the 2015 12th International Conference on Information Technology - New Generations},
pages = {445–450},
numpages = {6},
keywords = {Interaction Design, Gamification, End-user software engineering, Crowdsourcing Systems, Conceptual Framework},
series = {ITNG '15}
}

@inproceedings{10.1145/2987386.2987416,
author = {Lazarova-Molnar, Sanja and Logason, Halld\'{o}r \TH{}\'{o}r and Andersen, Peter Gr\o{}nb\ae{}k and Kj\ae{}rgaard, Mikkel Baun},
title = {Mobile Crowdsourcing of Data for Fault Detection and Diagnosis in Smart Buildings},
year = {2016},
isbn = {9781450344555},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2987386.2987416},
doi = {10.1145/2987386.2987416},
abstract = {Energy use of buildings represents roughly 40\% of the overall energy consumption. Most of the national agendas contain goals related to reducing the energy consumption and carbon footprint. Timely and accurate fault detection and diagnosis (FDD) in building management systems (BMS) have the potential to reduce energy consumption cost by approximately 15-30\%. Most of the FDD methods are data-based, meaning that their performance is tightly linked to the quality and availability of relevant data. Based on our experience, faults and relevant events data is very sparse and inadequate, mostly because of the lack of will and incentive for those that would need to keep track of faults. In this paper we introduce the idea of using crowdsourcing to support FDD data collection processes, and illustrate our idea through a mobile application that has been implemented for this purpose. Furthermore, we propose a strategy of how to successfully deploy this building occupants' crowdsourcing application.},
booktitle = {Proceedings of the International Conference on Research in Adaptive and Convergent Systems},
pages = {12–17},
numpages = {6},
keywords = {occupants, fault detection and diagnosis, energy performance, data collection, buildings, Crowdsourcing},
location = {Odense, Denmark},
series = {RACS '16}
}

@inproceedings{10.1145/2441776.2441848,
author = {Hansen, Derek L. and Schone, Patrick J. and Corey, Douglas and Reid, Matthew and Gehring, Jake},
title = {Quality control mechanisms for crowdsourcing: peer review, arbitration, \&amp; expertise at familysearch indexing},
year = {2013},
isbn = {9781450313315},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2441776.2441848},
doi = {10.1145/2441776.2441848},
abstract = {The FamilySearch Indexing project has enabled hundreds of thousands of volunteers to transcribe billions of records, making it one of the largest crowdsourcing initiatives in the world. Assuring high quality transcriptions (i.e., indexes) with a reasonable amount of volunteer effort is essential to keep pace with the mounds of newly digitized documents. Using historical data, we show the relationship between prior experience and native language on transcriber agreement. We then present a field experiment comparing the effectiveness (accuracy) and efficiency (time) of two quality control mechanisms: (1) Arbitration -- the existing mechanism wherein two volunteers independently transcribe records and disagreements go to an arbitrator, and (2) Peer Review -- a mechanism wherein one volunteer's work is reviewed by another volunteer. Peer Review is significantly more efficient, though not as effective for certain fields as Arbitration. Design suggestions for FamilySearch Indexing and related crowdsourcing initiatives are provided.},
booktitle = {Proceedings of the 2013 Conference on Computer Supported Cooperative Work},
pages = {649–660},
numpages = {12},
keywords = {transcription, quality control, peer review, historical documents, genealogy, familysearch indexing, crowdsourcing},
location = {San Antonio, Texas, USA},
series = {CSCW '13}
}

@inproceedings{10.1109/ICDCS.2014.10,
author = {Zhu, Yanmin and Zhang, Qian and Zhu, Hongzi and Yu, Jiadi and Cao, Jian and Ni, Lionel M.},
title = {Towards Truthful Mechanisms for Mobile Crowdsourcing with Dynamic Smartphones},
year = {2014},
isbn = {9781479951697},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/ICDCS.2014.10},
doi = {10.1109/ICDCS.2014.10},
abstract = {Stimulating participation from smartphone users is of paramount importance to mobile crowd sourcing systems and applications. A few incentive mechanisms have been proposed, but most of them have made the impractical assumption that smartphones remain static in the system and sensing tasks are known in advance. The existing mechanisms fail when being applied to the realistic scenario where smartphones dynamically arrive to the system and sensing tasks are submitted at random. It is particularly challenging to design an incentive mechanism for such a mobile crowd sourcing system, given dynamic smartphones, uncertain arrivals of tasks, strategic behaviors, and private information of smartphones. We propose two truthful auction mechanisms for two different cases of mobile crowd sourcing with dynamic smartphones. For the offline case, we design an optimal truthful mechanism with an optimal task allocation algorithm of polynomial-time computation complexity of O (n+γ)3, where n is the number of smartphones and γ is the number of sensing tasks. For the online case, we design a near-optimal truthful mechanism with an online task allocation algorithm that achieves a constant competitive ratio of 1 2. Rigorous theoretical analysis and extensive simulations have been performed, and the results demonstrate the proposed auction mechanisms achieve truthfulness, individual rationality, computational efficiency, and low overpayment.},
booktitle = {Proceedings of the 2014 IEEE 34th International Conference on Distributed Computing Systems},
pages = {11–20},
numpages = {10},
keywords = {Crowdsourcing, Truthful mechanisms, Online mechanisms},
series = {ICDCS '14}
}

@inproceedings{10.1145/2846661.2846665,
author = {Liang, Guangtai and Li, Shaochun},
title = {An energy-saving framework for mobile devices based on crowdsourcing intelligences},
year = {2015},
isbn = {9781450339063},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2846661.2846665},
doi = {10.1145/2846661.2846665},
abstract = {Rich mobile apps make mobile devices increasingly pervasive in our daily life. However, energy consumptions of mobile devices brings lots of users’ frustrations. To guarantee good experiences of mobile users, we propose an energy-saving framework for mobile devices, which uses a set of coarse-grained and general-purpose energy-waste heuristics as a starting point and then smartly takes advantages of crowdsourcing intelligence to refine energy-waste related knowledge to help detect/resolve energy wastes in mobile devices. In return, summarized energy-waste related knowledge can be presented to the developers of related mobile apps and guide them to identify/fix related energy bugs. Through initial evaluations, we demonstrate the proposed framework is able to extend the lifetime of a mobile device with one full charge to a large degree (e.g., 30\%-70\%).},
booktitle = {Proceedings of the 3rd International Workshop on Mobile Development Lifecycle},
pages = {5–6},
numpages = {2},
keywords = {mobile devices, energy-saving framework, crowdsourcing intelligence},
location = {Pittsburgh, PA, USA},
series = {MobileDeLi 2015}
}

@inproceedings{10.1145/3397166.3409142,
author = {Sarkar, Shamik and Baset, Aniqua and Singh, Harsimran and Smith, Phillip and Patwari, Neal and Kasera, Sneha and Derr, Kurt and Ramirez, Samuel},
title = {LLOCUS: learning-based localization using crowdsourcing},
year = {2020},
isbn = {9781450380157},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3397166.3409142},
doi = {10.1145/3397166.3409142},
abstract = {We present LLOCUS, a novel learning-based system that uses mobile crowdsourced RF sensing to estimate the location and power of unknown mobile transmitters in real time, while allowing unrestricted mobility of the crowdsourcing participants. We carefully identify and tackle several challenges in learning and localizing, based on RSS, in such a dynamic environment. We decouple the problem of localizing a transmitter with unknown transmit power into two problems, 1) predicting the power of a transmitter at an unknown location, and 2) localizing a transmitter with known transmit power. LLOCUS first estimates the power of the unknown transmitter and then scales the reported RSS values such that the unknown transmit power problem is transparent to the method of localization. We evaluate LLOCUS using three experiments in different indoor and outdoor environments. We find that LLOCUS reduces the localization error by 17-68\% compared to several non-learning methods.},
booktitle = {Proceedings of the Twenty-First International Symposium on Theory, Algorithmic Foundations, and Protocol Design for Mobile Networks and Mobile Computing},
pages = {201–210},
numpages = {10},
location = {Virtual Event, USA},
series = {Mobihoc '20}
}

@inproceedings{10.1145/2009916.2010170,
author = {Alonso, Omar and Lease, Matthew},
title = {Crowdsourcing for information retrieval: principles, methods, and applications},
year = {2011},
isbn = {9781450307574},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2009916.2010170},
doi = {10.1145/2009916.2010170},
abstract = {Crowdsourcing has emerged in recent years as a promising new avenue for leveraging today's digitally-connected, diverse, distributed workforce. Generally speaking, crowdsourcing describes outsourcing of tasks to a large group of people instead of assigning such tasks to an in-house employee or contractor. Crowdsourcing platforms such as Amazon Mechanical Turk and CrowdFlower have gained particular attention as active online market places for reaching and tapping into this still largely under-utilized workforce. Crowdsourcing also offers intriguing new opportunities for accomplishing different kinds of tasks or achieving broader participation than previously possible, as well as completing standard tasks more accurately in less time and at lower cost. Unlocking the potential of crowdsourcing in practice, however, requires a tri-partite understanding of principles, platforms, and best practices. We will introduce the opportunities and challenges of crowdsourcing while discussing the three issues above. This will provide a basic foundation to begin crowdsourcing in the context of one's own particular tasks},
booktitle = {Proceedings of the 34th International ACM SIGIR Conference on Research and Development in Information Retrieval},
pages = {1299–1300},
numpages = {2},
keywords = {human computation, crowdsourcing},
location = {Beijing, China},
series = {SIGIR '11}
}

@inproceedings{10.1145/3250538,
author = {Dontcheva, Mira},
title = {Session details: Crowdsourcing \&amp; peer production I},
year = {2012},
isbn = {9781450310154},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3250538},
doi = {10.1145/3250538},
booktitle = {Proceedings of the SIGCHI Conference on Human Factors in Computing Systems},
location = {Austin, Texas, USA},
series = {CHI '12}
}

@inproceedings{10.1007/978-3-662-44845-8_49,
author = {Schnitzler, Fran\c{c}ois and Artikis, Alexander and Weidlich, Matthias and Boutsis, Ioannis and Liebig, Thomas and Piatkowski, Nico and Bockermann, Christian and Morik, Katharina and Kalogeraki, Vana and Marecek, Jakub and Gal, Avigdor and Mannor, Shie and Kinane, Dermot and Gunopulos, Dimitrios},
title = {Heterogeneous Stream Processing and Crowdsourcing for Traffic Monitoring: Highlights},
year = {2014},
isbn = {978-3-662-44844-1},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-662-44845-8_49},
doi = {10.1007/978-3-662-44845-8_49},
abstract = {We give an overview of an intelligent urban traffic management system. Complex events related to congestions are detected from heterogeneous sources involving fixed sensors mounted on intersections and mobile sensors mounted on public transport vehicles. To deal with data veracity, sensor disagreements are resolved by crowdsourcing. To deal with data sparsity, a traffic model offers information in areas with low sensor coverage. We apply the system to a real-world use-case.},
booktitle = {Machine Learning and Knowledge Discovery in Databases},
pages = {520–523},
numpages = {4},
keywords = {big data, stream processing, traffic, event pattern matching, crowdsourcing, smart cities},
location = {Nancy
France}
}

@inproceedings{10.5555/2772879.2773400,
author = {Chen, Cen and Cheng, Shih-Fen and Misra, Archan and Lau, Hoong Chuin},
title = {Multi-Agent Task Assignment for Mobile Crowdsourcing under Trajectory Uncertainties},
year = {2015},
isbn = {9781450334136},
publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
address = {Richland, SC},
abstract = {In this work, we investigate the problem of mobile crowdsourcing, where workers are financially motivated to perform location-based tasks physically. Unlike current industry practice that relies on workers to manually browse and filter tasks to perform, we intend to automatically make task recommendations based on workers' historical trajectories and desired time budgets. However, predicting workers' trajectories is inevitably faced with uncertainties, as no one will take exactly the same route every day; yet such uncertainties are oftentimes abstracted away in the known literature. In this work, we depart from the deterministic modeling and study the stochastic task recommendation problem where each worker is associated with several predicted routine routes with probabilities. We formulate this problem as a stochastic integer linear program whose goal is to maximize the expected total utility achieved by all workers. We further exploit the separable structure of the formulation and apply the Lagrangian relaxation technique to scale up the solution approach. Experiments have been performed over the instances generated using the real Singapore transportation network. The results show that we can find significantly better solutions than the deterministic formulation.},
booktitle = {Proceedings of the 2015 International Conference on Autonomous Agents and Multiagent Systems},
pages = {1715–1716},
numpages = {2},
keywords = {multiagent planning, mobile crowdsourcing},
location = {Istanbul, Turkey},
series = {AAMAS '15}
}

@inproceedings{10.1145/2424321.2424383,
author = {Efentakis, Alexandros and Theodorakis, Dimitris and Pfoser, Dieter},
title = {Crowdsourcing computing resources for shortest-path computation},
year = {2012},
isbn = {9781450316910},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2424321.2424383},
doi = {10.1145/2424321.2424383},
abstract = {Crowdsourcing road network data, i.e., involving users to collect data including the detection and assessment of changes to the road network graph, poses a challenge to shortest-path algorithms that rely on preprocessing. Hence, current research challenges lie with improving performance by adequately balancing preprocessing with respect to fast-changing road networks. In this work, we take the crowdsourcing approach further in that we solicit the help of users not only for data collection, but also to provide us their computing resources. A promising approach is parallelization, which splits the graph into chunks of data that may be processed separately. This work extends this approach in that small-enough chunks allow us to use browser-based computing to solve the pre-computation problem. Essentially, we aim for a Web-based navigation service that whenever users request a route, the service uses their browsers for partially preprocessing a large, but changing road network. The paper gives performance studies that highlight the potential of the browser as a computing platform and showcases a scalable approach, which almost eliminates the computing load on the server.},
booktitle = {Proceedings of the 20th International Conference on Advances in Geographic Information Systems},
pages = {434–437},
numpages = {4},
keywords = {shortest path, preprocessing, graph separators, crowdsourcing},
location = {Redondo Beach, California},
series = {SIGSPATIAL '12}
}

@inproceedings{10.1109/ICCV.2013.373,
author = {Long, Chengjiang and Hua, Gang and Kapoor, Ashish},
title = {Active Visual Recognition with Expertise Estimation in Crowdsourcing},
year = {2013},
isbn = {9781479928408},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/ICCV.2013.373},
doi = {10.1109/ICCV.2013.373},
abstract = {We present a noise resilient probabilistic model for active learning of a Gaussian process classifier from crowds, i.e., a set of noisy labelers. It explicitly models both the overall label noises and the expertise level of each individual labeler in two levels of flip models. Expectation propagation is adopted for efficient approximate Bayesian inference of our probabilistic model for classification, based on which, a generalized EM algorithm is derived to estimate both the global label noise and the expertise of each individual labeler. The probabilistic nature of our model immediately allows the adoption of the prediction entropy and estimated expertise for active selection of data sample to be labeled, and active selection of high quality labelers to label the data, respectively. We apply the proposed model for three visual recognition tasks, i.e., object category recognition, gender recognition, and multi-modal activity recognition, on three datasets with real crowd-sourced labels from Amazon Mechanical Turk. The experiments clearly demonstrated the efficacy of the proposed model.},
booktitle = {Proceedings of the 2013 IEEE International Conference on Computer Vision},
pages = {3000–3007},
numpages = {8},
keywords = {Visual Recognition, Gaussian Processes, Expectation Propagation, Crowdsourcing, Active Learning},
series = {ICCV '13}
}

@inproceedings{10.1145/3338147.3338159,
author = {Guo, Dongpo and Li, Qing and Liu, Sanya and Chai, Huanyou},
title = {Research on the Ecology Model of Crowdfunding and Crowdsourcing for Digital Education Service and its Applications in China},
year = {2019},
isbn = {9781450362658},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3338147.3338159},
doi = {10.1145/3338147.3338159},
abstract = {Crowdfunding and crowdsourcing in digital education service has become an important content in educational informatization. Different personalized demands from learners for digital education service are increasing day by day, which leads to the asymmetry between supply and demand. This paper aims to solve the asymmetry of supply and demand in education sector under the SISC ecology model based on value chain theory, and also to elaborate the ecology environment in digital education service, as well as the subject and content of crowdfunding and crowdsourcing. Block chain is the protection mechanism for the rights of subject, which improves the development of personalized education. Empirical study and qualitative analysis are used to fulfill the effectiveness of SISC ecology model of CFCS in DES. A case of "Parallel" successfully recommends 10 resources that suitable for the students, which proves the feasibility of the SISC ecology model so as to improve the motivations of CFCS subjects and student' satisfaction. This paper also supports future research by developing a framework for crowdfunding and crowdsourcing in digital education service.},
booktitle = {Proceedings of the 2019 4th International Conference on Distance Education and Learning},
pages = {147–152},
numpages = {6},
keywords = {Ecology model, Digital education service, Crowdfunding and Crowdsourcing, Block chain},
location = {Shanghai, China},
series = {ICDEL '19}
}

@inproceedings{10.1109/CIT.2014.126,
author = {Trow, Josh and Liu, Lu and Li, Zhiyuan},
title = {An Investigation Into Internet Crowdsourcing for Enterprise Software Development},
year = {2014},
isbn = {9781479962396},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/CIT.2014.126},
doi = {10.1109/CIT.2014.126},
abstract = {Recent years have witnessed the continuing development of the Internet from its original communication purpose (e.g., email), content provision (e.g., Web) and software deployment platform (e.g. SaaS) to a software development platform for enterprise. This paper investigated crowd sourcing, in particular its application to the software development sector. By making use of this method developers can be recruited from around the world, resulting in a wide range of expertise and more manageable workload. This paper considers a number of ways which crowd sourcing can be applied to existing software development methods and techniques, as well as exploring and evaluating the benefits and constraints of such methods. The paper used data gathered from the Top Coder platform to assess the skill levels and demographics of the current crowd sourcing user base. This paper also examined a wider range of crowd sourcing platforms to discover which features makes platforms successful and how they can be applied to a software engineering platform. Recommendations are then made as to what should constitute a software engineering specific crowd sourcing platform.},
booktitle = {Proceedings of the 2014 IEEE International Conference on Computer and Information Technology},
pages = {474–481},
numpages = {8},
keywords = {software development, crowdsourcing, Internet},
series = {CIT '14}
}

@inproceedings{10.5555/2820116.2820125,
author = {Zhao, Mengyao and van der Hoek, Andr\'{e}},
title = {A brief perspective on microtask crowdsourcing workflows for interface design},
year = {2015},
publisher = {IEEE Press},
abstract = {User interface design, as a crucial part of software design, is complex. Current microtask crowdsourcing workflows do not support its complexity well. The difficulty particularly relates to the process to decompose an interface design task into microtasks. In order to make microtask crowdsourcing more supportive for interface design, we need a workflow that can help task owners to break down interface design tasks more easily. This paper briefly describes three experiments that help inform various aspects of workflow design for interface design through microtask crowdsourcing.},
booktitle = {Proceedings of the Second International Workshop on CrowdSourcing in Software Engineering},
pages = {45–46},
numpages = {2},
keywords = {interface design, crowdsourcing, complex work},
location = {Florence, Italy},
series = {CSI-SE '15}
}

@inproceedings{10.1145/2835596.2835610,
author = {Xu, Zheng and Sugumaran, Vijayan and Zhang, Hui},
title = {Crowdsourcing based spatial mining of urban emergency events using social media},
year = {2015},
isbn = {9781450339704},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2835596.2835610},
doi = {10.1145/2835596.2835610},
abstract = {With the advances of information communication technologies, it is critical to improve the efficiency and accuracy of emergency management systems through modern data processing techniques. The past decade has witnessed the tremendous technical advances in Sensor Networks, Internet/Web of Things, Cloud Computing, Mobile/Embedded Computing, Spatial/Temporal Data Processing, and Big Data, and these technologies have provided new opportunities and solutions to emergency management. GIS models and simulation capabilities are used to exercise response and recovery plans during non-disaster times. They help the decision-makers understand near real-time possibilities during an event. In this paper, a crowdsourcing based model for mining spatial information of urban emergency events is introduced. Firstly, basic definitions of the proposed method are given. Secondly, positive samples are selected to mine the spatial information of urban emergency events. Thirdly, location and GIS information are extracted from positive samples. At last, the real spatial information is determined based on address and GIS information. At last, a case study on an urban emergency event is given.},
booktitle = {Proceedings of the 1st ACM SIGSPATIAL International Workshop on the Use of GIS in Emergency Management},
articleno = {9},
numpages = {4},
keywords = {urban emergency events, urban computing, social media, crowdsourcing},
location = {Bellevue, Washington},
series = {EM-GIS '15}
}

@inproceedings{10.1109/MobServ.2014.30,
author = {Jarrett, Julian and Saleh, Iman and Blake, M. Brian and Thorpe, Sean and Grandison, Tyrone and Malcolm, Rohan},
title = {Mobile Services for Enhancing Human Crowdsourcing with Computing Elements},
year = {2014},
isbn = {9781479950607},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/MobServ.2014.30},
doi = {10.1109/MobServ.2014.30},
abstract = {Crowdsourcing enables one to leverage the power of the crowd. Normally, it involves utilizing humans for tasks that machines have difficulty performing. We propose a system, delivered as a mobile service, which dynamically adapts to the application domain and selects a combination of human and machine crowdsourcing components. Our work is towards the design of elastic systems that adaptively optimizes the use of human and automated software resources in order to maximize overall performance. We propose a performance model that predicts both human and machine outcomes for a certain task and then optimizes task assignment accordingly. Our experimentation shows that our proposed system significantly enhances the outcome precision of a crowdsourced task.},
booktitle = {Proceedings of the 2014 IEEE International Conference on Mobile Services},
pages = {149–152},
numpages = {4},
keywords = {Services, Mobile, Crowdsourcing},
series = {MS '14}
}

@inproceedings{10.5555/1866696.1866710,
author = {Higgins, Chiara and McGrath, Elizabeth and Moretto, Lailla},
title = {MTurk crowdsourcing: a viable method for rapid discovery of Arabic nicknames?},
year = {2010},
publisher = {Association for Computational Linguistics},
address = {USA},
abstract = {This paper presents findings on using crowdsourcing via Amazon Mechanical Turk (MTurk) to obtain Arabic nicknames as a contribution to exiting Named Entity (NE) lexicons. It demonstrates a strategy for increasing MTurk participation from Arab countries. The researchers validate the nicknames using experts, MTurk workers, and Google search and then compare them against the Database of Arabic Names (DAN). Additionally, the experiment looks at the effect of pay rate on speed of nickname collection and documents an advertising effect where MTurk workers respond to existing work batches, called Human Intelligence Tasks (HITs), more quickly once similar higher paying HITs are posted.},
booktitle = {Proceedings of the NAACL HLT 2010 Workshop on Creating Speech and Language Data with Amazon's Mechanical Turk},
pages = {89–92},
numpages = {4},
location = {Los Angeles, California},
series = {CSLDAMT '10}
}

@inproceedings{10.5555/3044805.3044917,
author = {Zhou, Yuan and Chen, Xi and Li, Jian},
title = {Optimal PAC multiple arm identification with applications to crowdsourcing},
year = {2014},
publisher = {JMLR.org},
abstract = {We study the problem of selecting K arms with the highest expected rewards in a stochastic n-armed bandit game. Instead of using existing evaluation metrics (e.g., misidentification probability (Bubeck et al., 2013) or the metric in EXPLORE-K (Kalyanakrishnan \&amp; Stone, 2010)), we propose to use the aggregate regret, which is defined as the gap between the average reward of the optimal solution and that of our solution. Besides being a natural metric by itself, we argue that in many applications, such as our motivating example from crowdsourcing, the aggregate regret bound is more suitable. We propose a new PAC algorithm, which, with probability at least 1 - δ, identifies a set of K arms with regret at most ε. We provide the sample complexity bound of our algorithm. To complement, we establish the lower bound and show that the sample complexity of our algorithm matches the lower bound. Finally, we report experimental results on both synthetic and real data sets, which demonstrates the superior performance of the proposed algorithm.},
booktitle = {Proceedings of the 31st International Conference on International Conference on Machine Learning - Volume 32},
pages = {II–217–II–225},
location = {Beijing, China},
series = {ICML'14}
}

@inproceedings{10.1145/2908131.2908209,
author = {Gadiraju, Ujwal and Siehndel, Patrick and Dietze, Stefan},
title = {Estimating domain specificity for effective crowdsourcing of link prediction and schema mapping},
year = {2016},
isbn = {9781450342087},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2908131.2908209},
doi = {10.1145/2908131.2908209},
abstract = {Crowdsourcing has been widely adopted in research and practice over the last decade. In this work, we first investigate the extent to which crowd workers can substitute expert-based judgments in the task of link prediction and schema mapping, which is the creation of explicit links between resources on the Semantic Web at the instance and schema level. This is important since human input is required to evaluate and improve automated approaches for these tasks. We present a novel method to assess the inherent specificity of the link prediction task, and the impact of task specificity on quality of the results. We propose a Wikipedia-based mechanism to estimate specificity and show the influence of concept familiarity in producing high quality link prediction. Our findings indicate that the effectiveness of crowdsourcing the task of link prediction can improve by estimating the specificity.},
booktitle = {Proceedings of the 8th ACM Conference on Web Science},
pages = {323–324},
numpages = {2},
keywords = {schema mapping, nichesourcing, link prediction, experts, crowdsourcing, crowd workers},
location = {Hannover, Germany},
series = {WebSci '16}
}

@inproceedings{10.1109/ICPADS.2015.28,
author = {Hao Wang and Dong Zhao and Huadong Ma and Huaiyu Xu and Xiabing Hou},
title = {Crowdsourcing Based Mobile Location Recognition with Richer Fingerprints from Smartphone Sensors},
year = {2015},
isbn = {9780769557854},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/ICPADS.2015.28},
doi = {10.1109/ICPADS.2015.28},
abstract = {With the rapid advancements of mobile computing, mobile location recognition is becoming an important and useful service, which recognizes the logical locations of places/scenes that users are interested in, instead of physical coordinates. Most of the existing mobile location recognition systems utilize the image as visual fingerprint of a place, and need to construct a large-scale visual fingerprint database in advance. However, collecting visual fingerprints is a labor-intensive and time-consuming procedure. In order to address this problem, we propose a novel crowdsourcing-based framework, and leverage a variety of sensors embedded in smartphones to collect richer location fingerprints for exploring their positive effects. To achieve higher recognition accuracy, we propose an object-centric fingerprint searching which can sufficiently take advantage of smartphone sensors and determine more accurate searching space than the traditional user-centric method. We build a crowdsourcing-based database with richer fingerprints and implement a location recognition system, called CrowdLR. Extensive experiments verify that our object-centric method can achieve promising results maintaining around 10\% precision higher than the user-centric method.},
booktitle = {Proceedings of the 2015 IEEE 21st International Conference on Parallel and Distributed Systems (ICPADS)},
pages = {156–163},
numpages = {8},
series = {ICPADS '15}
}

@inproceedings{10.1145/2502081.2503828,
author = {Chen, Kuan-Ta and Chu, Wei-Ta and Larson, Martha},
title = {ACM multimedia 2013 workshop on crowdsourcing for multimedia},
year = {2013},
isbn = {9781450324045},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2502081.2503828},
doi = {10.1145/2502081.2503828},
abstract = {The topic "Crowdsourcing for Multimedia" encompasses the full range of techniques that combine human intelligence and a large number of individual contributors to advance the state of the art in multimedia research. The ACM Multimedia 2013 Workshop on Crowdsourcing for Multimedia (CrowdMM 2013) provided a forum for presenting new crowdsourcing techniques, exchanging innovative crowdsourcing ideas, and discussing crowdsourcing best practices for multimedia. The workshop program consisted of presented papers, a keynote speech and a panel discussion. A special feature of this year's workshop was the "Crowdsourcing for Multimedia Ideas Competition", the results of which were presented at the workshop.},
booktitle = {Proceedings of the 21st ACM International Conference on Multimedia},
pages = {1089–1090},
numpages = {2},
keywords = {multimedia, human computation, crowdsourcing},
location = {Barcelona, Spain},
series = {MM '13}
}

@inproceedings{10.5555/2887352.2887358,
author = {Simperl, Elena and Norton, Barry and Vrande\v{c}i\'{c}, Denny},
title = {Crowdsourcing tasks in linked data management},
year = {2011},
publisher = {CEUR-WS.org},
address = {Aachen, DEU},
abstract = {Many aspects of Linked Data management - including exposing legacy data and applications to semantic formats, designing vocabularies to describe RDF data, identifying links between entities, query processing, and data curation - are necessarily tackled through the combination of human effort with algorithmic techniques. In the literature on traditional data management the theoretical and technical groundwork to realize and manage such combinations is being established. In this paper we build upon and extend these ideas to propose a framework by which human and computational intelligence can co-exist by augmenting existing Linked Data and Linked Service technology with crowdsourcing functionality. Starting from a motivational scenario we introduce a set of generic tasks which may feasibly be approached using crowdsourcing platforms such as Amazon's Mechanical Turk, explain how these tasks can be decomposed and translated into MTurk projects, and roadmap the extensions to SPARQL, D2RQ/R2R and Linked Data browsing that are required to achieve this vision.},
booktitle = {Proceedings of the Second International Conference on Consuming Linked Data - Volume 782},
pages = {61–72},
numpages = {12},
location = {Bonn, Germany},
series = {COLD'11}
}

@inproceedings{10.5555/3620237.3620297,
author = {Qiu, Wenjun and Lie, David and Austin, Lisa},
title = {Calpric: inclusive and fine-grained labeling of privacy policies with crowdsourcing and active learning},
year = {2023},
isbn = {978-1-939133-37-3},
publisher = {USENIX Association},
address = {USA},
abstract = {A significant challenge to training accurate deep learning models on privacy policies is the cost and difficulty of obtaining a large and comprehensive set of training data. To address these challenges, we present Calpric, which combines automatic text selection and segmentation, active learning and the use of crowdsourced annotators to generate a large, balanced training set for Android privacy policies at low cost. Automated text selection and segmentation simplifies the labeling task, enabling untrained annotators from crowdsourcing platforms, like Amazon's Mechanical Turk, to be competitive with trained annotators, such as law students, and also reduces inter-annotator agreement, which decreases labeling cost. Having reliable labels for training enables the use of active learning, which uses fewer training samples to efficiently cover the input space, further reducing cost and improving class and data category balance in the data set.The combination of these techniques allows Calpric to produce models that are accurate over a wider range of data categories, and provide more detailed, fine-grained labels than previous work. Our crowdsourcing process enables Calpric to attain reliable labeled data at a cost of roughly $0.92-$1.71 per labeled text segment. Calpric's training process also generates a labeled data set of 16K privacy policy text segments across 9 data categories with balanced assertions and denials.},
booktitle = {Proceedings of the 32nd USENIX Conference on Security Symposium},
articleno = {60},
numpages = {18},
location = {Anaheim, CA, USA},
series = {SEC '23}
}

@inproceedings{10.1109/MASCOTS.2014.28,
author = {Amor, Iheb Ben and Ouziri, Mourad and Sahri, Soror and Karam, Naouel},
title = {Be a Collaborator and a Competitor in Crowdsourcing System},
year = {2014},
isbn = {9781479956104},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/MASCOTS.2014.28},
doi = {10.1109/MASCOTS.2014.28},
abstract = {Crowd sourcing is emerging as a powerful paradigm to solve a wide range of tedious and complex problems in various enterprise applications. It spawns the issue of finding the unknown collaborative and competitive group of solvers. The formation of collaborative team should provide the best solution and treat that solution as a trade secret avoiding data leak between competitive teams due to reward behind the outsourcing of the issue. The formation of effective competitive teams not only requires adequate skills between members of each team, but also good team connectivity through social network and to provide the best solution and treat that solution as a trade secret avoiding data leak between teams due to reward behind the outsourcing of the issue. In this paper, we propose a data leak aware crowd sourcing system called Social Crowd. We introduce a clustering algorithm that uses social relationships between crowd workers to discover all possible teams while avoiding inter-team data leakage.},
booktitle = {Proceedings of the 2014 IEEE 22nd International Symposium on Modelling, Analysis \&amp; Simulation of Computer and Telecommunication Systems},
pages = {158–167},
numpages = {10},
series = {MASCOTS '14}
}

@inproceedings{10.1145/2875913.2875945,
author = {He, Meimei and Tang, Hongyin and Wu, Guoquan and Wei, Jun and Zhong, Hua},
title = {A Crowdsourcing framework for Detecting Cross-Browser Issues in Web Application},
year = {2015},
isbn = {9781450336413},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2875913.2875945},
doi = {10.1145/2875913.2875945},
abstract = {With the advent of Web 2.0 application, and the increasing number of browsers and platforms on which the applications can be executed, cross-browser incompatibilities (XBIs) are becoming a serious problem for organizations to develop web-based software with good user experience. Although some techniques and tools have been proposed to identify XBIs, some XBIs are still missed as only partial state space is explored (by the crawler) in the testing environment. To address this limitation, based on record/replay technique, this paper proposed a crowdsourcing framework to detect cross-browser issues for Web application deployed in the field. Our empirical evaluation shows that the proposed technique is effective and efficient, improves on the state of the art.},
booktitle = {Proceedings of the 7th Asia-Pacific Symposium on Internetware},
pages = {239–242},
numpages = {4},
keywords = {Web Application, Record/Replay, Crowdsourcing},
location = {Wuhan, China},
series = {Internetware '15}
}

@inproceedings{10.5555/2892753.2893000,
author = {Fang, Yili and Sun, Hailong and Zhang, Richong and Huai, Jinpeng and Mao, Yongyi},
title = {A model for aggregating contributions of synergistic crowdsourcing workflows},
year = {2014},
publisher = {AAAI Press},
abstract = {One of the most important crowdsourcing topics is to study the effective quality control methods so as to reduce the cost and to guarantee the quality of task processing. As an effective approach, iterative improvement workflow is known to choose the best result from multiple workflows. However, for complex crowdsourcing tasks that consists of a certain number of subtasks under some specific constraints, but cannot be split into subtasks to be crowdsourced, the approach merely considers the best workflow without integrating the contributions of all workflows, which potentially results in extra costs for more iterations. In this paper, we propose an assembly model to integrate the best output of subtasks from different workflows. Moreover, we devise an efficient iterative method based on POMDP to improve the quality of assembled output. Empirical studies confirms the superiority of our proposed model.},
booktitle = {Proceedings of the Twenty-Eighth AAAI Conference on Artificial Intelligence},
pages = {3102–3103},
numpages = {2},
location = {Qu\'{e}bec City, Qu\'{e}bec, Canada},
series = {AAAI'14}
}

@inproceedings{10.5555/2936924.2936941,
author = {Jain, Shweta and Ghalme, Ganesh and Bhat, Satyanath and Gujar, Sujit and Narahari, Y.},
title = {A Deterministic MAB Mechanism for Crowdsourcing with Logarithmic Regret and Immediate Payments},
year = {2016},
isbn = {9781450342391},
publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
address = {Richland, SC},
abstract = {We consider a general crowdsourcing setting with strategic workers whose qualities are unknown and design a multi-armed bandit (MAB) mechanism, CrowdUCB, which is deterministic, regret minimizing, and offers immediate payments to the workers. The problem involves sequentially selecting workers to process tasks in order to maximize the social welfare while learning the qualities of the strategic workers (strategic about their costs). Existing MAB mechanisms are either: (a) deterministic which potentially cause significant loss in social welfare, or (b) randomized which typically lead to high variance in payments. CrowdUCB completely addresses the above problems with the following features: (i) offers deterministic payments, (ii) achieves logarithmic regret in social welfare, (iii) renders allocations more effective by allocating blocks of tasks to a worker instead of a single task, and (iv) offers payment to a worker immediately upon completion of an assigned block of tasks. CrowdUCB is a mechanism with learning that learns the qualities of the workers while eliciting their true costs, irrespective of whether or not the workers know their own qualities. We show that CrowdUCB is ex-post individually rational (EPIR) and ex-post incentive compatible (EPIC) when the workers do not know their own qualities and when they update their beliefs in sync with the requester. When the workers know their own qualities, CrowdUCB is EPIR and εEPIC where ε is sub-linear in terms of the number of tasks.},
booktitle = {Proceedings of the 2016 International Conference on Autonomous Agents \&amp; Multiagent Systems},
pages = {86–94},
numpages = {9},
keywords = {multi-armed bandit, mechanism design, crowdsourcing},
location = {Singapore, Singapore},
series = {AAMAS '16}
}

@inproceedings{10.1109/ICTAI.2015.128,
author = {Fan, Yue and Sun, Hailong and Liu, Xudong},
title = {Truthful Incentive Mechanisms for Dynamic and Heterogeneous Tasks in Mobile Crowdsourcing},
year = {2015},
isbn = {9781509001637},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/ICTAI.2015.128},
doi = {10.1109/ICTAI.2015.128},
abstract = {Crowdsourcing has received tremendous attention for collecting various data with the distributed smartphones of people. For the mobile crowdsourcing applications to obtain high-quality data, stimulating user participation is of paramount importance. Although many incentive mechanisms have been designed, most of them ignore the dynamic arrivals and different sensing requirements of tasks. Thus, the existing mechanisms will fail when being applied to the realistic scenario where tasks are publicized dynamically and heterogeneous with different sensing requirements of locations, time durations and sensing times. In this work, we propose two auction-based truthful mechanisms, TRIMS and TRIMG, for realistic mobile crowdsourcing under special user model and more general model, respectively. Through extensive simulations and theoretical analysis, we demonstrate that our mechanisms can satisfy the desired properties of truthfulness, individual rationality, computational efficiency with both low social cost and low total payment.},
booktitle = {Proceedings of the 2015 IEEE 27th International Conference on Tools with Artificial Intelligence (ICTAI)},
pages = {881–888},
numpages = {8},
series = {ICTAI '15}
}

@inproceedings{10.1007/978-3-319-07626-3_64,
author = {Choi, Joohee and Choi, Heejin and So, Woonsub and Lee, Jaeki and You, Jongjun},
title = {A Study about Designing Reward for Gamified Crowdsourcing System},
year = {2014},
isbn = {9783319076256},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-319-07626-3_64},
doi = {10.1007/978-3-319-07626-3_64},
abstract = {The goal of this study is to understand the mechanism of gamification in crowdsourcing by investigating the ways of giving rewards. Perceived reward diversity is proposed as a construct to induce fun experience from participants based on previous studies about gamified crowdsourcing. With respect to system manipulation, explicating the anticipated level of rewards before task phase is conducted. The effect of explication on task outcome and psychological outcome is compared with control group. As a result, both perceived reward diversity and explicating the anticipated level of rewards significantly affect both quality and quantity of submitted answers, as well as feeling of fun during the task phase. The limitation and implication of the study is stated in the end.},
booktitle = {Proceedings of the Third International Conference on Design, User Experience, and Usability. User Experience Design for Diverse Interaction Platforms and Environments - Volume 8518},
pages = {678–687},
numpages = {10},
keywords = {reward, perceived diversity, gamification, fun experience, crowdsourcing}
}

@inproceedings{10.1109/WI-IAT.2012.86,
author = {Jiang, Huan and Matsubara, Shigeo},
title = {Improving Crowdsourcing Efficiency Based on Division Strategy},
year = {2012},
isbn = {9780769548807},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/WI-IAT.2012.86},
doi = {10.1109/WI-IAT.2012.86},
abstract = {This paper examines the efficiency in crowd sourcing, especially crowd sourcing for the software bug detection. Crowd sourcing has recently emerged as a lucrative paradigm for leveraging the collective intelligence of crowds. However, it has inherent weakness that a simple reward setting causes an uneven distribution of workers on each task, which reduces the efficiency of solving the tasks. A challenge is that the system designer is not allowed to set the reward to the arbitrary value because so-called "market wages" exist and if the reward is set to the value lower than the market wage, such a task fails to attract the sufficient number of workers. To solve this problem, we focus on the division strategy that divides the crowds into different groups and the workers compete with each other among the same group. We have developed a model that crowds write their codes independently and then try to find bugs in the codes written by the others. Next, we examine two division strategy, random grouping and ability grouping by analyzing the equilibrium behavior of each worker and carrying out simulations. The results show that the division strategy affect the efficiency of crowd sourcing bug detection and the random grouping leads to a higher efficiency compared to the ability grouping.},
booktitle = {Proceedings of the The 2012 IEEE/WIC/ACM International Joint Conferences on Web Intelligence and Intelligent Agent Technology - Volume 02},
pages = {425–429},
numpages = {5},
keywords = {software bug detection, division strategy, Crowdsourcing efficiency},
series = {WI-IAT '12}
}

@inproceedings{10.1109/ISM.2011.87,
author = {Hoβfeld, Tobias and Seufert, Michael and Hirth, Matthias and Zinner, Thomas and Tran-Gia, Phuoc and Schatz, Raimund},
title = {Quantification of YouTube QoE via Crowdsourcing},
year = {2011},
isbn = {9780769545899},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/ISM.2011.87},
doi = {10.1109/ISM.2011.87},
abstract = {This paper addresses the challenge of assessing and modeling Quality of Experience (QoE) for online video services that are based on TCP-streaming. We present a dedicated QoE model for You Tube that takes into account the key influence factors (such as stalling events caused by network bottlenecks) that shape quality perception of this service. As second contribution, we propose a generic subjective QoE assessment methodology for multimedia applications (like online video) that is based on crowd sourcing - a highly cost-efficient, fast and flexible way of conducting user experiments. We demonstrate how our approach successfully leverages the inherent strengths of crowd sourcing while addressing critical aspects such as the reliability of the experimental data obtained. Our results suggest that, crowd sourcing is a highly effective QoE assessment method not only for online video, but also for a wide range of other current and future Internet applications.},
booktitle = {Proceedings of the 2011 IEEE International Symposium on Multimedia},
pages = {494–499},
numpages = {6},
keywords = {reliability, YouTube, HTTP video streaming, Crowdsourcing},
series = {ISM '11}
}

@inproceedings{10.5555/2888116.2888337,
author = {Wu, Heting and Sun, Hailong and Fang, Yili and Hu, Kefan and Xie, Yongqing and Song, Yangqiu and Liu, Xudong},
title = {Combining machine learning and crowdsourcing for better understanding commodity reviews},
year = {2015},
isbn = {0262511290},
publisher = {AAAI Press},
abstract = {In e-commerce systems, customer reviews are important information for understanding market feedbacks on certain commodities. However, accurate analyzing reviews is challenging due to the complexity of natural language processing and informal descriptions in reviews. Existing methods mainly focus on studying efficient algorithms that cannot guarantee the accuracy for review analysis. Crowdsourcing can improve the accuracy of review analysis while it is subject to extra costs and low response time. In this work, we combine machine learning and crowdsourcing together for better understanding customer reviews. First, we collectively use multiple machine learning algorithms to pre-process review classification. Second, we select the reviews on which all machine learning algorithms cannot agree and assign them to humans to process. Third, the results from machine learning and crowdsourcing are aggregated to be the final analysis results. Finally, we perform real experiments with practical review data to confirm the effectiveness of our method.},
booktitle = {Proceedings of the Twenty-Ninth AAAI Conference on Artificial Intelligence},
pages = {4220–4221},
numpages = {2},
location = {Austin, Texas},
series = {AAAI'15}
}

@inproceedings{10.1007/978-3-319-27974-9_6,
author = {Goh, Dion Hoe-Lian and Pe-Than, Ei Pa and Lee, Chei Sian},
title = {Investigating the Antecedents of Playing Games for Crowdsourcing Location-Based Content},
year = {2015},
isbn = {9783319279732},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-319-27974-9_6},
doi = {10.1007/978-3-319-27974-9_6},
abstract = {Human Computation Games HCGs are games that harness human intelligence to tackle computational problems. Put differently, they are a means of crowdsourcing via games. Due to this entertainment-output generation duality, perceived enjoyment and perceived quality of outputs are potentially important determinants of HCG usage. This study adopts a multidimensional view of perceived enjoyment and output quality to investigate their influence on intention to use HCGs. This is done using SPLASH, our developed mobile HCG for crowdsourcing location-based content. Since SPLASH comprises various gaming features, we further study how the different dimensions of enjoyment vary across them. Using a survey of 105 undergraduate and graduate students, findings validated the multidimensionality of perceived enjoyment and output quality and showed their differing influence. As well, the different gaming features elicited different perceptions of enjoyment. Our results thus suggest that HCGs can be used for crowdsourcing tasks if they can fulfill enjoyment and assure output quality.},
booktitle = {Proceedings of the 17th International Conference on Asia-Pacific Digital Libraries - Volume 9469},
pages = {52–63},
numpages = {12},
keywords = {Output quality, Mobile devices, Location-based content, Human computation games, Enjoyment, Crowdsourcing}
}

@inproceedings{10.1109/SocialCom.2013.71,
author = {Wang, Yang and Huang, Yun and Louis, Claudia},
title = {Towards a Framework for Privacy-Aware Mobile Crowdsourcing},
year = {2013},
isbn = {9780769551371},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/SocialCom.2013.71},
doi = {10.1109/SocialCom.2013.71},
abstract = {The practice of employing "the crowd" to help solve an organization's problems first became popular in the business sector, and has since spread to public and not-for-profit organizations. Input from the crowd can be solicited using different mechanisms involving various types of web-based applications, or the more recent trend of employing mobile phones with sensing capabilities. However, these crowd sourcing systems may lead to various privacy and security risks which can then hinder the adoption of these services. How to identify and address these potential risks in such systems has both research and practical value. This paper presents two aspects of our work in this emerging space. First, we describe a survey of potential privacy and security risks in mobile crowd sourcing systems (MCSS). Second, we describe our PEALS framework to support privacy-aware mobile crowd sourcing.},
booktitle = {Proceedings of the 2013 International Conference on Social Computing},
pages = {454–459},
numpages = {6},
keywords = {security, privacy, crowdsourcing},
series = {SOCIALCOM '13}
}

@inproceedings{10.5555/2986459.2986677,
author = {Karger, David R. and Oh, Sewoong and Shah, Devavrat},
title = {Iterative learning for reliable crowdsourcing systems},
year = {2011},
isbn = {9781618395993},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Crowdsourcing systems, in which tasks are electronically distributed to numerous "information piece-workers", have emerged as an effective paradigm for human-powered solving of large scale problems in domains such as image classification, data entry, optical character recognition, recommendation, and proofreading. Because these low-paid workers can be unreliable, nearly all crowdsourcers must devise schemes to increase confidence in their answers, typically by assigning each task multiple times and combining the answers in some way such as majority voting. In this paper, we consider a general model of such crowdsourcing tasks, and pose the problem of minimizing the total price (i.e., number of task assignments) that must be paid to achieve a target overall reliability. We give a new algorithm for deciding which tasks to assign to which workers and for inferring correct answers from the workers' answers. We show that our algorithm significantly outperforms majority voting and, in fact, is asymptotically optimal through comparison to an oracle that knows the reliability of every worker.},
booktitle = {Proceedings of the 24th International Conference on Neural Information Processing Systems},
pages = {1953–1961},
numpages = {9},
location = {Granada, Spain},
series = {NIPS'11}
}

@inproceedings{10.5555/2050843.2050864,
author = {Viappiani, Paolo and Zilles, Sandra and Hamilton, Howard J. and Boutilier, Craig},
title = {Learning complex concepts using crowdsourcing: a Bayesian approach},
year = {2011},
isbn = {9783642248726},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {We develop a Bayesian approach to concept learning for crowdsourcing applications. A probabilistic belief over possible concept definitions is maintained and updated according to (noisy) observations from experts, whose behaviors are modeled using discrete types. We propose recommendation techniques, inference methods, and query selection strategies to assist a user charged with choosing a configuration that satisfies some (partially known) concept. Our model is able to simultaneously learn the concept definition and the types of the experts. We evaluate our model with simulations, showing that our Bayesian strategies are effective even in large concept spaces with many uninformative experts.},
booktitle = {Proceedings of the Second International Conference on Algorithmic Decision Theory},
pages = {277–291},
numpages = {15},
location = {Piscataway, NJ},
series = {ADT'11}
}

@inproceedings{10.1145/1982624.1982625,
author = {Ipeirotis, Panos},
title = {Crowdsourcing using Mechanical Turk: quality management and scalability},
year = {2011},
isbn = {9781450306201},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1982624.1982625},
doi = {10.1145/1982624.1982625},
abstract = {I will discuss the repeated acquisition of "labels" for data items when the labeling is imperfect. Labels are values provided by humans for specified variables on data items, such as "PG-13" for "Adult Content Rating on this Web Page." With the increasing popularity of micro-outsourcing systems, such as Amazon's Mechanical Turk, it often is possible to obtain less-than-expert labeling at low cost. We examine the improvement (or lack thereof) in data quality via repeated labeling, and focus especially on the improvement of training labels for supervised induction.},
booktitle = {Proceedings of the 8th International Workshop on Information Integration on the Web: In Conjunction with WWW 2011},
articleno = {1},
numpages = {1},
keywords = {spam detection, labeling},
location = {Hyderabad, India},
series = {IIWeb '11}
}

@inproceedings{10.1145/2384916.2384941,
author = {Burton, Michele A. and Brady, Erin and Brewer, Robin and Neylan, Callie and Bigham, Jeffrey P. and Hurst, Amy},
title = {Crowdsourcing subjective fashion advice using VizWiz: challenges and opportunities},
year = {2012},
isbn = {9781450313216},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2384916.2384941},
doi = {10.1145/2384916.2384941},
abstract = {Fashion is a language. How we dress signals to others who we are and how we want to be perceived. However, this language is primarily visual, making it inaccessible to people with vision impairments. Someone who is low-vision or completely blind cannot see what others are wearing or readily know what constitutes the norms and extremes of fashion, but most everyone they encounter can see (and judge) their fashion choices. We describe our findings of a diary study with people with vision impairments that revealed the many accessibility barriers fashion presents, and how an online survey revealed that clothing decisions are often made collaboratively, regardless of visual ability. Based on these findings, we identified a need for a collaborative and real-time environment for fashion advice. We have tested the feasibility of providing this advice through crowdsourcing using VizWiz, a mobile phone application where participants receive nearly real-time answers to visual questions. Our pilot study results show that this application has the potential to address a great need within the blind community, but remaining challenges include improving photo capture and assembling a set of crowd workers with the requisite expertise. More broadly our research highlights the feasibility of using crowdsourcing for subjective, opinion-based advice.},
booktitle = {Proceedings of the 14th International ACM SIGACCESS Conference on Computers and Accessibility},
pages = {135–142},
numpages = {8},
keywords = {fashion, crowdsourcing, blind users},
location = {Boulder, Colorado, USA},
series = {ASSETS '12}
}

@inproceedings{10.1145/2382636.2382642,
author = {Chorianopoulos, Konstantinos},
title = {Crowdsourcing user interactions with the video player},
year = {2012},
isbn = {9781450317061},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2382636.2382642},
doi = {10.1145/2382636.2382642},
abstract = {Every second millions of users enjoy content streaming on diverse video players (e.g., Web, Apps, social networks) and create billions of interactions within online video, such as play, pause, seek/scrub. This collective intelligence of video viewers might be leveraged into useful information for improved video navigation. For example, we can accurately detect and retrieve interesting scenes through the analysis of the aggregated users' replay interactions with the video player. Effective crowdsourcing of video interactions is grounded on previous work in multimedia, user modeling, and controlled user experiments. These research issues are described for the case of user-based detection of video thumbnails that stand for the semantics of the video. Moreover, we demonstrate the respective experimental environment with a focus on educational and user generated (e.g., how-to, lecture) videos.},
booktitle = {Proceedings of the 18th Brazilian Symposium on Multimedia and the Web},
pages = {13–16},
numpages = {4},
keywords = {video, user-based, signal processing., replay, key-frame},
location = {S\~{a}o Paulo/SP, Brazil},
series = {WebMedia '12}
}

@inproceedings{10.1109/MDM.2014.70,
author = {Nandan, Naveen and Pursche, Andreas and Zhe, Xing},
title = {Challenges in Crowdsourcing Real-Time Information for Public Transportation},
year = {2014},
isbn = {9781479957057},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/MDM.2014.70},
doi = {10.1109/MDM.2014.70},
abstract = {Public transportation is a key enabler of mobility in today's urban environment. Transportation service operators are exploring various technologies in order to be able to provide passengers accurate real-time information to plan their journeys. For them, the challenge is often in understanding where, when and how there is a demand. With rapid advancements in mobile technology, crowd sourcing or participatory sensing can be thought of as a medium by which information can be collected, augmented and used for better planning as well as a mode to deliver real-time information to commuters. In this paper, we conduct an extensive review of both literature and applications using mobile crowd sourcing with a focus on the public transportation domain. We identify certain common challenges across various techniques proposed and applied, categorize them and discuss possible future research directions into these areas.},
booktitle = {Proceedings of the 2014 IEEE 15th International Conference on Mobile Data Management - Volume 02},
pages = {67–72},
numpages = {6},
keywords = {Real-time passenger information systems, Participatory sensing, Mobile crowdsourcing, Data collection},
series = {MDM '14}
}

@inproceedings{10.1145/1979742.1979593,
author = {Bernstein, Michael and Chi, Ed H. and Chilton, Lydia and Hartmann, Bj\"{o}rn and Kittur, Aniket and Miller, Robert C.},
title = {Crowdsourcing and human computation: systems, studies and platforms},
year = {2011},
isbn = {9781450302685},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1979742.1979593},
doi = {10.1145/1979742.1979593},
abstract = {Crowdsourcing and human computation are transforming human-computer interaction, and CHI has led the way. The seminal publication in human computation was initially published in CHI in 2004 [1], and the first paper investigating Mechanical Turk as a user study platform has amassed over one hundred citations in two years [5]. However, we are just beginning to stake out a coherent research agenda for the field. This workshop will bring together researchers in the young field of crowdsourcing and human computation and produce three artifacts: a research agenda for the field, a vision for ideal crowdsourcing platforms, and a group-edited bibliography. These resources will be publically disseminated on the web and evolved and maintained by the community.},
booktitle = {CHI '11 Extended Abstracts on Human Factors in Computing Systems},
pages = {53–56},
numpages = {4},
keywords = {workshop, platform, human computation, crowdsourcing, bibliography},
location = {Vancouver, BC, Canada},
series = {CHI EA '11}
}

@inproceedings{10.1109/SCC.2013.58,
author = {Vukovic, Maja and Natarajan, Arjun},
title = {Operational Excellence in IT Services Using Enterprise Crowdsourcing},
year = {2013},
isbn = {9780769550268},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/SCC.2013.58},
doi = {10.1109/SCC.2013.58},
abstract = {The IT services industry has been undergoing a significant transformation over the last decade primarily driven by the global delivery model - services are provided from delivery centers across the globe based on skill and cost. While extremely effective a key challenge is being able to harness distributed knowledge, especially to drive operational and business process optimizations and a superior client experience. The knowledge about service requirements, client experience and delivery quality is in collective possession of different communities, such as clients, service designers and delivery teams. Current practices to discovering this distributed and unstructured knowledge are semi-automated, and as such they fail to scale and provide accurate insights on demand. Enterprise crowdsourcing provides a mechanism to harness the tacit knowledge from a large group of network-connected humans. In this paper we describe a novel approach to improving operational excellence, with focus on compliance posture, using enterprise crowdsourcing. We demonstrate how enterprise crowdsourcing accelerated deployment of a novel identity access management capability in a global IT service delivery center, reducing the time to discover the required knowledge by 80\%. We discuss how the uncovered knowledge networks can be engaged for various on-going operational activities as well as large-scale business transformation initiatives.},
booktitle = {Proceedings of the 2013 IEEE International Conference on Services Computing},
pages = {494–501},
numpages = {8},
keywords = {operational excellence, crowdsourcing, component, IT services},
series = {SCC '13}
}

@inproceedings{10.5555/2615731.2616098,
author = {Bachrach, Yoram and Ceppi, Sofia and Kash, Ian A. and Key, Peter and Radlinski, Filip and Porat, Ely and Armstrong, Michael and Sharma, Vijay},
title = {Building a personalized tourist attraction recommender system using crowdsourcing},
year = {2014},
isbn = {9781450327381},
publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
address = {Richland, SC},
abstract = {We demonstrate how crowdsourcing can be used to automatically build a personalized tourist attraction recommender system, which tailors recommendations to specific individuals, so different people who use the system each get their own list of recommendations, appropriate to their own traits. Recommender systems crucially depend on the availability of reliable and large scale data that allows predicting how a new individual is likely to rate items from the catalog of possible items to recommend. We show how to automate the process of generating this data using crowdsourcing, so that such a system can be built even when such a dataset is not initially available. We first find possible tourist attractions to recommend by scraping such information from Wikipedia. Next, we use crowdsourced workers to filter the data, then provide their opinions regarding these items. Finally, we use machine learning methods to predict how new individuals are likely to rate each attraction, and recommend the items with the highest predicted ratings.},
booktitle = {Proceedings of the 2014 International Conference on Autonomous Agents and Multi-Agent Systems},
pages = {1631–1632},
numpages = {2},
keywords = {human factors, economics, algorithms},
location = {Paris, France},
series = {AAMAS '14}
}

@inproceedings{10.1145/2389176.2389189,
author = {V\"{a}\"{a}t\"{a}j\"{a}, Heli and Vainio, Teija and Sirkkunen, Esa},
title = {Location-based crowdsourcing of hyperlocal news: dimensions of participation preferences},
year = {2012},
isbn = {9781450314862},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2389176.2389189},
doi = {10.1145/2389176.2389189},
abstract = {We studied the mobile users' preferences and concerns of using location-based assignments (LBA) and geotagging in crowdsourced news making. First, nine readers who had submitted reader's photos were interviewed about their perceptions of LBA and geotagging scenarios. Second, a quasi-experiment in field conditions was carried out with nineteen participants. After completing four LBA tasks with a mobile phone, participants were interviewed on their perceptions and asked to complete a questionnaire on their preferences for receiving LBA and usage of geotags. Findings indicate that the perceived benefits of LBA and geotagging are greater than the perceived risks. The task type, temporal context, preciseness of location query, proximity to the reporting location, parallel tasks, social context and incentives affected the participation preferences. We propose a framework for participation preferences to support further studies in location-based crowdsourcing and in the development of crowdsourcing processes and systems.},
booktitle = {Proceedings of the 2012 ACM International Conference on Supporting Group Work},
pages = {85–94},
numpages = {10},
keywords = {ugc, reader, privacy, news, location, crowdsourcing, assignment},
location = {Sanibel Island, Florida, USA},
series = {GROUP '12}
}

@inproceedings{10.1145/2390803.2390816,
author = {Park, Sunghyun and Mohammadi, Gelareh and Artstein, Ron and Morency, Louis-Philippe},
title = {Crowdsourcing micro-level multimedia annotations: the challenges of evaluation and interface},
year = {2012},
isbn = {9781450315890},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2390803.2390816},
doi = {10.1145/2390803.2390816},
abstract = {This paper presents a new evaluation procedure and tool for crowdsourcing micro-level multimedia annotations and shows that such annotations can achieve a quality comparable to that of expert annotations. We propose a new evaluation procedure, called MM-Eval (Micro-level Multimedia Evaluation), which compares fine time-aligned annotations using Krippendorff's alpha metric and introduce two new metrics to evaluate the types of disagreement between coders. We also introduce OCTAB (Online Crowdsourcing Tool for Annotations of Behaviors), a web-based annotation tool that allows precise and convenient multimedia behavior annotations, directly from Amazon Mechanical Turk interface. With an experiment using the above tool and evaluation procedure, we show that a majority vote among annotations from 3 crowdsource workers leads to a quality comparable to that of local expert annotations.},
booktitle = {Proceedings of the ACM Multimedia 2012 Workshop on Crowdsourcing for Multimedia},
pages = {29–34},
numpages = {6},
keywords = {video annotation, octab, inter-rater reliability, inter-coder agreement, crowdsourcing, behavior annotation, amazon mechanical turk},
location = {Nara, Japan},
series = {CrowdMM '12}
}

@inproceedings{10.1109/INFOCOM.2016.7524547,
author = {Zhuo, Gaoqiang and Jia, Qi and Guo, Linke and Li, Ming and Li, Pan},
title = {Privacy-preserving verifiable data aggregation and analysis for cloud-assisted mobile crowdsourcing},
year = {2016},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/INFOCOM.2016.7524547},
doi = {10.1109/INFOCOM.2016.7524547},
abstract = {Crowdsourcing is a crowd-based outsourcing, where a requester (task owner) can outsource tasks to workers (public crowd). Recently, mobile crowdsourcing, which can leverage workers' data from smartphones for data aggregation and analysis, has attracted much attention. However, when the data volume is getting large, it becomes a difficult problem for a requester to aggregate and analyze the incoming data, especially when the requester is an ordinary smartphone user or a start-up company with limited storage and computation resources. Besides, workers are concerned about their identity and data privacy. To tackle these issues, we introduce a three-party architecture for mobile crowdsourcing, where the cloud is implemented between workers and requesters to ease the storage and computation burden of the resource-limited requester. Identity privacy and data privacy are also achieved. With our scheme, a requester is able to verify the correctness of computation results from the cloud. We also provide several aggregated statistics in our work, together with efficient data update methods. Extensive simulation shows both the feasibility and efficiency of our proposed solution.},
booktitle = {IEEE INFOCOM 2016 - The 35th Annual IEEE International Conference on Computer Communications},
pages = {1–9},
numpages = {9},
location = {San Francisco, CA, USA}
}

@inproceedings{10.1145/2470654.2466269,
author = {Lasecki, Walter S. and Miller, Christopher D. and Bigham, Jeffrey P.},
title = {Warping time for more effective real-time crowdsourcing},
year = {2013},
isbn = {9781450318990},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2470654.2466269},
doi = {10.1145/2470654.2466269},
abstract = {In this paper, we introduce the idea of "warping time" to improve crowd performance on the difficult task of captioning speech in real-time. Prior work has shown that the crowd can collectively caption speech in real-time by merging the partial results of multiple workers. Because non-expert workers cannot keep up with natural speaking rates, the task is frustrating and prone to errors as workers buffer what they hear to type later. The TimeWarp approach automatically increases and decreases the speed of speech playback systematically across individual workers who caption only the periods played at reduced speed. Studies with 139 remote crowd workers and 24 local participants show that this approach improves median coverage (14.8\%), precision (11.2\%), and per-word latency (19.1\%). Warping time may also help crowds outperform individuals on other difficult real-time performance tasks.},
booktitle = {Proceedings of the SIGCHI Conference on Human Factors in Computing Systems},
pages = {2033–2036},
numpages = {4},
keywords = {real-time crowdsourcing, human computation, captioning},
location = {Paris, France},
series = {CHI '13}
}

@inproceedings{10.1109/SMC.2013.122,
author = {Barbosa, Carlos Eduardo and Epelbaum, Vanessa Janni and Antelio, Marcio and Oliveira, Jonice and Souza, Jano Moreira de},
title = {Crowdsourcing Environments in E-Learning Scenario: A Classification Based on Educational and Collaboration Criteria},
year = {2013},
isbn = {9781479906529},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/SMC.2013.122},
doi = {10.1109/SMC.2013.122},
abstract = {Recent crowd sourcing tools are changing the Education. However, each tool has a specific approach, business model and e-Learning goals. Due the increasing contribution of crowd sourcing in the e-Learning process, it has become important to study it and map the challenges intrinsic to this activity. This work discusses and classifies several crowd sourcing e-Learning tools that can be found in the Internet. We defined and used eleven dimensions to classify the 22 tools found, ranging from online Universities to marketplaces for online courses. The work contributions are: provide the first map the crowd sourcing tools and a framework for classify the future tools.},
booktitle = {Proceedings of the 2013 IEEE International Conference on Systems, Man, and Cybernetics},
pages = {687–692},
numpages = {6},
keywords = {e-Learning, Crowdsourcing, Classification},
series = {SMC '13}
}

@inproceedings{10.5555/2900728.2900741,
author = {Lin, Christopher H. and Mausam, Mausam and Weld, Daniel S.},
title = {Dynamically switching between synergistic workflows for crowdsourcing},
year = {2012},
publisher = {AAAI Press},
abstract = {To ensure quality results from unreliable crowdsourced workers, task designers often construct complex workflows and aggregate worker responses from redundant runs. Frequently, they experiment with several alternative workflows to accomplish the task, and eventually deploy the one that achieves the best performance during early trials.Surprisingly, this seemingly natural design paradigm does not achieve the full potential of crowdsourcing. In particular, using a single workflow (even the best) to accomplish a task is suboptimal. We show that alternative workflows can compose synergistically to yield much higher quality output. We formalize the insight with a novel probabilistic graphical model. Based on this model, we design and implement AGENTHUNT, a POMDP-based controller that dynamically switches between these workflows to achieve higher returns on investment. Additionally, we design offline and online methods for learning model parameters. Live experiments on Amazon Mechanical Turk demonstrate the superiority of AGENTHUNT for the task of generating NLP training data, yielding up to 50\% error reduction and greater net utility compared to previous methods.},
booktitle = {Proceedings of the Twenty-Sixth AAAI Conference on Artificial Intelligence},
pages = {87–93},
numpages = {7},
location = {Toronto, Ontario, Canada},
series = {AAAI'12}
}

@inproceedings{10.1109/SOCA.2014.26,
author = {Allahbakhsh, Mohammad and Samimi, Samira and Motahari-Nezhad, Hamid-Reza and Benatallah, Boualem},
title = {Harnessing Implicit Teamwork Knowledge to Improve Quality in Crowdsourcing Processes},
year = {2014},
isbn = {9781479968336},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/SOCA.2014.26},
doi = {10.1109/SOCA.2014.26},
abstract = {Workers in online crowd sourcing systems have different levels of expertise, trustworthiness, incentives and motivations. Therefore, recruiting sufficient number of well-suited workers is always a challenge. Existing methods usually recruit workers through open calls, friendships relations, matching their profiles with task requirements or recruiting teams of workers. But there are still challenges that need more investigations, mainly all existing recruitment methods are highly vulnerable to collaborating misbehaviour, i.e., Collusion. \%These groups are highly vulnerable to collusion attacks. In this paper, we propose a recruitment method which takes into account individual and social attributes of workers to find suitable workers. The method discovers indirect collaborations between workers to harness implicit teamwork knowledge in order to increase the quality of crowd sourcing tasks' outcome and in the same time prevent collusion attacks. The proposed method is implemented and tested using the simulated data, build based on a public data dump from Stack overflow. The evaluation results show the accuracy of the obtained results and superiority of our proposed method over the other related work.},
booktitle = {Proceedings of the 2014 IEEE 7th International Conference on Service-Oriented Computing and Applications},
pages = {17–24},
numpages = {8},
keywords = {Suitability, Recruitment, Quality, Implicit Teamwork, Crowdsourcing},
series = {SOCA '14}
}

@inproceedings{10.1145/2908131.2908187,
author = {El Maarry, Kinda and Balke, Wolf-Tilo},
title = {An impact-driven model for quality control in skewed-domain crowdsourcing tasks},
year = {2016},
isbn = {9781450342087},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2908131.2908187},
doi = {10.1145/2908131.2908187},
abstract = {Not only do the highly-distributed digital crowdsourcing solutions surpass both borders and time-zones, but they materialize the vision of impact sourcing, by tapping into new labor markets in developing countries. Unfortunately, crowdsourcing is associated with severe quality issues. To that end, many countermeasures have been designed to detect spammers, except in practice, also honest, yet not perfect workers will often be exposed and deprived of much-needed earnings. Here, we argue for the need of an impact-driven quality control measure, especially for skewed-domain tasks. Such a measure should ensure high quality results, while simultaneously fulfilling the social responsibility aspect of crowdsourcing.},
booktitle = {Proceedings of the 8th ACM Conference on Web Science},
pages = {320–322},
numpages = {3},
keywords = {quality control, impact sourcing, fraud detection, crowdsourcing},
location = {Hannover, Germany},
series = {WebSci '16}
}

@inproceedings{10.1145/2072298.2071901,
author = {Freiburg, Bauke and Kamps, Jaap and Snoek, Cees G.M.},
title = {Crowdsourcing visual detectors for video search},
year = {2011},
isbn = {9781450306164},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2072298.2071901},
doi = {10.1145/2072298.2071901},
abstract = {In this paper, we study social tagging at the video fragment-level using a combination of automated content understanding and the wisdom of the crowds. We are interested in the question whether crowdsourcing can be beneficial to a video search engine that automatically recognizes video fragments on a semantic level. To answer this question, we perform a 3-month online field study with a concert video search engine targeted at a dedicated user-community of pop concert enthusiasts. We harvest the feedback of more than 500 active users and perform two experiments. In experiment 1 we measure user incentive to provide feedback, in experiment 2 we determine the tradeoff between feedback quality and quantity when aggregated over multiple users. Results show that users provide sufficient feedback, which becomes highly reliable when a crowd agreement of 67\% is enforced.},
booktitle = {Proceedings of the 19th ACM International Conference on Multimedia},
pages = {913–916},
numpages = {4},
keywords = {video retrieval, semantic indexing, information visualization},
location = {Scottsdale, Arizona, USA},
series = {MM '11}
}

@inproceedings{10.1145/2556288.2557217,
author = {Dontcheva, Mira and Morris, Robert R. and Brandt, Joel R. and Gerber, Elizabeth M.},
title = {Combining crowdsourcing and learning to improve engagement and performance},
year = {2014},
isbn = {9781450324731},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2556288.2557217},
doi = {10.1145/2556288.2557217},
abstract = {Crowdsourcing complex creative tasks remains difficult, in part because these tasks require skilled workers. Most crowdsourcing platforms do not help workers acquire the skills necessary to accomplish complex creative tasks. In this paper, we describe a platform that combines learning and crowdsourcing to benefit both the workers and the requesters. Workers gain new skills through interactive step-by-step tutorials and test their knowledge by improving real-world images submitted by requesters. In a series of three deployments spanning two years, we varied the design of our platform to enhance the learning experience and improve the quality of the crowd work. We tested our approach in the context of LevelUp for Photoshop, which teaches people how to do basic photograph improvement tasks using Adobe Photoshop. We found that by using our system workers gained new skills and produced high-quality edits for requested images, even if they had little prior experience editing images.},
booktitle = {Proceedings of the SIGCHI Conference on Human Factors in Computing Systems},
pages = {3379–3388},
numpages = {10},
keywords = {training, games, crowdsourcing},
location = {Toronto, Ontario, Canada},
series = {CHI '14}
}

@inproceedings{10.1145/2661829.2661946,
author = {Rokicki, Markus and Chelaru, Sergiu and Zerr, Sergej and Siersdorfer, Stefan},
title = {Competitive Game Designs for Improving the Cost Effectiveness of Crowdsourcing},
year = {2014},
isbn = {9781450325981},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2661829.2661946},
doi = {10.1145/2661829.2661946},
abstract = {Crowd based online work is leveraged in a variety of applications such as semantic annotation of images, translation of texts in foreign languages, and labeling of training data for machine learning models. However, annotating large amounts of data through crowdsourcing can be slow and costly. In order to improve both cost and time efficiency of crowdsourcing we examine alternative reward mechanisms compared to the "Pay-per-HIT" scheme commonly used in platforms such as Amazon Mechanical Turk. To this end, we explore a wide range of monetary reward schemes that are inspired by the success of competitions, lotteries, and games of luck. Our large-scale experimental evaluation with an overall budget of more than 1,000 USD and with 2,700 hours of work spent by crowd workers demonstrates that our alternative reward mechanisms are well accepted by online workers and lead to substantial performance boosts.},
booktitle = {Proceedings of the 23rd ACM International Conference on Conference on Information and Knowledge Management},
pages = {1469–1478},
numpages = {10},
keywords = {competitions, crowdsourcing, lotteries, reward schemes},
location = {Shanghai, China},
series = {CIKM '14}
}

@inproceedings{10.1145/3253131,
author = {Jacknis, Norm},
title = {Session details: Social media and crowdsourcing},
year = {2011},
isbn = {9781450307628},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3253131},
doi = {10.1145/3253131},
booktitle = {Proceedings of the 12th Annual International Digital Government Research Conference: Digital Government Innovation in Challenging Times},
location = {College Park, Maryland, USA},
series = {dg.o '11}
}

@inproceedings{10.1145/3251097,
author = {Clarke, Charlie},
title = {Session details: Session 6: Crowdsourcing, Temporal and Location-based mining},
year = {2015},
isbn = {9781450333177},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3251097},
doi = {10.1145/3251097},
booktitle = {Proceedings of the Eighth ACM International Conference on Web Search and Data Mining},
location = {Shanghai, China},
series = {WSDM '15}
}

@inproceedings{10.1145/2661334.2661401,
author = {Calvo, Roc\'{\i}o and Kane, Shaun K. and Hurst, Amy},
title = {Evaluating the accessibility of crowdsourcing tasks on Amazon's mechanical turk},
year = {2014},
isbn = {9781450327206},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2661334.2661401},
doi = {10.1145/2661334.2661401},
abstract = {Crowd work web sites such as Amazon Mechanical Turk enable individuals to work from home, which may be useful for people with disabilities. However, the web sites for finding and performing crowd work tasks must be accessible if people with disabilities are to use them. We performed a heuristic analysis of one crowd work site, Amazon's Mechanical Turk, using the Web Content Accessibility Guidelines 2.0. This paper presents the accessibility problems identified in our analysis and offers suggestions for making crowd work platforms more accessible},
booktitle = {Proceedings of the 16th International ACM SIGACCESS Conference on Computers \&amp; Accessibility},
pages = {257–258},
numpages = {2},
keywords = {mechanical turk, evaluation, crowdsourcing., accessibility},
location = {Rochester, New York, USA},
series = {ASSETS '14}
}

@inproceedings{10.5555/2900728.2900734,
author = {Gao, Xi Alice and Bachrach, Yoram and Key, Peter and Graepel, Thore},
title = {Quality expectation-variance tradeoffs in crowdsourcing contests},
year = {2012},
publisher = {AAAI Press},
abstract = {We examine designs for crowdsourcing contests, where participants compete for rewards given to superior solutions of a task. We theoretically analyze tradeoffs between the expectation and variance of the principal's utility (i.e. the best solution's quality), and empirically test our theoretical predictions using a controlled experiment on Amazon Mechanical Turk. Our evaluation method is also crowdsourcing based and relies on the peer prediction mechanism. Our theoretical analysis shows an expectation-variance tradeoff of the principal's utility in such contests through a Pareto efficient frontier. In particular, we show that the simple contest with 2 authors and the 2-pair contest have good theoretical properties. In contrast, our empirical results show that the 2-pair contest is the superior design among all designs tested, achieving the highest expectation and lowest variance of the principal's utility.},
booktitle = {Proceedings of the Twenty-Sixth AAAI Conference on Artificial Intelligence},
pages = {38–44},
numpages = {7},
location = {Toronto, Ontario, Canada},
series = {AAAI'12}
}

@inproceedings{10.1145/1837885.1837895,
author = {Stewart, Osamuyimen and Lubensky, David and Huerta, Juan M.},
title = {Crowdsourcing participation inequality: a SCOUT model for the enterprise domain},
year = {2010},
isbn = {9781450302227},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1837885.1837895},
doi = {10.1145/1837885.1837895},
abstract = {In large scale online multi-user communities, the phenomenon of 'participation inequality,' has been described as generally following a more or less 90-9-1 rule [9]. In this paper, we examine crowdsourcing participation levels inside the enterprise (within a company's firewall) and show that it is possible to achieve a more equitable distribution of 33-66-1. Accordingly, we propose a SCOUT ((S)uper Contributor, (C)ontributor, and (OUT)lier)) model for describing user participation based on quantifiable effort-level metrics. In support of this framework, we present an analysis that measures the quantity of contributions correlated with responses to motivation and incentives. In conclusion, SCOUT provides the task-based categories to characterize participation inequality that is evident in online communities, and crucially, also demonstrates the inequality curve (and associated characteristics) in the enterprise domain.},
booktitle = {Proceedings of the ACM SIGKDD Workshop on Human Computation},
pages = {30–33},
numpages = {4},
keywords = {online community, motivation, incentives, crowdsourcing},
location = {Washington DC},
series = {HCOMP '10}
}

@inproceedings{10.1109/NBiS.2015.74,
author = {Matsushita, Yuichi and Uchiya, Takahiro and Nishimuray, Ryota and Yamamoto, Daisuke and Takumi, Ichi},
title = {Crowdsourcing Environment to Create Voice Interaction Scenario of Spoken Dialogue System},
year = {2015},
isbn = {9781479999422},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/NBiS.2015.74},
doi = {10.1109/NBiS.2015.74},
abstract = {This university has published the "MMDAgent" voice interaction system toolkit as open source software. Voice Interaction Agent, produced using MMDAgent, is active in various scenarios and provides voice interaction capability. Voice interaction agents produced using MMDAgent talk to people by loading a script file that describes a voice interaction scenario. To increase the agent's vocabulary, it is important to describe many voice interaction scenarios. To date, voice interaction scenarios have been described by a small organization. Therefore, they require high costs to describe many voice interaction scenarios. To resolve this problem, we construct a crowdsourcing[1] model for the creation of a voice interaction scenario. In this study, the works of creation of voice interaction scenario are divided and distributed to numerous people via the internet. Furthermore, we provide in this study a tool for creation voice interaction scenario and test operation of voice interaction agent as a web application service. We hereby reduce the cost of installation MMDAgent and provide a comfortable environment for the creation voice interaction scenario.},
booktitle = {Proceedings of the 2015 18th International Conference on Network-Based Information Systems},
pages = {499–504},
numpages = {6},
keywords = {Voice Interaction System, MMDAgent, Crowdsourcing},
series = {NBIS '15}
}

@inproceedings{10.1145/2998476.2998498,
author = {Agarwal, Bhoomika and Ravikumar, Abhiram and Saha, Snehanshu},
title = {A Novel Approach to Big Data Veracity using Crowdsourcing Techniques and Bayesian Predictors},
year = {2016},
isbn = {9781450348089},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2998476.2998498},
doi = {10.1145/2998476.2998498},
abstract = {In today's world data is being generated at a tremendous pace and there have to be enough measures in place to verify the nature of big data. Analysis performed on 'dirty' data may lead to erroneous insights and thereby shaping decisions poorly. The aspect of big data that deals with its correctness is known as big data veracity. Trusting the data acquired goes a long way in implementing decisions from an automated decision-making system and veracity helps to validate the data acquired. In this paper, we present our solution to the big data veracity problem using crowdsourcing techniques. Our solution involves the use of sentiment analysis, which deals with identifying the sentiment expressed in a piece of text. As a proof of concept, we have developed an app that requires users to tag tweets as per the sentiment it evokes in them. Each tweet would therefore get ratified by hundreds of our participants and the sentiment associated to the tweet gets tagged. The tagged emotion was then evaluated against the verified emotion as compared to a verified data set. This analysis was then plotted on a ROC curve and also evaluated against verified data using a Bayesian predictor trained with a trinomial function. As can be seen, an accuracy of 81\% was obtained as displayed by the ROC curve and 89\% through the Bayesian predictor. Also, a MAP analysis of the Bayesian predictor yields neutral sentiment as the most probable hypothesis. By doing this, we have proven that crowdsourcing of sentiment analysis is a viable solution to the problem of big data veracity and therefore an aid in making better decisions.},
booktitle = {Proceedings of the 9th Annual ACM India Conference},
pages = {153–160},
numpages = {8},
keywords = {Tweet Mining, Sentiment Analysis, Machine Learning, Crowdsourcing, Big Data, Bayesian Predictor},
location = {Gandhinagar, India},
series = {COMPUTE '16}
}

@inproceedings{10.1145/2967413.2967421,
author = {Zheng, Yu and Chen, Zhenhua(Jimmy) and Velipasalar, Senem and Tang, Jian},
title = {Person Detection and Re-identification Across Multiple Images and Videos Obtained via Crowdsourcing},
year = {2016},
isbn = {9781450347860},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2967413.2967421},
doi = {10.1145/2967413.2967421},
abstract = {Person re-identification is indispensable for consistent labeling across different camera views. Most existing studies use static cameras, apply background subtraction to detect moving people, and then focus on the matching of detection results. However, if cameras are mobile or only single image frames (not videos) are available, then background subtraction cannot be used, and human detection needs to be performed on entire images. In this paper, different from most of the existing work, we focus on a crowdsourcing scenario to find and follow person(s) of interest in the collected images/videos. We propose a novel approach combining R-CNN based person detection with the GPU implementation of color histogram and SURF-based re-identification. Moreover, GeoTags are extracted from the EXIF data of videos captured by smart phones, and are displayed on a map together with the time-stamps. All the processing is performed on a GPU, and the average processing time is 5 ms per frame.},
booktitle = {Proceedings of the 10th International Conference on Distributed Smart Camera},
pages = {178–183},
numpages = {6},
location = {Paris, France},
series = {ICDSC '16}
}

@inproceedings{10.1145/1835449.1835617,
author = {Potthast, Martin},
title = {Crowdsourcing a wikipedia vandalism corpus},
year = {2010},
isbn = {9781450301534},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1835449.1835617},
doi = {10.1145/1835449.1835617},
abstract = {We report on the construction of the PAN Wikipedia vandalism corpus, PAN-WVC-10, using Amazon's Mechanical Turk. The corpus compiles 32452 edits on 28468 Wikipedia articles, among which 2391 vandalism edits have been identified. 753 human annotators cast a total of 193022 votes on the edits, so that each edit was reviewed by at least 3 annotators, whereas the achieved level of agreement was analyzed in order to label an edit as "regular" or "vandalism." The corpus is available free of charge.},
booktitle = {Proceedings of the 33rd International ACM SIGIR Conference on Research and Development in Information Retrieval},
pages = {789–790},
numpages = {2},
keywords = {wikipedia, vandalism detection, evaluation, corpus},
location = {Geneva, Switzerland},
series = {SIGIR '10}
}

@inproceedings{10.1145/2785971.2785976,
author = {Marina, Mahesh K. and Radu, Valentin and Balampekos, Konstantinos},
title = {Impact of Indoor-Outdoor Context on Crowdsourcing based Mobile Coverage Analysis},
year = {2015},
isbn = {9781450335386},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2785971.2785976},
doi = {10.1145/2785971.2785976},
abstract = {We consider the crowdsourcing based mobile cellular network measurement paradigm that is becoming increasingly popular. In particular, we aim to study the impact of user indoor/outdoor environment context at time of measurement. Focusing on signal strength as the measurement metric and using a real large crowdsourced measurement dataset for central London area along with estimated environment state (indoor or outdoor), we show that indoor-outdoor context has a significant impact, suggesting that conflating indoor and outdoor measurements can lead to unreliable results. We validate these observations using a set of diverse and controlled measurements with indoor/outdoor ground truth information. We also discuss some opportunities for future work (e.g., accurate and efficient context detection) relevant to crowdsourced mobile network measurement systems.},
booktitle = {Proceedings of the 5th Workshop on All Things Cellular: Operations, Applications and Challenges},
pages = {45–50},
numpages = {6},
keywords = {indoor-outdoor environment context, crowdsourced mobile network measurement, cellular coverage},
location = {London, United Kingdom},
series = {AllThingsCellular '15}
}

@inproceedings{10.1109/SCC.2010.74,
author = {Lopez, Mariana and Vukovic, Maja and Laredo, Jim},
title = {PeopleCloud Service for Enterprise Crowdsourcing},
year = {2010},
isbn = {9780769541266},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/SCC.2010.74},
doi = {10.1109/SCC.2010.74},
abstract = {Web 2.0 provides the technological foundations upon which the crowdsourcing paradigm evolves and operates, enabling enterprises, universities and eGovernments, to access scalable networks of knowledge experts on-line. However, there is no existing practice allowing for coordination of crowdsourcing tasks, and their integration with existing business processes and embedding these services into the Web fabric. In this paper, we examine two applications of enterprise crowdsourcing service in the domain of IT Service Delivery: 1) IT Inventory Management and 2) End-User Support. We illustrate how a) expert discovery mechanism, b) virtual team building capabilities, c) task management and d) provisioning of task-based services, enable enterprises to effectively build knowledge networks, which are able to execute complex and transformative knowledge-intensive tasks. Finally, based on the application analysis, we propose PeopleCloud, an on-demand service system, which spawns and manages scalable virtual teams of knowledge workers by either (1) building on the wisdom of crowds within an enterprise or across a value chain or (2) creating a marketplace for accessing specialists on-line.},
booktitle = {Proceedings of the 2010 IEEE International Conference on Services Computing},
pages = {538–545},
numpages = {8},
keywords = {Crowdsourcing Service, Collaborative Production},
series = {SCC '10}
}

@inproceedings{10.1145/2486084.2486085,
author = {Shahabi, Cyrus},
title = {Towards a generic framework for trustworthy spatial crowdsourcing},
year = {2013},
isbn = {9781450321976},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2486084.2486085},
doi = {10.1145/2486084.2486085},
abstract = {Many studies foresee significant future growth in the number of mobile smart phone users, the phone's hardware and software features, and the broadband bandwidth. Therefore, a transformative area of research is to fully utilize this new platform for various tasks, among which the most promising is spatial crowdsourcing. Spatial crowdsourcing (SC) engages individuals, groups, and communities in the act of collecting, analyzing, and disseminating urban, social, and other spatiotemporal information. This new paradigm of data collection has shown to be useful when traditional means fail (e.g., due to disaster), are censored or do not scale in time and space.Two major impediments to the success of spatial crowdsourcing in real-world applications are scalability and trust issues. Without scale considerations, it is impossible to develop a generic multi-campaign spatial crowdsourcing system (SC-system) that can efficiently and in real-time match many requesters' tasks to numerous workers. Without trust, the SC-system cannot evaluate the credibility of the contributed data, rendering it ineffective for replacing the traditional data collection means. In this paper, we survey and study both issues of scale and trust in spatial crowdsourcing.},
booktitle = {Proceedings of the 12th International ACM Workshop on Data Engineering for Wireless and Mobile Acess},
pages = {1–4},
numpages = {4},
keywords = {spatial data management, spatial crowdsourcing, mobile applications, crowdsourcing},
location = {New York, New York},
series = {MobiDE '13}
}

@inproceedings{10.1145/2470654.2470708,
author = {Massung, Elaine and Coyle, David and Cater, Kirsten F. and Jay, Marc and Preist, Chris},
title = {Using crowdsourcing to support pro-environmental community activism},
year = {2013},
isbn = {9781450318990},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2470654.2470708},
doi = {10.1145/2470654.2470708},
abstract = {Community activist groups typically rely on core groups of highly motivated members. In this paper we consider how crowdsourcing strategies can be used to supplement the activities of pro-environmental community activists, thus increasing the scalability of their campaigns. We focus on mobile data collection applications and strategies that can be used to engage casual participants in pro-environmental data collection. We report the results of a study that used both quantitative and qualitative methods to investigate the impact of different motivational factors and strategies, including both intrinsic and extrinsic motivators. The study compared and provides empirical evidence for the effectiveness of two extrinsic motivation strategies, pointification - a subset of gamification - and financial incentives. Prior environmental interest is also assessed as an intrinsic motivation factor. In contrast to previous HCI research on pro-environmental technology, much of which has focused on individual behavior change, this paper offers new insights and recommendations on the design of systems that target groups and communities.},
booktitle = {Proceedings of the SIGCHI Conference on Human Factors in Computing Systems},
pages = {371–380},
numpages = {10},
keywords = {sustainability, participatory urbanism, motivation, gamification, crowdsourcing, community activism},
location = {Paris, France},
series = {CHI '13}
}

@inproceedings{10.1145/2442882.2442904,
author = {Roy, Shourya and Balamurugan, Chithralekha and Gujar, Sujit},
title = {Sustainable employment in India by crowdsourcing enterprise tasks},
year = {2013},
isbn = {9781450318563},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2442882.2442904},
doi = {10.1145/2442882.2442904},
abstract = {This paper explains how crowdsourcing would constitute for a promising and a successful alternate business model in India, especially at the juncture where the advantages of outsourcing is beginning to fade. The success of this alternate business model depends on the solutions for two challenges -- by taking work to people to leverage large educated population in India and making enough work available for the workforce to work and earn. Though solution to the first challenge is implicitly available through increasing penetration of Information and Communication Technologies (ICT) in India, the second challenge requires enough tasks to be available by enabling business organizations to adopt crowdsourcing. Since enterprise tasks are not readily crowdsourcable owing to security, compliance and contractual reasons, this paper proceeds to describe an end to end system which encompasses technical solutions that could help crowdsourcing business tasks, by tactfully overcoming the existing business constraints. For business tasks, we consider Insurance Claim Form Digitization which is one of the most common tasks taken up by outsourcing enterprises.},
booktitle = {Proceedings of the 3rd ACM Symposium on Computing for Development},
articleno = {16},
numpages = {2},
keywords = {microtasking, employment models, data digitization, crowdsourcing, Amazon mechanical turk},
location = {Bangalore, India},
series = {ACM DEV '13}
}

@inproceedings{10.1007/978-3-642-41428-2_14,
author = {Zhang, Gang and Chen, Haopeng},
title = {Quality Control for Crowdsourcing with Spatial and Temporal Distribution},
year = {2013},
isbn = {9783642414275},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-642-41428-2_14},
doi = {10.1007/978-3-642-41428-2_14},
abstract = {In the past decade, crowdsourcing has become a prospective paradigm for commercial purposes, for it brings a lot of benefits such as low cost and high immediacy, particularly in location-based services LBS. On the other side, there also exist many problems need to be solved in crowdsourcing. For example, the quality control for crowdsourcing systems has been identified as a significant challenge, which includes how to handle massive data more efficiently, how to discriminate poor quality content in workers' submissions and so on. In this paper, we put forward an approach to control the crowdsourcing quality from spatial and temporal distribution. Our experiments have demonstrated the effectiveness and efficiency of the approach.},
booktitle = {Proceedings of the 6th International Conference on Internet and Distributed Computing Systems - Volume 8223},
pages = {169–182},
numpages = {14},
keywords = {spatial and temporal distribution, quality control, location-based service LBS, crowdsourcing},
location = {Hangzhou, China},
series = {IDCS 2013}
}

@inproceedings{10.1145/2810188.2810193,
author = {Gardlo, Bruno and Egger, Sebastian and Hossfeld, Tobias},
title = {Do Scale-Design and Training Matter for Video QoE Assessments through Crowdsourcing?},
year = {2015},
isbn = {9781450337465},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2810188.2810193},
doi = {10.1145/2810188.2810193},
abstract = {Crowdsourcing (CS) has evolved into a mature assessment methodology for subjective experiments in diverse scientific fields and in particular for QoE assessment. However, the results acquired for absolute category rating (ACR) scales through CS are often not fully comparable to QoE assessments done in laboratory environments. A possible reason for such differences may be the scale usage heterogeneity problem caused by deviant scale usage of the crowd workers. In this paper, we study different implementations of (quality) rating scales (in terms of design and number of answer categories) in order to identify if certain scales can help to overcome scale usage problems in crowdsourcing. Additionally, training of subjects is well known to enhance result quality for laboratory ACR evaluations. Hence, we analyzed the appropriateness of training conditions to overcome scale usage problems across different samples in crowdsourcing. As major results, we found that filtering of user ratings and different scale designs are not sufficient to overcome scale usage heterogeneity, but training sessions despite their additional costs, enhance result quality in CS and properly counterfeit the identified scale usage heterogeneity problems.},
booktitle = {Proceedings of the Fourth International Workshop on Crowdsourcing for Multimedia},
pages = {15–20},
numpages = {6},
keywords = {scales, reliability, methodology, crowdsourcing, QoE},
location = {Brisbane, Australia},
series = {CrowdMM '15}
}

@inproceedings{10.1109/SRII.2012.17,
author = {Varshney, Lav R.},
title = {Privacy and Reliability in Crowdsourcing Service Delivery},
year = {2012},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/SRII.2012.17},
doi = {10.1109/SRII.2012.17},
abstract = {Due to the anonymity and low pay of workers in crowd sourcing platforms, there may be concerns regarding reliability and privacy-preservation when using such platforms to deliver services. This paper describes a technique for jointly providing privacy and reliability through stochastic perturbation of micro task definitions and fusion rules to combine the work of several workers. A mathematical model of a crowd sourcing system using this technique is proposed and precise threshold conditions on loss of privacy when workers collude are provided. Tradeoffs between privacy, reliability, and cost are determined.},
booktitle = {Proceedings of the 2012 Annual SRII Global Conference},
pages = {55–60},
numpages = {6},
series = {SRII '12}
}

@proceedings{10.5555/2820116,
title = {CSI-SE '15: Proceedings of the Second International Workshop on CrowdSourcing in Software Engineering},
year = {2015},
publisher = {IEEE Press},
abstract = {It is our pleasure to welcome the reader to the (pre-workshop) proceedings of the 2nd International Workshop on Crowd Sourcing in Software Engineering (CSI-SE 2015), co-located with the 37th International Conference on Software Engineering (ICSE 2015) held in Florence, Italy, May 19, 2015.A number of trends under the broad banner of crowdsourcing are beginning to fundamentally disrupt the way in which software is engineered. Programmers increasingly rely on crowdsourced knowledge and code, as they look to Q&amp;A sites for answers or use code from publicly posted snippets. Programmers play, compete, and learn with the crowd, engaging in programming competitions and puzzles with crowds of programmers. Online IDEs make radically new forms of collaboration possible, allowing developers to synchronously program with crowds of distributed programmers. Programmer reputation is increasingly visible on Q&amp;A sites and public code repositories, opening new possibilities in how developers find jobs and companies identify talent. Crowds of non-programmers increasingly participate in development, usability testing software or even constructing specifications while playing games. Crowdfunding democratizes choices about which software is built, broadening the software which might be feasibly constructed. Approaches for crowd development seek to microtask software development, dramatically increasing participation in open source by enabling software projects to be built through casual, transient work.},
location = {Florence, Italy}
}

@inproceedings{10.1007/978-3-319-15168-7_54,
author = {Pereira Santos, Carlos and Khan, Vassilis-Javed and Markopoulos, Panos},
title = {On Utilizing Player Models to Predict Behavior in Crowdsourcing Tasks},
year = {2015},
isbn = {978-3-319-15167-0},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-319-15168-7_54},
doi = {10.1007/978-3-319-15168-7_54},
abstract = {Player Modeling is a research field that studies player characteristics by analyzing in-game behavior. We aim to develop independent models, which are transferable and useful beyond a game’s context. We shall demonstrate the feasibility of this approach by applying player models to crowdsourcing to predict workers’ task completion effectiveness. Specifically, we model a user’s Need for Cognition based on in-game behavior, and based on that try to assign appropriate tasks to workers.},
booktitle = {Social Informatics: SocInfo 2014 International Workshops, Barcelona, Spain, November 11, 2014, Revised Selected Papers},
pages = {448–451},
numpages = {4},
keywords = {Predict Behavior, Profile Tool, Player Experience, Player Skill, Personality Trait},
location = {Barcelona, Spain}
}

@inproceedings{10.1109/HICSS.2011.207,
author = {Wiggins, Andrea and Crowston, Kevin},
title = {From Conservation to Crowdsourcing: A Typology of Citizen Science},
year = {2011},
isbn = {9780769542829},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/HICSS.2011.207},
doi = {10.1109/HICSS.2011.207},
abstract = {Citizen science is a form of research collaboration involving members of the public in scientific research projects to address real-world problems. Often organized as a virtual collaboration, these projects are a type of open movement, with collective goals addressed through open participation in research tasks. Existing typologies of citizen science projects focus primarily on the structure of participation, paying little attention to the organizational and macrostructural properties that are important to designing and managing effective projects and technologies. By examining a variety of project characteristics, we identified five types-Action, Conservation, Investigation, Virtual, and Education- that differ in primary project goals and the importance of physical environment to participation.},
booktitle = {Proceedings of the 2011 44th Hawaii International Conference on System Sciences},
pages = {1–10},
numpages = {10},
series = {HICSS '11}
}

@inproceedings{10.1145/1753326.1753357,
author = {Heer, Jeffrey and Bostock, Michael},
title = {Crowdsourcing graphical perception: using mechanical turk to assess visualization design},
year = {2010},
isbn = {9781605589299},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1753326.1753357},
doi = {10.1145/1753326.1753357},
abstract = {Understanding perception is critical to effective visualization design. With its low cost and scalability, crowdsourcing presents an attractive option for evaluating the large design space of visualizations; however, it first requires validation. In this paper, we assess the viability of Amazon's Mechanical Turk as a platform for graphical perception experiments. We replicate previous studies of spatial encoding and luminance contrast and compare our results. We also conduct new experiments on rectangular area perception (as in treemaps or cartograms) and on chart size and gridline spacing. Our results demonstrate that crowdsourced perception experiments are viable and contribute new insights for visualization design. Lastly, we report cost and performance data from our experiments and distill recommendations for the design of crowdsourced studies.},
booktitle = {Proceedings of the SIGCHI Conference on Human Factors in Computing Systems},
pages = {203–212},
numpages = {10},
keywords = {user study, mechanical turk, information visualization, graphical perception, experimentation, evaluation, crowdsourcing},
location = {Atlanta, Georgia, USA},
series = {CHI '10}
}

@inproceedings{10.1145/2468356.2468522,
author = {Ha, Seyong and Kim, Dongwhan and Lee, Joonhwan},
title = {Crowdsourcing as a method for indexing digital media},
year = {2013},
isbn = {9781450319522},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2468356.2468522},
doi = {10.1145/2468356.2468522},
abstract = {As people spend more time online, watching YouTube or playing games, a number of research studies arose in ways to make use of the time and energy from the crowd in doing such activities. In this paper, we have explored the possibility of converting the collective resources from the crowd in making useful information back to people. We collected posts from the online forums about soap operas on the air, and extracted instances when the name of characters in the play has been mentioned. These crowdsourced indexes become good search keywords to find the scenes where the characters mentioned in the posts appear.},
booktitle = {CHI '13 Extended Abstracts on Human Factors in Computing Systems},
pages = {931–936},
numpages = {6},
keywords = {social media, digital media index, crowdsourcing, collective intelligence.},
location = {Paris, France},
series = {CHI EA '13}
}

@inproceedings{10.4108/icst.urb-iot.2014.257267,
author = {Prandi, Catia and Mirri, Silvia and Salomoni, Paola},
title = {Trustworthiness assessment in mapping urban accessibility via sensing and crowdsourcing},
year = {2014},
isbn = {9781631900372},
publisher = {ICST (Institute for Computer Sciences, Social-Informatics and Telecommunications Engineering)},
address = {Brussels, BEL},
url = {https://doi.org/10.4108/icst.urb-iot.2014.257267},
doi = {10.4108/icst.urb-iot.2014.257267},
abstract = {In this work we present the trustworthiness assessment in mPASS (mobile Pervasive Accessibility Social Sensing), a location and context aware system that collects data from crowdsourcing and sensing in order to map urban and architectural accessibility. The fusion of heterogeneous urban sources allows mPASS to provide users with personalized paths, computed on the basis of their preferences and needs. To perform this task, the system needs a set of georeferenced data that is dense enough and trustworthy enough to avoid false positives and negatives, e.g. to prevent users from encounter on their path an unknown barrier or a non-existing facility. In order to reach this goal, we propose a trustworthiness assessment which combines accuracy of sensors, source credibility of the crowd and the authority of experts. To evaluate the effectiveness of our trustworthiness assessment, we conducted a set of simulations by exploiting OSM data and we have obtained interesting results.},
booktitle = {Proceedings of the First International Conference on IoT in Urban Space},
pages = {108–110},
numpages = {3},
keywords = {crowdsourcing, sensing, trustworthiness, urban accessibility},
location = {Rome, Italy},
series = {URB-IOT '14}
}

@inproceedings{10.1145/3136755.3136767,
author = {Ramanarayanan, Vikram and Leong, Chee Wee and Suendermann-Oeft, David and Evanini, Keelan},
title = {Crowdsourcing ratings of caller engagement in thin-slice videos of human-machine dialog: benefits and pitfalls},
year = {2017},
isbn = {9781450355438},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3136755.3136767},
doi = {10.1145/3136755.3136767},
abstract = {We analyze the efficacy of different crowds of naive human raters in rating engagement during human--machine dialog interactions. Each rater viewed multiple 10 second, thin-slice videos of native and non-native English speakers interacting with a computer-assisted language learning (CALL) system and rated how engaged and disengaged those callers were while interacting with the automated agent. We observe how the crowd's ratings compared to callers' self ratings of engagement, and further study how the distribution of these rating assignments vary as a function of whether the automated system or the caller was speaking. Finally, we discuss the potential applications and pitfalls of such crowdsourced paradigms in designing, developing and analyzing engagement-aware dialog systems.},
booktitle = {Proceedings of the 19th ACM International Conference on Multimodal Interaction},
pages = {281–287},
numpages = {7},
keywords = {thin-slicing, multimodal dialog, engagement, crowdsourcing},
location = {Glasgow, UK},
series = {ICMI '17}
}

@inproceedings{10.1145/2468356.2468506,
author = {Nguyen, Phu and Kim, Juho and Miller, Robert C.},
title = {Generating annotations for how-to videos using crowdsourcing},
year = {2013},
isbn = {9781450319522},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2468356.2468506},
doi = {10.1145/2468356.2468506},
abstract = {How-to videos can be valuable for learning, but searching for and following along with them can be difficult. Having labeled events such as the tools used in how-to videos could improve video indexing, searching, and browsing. We introduce a crowdsourcing annotation tool for Photoshop how-to videos with a three-stage method that consists of: (1) gathering timestamps of important events, (2) labeling each event, and (3) capturing how each event affects the task of the tutorial. Our ultimate goal is to generalize our method to be applied to other domains of how-to videos. We evaluate our annotation tool with Amazon Mechanical Turk workers to investigate the accuracy, costs, and feasibility of our three-stage method for annotating large numbers of video tutorials. Improvements can be made for stages 1 and 3, but stage 2 produces accurate labels over 90\% of the time using majority voting. We have observed that changes in the instructions and interfaces of each task can improve the accuracy of the results significantly.},
booktitle = {CHI '13 Extended Abstracts on Human Factors in Computing Systems},
pages = {835–840},
numpages = {6},
keywords = {video tutorials, how-to videos, crowd workers},
location = {Paris, France},
series = {CHI EA '13}
}

@inproceedings{10.1109/ICGSE.2014.26,
author = {Wang, Hao and Wang, Yasha and Wang, Jiangtao},
title = {A Participant Recruitment Framework for Crowdsourcing Based Software Requirement Acquisition},
year = {2014},
isbn = {9781479943609},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/ICGSE.2014.26},
doi = {10.1109/ICGSE.2014.26},
abstract = {The opportunity to leverage crowd sourcing-based model to facilitate software requirements acquisition has been recognized to maximize the advantages of the diversity of talents and expertise available within the crowd. Identifying well-suited participants is a common issue in crowd sourcing system. Requirements acquisition tasks call for participants with particular kind of domain knowledge. However, current crowd sourcing system failed to provide such kind of identification among participants. We observed that participants with a particular kind of domain knowledge often have the opportunity to cluster in particular spatiotemporal spaces. Based on this observation, we propose a novel opportunistic participant recruitment framework to enable organizers to recruit participants with desired kind of domain knowledge in a more efficient way. We analyzed the feasibility of our opportunistic approach through both theoretic study on analytical model and simulated experiment on real world mobility model. The results showed the feasibility of our approach.},
booktitle = {Proceedings of the 2014 IEEE 9th International Conference on Global Software Engineering},
pages = {65–73},
numpages = {9},
keywords = {Opportunistic Network, Crowdsourcing},
series = {ICGSE '14}
}

@inproceedings{10.1145/3009912.3009916,
author = {Padhariya, Nilesh and Mondal, Anirban and Madria, Sanjay Kumar},
title = {Efficient Processing of Mobile Crowdsourcing Queries with Multiple Sub-tasks for Facilitating Smart Cities},
year = {2016},
isbn = {9781450346672},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3009912.3009916},
doi = {10.1145/3009912.3009916},
abstract = {The proliferation and ever-increasing popularity of mobile devices have dramatically increased the potential for mobile crowdsourcing across a wide gamut of applications that are relevant to smart cities. While existing works have mostly focused on mobile crowdsourcing for queries involving a single given major task, the issue of addressing complex queries with multiple related large sub-tasks (with spatio-temporal dependencies) has received little attention. In this regard, the key contributions of the paper are three-fold. First, we present a scheme, designated as BMS (Broker-based processing of Multiple Sub-tasks), in which a broker coordinates the processing of multiple related large subtasks in a given query among a set of mobile peers. Second, we propose the DMS (Distributed processing of Multiple Sub-tasks) scheme in which the processing of multiple sub-tasks in a query occurs in a distributed manner without the existence of any brokers. Third, the results of our performance evaluation demonstrate the effectiveness of both of the proposed schemes in terms of relatively low query response times, high query success rates and reasonable communication costs.},
booktitle = {Proceedings of the 2nd International Workshop on Smart},
articleno = {8},
numpages = {6},
keywords = {sub-tasks, smart cities, queries, Mobile crowdsourcing},
location = {Trento, Italy},
series = {SmartCities '16}
}

@inproceedings{10.1145/2658779.2658795,
author = {Embiricos, Alex and Rahmati, Negar and Zhu, Nicole and Bernstein, Michael S.},
title = {Structured handoffs in expert crowdsourcing improve communication and work output},
year = {2014},
isbn = {9781450330688},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2658779.2658795},
doi = {10.1145/2658779.2658795},
abstract = {Expert crowdsourcing allows specialized, remote teams to complete projects, often large and involving multiple stages. Its execution is complicated due to communication difficulties between remote workers. This paper investigates whether structured handoff methods, from one worker to the next, improve final product quality by helping the workers understand the input of their tasks and reduce overall integration cost. We investigate this question through 1) a "live" handoff method where the next worker shadows the former via screen sharing technology and 2) a "recorded" handoff, where workers summarize work done for the next, via a screen capture and narration. We confirm the need for a handoff process. We conclude that structured handoffs result in higher quality work, improved satisfaction (especially for workers with creative tasks), improved communication of non-obvious instructions, and increased adherence to the original intent of the project.},
booktitle = {Adjunct Proceedings of the 27th Annual ACM Symposium on User Interface Software and Technology},
pages = {99–100},
numpages = {2},
keywords = {expert crowdsourcing, crowdsourcing, CSCW},
location = {Honolulu, Hawaii, USA},
series = {UIST '14 Adjunct}
}

@inproceedings{10.1145/2037556.2037582,
author = {Warner, Janice},
title = {Next steps in e-government crowdsourcing},
year = {2011},
isbn = {9781450307628},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2037556.2037582},
doi = {10.1145/2037556.2037582},
abstract = {In the US, the Federal Administration's Open Government Initiative has spurred use of crowdsourcing tools for several types of citizen participation. Likewise, US states and municipalities have implemented several forums. A survey of crowdsourcing initiatives as well as a discussion of important characteristics and features are provided in this paper. Most critical to crowdsourcing success is the feeling by participants that their efforts were considered and that results came from the initiative. This requires moderators who are knowledge workers adept at working in a social network environment, flexible crowdsourcing tools that make linking and feedback easy to provide, as well as a change in processes used to develop governmental services.},
booktitle = {Proceedings of the 12th Annual International Digital Government Research Conference: Digital Government Innovation in Challenging Times},
pages = {177–181},
numpages = {5},
keywords = {e-government, crowdsourcing},
location = {College Park, Maryland, USA},
series = {dg.o '11}
}

@inproceedings{10.1145/1693042.1693093,
author = {Shah, Neeta and Dhanesha, Ashutosh and Seetharam, Dravida},
title = {Crowdsourcing for e-governance: case study},
year = {2009},
isbn = {9781605586632},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1693042.1693093},
doi = {10.1145/1693042.1693093},
abstract = {In this case study, we share our experience with creating and running a large scale initiative in the state of Gujarat in India wherein Crowdsourcing [1] was used to develop e-Governance applications. The case study provides details on the methodology used, lessons learnt, key success factors, challenges faced, and recommendations for future usage of crowdsourcing for building e-Governance applications.In this initiative, named INVITE, students were engaged in e-Governance application design and development for real project scenarios of various government departments. In a span of 2 years over 5000 students pursuing undergraduate and graduate courses from nearly 100 colleges and universities across the state participated. Over 500 faculty members also participated as project guides for the students who worked in teams of 3 or 4 members. Use of open source software tools was made mandatory. This community e-Governance initiative was supported by the three main pillars of the IT ecosystem - Government, Industry and Academia.A nodal IT agency of the state government documented 27 real world e-Governance project scenarios for various departments for the INVITE crowdsourcing experiment. Village kiosks for farmers, land records, animal husbandry, and museum administration are some of the areas for which the students developed applications. A programming contest was announced for attracting students to participate in the initiative.The state department of technical education provided impetus to INVITE by sending official circulars to heads of universities and colleges to encourage the students to take up the projects as part of their course requirements.An industry sponsor funded the basic infrastructure of a website for team registration and project submission, program management agency for mass communications, posters, and prizes for best applications for each project scenario. Provision was made to reward the faculty guides of winning teams and their institutes as well. As the student teams competed, the state government benefited by getting very good e-Governance applications developed for free by the crowdsourcing model.Various government departments nominated "resource persons" who interacted with the project teams to answer their queries about the prevailing manual systems and their vision of e-Governance for those functions.In summary, INVITE became a great platform for crowdsourcing resulting in "e-Governance for the people, by the people".},
booktitle = {Proceedings of the 3rd International Conference on Theory and Practice of Electronic Governance},
pages = {253–258},
numpages = {6},
keywords = {students, ecosystem, crowdsourcing, community},
location = {Bogota, Colombia},
series = {ICEGOV '09}
}

