@inproceedings{10.1145/2810188.2810191,
author = {Melenhorst, Mark and Novak, Jasminko and Micheel, Isabel and Larson, Martha and Boeckle, Martin},
title = {Bridging the Utilitarian-Hedonic Divide in Crowdsourcing Applications},
year = {2015},
isbn = {9781450337465},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2810188.2810191},
doi = {10.1145/2810188.2810191},
abstract = {This paper introduces a novel perspective on the gamification of crowdsourcing tasks by conceptualizing it as the introduction of hedonic quality into the solution of utilitarian tasks and into the design of corresponding systems. We demonstrate how such a conceptualization can enable crowdsourcing applications to involve new kinds of crowds in everyday contexts that cannot be reached with existing models. We illustrate its application with the design of TrendRack, a gamified crowdsourcing application in the domain of fashion. We then discuss the results from a first evaluation, suggesting successful engagement of fashion customers in everyday contexts.},
booktitle = {Proceedings of the Fourth International Workshop on Crowdsourcing for Multimedia},
pages = {9–14},
numpages = {6},
keywords = {user study, motivation, mobile application development, hedonic quality, crowdsourcing, GWAPs},
location = {Brisbane, Australia},
series = {CrowdMM '15}
}

@inproceedings{10.1145/2187836.2187970,
author = {Ghosh, Arpita and McAfee, Preston},
title = {Crowdsourcing with endogenous entry},
year = {2012},
isbn = {9781450312295},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2187836.2187970},
doi = {10.1145/2187836.2187970},
abstract = {We investigate the design of mechanisms to incentivize high quality outcomes in crowdsourcing environments with strategic agents, when entry is an endogenous, strategic choice. Modeling endogenous entry in crowdsourcing markets is important because there is a nonzero cost to making a contribution of any quality which can be avoided by not participating, and indeed many sites based on crowdsourced content do not have adequate participation. We use a mechanism with monotone, rank-based, rewards in a model where agents strategically make participation and quality choices to capture a wide variety of crowdsourcing environments, ranging from conventional crowdsourcing contests with monetary rewards such as TopCoder, to crowdsourced content as in online Q&amp;A forums.We begin by explicitly constructing the unique mixed-strategy equilibrium for such monotone rank-order mechanisms, and use the participation probability and distribution of qualities from this construction to address the question of designing incentives for two kinds of rewards that arise in the context of crowdsourcing. We first show that for attention rewards that arise in the crowdsourced content setting, the entire equilibrium distribution and therefore every increasing statistic including the maximum and average quality (accounting for participation), improves when the rewards for every rank but the last are as high as possible. In particular, when the cost of producing the lowest possible quality content is low, the optimal mechanism displays all but the poorest contribution. We next investigate how to allocate rewards in settings where there is a fixed total reward that can be arbitrarily distributed amongst participants, as in crowdsourcing contests. Unlike models with exogenous entry, here the expected number of participants can be increased by subsidizing entry, which could potentially improve the expected value of the best contribution. However, we show that subsidizing entry does not improve the expected quality of the best contribution, although it may improve the expected quality of the average contribution. In fact, we show that free entry is dominated by taxing entry---making all entrants pay a small fee, which is rebated to the winner along with whatever rewards were already assigned, can improve the quality of the best contribution over a winner-take-all contest with no taxes.},
booktitle = {Proceedings of the 21st International Conference on World Wide Web},
pages = {999–1008},
numpages = {10},
keywords = {user generated content (UGC), social computing, mechanism design, game theory, crowdsourcing, contest design},
location = {Lyon, France},
series = {WWW '12}
}

@inproceedings{10.1145/2670518.2673866,
author = {Shi, Jinghao and Guan, Zhangyu and Qiao, Chunming and Melodia, Tommaso and Koutsonikolas, Dimitrios and Challen, Geoffrey},
title = {Crowdsourcing Access Network Spectrum Allocation Using Smartphones},
year = {2014},
isbn = {9781450332569},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2670518.2673866},
doi = {10.1145/2670518.2673866},
abstract = {The hundreds of millions of deployed smartphones provide an unprecedented opportunity to collect data to monitor, debug, and continuously adapt wireless networks to improve performance. In contrast with previous mobile devices, such as laptops, smartphones are always on but mostly idle, making them available to perform measurements that help other nearby active devices make better use of available network resources. We present the design of PocketSniffer, a system delivering wireless measurements from smartphones both to network administrators for monitoring and debugging purposes and to algorithms performing realtime network adaptation. By collecting data from smartphones, PocketSniffer supports novel adaptation algorithms designed around common deployment scenarios involving both cooperative and self-interested clients and networks. We present preliminary results from a prototype and discuss challenges to realizing this vision.},
booktitle = {Proceedings of the 13th ACM Workshop on Hot Topics in Networks},
pages = {1–7},
numpages = {7},
keywords = {monitoring, crowdsourcing, Smartphones},
location = {Los Angeles, CA, USA},
series = {HotNets-XIII}
}

@inproceedings{10.5220/0005084800540063,
author = {Silva, C\^{a}ndida and Ramos, Isabel},
title = {An Ontology Roadmap for Crowdsourcing Innovation Intermediaries},
year = {2014},
isbn = {9789897580505},
publisher = {SCITEPRESS - Science and Technology Publications, Lda},
address = {Setubal, PRT},
url = {https://doi.org/10.5220/0005084800540063},
doi = {10.5220/0005084800540063},
abstract = {Ontologies have proliferated in the last years, essentially justified by the need of achieving a consensus in the multiple representations of reality inside computers, and therefore the accomplishment of interoperability between machines and systems. Ontologies provide an explicit conceptualization that describes the semantics of the data. Crowdsourcing innovation intermediaries are organizations that mediate the communication and relationship between companies that aspire to solve some problem or to take advantage of any business opportunity with a crowd that is prone to give ideas based on their knowledge, experience and wisdom, taking advantage of web 2.0 tools. Various ontologies have emerged, but at the best of our knowledge, there isn't any ontology that represents the entire process of intermediation of crowdsourcing innovation. In this paper we present an ontology roadmap for developing crowdsourcing innovation ontology of the intermediation process. Over the years, several authors have proposed some distinct methodologies, by different proposals of combining practices, activities, languages, according to the project they were involved in. We start making a literature review on ontology building, and analyse and compare ontologies that propose the development from scratch with the ones that propose reusing other ontologies. We also review enterprise and innovation ontologies known in literature. Finally, are presented the criteria for selecting the methodology and the roadmap for building crowdsourcing innovation intermediary ontology.},
booktitle = {Proceedings of the International Joint Conference on Knowledge Discovery, Knowledge Engineering and Knowledge Management - Volume 3},
pages = {54–63},
numpages = {10},
keywords = {Ontology Enterprise., Ontology Building Methodologies, Innovation Ontology, Crowdsourcing Innovation},
location = {Rome, Italy},
series = {IC3K 2014}
}

@inproceedings{10.1145/2948649.2948657,
author = {To, Hien and Geraldes, R\'{u}ben and Shahabi, Cyrus and Kim, Seon Ho and Prendinger, Helmut},
title = {An empirical study of workers' behavior in spatial crowdsourcing},
year = {2016},
isbn = {9781450343091},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2948649.2948657},
doi = {10.1145/2948649.2948657},
abstract = {With the ubiquity of smartphones, spatial crowdsourcing (SC) has emerged as a new paradigm that engages mobile users to perform tasks in the physical world. Thus, various SC techniques have been studied for performance optimization. However, little research has been done to understand workers' behavior in the real world. In this study, we designed and performed two real world SC campaigns utilizing our mobile app, called Genkii, which is a GPS-enabled app for users to report their affective state (e.g., happy, sad). We used Yahoo! Japan Crowdsourcing as the payment platform to reward users for reporting their affective states at different locations and times. We studied the relationship between incentives and participation by analyzing the impact of offering a fixed reward versus an increasing reward scheme. We observed that users tend to stay in a campaign longer when the provided incentives gradually increase over time. We also found that the degree of mobility is correlated with the reported information. For example, users who travel more are observed to be happier than the ones who travel less. Furthermore, analyzing the spatiotemporal information of the reports reveals interesting mobility patterns that are unique to spatial crowdsourcing.},
booktitle = {Proceedings of the Third International ACM SIGMOD Workshop on Managing and Mining Enriched Geo-Spatial Data},
articleno = {8},
numpages = {6},
keywords = {spatial crowdsourcing, mobility, incentives},
location = {San Francisco, California},
series = {GeoRich '16}
}

@inproceedings{10.1145/2702123.2702443,
author = {Gadiraju, Ujwal and Kawase, Ricardo and Dietze, Stefan and Demartini, Gianluca},
title = {Understanding Malicious Behavior in Crowdsourcing Platforms: The Case of Online Surveys},
year = {2015},
isbn = {9781450331456},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2702123.2702443},
doi = {10.1145/2702123.2702443},
abstract = {Crowdsourcing is increasingly being used as a means to tackle problems requiring human intelligence. With the ever-growing worker base that aims to complete microtasks on crowdsourcing platforms in exchange for financial gains, there is a need for stringent mechanisms to prevent exploitation of deployed tasks. Quality control mechanisms need to accommodate a diverse pool of workers, exhibiting a wide range of behavior. A pivotal step towards fraud-proof task design is understanding the behavioral patterns of microtask workers. In this paper, we analyze the prevalent malicious activity on crowdsourcing platforms and study the behavior exhibited by trustworthy and untrustworthy workers, particularly on crowdsourced surveys. Based on our analysis of the typical malicious activity, we define and identify different types of workers in the crowd, propose a method to measure malicious activity, and finally present guidelines for the efficient design of crowdsourced surveys.},
booktitle = {Proceedings of the 33rd Annual ACM Conference on Human Factors in Computing Systems},
pages = {1631–1640},
numpages = {10},
keywords = {user behavior, online surveys, microtasks, malicious intent, crowdsourcing},
location = {Seoul, Republic of Korea},
series = {CHI '15}
}

@inproceedings{10.5555/2002472.2002626,
author = {Zaidan, Omar F. and Callison-Burch, Chris},
title = {Crowdsourcing translation: professional quality from non-professionals},
year = {2011},
isbn = {9781932432879},
publisher = {Association for Computational Linguistics},
address = {USA},
abstract = {Naively collecting translations by crowd-sourcing the task to non-professional translators yields disfluent, low-quality results if no quality control is exercised. We demonstrate a variety of mechanisms that increase the translation quality to near professional levels. Specifically, we solicit redundant translations and edits to them, and automatically select the best output among them. We propose a set of features that model both the translations and the translators, such as country of residence, LM perplexity of the translation, edit rate from the other translations, and (optionally) calibration against professional translators. Using these features to score the collected translations, we are able to discriminate between acceptable and unacceptable translations. We recreate the NIST 2009 Urdu-to-English evaluation set with Mechanical Turk, and quantitatively show that our models are able to select translations within the range of quality that we expect from professional translators. The total cost is more than an order of magnitude lower than professional translation.},
booktitle = {Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies - Volume 1},
pages = {1220–1229},
numpages = {10},
location = {Portland, Oregon},
series = {HLT '11}
}

@inproceedings{10.1109/MASCOTS.2014.31,
author = {Xie, Hong and Lui, John C. S. and Jiang, Wenjie},
title = {Mathematical Modeling of Crowdsourcing Systems: Incentive Mechanism and Rating System Design},
year = {2014},
isbn = {9781479956104},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/MASCOTS.2014.31},
doi = {10.1109/MASCOTS.2014.31},
abstract = {Crowd sourcing systems like Yahoo! Answers, Amazon Mechanical Turk, and Google Helpouts, etc., have seen an increasing prevalence in the past few years. The participation of users, high quality solutions, and a fair rating system are critical to the revenue of a crowd sourcing system. In this paper, we design a class of simple but effective incentive mechanisms to attract users participating, and providing high quality solutions. Our incentive mechanism consists of a task bundling scheme and a rating system, and pay workers according to solution ratings from requesters. We also propose a probabilistic model to capture various human factors like biases in rating, and we quantify its impact on the incentive mechanism, which is shown to be highly robust. We develop a model to characterize the design space of a class of commonly used rating systems - threshold based rating systems. We quantify the impact of such rating systems and the bundling scheme on the incentive mechanism.},
booktitle = {Proceedings of the 2014 IEEE 22nd International Symposium on Modelling, Analysis \&amp; Simulation of Computer and Telecommunication Systems},
pages = {181–186},
numpages = {6},
keywords = {rating system, incentive mechanism, crowdsourcing, bundling},
series = {MASCOTS '14}
}

@inproceedings{10.1007/978-3-642-45005-1_54,
author = {Vukovic, Maja and Das, Rajarshi},
title = {Decision Making in Enterprise Crowdsourcing Services},
year = {2013},
isbn = {9783642450044},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-642-45005-1_54},
doi = {10.1007/978-3-642-45005-1_54},
abstract = {Enterprises are increasingly employing crowdsourcing to engage employees and public as part of their business processes, given a promising, low cost, access to scalable workforce online. Common examples include harnessing of crowd expertise for enterprise knowledge discovery, software development, product support and innovation. Crowdsourcing tasks vary in their complexity, required level of business support and investment, and most importantly the quality of outcome. As such, not every step in a business process can successfully lend itself to crowdsourcing. In this paper, we present a decision-making and execution service, called CrowdArb, operating on crowdsourcing tasks in the large global enterprise. The system employs decision theoretic methodology to assess whether to crowdsource or not a selected step of the knowledge discovery process. The system addresses the challenges of trade-off between the quality and time of the crowdsourcing responses, as well as the trade-off between the cost of crowdsourcing experts and time required to complete the entire campaign. We present evaluation results from simulations of CrowdArb in enterprise crowdsourcing campaign that engaged over 560 client representatives to obtain actionable insights. We discuss how proposed solution addresses the opportunity to close the gap of semi-automated task coordination in crowdsourcing environments.},
booktitle = {Proceedings of the 11th International Conference on Service-Oriented Computing - Volume 8274},
pages = {624–638},
numpages = {15},
keywords = {Organizational Services, Enterprise, Crowdsourcing},
location = {Berlin, Germany},
series = {ICSOC 2013}
}

@inproceedings{10.1145/3144457.3144499,
author = {Zhao, Ziming and Liu, Fang and Cai, Zhiping and Xiao, Nong},
title = {Edge-based Content-aware Crowdsourcing Approach for Image Sensing in Disaster Environment},
year = {2017},
isbn = {9781450353687},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3144457.3144499},
doi = {10.1145/3144457.3144499},
abstract = {Photos obtained via crowdsourcing can be used in image sensing for disaster management. Due to the weak communication environment after a disaster, it is difficult to transfer the huge amount of crowdsourced photos. To address this problem, we propose COCO, a content-aware crowdsourcing system that leverages edge computing to support real-time image sensing in disaster environment. COCO filters the crowdsourced images at the data source and only uploads the images that contain relevant objects which the application is interested in. We use a machine-learning based computer vision detector to understand the content of images. Considering the resource constraints of mobile devices, we implement the computer vision detector at the edge server which located in the close proximity to data source. As the unstable network bandwidth is normal in disaster environment, we propose an adaptive mechanism to further improve the sensing performance. We have implemented the COCO prototype which is evaluated via a real-world dataset. The experimental results demonstrate the effectiveness of COCO.},
booktitle = {Proceedings of the 14th EAI International Conference on Mobile and Ubiquitous Systems: Computing, Networking and Services},
pages = {225–231},
numpages = {7},
keywords = {Image Sensing, Edge Computing, Crowdsourcing},
location = {Melbourne, VIC, Australia},
series = {MobiQuitous 2017}
}

@proceedings{10.5555/3100549,
title = {CSI-SE '17: Proceedings of the 4th International Workshop on CrowdSourcing in Software Engineering},
year = {2017},
isbn = {9781538640418},
publisher = {IEEE Press},
abstract = {It is our pleasure to welcome the reader to the (pre-workshop) proceedings of the 4th International Workshop on CrowdSourcing in Software Engineering (CSI-SE 2017), co-located with the 39th International Conference on Software Engineering (ICSE 2017) held in Buenos Aires, Argentina, during May 20-28, 2017.A number of trends under the broad banner of crowdsourcing are beginning to fundamentally disrupt the way in which software is engineered. Programmers increasingly rely on crowdsourced knowledge and code, as they look to Question \&amp; Answer (Q&amp;A) sites for answers or use code from publicly posted snippets. Programmers play, compete, and learn with the crowd, engaging in programming competitions and puzzles with crowds of programmers. Online IDEs make possible radically new forms of collaboration, allowing developers to synchronously program with crowds of distributed programmers. Programmers' reputation is increasingly visible on Q&amp;A sites and public code repositories, opening new possibilities in how developers find jobs and companies identify talent. Crowds of non-programmers increasingly participate in development, usability testing software or even constructing specifications while playing games. Crowds play an increasing role in shaping software requirements, broadening the software which might be feasibly constructed. Approaches for crowd development seek to microtask software development, dramatically increasing participation in open source by enabling software projects to be built through casual, transient work.},
location = {Buenos Aires, Argentina}
}

@inproceedings{10.1145/2786451.2786492,
author = {Timmermans, Benjamin and Aroyo, Lora and Welty, Chris},
title = {Crowdsourcing ground truth for Question Answering using CrowdTruth},
year = {2015},
isbn = {9781450336727},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2786451.2786492},
doi = {10.1145/2786451.2786492},
abstract = {Gathering training and evaluation data for open domain tasks, such as general question answering, is a challenging task. Typically, ground truth data is provided by human expert annotators, however, in an open domain experts are difficult to define. Moreover, the overall process for annotating examples can be lengthy and expensive. Naturally, crowdsourcing has become a mainstream approach for filling this gap, i.e. gathering human interpretation data. However, similar to the traditional expert annotation tasks, most of those methods use majority voting to measure the quality of the annotations and thus aim at identifying a single right answer for each example, despite the fact that many annotation tasks can have multiple interpretations, which results in multiple correct answers to the same question. We present a crowdsourcing-based approach for efficiently gathering ground truth data called CrowdTruth, where disagreement-based metrics are used to harness the multitude of human interpretation and measure the quality of the resulting ground truth. We exemplify our approach in two semantic interpretation use cases for answering questions.},
booktitle = {Proceedings of the ACM Web Science Conference},
articleno = {61},
numpages = {2},
keywords = {Gold Standard Annotation, Disagreement, Crowdsourcing},
location = {Oxford, United Kingdom},
series = {WebSci '15}
}

@inproceedings{10.5555/2729485.2729514,
author = {Hourcade, Juan Pablo and Gehrt, Laura},
title = {Crowdsourcing for delivering research results to patients},
year = {2014},
isbn = {9788968487521},
publisher = {Hanbit Media, Inc.},
address = {Seoul, KOR},
abstract = {Participatory research, where participants in research projects take an active role in research activities is an emerging trend that helps participants understand the long- term impact of research, reduces participant dropout rates, and shapes research for greater value to patients. One way to involve participants in research is to have them participate in making research results available to the general public. To learn about the feasibility of this approach, we conducted a study comparing crowdsourced medication warnings derived from research literature to expert-produced warnings derived from the same literature. Through a survey with 159 respondents, we learned that they mostly favored the expert-produced warnings, although for one set of warnings the respondents preferred the crowdsourced warnings in terms of how comprehensive they were. The study suggests that crowdsourcing techniques combined with expert supervision and input could provide reasonable ways for participants in clinical trials to be more active participants in the studies in which they participate.},
booktitle = {Proceedings of HCI Korea},
pages = {196–202},
numpages = {7},
keywords = {translation, research, older adults, crowdsourcing, clinical, Amazon mechanical turk},
location = {Seoul, Republic of Korea},
series = {HCIK '15}
}

@inproceedings{10.1109/HICSS.2015.580,
author = {Dissanayake, Indika and Zhang, Jie and Gu, Bin},
title = {Virtual Team Performance in Crowdsourcing Contest: A Social Network Perspective},
year = {2015},
isbn = {9781479973675},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/HICSS.2015.580},
doi = {10.1109/HICSS.2015.580},
abstract = {Social media technologies have made it feasible for organizations to tap "wisdom of the crowd" beyond their own workforce. Many organizations use online crowd sourcing contests to find solutions for their business problems. In these contests, self-organized virtual teams compete for monetary reward. Motivated by this phenomenon, this research investigates how the social network structure of a virtual team impacts its performance in the context of online crowd sourcing contests. Specifically, we empirically assess the impacts of member social-capital, intellectual-capital, and the alignment of these two measures on team performances. Our analysis suggests that the alignment of member social-capital and intellectual-capital has a negative impact on team performances. Our findings have strategic implications to participants of virtual crowd sourcing competitions and to the design of virtual work teams.},
booktitle = {Proceedings of the 2015 48th Hawaii International Conference on System Sciences},
pages = {4894–4897},
numpages = {4},
keywords = {Virtual teams, Social network analysis, Social capital, Intellectual capital, Crowdsourcing},
series = {HICSS '15}
}

@inproceedings{10.1109/BigDataCongress.2015.61,
author = {Tung, Wei-Feng and Jordann, Guillaume},
title = {Crowdsourcing Service Design for Social Enterprise Insight Innovation},
year = {2015},
isbn = {9781467372787},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/BigDataCongress.2015.61},
doi = {10.1109/BigDataCongress.2015.61},
abstract = {Social enterprises (SE) have started up not a few innovative business models or cause-related marketing in modern society, which can discover more and more critical social issues need to solve urgently. In recently years, SEs often use social media of Web 2.0 (e.g., Facebook, Twitter) to seek various opportunities, resources, and supports. Thus, this research is to develop an integrative 'crowd sourcing' as an altruism social media that can connect and leverage external resources to help people undertake some cause-related projects (e.g., Volunteer activity) or start-ups of SE. It can answer what would new SE look like? How could SE work? Is this possible sustainable? A proposed a crowd sourcing platform called 'HIVE' means a kind of corporation concepts. As we know, all bees work in cooperation with each other and settled in a hive. Thus, the various social issues and resources information can be integrated and emerged for innovative business insights to support the developments of social enterprises.},
booktitle = {Proceedings of the 2015 IEEE International Congress on Big Data},
pages = {367–373},
numpages = {7},
keywords = {Start-up, Social Enterprises, SE, Hive, Crowdsourcing, Collective Intelligence},
series = {BIGDATACONGRESS '15}
}

@inproceedings{10.1007/978-3-319-94277-3_5,
author = {Li, Liangcheng and Bu, Jiajun and Wang, Can and Yu, Zhi and Wang, Wei and Wu, Yue and Gu, Chunbin and Zhou, Qin},
title = {CrowdAE: A Crowdsourcing System with Human Inspection Quality Enhancement for Web Accessibility Evaluation},
year = {2018},
isbn = {978-3-319-94276-6},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-319-94277-3_5},
doi = {10.1007/978-3-319-94277-3_5},
abstract = {Crowdsourcing technology can help manual testing by soliciting the contributions from volunteer evaluators. But crowd evaluators may give inaccurate or invalid evaluation results. This paper proposes an advanced crowdsourcing-based web accessibility evaluation system called CrowdAE by enhancing the crowdsourcing-based manual testing module of the previous version. Through three main process namely learning system, task assignment and task review, we can improve the quality of evaluation results from the crowd. From the comparison on the two years’ evaluation process of Chinese government websites, our CrowdAE outperforms the previous version and improve the accuracy of the evaluation results.},
booktitle = {Computers Helping People with Special Needs: 16th International Conference, ICCHP 2018, Linz, Austria, July 11-13, 2018, Proceedings, Part I},
pages = {27–30},
numpages = {4},
keywords = {Crowdsourcing, Web accessibility evaluation},
location = {Linz, Austria}
}

@inproceedings{10.1145/2702123.2702296,
author = {Dergousoff, Kristen and Mandryk, Regan L.},
title = {Mobile Gamification for Crowdsourcing Data Collection: Leveraging the Freemium Model},
year = {2015},
isbn = {9781450331456},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2702123.2702296},
doi = {10.1145/2702123.2702296},
abstract = {Classic ways of gathering data on human behaviour are time-consuming, costly and are subject to limited participant pools. Crowdsourcing offers a reduction in operating costs and access to a diverse and large participant pool; however issues arise concerning low worker pay and questions about data quality. Gamification provides a motivation to participate, but also requires the development of specialized, research-question specific games that can be costly to produce. Our solution combines gamification and crowdsourcing in a smartphone-based system that emulates the popular Freemium model of play to motivate voluntary participation through in-game rewards, using a robust framework to study multiple unrelated research questions within the same system. We deployed our game on the Android store and compared it to a gamified laboratory version and a non-gamified laboratory version, and found that players who used the in-game rewards were motivated to do experimental tasks. There was no difference between the systems for performance on a motor task; however, performance on the cognitive task was worse for the crowdsourced game. We discuss options for improving performance on tasks requiring attention.},
booktitle = {Proceedings of the 33rd Annual ACM Conference on Human Factors in Computing Systems},
pages = {1065–1074},
numpages = {10},
keywords = {psychophysics, gamification, freemium, crowdsourcing},
location = {Seoul, Republic of Korea},
series = {CHI '15}
}

@inproceedings{10.1145/2757226.2764775,
author = {Kim, Joy},
title = {Designing Crowdsourcing Techniques Based on Expert Creative Practice},
year = {2015},
isbn = {9781450335980},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2757226.2764775},
doi = {10.1145/2757226.2764775},
abstract = {Current crowdsourcing workflows comprise of discrete tasks that guide the crowd towards predetermined goals. However, this approach is ill-suited towards supporting massively collaborative open-ended creative work, which often involves an exploration of possible end results and revision of creative goals. My dissertation explores how existing expert creative practice can inform new crowdsourcing techniques that allow the crowd to collaborate on complex creative tasks such as writing short stories.},
booktitle = {Proceedings of the 2015 ACM SIGCHI Conference on Creativity and Cognition},
pages = {349–350},
numpages = {2},
keywords = {storytelling, social computing, novices, experts, crowdsourcing, creativity},
location = {Glasgow, United Kingdom},
series = {C&amp;C '15}
}

@inproceedings{10.1145/2593728.2593731,
author = {Stol, Klaas-Jan and Fitzgerald, Brian},
title = {Researching crowdsourcing software development: perspectives and concerns},
year = {2014},
isbn = {9781450328579},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2593728.2593731},
doi = {10.1145/2593728.2593731},
abstract = {Crowdsourcing is an emerging form of `outsourcing’ software development. While there has been considerable research in the area of crowdsourcing in general, very little research has focused specifically on how crowdsourcing works in a software development context, and as far as we know, there have been no published studies of crowdsourcing software development from a customer perspective. Based on a review of the literature, we identified a number of key concerns related to crowdsourcing that are of particular importance in a software development context. Furthermore, we observed a number of recurring key stakeholders, or actors, each of whom has a unique perspective on crowdsourcing. This paper presents a research framework that consists of the various combinations of stakeholders and key concerns. The framework can be used to guide future research on the use of crowdsourcing as a `sourcing’ strategy, as well as a means to review and synthesize research findings so as to be able to compare studies on crowdsourcing in a software development context.},
booktitle = {Proceedings of the 1st International Workshop on CrowdSourcing in Software Engineering},
pages = {7–10},
numpages = {4},
keywords = {research framework, crowdsourcing software development},
location = {Hyderabad, India},
series = {CSI-SE 2014}
}

@inproceedings{10.1145/2736277.2741685,
author = {Difallah, Djellel Eddine and Catasta, Michele and Demartini, Gianluca and Ipeirotis, Panagiotis G. and Cudr\'{e}-Mauroux, Philippe},
title = {The Dynamics of Micro-Task Crowdsourcing: The Case of Amazon MTurk},
year = {2015},
isbn = {9781450334693},
publisher = {International World Wide Web Conferences Steering Committee},
address = {Republic and Canton of Geneva, CHE},
url = {https://doi.org/10.1145/2736277.2741685},
doi = {10.1145/2736277.2741685},
abstract = {Micro-task crowdsourcing is rapidly gaining popularity among research communities and businesses as a means to leverage Human Computation in their daily operations. Unlike any other service, a crowdsourcing platform is in fact a marketplace subject to human factors that affect its performance, both in terms of speed and quality. Indeed, such factors shape the dynamics of the crowdsourcing market. For example, a known behavior of such markets is that increasing the reward of a set of tasks would lead to faster results. However, it is still unclear how different dimensions interact with each other: reward, task type, market competition, requester reputation, etc. In this paper, we adopt a data-driven approach to (A) perform a long-term analysis of a popular micro-task crowdsourcing platform and understand the evolution of its main actors (workers, requesters, and platform). (B) We leverage the main findings of our five year log analysis to propose features used in a predictive model aiming at determining the expected performance of any batch at a specific point in time. We show that the number of tasks left in a batch and how recent the batch is are two key features of the prediction. (C) Finally, we conduct an analysis of the demand (new tasks posted by the requesters) and supply (number of tasks completed by the workforce) and show how they affect task prices on the marketplace.},
booktitle = {Proceedings of the 24th International Conference on World Wide Web},
pages = {238–247},
numpages = {10},
keywords = {trend identification, tracking, human factors, forecasting, experimentation, design, crowdsourcing},
location = {Florence, Italy},
series = {WWW '15}
}

@inproceedings{10.1145/2517351.2517397,
author = {Faggiani, Adriano and Gregori, Enrico and Lenzini, Luciano and Luconi, Valerio and Vecchio, Alessio},
title = {Network sensing through smartphone-based crowdsourcing},
year = {2013},
isbn = {9781450320276},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2517351.2517397},
doi = {10.1145/2517351.2517397},
abstract = {Portolan is a crowdsourcing system, aimed at monitoring and measuring large-scale networks, that uses smartphones as mobile observation elements. Currently, Portolan is able to collect information about both wired and wireless networks, in particular it is used to obtain the graph of the Internet with unprecedented resolution and to associate performance indexes (received signal strength, maximum throughput) of cellular networks to geographic locations.},
booktitle = {Proceedings of the 11th ACM Conference on Embedded Networked Sensor Systems},
articleno = {31},
numpages = {2},
keywords = {smartphone, network sensing, crowdsourcing},
location = {Roma, Italy},
series = {SenSys '13}
}

@inproceedings{10.5555/2863694.2864275,
author = {Qiang, Yang Mao and Lue, Fan Li},
title = {Crowdsourcing Translation Based on the Divide and Conquer},
year = {2015},
isbn = {9781467393935},
publisher = {IEEE Computer Society},
address = {USA},
abstract = {With the rapid development of Internet technology, the network has become the necessary media of every-day life. It is the most convenient, fastest and most effective way to learn about the world through the Internet. Text translation is one of the most important techniques, however the existed human-translation has the shortcomings of high price and long waiting time, and it is difficult to guarantee the quality for machine translation. This paper proposes crowdsourcing-based translation measures using the thought of dividing and conquering. The experiments show that this way can translate text with lower monetary cost, smaller latency and higher quality, which can meet the practical needs.},
booktitle = {Proceedings of the 2015 Sixth International Conference on Intelligent Systems Design and Engineering Applications},
pages = {251–255},
numpages = {5},
series = {ISDEA '15}
}

@inproceedings{10.1109/SAHCN.2015.7338353,
author = {Fox, Andrew and Kumar, B.V.K. Vijaya and Chen, Jinzhu and Bai, Fan},
title = {Crowdsourcing undersampled vehicular sensor data for pothole detection},
year = {2015},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/SAHCN.2015.7338353},
doi = {10.1109/SAHCN.2015.7338353},
abstract = {The increased availability of embedded vehicle sensors allows for the detection of road features such as potholes. Despite being a promising approach, current vehicle embedded sensors operate at low frequencies and undersample sensor signals, thus degrading detection accuracy. One emerging solution is to crowdsource such undersampled sensor data from multiple vehicles to increase the detection accuracy. Aggregating sensor data from multiple vehicles, nonetheless, is a challenging task given the heterogeneity among vehicles, asynchronous sensor operation, GPS error, and sensor noise. Additionally, there may be bandwidth restrictions in vehicular networks which limit the amount of data available for aggregation. We investigate these issues by focusing on the problem of pothole detection. To quantify the detection accuracies and effects of real-world limitations, we design and evaluate three crowdsourcing pothole detection schemes involving vehicles and the Cloud. We also address the issue of lack of extensive model training data by demonstrating that a detection model applicable to real-world systems can be derived using simulated data. We validate our pothole detection methods using 38.1 km of real-world data collected from driving on roads in Warren, Michigan.},
booktitle = {2015 12th Annual IEEE International Conference on Sensing, Communication, and Networking (SECON)},
pages = {515–523},
numpages = {9},
location = {Seattle, WA, USA}
}

@inproceedings{10.1007/978-3-319-49169-1_23,
author = {Granell, Emilio and Mart\'{\i}nez-Hinarejos, Carlos-D.},
title = {Collaborator Effort Optimisation in Multimodal Crowdsourcing for Transcribing Historical Manuscripts},
year = {2016},
isbn = {978-3-319-49168-4},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-319-49169-1_23},
doi = {10.1007/978-3-319-49169-1_23},
abstract = {Crowdsourcing is a powerful tool for massive transcription at a relatively low cost, since the transcription effort is distributed into a set of collaborators, and therefore, supervision effort of professional transcribers may be dramatically reduced. Nevertheless, collaborators are a scarce resource, which makes optimisation very important in order to get the maximum benefit from their efforts. In this work, the optimisation of the work load in the side of collaborators is studied in a multimodal crowdsourcing platform where speech dictation of handwritten text lines is used as transcription source. The experiments explore how this optimisation allows to obtain similar results reducing the number of collaborators and the number of text lines that they have to read.},
booktitle = {Advances in Speech and Language Technologies for Iberian Languages: Third International Conference, IberSPEECH 2016, Lisbon, Portugal, November 23-25, 2016, Proceedings},
pages = {234–244},
numpages = {11},
keywords = {Crowdsourcing framework, Collaborator effort, Speech recognition, Document transcription},
location = {Lisbon, Portugal}
}

@inproceedings{10.1145/2046396.2046421,
author = {Weld, Daniel S. and Mausam and Dai, Peng},
title = {Execution control for crowdsourcing},
year = {2011},
isbn = {9781450310147},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2046396.2046421},
doi = {10.1145/2046396.2046421},
abstract = {Crowdsourcing marketplaces enable a wide range of applications, but constructing any new application is challenging - usually requiring a complex, self-managing workflow in order to guarantee quality results. We report on the CLOWDER project, which uses machine learning to continually refine models of worker performance and task difficulty. We present decision-theoretic optimization techniques that can select the best parameters for a range of workflows. Initial experiments show our optimized workflows are significantly more economical than with manually set parameters.},
booktitle = {Proceedings of the 24th Annual ACM Symposium Adjunct on User Interface Software and Technology},
pages = {57–58},
numpages = {2},
keywords = {pomdp, human computation, execution control, decision theory, crowdsourcing},
location = {Santa Barbara, California, USA},
series = {UIST '11 Adjunct}
}

@inproceedings{10.1109/ICMeCG.2014.39,
author = {Liu, Nianzu and Chen, Xiao},
title = {Contribution-Based Incentive Design for Mobile Crowdsourcing},
year = {2014},
isbn = {9781479965434},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/ICMeCG.2014.39},
doi = {10.1109/ICMeCG.2014.39},
abstract = {In this paper, we propose COIN as a general pattern for carrying out effective mobile crowd sourcing. As a demonstration, we apply the pattern to design a crowd-based system for realizing smart parking. Compared with existing solutions, our proposal possesses several desirable features and improves the efficiency of crowd sourcing in the process of problem solving.},
booktitle = {Proceedings of the 2014  International Conference on Management of E-Commerce and e-Government},
pages = {151–155},
numpages = {5},
keywords = {Mobile application, Intelligent Transportation, Crowdsourcing},
series = {ICMECG '14}
}

@inproceedings{10.5555/2999792.2999826,
author = {Liu, Qiang and Steyvers, Mark and Ihler, Alexander},
title = {Scoring workers in crowdsourcing: how many control questions are enough?},
year = {2013},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {We study the problem of estimating continuous quantities, such as prices, probabilities, and point spreads, using a crowdsourcing approach. A challenging aspect of combining the crowd's answers is that workers' reliabilities and biases are usually unknown and highly diverse. Control items with known answers can be used to evaluate workers' performance, and hence improve the combined results on the target items with unknown answers. This raises the problem of how many control items to use when the total number of items each workers can answer is limited: more control items evaluates the workers better, but leaves fewer resources for the target items that are of direct interest, and vice versa. We give theoretical results for this problem under different scenarios, and provide a simple rule of thumb for crowdsourcing practitioners. As a byproduct, we also provide theoretical analysis of the accuracy of different consensus methods.},
booktitle = {Proceedings of the 26th International Conference on Neural Information Processing Systems - Volume 2},
pages = {1914–1922},
numpages = {9},
location = {Lake Tahoe, Nevada},
series = {NIPS'13}
}

@inproceedings{10.1109/BigData.Congress.2013.43,
author = {Roopa, T. and Iyer, Anantharaman Narayana and Rangaswamy, Shanta},
title = {CroTIS-Crowdsourcing Based Traffic Information System},
year = {2013},
isbn = {9780769550060},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/BigData.Congress.2013.43},
doi = {10.1109/BigData.Congress.2013.43},
abstract = {Road Traffic congestion is a perennial problem across the globe. The nature of the problems associated with vehicular traffic is not uniform across the globe. In the recent past, several solutions were proposed to improve traffic management. However most of these solutions cater to the traffic scenarios in the developed countries where traffic is lane based and the issues being encountered is quite different from that of developing countries. The existing solutions cannot be readily applied to the traffic situation of developing countries. In this paper, we propose an architecture which addresses the challenges of real time traffic management of non-lane based and chaotic traffic of developing countries. The big data collected for traffic analysis can be used to provide predictive analysis and patterns to enable the Traffic Department to manage the traffic better.},
booktitle = {Proceedings of the 2013 IEEE International Congress on Big Data},
pages = {271–277},
numpages = {7},
keywords = {Smartphone, Intelligent Traffic systems, Crowdsourcing, Big data},
series = {BIGDATACONGRESS '13}
}

@inproceedings{10.1109/IIKI.2015.60,
author = {Liu, Ziwei and Niu, Xiaoguang and Wei, Chuanbo and Huang, Zhen and Wu, Yunlong and Li, Hui},
title = {A Data-centric Cooperative Sensing Scheme in Crowdsourcing Systems},
year = {2015},
isbn = {9781467386371},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/IIKI.2015.60},
doi = {10.1109/IIKI.2015.60},
abstract = {In a densely deployed crowdsourcing system, data observed at neighboring participants often exhibit strong spatial correlation. Exploiting this property, one may put a portion of participants into low power sleep mode without compromising the quality of sensing or the connectivity of the network. In this work, two fundamental scheduling questions are considered: (a) how to select a maximum number of participants to be put into sleep mode so that the overall sensing data integrity is maintained above a given threshold, and (b) how to divide the participants into two "shifts" such that participants at different shifts will perform sensing alternatively while sensing data integrity is being maximized. For question (a), we propose a novel data centric approach to explicitly exploit data correlation among participants. We formulate this subset selection problem as a constrained optimization problem and propose an efficient polynomial time algorithm. For question (b), we formulate this set partitioning problem as a constrained mini-max optimization problem. We validate these algorithms using the New Library of Wuhan University data set and observe very satisfactory results.},
booktitle = {Proceedings of the 2015 International Conference on Identification, Information, and Knowledge in the Internet of Things (IIKI)},
pages = {250–253},
numpages = {4},
series = {IIKI '15}
}

@inproceedings{10.5555/2892753.2892804,
author = {Fang, Meng and Yin, Jie and Tao, Dacheng},
title = {Active learning for crowdsourcing using knowledge transfer},
year = {2014},
publisher = {AAAI Press},
abstract = {This paper studies the active learning problem in crowd-sourcing settings, where multiple imperfect annotators with varying levels of expertise are available for labeling the data in a given task. Annotations collected from these labelers may be noisy and unreliable, and the quality of labeled data needs to be maintained for data mining tasks. Previous solutions have attempted to estimate individual users' reliability based on existing knowledge in each task, but for this to be effective each task requires a large quantity of labeled data to provide accurate estimates. In practice, annotation budgets for a given task are limited, so each instance can be presented to only a few users, each of whom can only label a few examples. To overcome data scarcity we propose a new probabilistic model that transfers knowledge from abundant unlabeled data in auxiliary domains to help estimate labelers' expertise. Based on this model we present a novel active learning algorithm that: a) simultaneously selects the most informative example and b) queries its label from the labeler with the best expertise. Experiments on both text and image datasets demonstrate that our proposed method outperforms other state-of-the-art active learning methods.},
booktitle = {Proceedings of the Twenty-Eighth AAAI Conference on Artificial Intelligence},
pages = {1809–1815},
numpages = {7},
location = {Qu\'{e}bec City, Qu\'{e}bec, Canada},
series = {AAAI'14}
}

@inproceedings{10.1145/2851613.2851933,
author = {Saab, Farah and Elhajj, Imad and Kayssi, Ayman and Chehab, Ali},
title = {A crowdsourcing game-theoretic intrusion detection and rating system},
year = {2016},
isbn = {9781450337397},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2851613.2851933},
doi = {10.1145/2851613.2851933},
abstract = {One of the main concerns for smartphone users is the quality of apps they download. Before installing any app from the market, users first check its rating and reviews. However, these ratings are not computed by experts and most times are not associated with malicious behavior. In this work, we present an IDS/rating system based on a game theoretic model with crowdsourcing. Our results show that, with minor control over the error in categorizing users and the fraction of experts in the crowd, our system provides proper ratings while flagging all malicious apps.},
booktitle = {Proceedings of the 31st Annual ACM Symposium on Applied Computing},
pages = {622–625},
numpages = {4},
keywords = {IDS, app market, app rating, crowdsourcing, game theory},
location = {Pisa, Italy},
series = {SAC '16}
}

@inproceedings{10.1145/2390034.2390038,
author = {Lukyanenko, Roman and Parsons, Jeffrey},
title = {Conceptual modeling principles for crowdsourcing},
year = {2012},
isbn = {9781450317153},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2390034.2390038},
doi = {10.1145/2390034.2390038},
abstract = {Traditionally, the research and practice of conceptual modeling assumed all relevant information about a domain could be discovered through user-analyst communication. The increasing ubiquity of crowdsourcing challenges a number of long-held propositions about conceptual modeling. Given significant differences in levels of domain expertise among contributors in crowdsourcing projects, it is often impossible to predict all valid conceptualizations of a domain by potential users. Approaching conceptual modeling in crowdsourcing using traditional principles of modeling is highly constraining. This paper explores fundamental conceptual modeling challenges in crowdsourcing domains. We then use theoretical foundations in philosophy (ontology) to offer potential solutions.},
booktitle = {Proceedings of the 1st International Workshop on Multimodal Crowd Sensing},
pages = {3–6},
numpages = {4},
keywords = {user-generated content, crowdsourcing, conceptual modeling},
location = {Maui, Hawaii, USA},
series = {CrowdSens '12}
}

@inproceedings{10.1145/2362456.2362479,
author = {Sabou, Marta and Bontcheva, Kalina and Scharl, Arno},
title = {Crowdsourcing research opportunities: lessons from natural language processing},
year = {2012},
isbn = {9781450312424},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2362456.2362479},
doi = {10.1145/2362456.2362479},
abstract = {Although the field has led to promising early results, the use of crowdsourcing as an integral part of science projects is still regarded with skepticism by some, largely due to a lack of awareness of the opportunities and implications of utilizing these new techniques. We address this lack of awareness, firstly by highlighting the positive impacts that crowdsourcing has had on Natural Language Processing research. Secondly, we discuss the challenges of more complex methodologies, quality control, and the necessity to deal with ethical issues. We conclude with future trends and opportunities of crowdsourcing for science, including its potential for disseminating results, making science more accessible, and enriching educational programs.},
booktitle = {Proceedings of the 12th International Conference on Knowledge Management and Knowledge Technologies},
articleno = {17},
numpages = {8},
keywords = {resource acquisition, natural language processing, games with a purpose, crowdsourcing},
location = {Graz, Austria},
series = {i-KNOW '12}
}

@inproceedings{10.1109/FOCS.2014.36,
author = {Anari, Nima and Goel, Gagan and Nikzad, Afshin},
title = {Mechanism Design for Crowdsourcing: An Optimal 1-1/e Competitive Budget-Feasible Mechanism for Large Markets},
year = {2014},
isbn = {9781479965175},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/FOCS.2014.36},
doi = {10.1109/FOCS.2014.36},
abstract = {In this paper we consider a mechanism design problem in the context of large-scale crowdsourcing markets such as Amazon's Mechanical Turk mturk, ClickWorker clickworker, CrowdFlower crowdflower. In these markets, there is a requester who wants to hire workers to accomplish some tasks. Each worker is assumed to give some utility to the requester on getting hired. Moreover each worker has a minimum cost that he wants to get paid for getting hired. This minimum cost is assumed to be private information of the workers. The question then is--if the requester has a limited budget, how to design a direct revelation mechanism that picks the right set of workers to hire in order to maximize the requester's utility? We note that although the previous work (Singer (2010) chen et al. (2011)) has studied this problem, a crucial difference in which we deviate from earlier work is the notion of large-scale markets that we introduce in our model. Without the large market assumption, it is known that no mechanism can achieve a competitive ratio better than 0.414 and 0.5 for deterministic and randomized mechanisms respectively (while the best known deterministic and randomized mechanisms achieve an approximation ratio of 0.292 and 0.33 respectively). In this paper, we design a budget-feasible mechanism for large markets that achieves a competitive ratio of 1--1/e = 0.63. Our mechanism can be seen as a generalization of an alternate way to look at the proportional share mechanism, which is used in all the previous works so far on this problem. Interestingly, we can also show that our mechanism is optimal by showing that no truthful mechanism can achieve a factor better than 1--1/e, thus, fully resolving this setting. Finally we consider the more general case of submodular utility functions and give new and improved mechanisms for the case when the market is large.},
booktitle = {Proceedings of the 2014 IEEE 55th Annual Symposium on Foundations of Computer Science},
pages = {266–275},
numpages = {10},
keywords = {Truthful Mechanisms, Large Markets, Crowdsourcing, Budget-feasibility},
series = {FOCS '14}
}

@inproceedings{10.1145/2740908.2744109,
author = {Difallah, Djellel Eddine and Catasta, Michele and Demartini, Gianluca and Ipeirotis, Panagiotis G. and Cudr\'{e}-Mauroux, Philippe},
title = {The Dynamics of Micro-Task Crowdsourcing: The Case of Amazon MTurk},
year = {2015},
isbn = {9781450334730},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2740908.2744109},
doi = {10.1145/2740908.2744109},
abstract = {Micro-task crowdsourcing is rapidly gaining popularity among research communities and businesses as a means to leverage Human Computation in their daily operations. Unlike any other service, a crowdsourcing platform is in fact a marketplace subject to human factors that affect its performance, both in terms of speed and quality. Indeed, such factors shape the emph{dynamics} of the crowdsourcing market. For example, a known behavior of such markets is that increasing the reward of a set of tasks would lead to faster results. However, it is still unclear how different dimensions interact with each other: reward, task type, market competition, requester reputation, etc.In this paper, we adopt a data-driven approach to (A) perform a long-term analysis of a popular micro-task crowdsourcing platform and understand the evolution of its main actors (workers, requesters, tasks, and platform). (B) We leverage the main findings of our five year log analysis to propose features used in a predictive model aiming at determining the expected performance of any batch at a specific point in time. We show that the number of tasks left in a batch and how recent the batch is are two key features of the prediction. (C) Finally, we conduct an analysis of the demand (new tasks posted by the requesters) and supply (number of tasks completed by the workforce) and show how they affect task prices on the marketplace.},
booktitle = {Proceedings of the 24th International Conference on World Wide Web},
pages = {617},
numpages = {1},
keywords = {trend identification, tracking, forecasting, crowdsourcing},
location = {Florence, Italy},
series = {WWW '15 Companion}
}

@inproceedings{10.1109/CCNC.2018.8319237,
author = {Liu, Zhan and Shabani, Shaban and Balet, Nicole Glassey and Sokhn, Maria and Cretton, Fabian},
title = {How to motivate participation and improve quality of crowdsourcing when building accessibility maps},
year = {2018},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/CCNC.2018.8319237},
doi = {10.1109/CCNC.2018.8319237},
abstract = {Crowdsourcing, as one of the most promising techniques for distributed problem-solving, requires sustained human involvement. Therefore, it also brings new challenges to data management, fundamentally data input and its quality. In this paper, we looked at various forms of user motivations and quality control of crowdsourcing when building accessibility maps mobile applications. We discuss how motivations could be used to contribute to our accessibility maps scenarios, and how data can be improved for two types of participants: individual participants and organization participants. We identified three useful techniques for improving data quality: qualification-based, reputation-based, and aggregation-based. In addition, based on our own mobile application (named WEMAP), we evaluated our approaches through focus group discussions and in-depth interviews.},
booktitle = {2018 15th IEEE Annual Consumer Communications \&amp; Networking Conference (CCNC)},
pages = {1–6},
numpages = {6},
location = {Las Vegas, NV, USA}
}

@inproceedings{10.1145/2872518.2890087,
author = {Schnitzer, Steffen and Neitzel, Svenja and Schmidt, Sebastian and Rensing, Christoph},
title = {Perceived Task Similarities for Task Recommendation in Crowdsourcing Systems},
year = {2016},
isbn = {9781450341448},
publisher = {International World Wide Web Conferences Steering Committee},
address = {Republic and Canton of Geneva, CHE},
url = {https://doi.org/10.1145/2872518.2890087},
doi = {10.1145/2872518.2890087},
abstract = {Crowdsourcing platforms support the assignment of jobs while relying on the workers' search capabilities. Recommenders can support the workers' decisions to improve quality and outcome for both worker and requester. A precedent study showed, that many workers expect to get tasks recommended, which are similar to previously finished ones. In order to create genuine task recommendation, similarities between tasks have to be identified and analyzed. Therefore, this work provides an empirical study about how workers perceive task similarities. The perceived task similarities may vary between workers with different cultural background and may depend e.g. on the complexity, required action or the requester of the task.},
booktitle = {Proceedings of the 25th International Conference Companion on World Wide Web},
pages = {585–590},
numpages = {6},
keywords = {user survey, recommender systems, crowdsourcing},
location = {Montr\'{e}al, Qu\'{e}bec, Canada},
series = {WWW '16 Companion}
}

@inproceedings{10.1109/CSI-SE.2017.1,
author = {Diaz-Mosquera, Juan D. and Sanabria, Pablo and Neyem, Andres and Parra, Denis and Navon, Jaime},
title = {Enriching capstone project-based learning experiences using a crowdsourcing recommender engine},
year = {2017},
isbn = {9781538640418},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/CSI-SE.2017.1},
doi = {10.1109/CSI-SE.2017.1},
abstract = {Capstone project-based learning courses generate a suitable space where students can put into action knowledge specific to an area. In the case of Software Engineering (SE), students must apply knowledge at the level of Analysis, Design, Development, Implementation and Management of Software Projects. There is a large number of supportive resources for SE that one can find on the web, however, information overload ends up saturating the students who wish to find resources more accurate depending on their needs. This is why we propose a crowdsourcing recommender engine as part of an educational software platform. This engine based its recommendations on content from StackExchange posts using the project's profile in which a student is currently working. To generate the project's profile, our engine takes advantage of the information stored by students in the aforementioned platform.Content-based algorithms based on Okapi BM25 and Latent Dirichlet Allocation (LDA) are used to provide suitable recommendations. The evaluation of the engine was held with students from the capstone course in SE of the University Catholic of Chile. Results show that Cosine similarity over traditional bag-of-words TF-IDF content vectors yield interesting results, but they are outperformed by the integration of BM25 with LDA.},
booktitle = {Proceedings of the 4th International Workshop on CrowdSourcing in Software Engineering},
pages = {25–29},
numpages = {5},
keywords = {software engineering, recommender systems, crowdsourcing, capstones},
location = {Buenos Aires, Argentina},
series = {CSI-SE '17}
}

@inproceedings{10.1109/SOSE.2014.63,
author = {Chu, Chih-Han and Wan, Menghsi and Yang, Yufan and Gao, Jerry and Deng, Lei},
title = {Building On-demand Marketing SaaS for Crowdsourcing},
year = {2014},
isbn = {9781479936168},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/SOSE.2014.63},
doi = {10.1109/SOSE.2014.63},
abstract = {The concept of crowdsourcing has been introduced for years and greatly accepted by companies and enterprises. As opposed to current marketing systems, this paper designs a new marketing system by merging the idea of crowdsourcing into marketing information system (MkIS). Our purpose is to develop a cloud-based MkIS which serves as a platform and provides info publishing/tracking via various media and crowdsourcing features. To ensure accessibility and cost-efficiency, the product will be deployed as Software as a Service (SaaS), which will be capable of supporting large number of customers. The implementation of this idea is to construct a prototype system which targets enterprise and crowdsourcing service providers. The outcome has two deliverables. One is application server, which hosts the service API for web clients. Another is database server, which stores and replicates application data. The work serves as one of the first attempts for an information system flexible enough to adapt marketing, crowdsourcing, and other business workflows. Moreover, it shows the potential of the form of MkIS. This paper also presents the design and implementation of the system.},
booktitle = {Proceedings of the 2014 IEEE 8th International Symposium on Service Oriented System Engineering},
pages = {430–438},
numpages = {9},
keywords = {workflow, software as a service, marketing information system, crowdsourcing, SaaS, MkIS},
series = {SOSE '14}
}

@inproceedings{10.1109/UCC.2013.99,
author = {Springer, Thomas and Hara, Tenshi and Schill, Alexander},
title = {On Providing Crowdsourcing as a Service},
year = {2013},
isbn = {9780769551524},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/UCC.2013.99},
doi = {10.1109/UCC.2013.99},
abstract = {In this paper we present an approach to separate submission capturing and processing from basic crowd sourcing functionality to create reusable crowd sourcing services which are more flexible and secure.},
booktitle = {Proceedings of the 2013 IEEE/ACM 6th International Conference on Utility and Cloud Computing},
pages = {510–511},
numpages = {2},
keywords = {proxy approach, implicit and explicit crowdsourcing, crowdsourcing platform},
series = {UCC '13}
}

@inproceedings{10.1109/HASE.2016.20,
author = {Awwad, Tarek and Bennani, Nadia and Brunie, Lionel and Coquil, David and Kosch, Harald and Rehn-Sonigo, Veronika},
title = {Task Characterization for an Effective Worker Targeting in Crowdsourcing},
year = {2016},
isbn = {9781467399135},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/HASE.2016.20},
doi = {10.1109/HASE.2016.20},
abstract = {In the last decade, crowdsourcing (CS) has emergedas a very promising approach for obtaining services, feedbackor data from a large number of people connected through theInternet, in a short time and at a reasonable cost. CS has beenused in a large range of contexts, thus proving its versatility. However, the quality of the services or data provided by theworkers (the "crowd") is not guaranteed, and therefore mustbe verified. This verification usually results in additional timeand cost. We propose a novel approach of quality control incrowdsourcing that reduces, and in some cases eliminates, thisoverhead. Our approach uses a learning technique to characterizeand cluster tasks, and selects, within the available crowd, the mostreliable group of workers for a given type of tasks.},
booktitle = {Proceedings of the 2016 IEEE 17th International Symposium on High Assurance Systems Engineering (HASE)},
pages = {63–64},
numpages = {2},
series = {HASE '16}
}

@inproceedings{10.5555/2832249.2832277,
author = {Yin, Ming and Chen, Yiling},
title = {Bonus or not? learn to reward in crowdsourcing},
year = {2015},
isbn = {9781577357384},
publisher = {AAAI Press},
abstract = {Recent work has shown that the quality of work produced in a crowdsourcing working session can be influenced by the presence of performance-contingent financial incentives, such as bonuses for exceptional performance, in the session. We take an algorithmic approach to decide when to offer bonuses in a working session to improve the overall utility that a requester derives from the session. Specifically, we propose and train an input-output hidden Markov model to learn the impact of bonuses on work quality and then use this model to dynamically decide whether to offer a bonus on each task in a working session to maximize a requester's utility. Experiments on Amazon Mechanical Turk show that our approach leads to higher utility for the requester than fixed and random bonus schemes do. Simulations on synthesized data sets further demonstrate the robustness of our approach against different worker population and worker behavior in improving requester utility.},
booktitle = {Proceedings of the 24th International Conference on Artificial Intelligence},
pages = {201–207},
numpages = {7},
location = {Buenos Aires, Argentina},
series = {IJCAI'15}
}

@inproceedings{10.1145/2542355.2542388,
author = {Tan, Chek Tien and Rosser, Daniel and Harrold, Natalie},
title = {Crowdsourcing facial expressions using popular gameplay},
year = {2013},
isbn = {9781450326292},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2542355.2542388},
doi = {10.1145/2542355.2542388},
abstract = {Facial expression analysis systems often employ machine learning algorithms that depend a lot on the quality of the face database they are trained on. Unfortunately, generating high quality face databases is a major challenge that is rather time consuming. We have developed BeFaced, a tile-matching casual tablet game to enable massive crowdsourcing of facial expressions for the purpose of such machine learning algorithms. Based on the popular tile-matching gameplay mechanic, players are required to make facial expressions shown on matched tiles in order to clear them and advance in the game. Dynamic difficulty adjustment of the recognition accuracy is employed in the game in order to increase engagement and hence increase the quantity of varied facial expressions obtained. Each facial expression is automatically captured, labelled and sent to our online face database. At a more abstract level, BeFaced investigates a novel method of using popular game mechanics to aid the advancement of computer vision algorithms.},
booktitle = {SIGGRAPH Asia 2013 Technical Briefs},
articleno = {26},
numpages = {4},
keywords = {serious games, facial expression analysis, crowdsourcing},
location = {Hong Kong, Hong Kong},
series = {SA '13}
}

@inproceedings{10.1109/CBI.2014.16,
author = {Lofi, Christoph and Maarry, Kinda El},
title = {Design Patterns for Hybrid Algorithmic-Crowdsourcing Workflows},
year = {2014},
isbn = {9781479957798},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/CBI.2014.16},
doi = {10.1109/CBI.2014.16},
abstract = {Crowd sourcing has shown to be a powerful technique for over-coming many challenges in data and information processing where current state-of-the-art algorithms are still struggling. This is especially true for workflows that transparently combine algorithmic heuristics and dynamically crowd sourced tasks that are performed by human workers, and which promise to solve even more complex tasks effectively and efficiently. But still, such hybrid crowd sourcing workflows can be difficult to approach, and they are often designed in an ad-hoc fashion. Therefore, in this paper, we extensively investigate such crowd sourcing workflows as described in the literature, and abstract generic design patterns, which codify commonly recurring challenges and their best-practice solutions. Each design pattern is described and discussed with a special focus on its requirements, constraints, and effects on the overall workflow. We illustrate the practicality of these patterns by providing real-world application examples where such patterns can or have been applied. Furthermore, we show-case how the individual design patterns can be extended and combined to support more complex workflows. Our design patterns provide an extensive overview of the hybrid crowd sourcing workflows' design space, and allow for a more efficient modeling, analysis, and documentation of such workflows.},
booktitle = {Proceedings of the 2014 IEEE 16th Conference on Business Informatics - Volume 01},
pages = {1–8},
numpages = {8},
keywords = {information processing, crowdsourcing, workflows, design patterns},
series = {CBI '14}
}

@inproceedings{10.1109/ICALT.2014.151,
author = {Erdt, Mojisola and Rensing, Christoph},
title = {Evaluating Recommender Algorithms for Learning Using Crowdsourcing},
year = {2014},
isbn = {9781479940387},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/ICALT.2014.151},
doi = {10.1109/ICALT.2014.151},
abstract = {Keeping focused on a certain goal or topic when learning with resources found on the Web is a challenge. Creating a hierarchical learning goal structure with activities and sub-activities can help the learner to keep on track. Moreover, providing useful recommendations to such activities can further support the learner. However, recommendations need to be relevant to the specific goal or activity the learner is currently working on, as well as being novel and diverse to the learner. Such user-centric metrics like novelty and diversity are best measured by asking the users themselves. Nonetheless, conducting user experiments are notoriously time-consuming and access to an adequate amount of users is often very limited. Crowd sourcing offers a means to evaluate TEL recommender algorithms by reaching out to sufficient participants in a shorter time-frame and with less effort. In this paper, a concept for evaluating TEL recommender algorithms using crowd sourcing is presented as well as a repeated proof-of-concept evaluation experiment of a TEL graph-based recommender algorithm AScore that exploits hierarchical activity structures. Results from both experiments support the postulated hypotheses, thereby showing that crowd sourcing can be successfully applied to evaluate TEL recommender algorithms.},
booktitle = {Proceedings of the 2014 IEEE 14th International Conference on Advanced Learning Technologies},
pages = {513–517},
numpages = {5},
keywords = {crowdsourcing, TEL, recommender systems, evaluation methods},
series = {ICALT '14}
}

@inproceedings{10.1145/2600428.2609577,
author = {Zhang, Yinglong and Zhang, Jin and Lease, Matthew and Gwizdka, Jacek},
title = {Multidimensional relevance modeling via psychometrics and crowdsourcing},
year = {2014},
isbn = {9781450322577},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2600428.2609577},
doi = {10.1145/2600428.2609577},
abstract = {While many multidimensional models of relevance have been posited, prior studies have been largely exploratory rather than confirmatory. Lacking a methodological framework to quantify the relationships among factors or measure model fit to observed data, many past models could not be empirically tested or falsified. To enable more positivist experimentation, Xu and Chen [77] proposed a psychometric framework for multidimensional relevance modeling. However, we show their framework exhibits several methodological limitations which could call into question the validity of findings drawn from it. In this work, we identify and address these limitations, scale their methodology via crowdsourcing, and describe quality control methods from psychometrics which stand to benefit crowdsourcing IR studies in general. Methodology we describe for relevance judging is expected to benefit both human-centered and systems-centered IR.},
booktitle = {Proceedings of the 37th International ACM SIGIR Conference on Research \&amp; Development in Information Retrieval},
pages = {435–444},
numpages = {10},
keywords = {relevance judgment, psychometrics, crowdsourcing},
location = {Gold Coast, Queensland, Australia},
series = {SIGIR '14}
}

@inproceedings{10.1145/2660168.2660186,
author = {Saitis, Charalampos and Hankinson, Andrew and Fujinaga, Ichiro},
title = {Correcting Large-Scale OMR Data with Crowdsourcing},
year = {2014},
isbn = {9781450330022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2660168.2660186},
doi = {10.1145/2660168.2660186},
abstract = {This paper discusses several technical challenges in using crowdsourcing for distributed correction interfaces. The specific scenario under investigation involves the implementation of a crowd-sourced adaptive optical music recognition system (Single Interface for Music Score Searching and Analysis project). We envisage the distribution of correction tasks beyond a single workstation to potentially thousands of users around the globe. This will have the effect of producing human-checked transcriptions, as well as significant quantities of human-provided ground-truth data, which may be re-integrated into an adaptive recognition process, allowing an OMR system to "learn" from its mistakes. Drawing from existing crowdsourcing approaches and user interfaces in music (e.g., Bodleian Libraries) and non-music (e.g., CAPTCHAs) applications, this project aims to develop a scientific understanding of what makes crowdsourcing work, how to entice, engage and reward contributors, and how to evaluate their reliability. While results will be considered based on the specific needs of SIMSSA, such knowledge can be useful to a variety of musicological investigations that involve labour-intensive methods.},
booktitle = {Proceedings of the 1st International Workshop on Digital Libraries for Musicology},
pages = {1–3},
numpages = {3},
keywords = {web applications, music score searching, music notation, crowdsourcing, Optical music recognition},
location = {London, United Kingdom},
series = {DLfM '14}
}

@inproceedings{10.1145/2615569.2615644,
author = {Oosterman, Jasper and Nottamkandath, Archana and Dijkshoorn, Chris and Bozzon, Alessandro and Houben, Geert-Jan and Aroyo, Lora},
title = {Crowdsourcing knowledge-intensive tasks in cultural heritage},
year = {2014},
isbn = {9781450326223},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2615569.2615644},
doi = {10.1145/2615569.2615644},
abstract = {Large datasets such as Cultural Heritage collections require detailed annotations when digitised and made available online. Annotating different aspects of such collections requires a variety of knowledge and expertise which is not always possessed by the collection curators. Artwork annotation is an example of a knowledge intensive image annotation task, i.e. a task that demands annotators to have domain-specific knowledge in order to be successfully completed.This paper describes the results of a study aimed at investigating the applicability of crowdsourcing techniques to knowledge intensive image annotation tasks. We observed a clear relationship between the annotation difficulty of an image, in terms of number of items to identify and annotate, and the performance of the recruited workers.},
booktitle = {Proceedings of the 2014 ACM Conference on Web Science},
pages = {267–268},
numpages = {2},
keywords = {knowledge intensive tasks, cultural heritage, crowdsourcing},
location = {Bloomington, Indiana, USA},
series = {WebSci '14}
}

@inproceedings{10.1145/2505515.2507858,
author = {Cheng, Yu and Chen, Zhengzhang and Wang, Jiang and Agrawal, Ankit and Choudhary, Alok},
title = {Bootstrapping active name disambiguation with crowdsourcing},
year = {2013},
isbn = {9781450322638},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2505515.2507858},
doi = {10.1145/2505515.2507858},
abstract = {Name disambiguation is a challenging and important problem in many domains, such as digital libraries, social media management and people search systems. Traditional methods, based on direct assignment using supervised machine learning techniques, seem to be the most effective, but their performances are highly dependent on the amount of training data, while large data annotation can be expensive and time-consuming requiring hours of manual inspection by a domain expert. To efficiently acquire labeled data, we propose a bootstrapping algorithm for the name disambiguation task based on active learning and crowdsourced labeling. We show that the proposed method can leverage the advantages of exploration and exploitation by combining two strategies, thereby improving the overall quality of the training data at minimal expense. The experimental results on two datasets DBLP and ArnetMiner demonstrate the superiority of our framework over existing methods.},
booktitle = {Proceedings of the 22nd ACM International Conference on Information \&amp; Knowledge Management},
pages = {1213–1216},
numpages = {4},
keywords = {active learning, bootstrapping, crowdsourcing, name disambiguation},
location = {San Francisco, California, USA},
series = {CIKM '13}
}

@inproceedings{10.1109/WI-IAT.2015.81,
author = {Ashikawa, Masayuki and Kawamura, Takahiro and Ohsuga, Akihiko},
title = {Deployment of Private Crowdsourcing System with Quality Control Methods},
year = {2015},
isbn = {9781467396189},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/WI-IAT.2015.81},
doi = {10.1109/WI-IAT.2015.81},
abstract = {Current crowdsourcing platforms such as Amazon Mechanical Turk provide an attractive solution for processing of high-volume tasks at low cost. However, problems of quality control remain a major concern. In the present work, we developed a private crowdsourcing system(PCSS) running in a intranetwork, that allow us to devise for quality control methods. For quality control, we introduce four worker selection methods: preprocessing filtering, real-time filtering, post-processing filtering, and guess processing filtering. In addition to a basic approach involving initial training or the use of gold standard data, these methods include a novel approach, utilizing collaborative filtering techniques. Furthermore, we collected a large amount of vocabulary data for natural language processing, such as voice recognition and text to speech using PCSS. The quality control methods increased accuracy by 32.4\% in collecting vocabulary task. Then, we got 138 thousand vocabulary data. We found that PCSS is a practical system to collect data, and used for three years since 2011.},
booktitle = {Proceedings of the 2015 IEEE / WIC / ACM International Conference on Web Intelligence and Intelligent Agent Technology (WI-IAT) - Volume 01},
pages = {9–16},
numpages = {8},
series = {WI-IAT '15}
}

@inproceedings{10.1145/3248609,
author = {Goncalves, Jorge and Hosio, Simo and Vukovic, Maja and Konomi, Shin'ichi and Lee, Uichin},
title = {Session details: (WMSC) second workshop on mobile and situated crowdsourcing},
year = {2016},
isbn = {9781450344623},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3248609},
doi = {10.1145/3248609},
booktitle = {Proceedings of the 2016 ACM International Joint Conference on Pervasive and Ubiquitous Computing: Adjunct},
location = {Heidelberg, Germany},
series = {UbiComp '16}
}

@inproceedings{10.1145/2745844.2745871,
author = {Lee, Donghyeon and Kim, Joonyoung and Lee, Hyunmin and Jung, Kyomin},
title = {Reliable Multiple-choice Iterative Algorithm for Crowdsourcing Systems},
year = {2015},
isbn = {9781450334860},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2745844.2745871},
doi = {10.1145/2745844.2745871},
abstract = {The appearance of web-based crowdsourcing systems gives a promising solution to exploiting the wisdom of crowds efficiently in a short time with a relatively low budget. Despite their efficiency, crowdsourcing systems have an inherent problem in that responses from workers can be unreliable since workers are low-paid and have low responsibility. Although simple majority voting can be a solution, various research studies have sought to aggregate noisy responses to obtain greater reliability in results through effective techniques such as Expectation-Maximization (EM) based algorithms. While EM-based algorithms get the limelight in crowdsourcing systems due to their useful inference techniques, Karger et al. made a significant breakthrough by proposing a novel iterative algorithm based on the idea of low-rank matrix approximations and the message passing technique. They showed that the performance of their iterative algorithm is order-optimal, which outperforms majority voting and EM-based algorithms. However, their algorithm is not always applicable in practice since it can only be applied to binary-choice questions. Recently, they devised an inference algorithm for multi-class labeling, which splits each task into a bunch of binary-choice questions and exploits their existing algorithm. However, it has difficulty in combining into real crowdsourcing systems since it overexploits redundancy in that each split question should be queried in multiple times to obtain reliable results.In this paper, we design an iterative algorithm to infer true answers for multiple-choice questions, which can be directly applied to real crowdsourcing systems. Our algorithm can also be applicable to short-answer questions as well. We analyze the performance of our algorithm, and prove that the error bound decays exponentially. Through extensive experiments, we verify that our algorithm outperforms majority voting and EM-based algorithm in accuracy.},
booktitle = {Proceedings of the 2015 ACM SIGMETRICS International Conference on Measurement and Modeling of Computer Systems},
pages = {205–216},
numpages = {12},
keywords = {resource allocation, multiple-choice, iterative learning, crowdsourcing},
location = {Portland, Oregon, USA},
series = {SIGMETRICS '15}
}

@article{10.1145/2796314.2745871,
author = {Lee, Donghyeon and Kim, Joonyoung and Lee, Hyunmin and Jung, Kyomin},
title = {Reliable Multiple-choice Iterative Algorithm for Crowdsourcing Systems},
year = {2015},
issue_date = {June 2015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {43},
number = {1},
issn = {0163-5999},
url = {https://doi.org/10.1145/2796314.2745871},
doi = {10.1145/2796314.2745871},
abstract = {The appearance of web-based crowdsourcing systems gives a promising solution to exploiting the wisdom of crowds efficiently in a short time with a relatively low budget. Despite their efficiency, crowdsourcing systems have an inherent problem in that responses from workers can be unreliable since workers are low-paid and have low responsibility. Although simple majority voting can be a solution, various research studies have sought to aggregate noisy responses to obtain greater reliability in results through effective techniques such as Expectation-Maximization (EM) based algorithms. While EM-based algorithms get the limelight in crowdsourcing systems due to their useful inference techniques, Karger et al. made a significant breakthrough by proposing a novel iterative algorithm based on the idea of low-rank matrix approximations and the message passing technique. They showed that the performance of their iterative algorithm is order-optimal, which outperforms majority voting and EM-based algorithms. However, their algorithm is not always applicable in practice since it can only be applied to binary-choice questions. Recently, they devised an inference algorithm for multi-class labeling, which splits each task into a bunch of binary-choice questions and exploits their existing algorithm. However, it has difficulty in combining into real crowdsourcing systems since it overexploits redundancy in that each split question should be queried in multiple times to obtain reliable results.In this paper, we design an iterative algorithm to infer true answers for multiple-choice questions, which can be directly applied to real crowdsourcing systems. Our algorithm can also be applicable to short-answer questions as well. We analyze the performance of our algorithm, and prove that the error bound decays exponentially. Through extensive experiments, we verify that our algorithm outperforms majority voting and EM-based algorithm in accuracy.},
journal = {SIGMETRICS Perform. Eval. Rev.},
month = jun,
pages = {205–216},
numpages = {12},
keywords = {resource allocation, multiple-choice, iterative learning, crowdsourcing}
}

@inproceedings{10.1109/iThings/CPSCom.2011.128,
author = {Yuen, Man-Ching and King, Irwin and Leung, Kwong-Sak},
title = {Task Matching in Crowdsourcing},
year = {2011},
isbn = {9780769545806},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/iThings/CPSCom.2011.128},
doi = {10.1109/iThings/CPSCom.2011.128},
abstract = {Crowd sourcing is evolving as a distributed problem-solving and business production model in recent years. In crowd sourcing paradigm, tasks are distributed to networked people to complete such that a company s production cost can be greatly reduced. A crowd sourcing process involves operations of both requesters and workers. A requester submits a task request, a worker selects and completes a task, and the requester only pays the worker for the successful completion of the task. Obviously, it is not efficient that the amount of time spent on selecting a task is comparable with that spent on working on a task, but the monetary reward of a task is just a small amount. Literature mainly focused on exploring what type of tasks can be deployed to the crowd and analyzing the performance of crowd sourcing platforms. However, no existing work investigates on how to support workers to select tasks on crowd sourcing platforms easily and effectively. In this paper, we propose a novel idea on task matching in crowd sourcing to motivate workers to keep on working on crowd sourcing platforms in long run. The idea utilizes the past task preference and performance of a worker to produce a list of available tasks in the order of best matching with the worker during his task selection stage. It aims to increase the efficiency of task completion. We present some preliminary experimental results in case studies. Finally, we address the possible challenges and discuss the future directions.},
booktitle = {Proceedings of the 2011 International Conference on Internet of Things and 4th International Conference on Cyber, Physical and Social Computing},
pages = {409–412},
numpages = {4},
keywords = {task model, task matching algorithm, crowdsourcing},
series = {ITHINGSCPSCOM '11}
}

@inproceedings{10.1145/2674396.2674466,
author = {Boutsis, Ioannis and Tomaras, Dimitrios and Kalogeraki, Vana},
title = {Towards real-time emergency response using crowdsourcing},
year = {2014},
isbn = {9781450327466},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2674396.2674466},
doi = {10.1145/2674396.2674466},
abstract = {Crowdsourcing has emerged as an attractive paradigm in recent years for information collection for disaster response, which utilizes data received from the human crowd, to provide critical information collection and dissemination during emergency situations and visualize this data to generate emergency maps for the human crowd. In this paper we investigate the use of crowdsourcing mechanisms for real-time emergency response and describe our approach for developing a crowdsourcing tool that can be effectively used to formulate questions and seek answers from the human crowd using a MapReduce programming model, and integrate this information into a novel spatiotemporal data structure and create a visual emergency map. Our experimental evaluation shows that our approach is practical, efficient and can be used for applications with real-time demands.},
booktitle = {Proceedings of the 7th International Conference on PErvasive Technologies Related to Assistive Environments},
articleno = {62},
numpages = {4},
keywords = {spatiotemporal data, emergency response, distributed sensor systems, crowdsourcing},
location = {Rhodes, Greece},
series = {PETRA '14}
}

@inproceedings{10.1145/3184558.3191545,
author = {Zequeira Jim\'{e}nez, Rafael and Fern\'{a}ndez Gallardo, Laura and M\"{o}ller, Sebastian},
title = {Outliers Detection vs. Control Questions to Ensure Reliable Results in Crowdsourcing.: A Speech Quality Assessment Case Study},
year = {2018},
isbn = {9781450356404},
publisher = {International World Wide Web Conferences Steering Committee},
address = {Republic and Canton of Geneva, CHE},
url = {https://doi.org/10.1145/3184558.3191545},
doi = {10.1145/3184558.3191545},
abstract = {Crowdsourcing provides an exceptional opportunity for the rapid collection of human input for data acquisition and labelling. This approach have been adopted in multiple domains and researchers are now able to reach a demographically diverse audience at low cost. However, it remains the question of whether the results are still valid and reliable. Previous work have introduced different mechanisms to ensure data reliability in crowdsourcing. This work examines to which extend, "trapping question" or "outliers detection" assure reliable results to the detriment of, overloading task content with stimuli that are not of interest for the researcher, or by discarding data points that might be the true opinion of a worker. To this end, a speech quality assessment study have been conducted in a web crowdsourcing platform, following the ITU-T Rec. P.800. Workers assessed the speech stimuli of the database 501 from the ITU-T Rec. P.863. We examine results' validity in terms of correlations to previous ratings collected in laboratory. Our outcomes shows that neither of the techniques under investigation improve results accuracy by itself, but a combination of both. Our goal is to provide empirical guidance for designing experiments in crowdsourcing while ensuring data reliability.},
booktitle = {Companion Proceedings of the The Web Conference 2018},
pages = {1127–1130},
numpages = {4},
keywords = {crowdsourcing, data validity, gold-strandard questions, outliers detection, speech quality assessment, trapping questions, users reliability},
location = {Lyon, France},
series = {WWW '18}
}

@inproceedings{10.1145/2442657.2442661,
author = {Yuen, Man-Ching and King, Irwin and Leung, Kwong-Sak},
title = {Task recommendation in crowdsourcing systems},
year = {2012},
isbn = {9781450315579},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2442657.2442661},
doi = {10.1145/2442657.2442661},
abstract = {In crowdsourcing systems, tasks are distributed to networked people to complete such that a company's production cost can be greatly reduced. Obviously, it is not efficient that the amount of time for a worker spent on selecting a task is comparable with that spent on working on a task, but the monetary reward of a task is just a small amount. The available worker history makes it possible to mine workers' preference on tasks and to provide favorite recommendations. Our exploratory study on the survey results collected from Amazon Mechanical Turk (MTurk) shows that workers' histories can reflect workers' preferences on tasks in crowdsourcing systems. Task recommendation can help workers to find their right tasks faster as well as help requesters to receive good quality output quicker. However, previously proposed classification based task recommendation approach only considers worker performance history, but does not explore worker task searching history. In our paper, we propose a task recommendation framework for task preference modeling and preference-based task recommendation, aiming to recommend tasks to workers who are likely to prefer to work on and provide output that accepted by requesters. We consider both worker performance history and worker task searching history to reflect workers' task preference more accurately. To the best of our knowledge, we are the first to use matrix factorization for task recommendation in crowdsourcing systems.},
booktitle = {Proceedings of the First International Workshop on Crowdsourcing and Data Mining},
pages = {22–26},
numpages = {5},
keywords = {task recommendation, matrix factorization, crowdsourcing},
location = {Beijing, China},
series = {CrowdKDD '12}
}

@inproceedings{10.5555/2772879.2772901,
author = {Davami, Erfan and Sukthankar, Gita},
title = {Improving the Performance of Mobile Phone Crowdsourcing Applications},
year = {2015},
isbn = {9781450334136},
publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
address = {Richland, SC},
abstract = {Mobile phone crowdsourcing is a powerful tool for many types of distributed sensing problems. However, a central issue with this type of system is that it relies on user contributed data, which may be sparse or erroneous. This paper describes our experiences developing a mobile phone crowdsourcing app, Kpark, for monitoring parking availability on a university campus. Our system combines multiple trust- based data fusion techniques to improve the quality of user submitted parking reports and is currently being used by over 1500 students.},
booktitle = {Proceedings of the 2015 International Conference on Autonomous Agents and Multiagent Systems},
pages = {145–153},
numpages = {9},
keywords = {trust-based fusion, mobile phone crowdsourcing},
location = {Istanbul, Turkey},
series = {AAMAS '15}
}

@inproceedings{10.1145/2685553.2702671,
author = {LaLone, Nicolas and Tapia, Andrea and MacDonald, Elizabeth and Case, Nathan and Hall, Michelle and Clayton, Jessica and Heavner, Matthew J.},
title = {Harnessing Twitter and Crowdsourcing to Augment Aurora Forecasting},
year = {2015},
isbn = {9781450329460},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2685553.2702671},
doi = {10.1145/2685553.2702671},
abstract = {The aurora borealis and aurora australis are beautiful space weather driven events whose sighting is typically based on luck given that forecasting is not spatially or temporally precise. To help increase the accuracy and timeliness of auroral forecasting, we have designed a multi-faceted system called Aurorasaurus. This system allows crisis management specialists to test reactions to rare event notifications, space weather scientists to get direct sighting information of auroras (complete with pictures), and science education researchers to evaluate the impact of educational materials about the aurora and the physics surrounding this unique phenomenon. Through manual tweet verification and directly reported aurora borealis or aurora australis sightings, everyday users help make space weather and aurora forecasting more accurate.},
booktitle = {Proceedings of the 18th ACM Conference Companion on Computer Supported Cooperative Work \&amp; Social Computing},
pages = {9–12},
numpages = {4},
keywords = {twitter, social media, early warning systems, disaster, crisis, aurora borealis},
location = {Vancouver, BC, Canada},
series = {CSCW'15 Companion}
}

@inproceedings{10.1145/2346676.2346685,
author = {Xie, Yusheng and Cheng, Yu and Honbo, Daniel and Zhang, Kunpeng and Agrawal, Ankit and Choudhary, Alok},
title = {Crowdsourcing recommendations from social sentiment},
year = {2012},
isbn = {9781450315432},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2346676.2346685},
doi = {10.1145/2346676.2346685},
abstract = {In this paper, we investigate an innovative recommendation system by incorporating relevant social opinion and sentiment information. Our recommendation system, a powerful application of social sentiment analysis, differs from many existing models, which investigate the situation where the social network itself is structured to work with the product ranking and is specially built inside an e-commerce website. In contrast, our proposed system focuses on constructing and inferring product recommendations from external social network services (SNS) such as Facebook. In our system, we process product features in a finite-dimensional polynomial linear space. Additional components of our proposed system include an asymmetric similarity measurement and an asymmetric advantage measurement. We also show that our definitions for the two measurements include specific properties that reduce the computational overhead in the experiments. An important aspect of our modeling is to incorporate user-generated high-level semantic sentiment in the analysis. We apply our models to real time data and observe promising results for not only product recommendation but also job recommendation.},
booktitle = {Proceedings of the First International Workshop on Issues of Sentiment Discovery and Opinion Mining},
articleno = {9},
numpages = {8},
keywords = {crowdsourcing, business intelligence},
location = {Beijing, China},
series = {WISDOM '12}
}

@inproceedings{10.1109/ASEW.2015.22,
author = {Saremi, Razieh Lotfalian and Yang, Ye},
title = {Empirical Analysis on Parallel Tasks in Crowdsourcing Software Development},
year = {2015},
isbn = {9781467397759},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/ASEW.2015.22},
doi = {10.1109/ASEW.2015.22},
abstract = {Crowdsourcing has become a popular option for rapid acquisition, with reported benefits such as shortened schedule due to mass parallel development, innovative solutions based on the "wisdom of crowds", and reduced cost due to the pre-pricing and bidding effects. However, most of existing studies on software crowdsourcing are focusing on individual task level, providing limited insights on the practice as well as outcomes at overall project level. To develop better understanding of crowdsourcing-based software projects, this paper reports an empirical study on analyzing four largest projects on Topcoder platform that intensively leverage crowdsourcing throughout the product implementation, testing, and assembly phases. The analysis results conclude that: (1) crowdsourcing task scheduling follows typical patterns including prototyping, component development, bug hunt, and assembly and coding (2) budget phase distribution patterns does not following traditional patterns, and uploading task rate is not representing same budget rate associated with them as about 75\% of uploaded tasks would price under 67\% of total project budget, (3) Higher degree of parallelism would lead to higher demand for competing on tasks and shorter planning schedule to complete the project consequently better resource allocation.},
booktitle = {Proceedings of the 2015 30th IEEE/ACM International Conference on Automated Software Engineering Workshop (ASEW)},
pages = {28–34},
numpages = {7},
series = {ASEW '15}
}

@inproceedings{10.1145/2685553.2699324,
author = {Huang, Yi-Ching},
title = {Designing a Micro-Volunteering Platform for Situated Crowdsourcing},
year = {2015},
isbn = {9781450329460},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2685553.2699324},
doi = {10.1145/2685553.2699324},
abstract = {Situated crowdsourcing has emerged to overcome the limitations of online and mobile crowdsourcing to allow people to perform a task by embedding an interface in a physical space. However, crowdsourcing for non-profits is a challenge in situated crowdsourcing platform. My dissertation investigates whether micro-volunteering can be applied successfully to a situated crowdsourcing platform for contributing problem-solving efforts with high-quality results.},
booktitle = {Proceedings of the 18th ACM Conference Companion on Computer Supported Cooperative Work \&amp; Social Computing},
pages = {73–76},
numpages = {4},
keywords = {situated crowdsourcing, micro-volunteering, micro-tasking},
location = {Vancouver, BC, Canada},
series = {CSCW'15 Companion}
}

@inproceedings{10.1007/978-3-642-39320-4_7,
author = {Martin, Ursula and Pease, Alison},
title = {Mathematical practice, crowdsourcing, and social machines},
year = {2013},
isbn = {9783642393198},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-642-39320-4_7},
doi = {10.1007/978-3-642-39320-4_7},
abstract = {The highest level of mathematics has traditionally been seen as a solitary endeavour, to produce a proof for review and acceptance by research peers. Mathematics is now at a remarkable inflexion point, with new technology radically extending the power and limits of individuals. Crowdsourcing pulls together diverse experts to solve problems; symbolic computation tackles huge routine calculations; and computers check proofs too long and complicated for humans to comprehend.The Study of Mathematical Practice is an emerging interdisciplinary field which draws on philosophy and social science to understand how mathematics is produced. Online mathematical activity provides a novel and rich source of data for empirical investigation of mathematical practice - for example the community question-answering system mathoverflow contains around 40,000 mathematical conversations, and polymath collaborations provide transcripts of the process of discovering proofs. Our preliminary investigations have demonstrated the importance of "soft" aspects such as analogy and creativity, alongside deduction and proof, in the production of mathematics, and have given us new ways to think about the roles of people and machines in creating new mathematical knowledge. We discuss further investigation of these resources and what it might reveal.Crowdsourced mathematical activity is an example of a "social machine", a new paradigm, identified by Berners-Lee, for viewing a combination of people and computers as a single problem-solving entity, and the subject of major international research endeavours. We outline a future research agenda for mathematics social machines, a combination of people, computers, and mathematical archives to create and apply mathematics, with the potential to change the way people do mathematics, and to transform the reach, pace, and impact of mathematics research.},
booktitle = {Proceedings of the 2013 International Conference on Intelligent Computer Mathematics},
pages = {98–119},
numpages = {22},
location = {Bath, UK},
series = {CICM'13}
}

@inproceedings{10.1145/2506364.2506366,
author = {Figuerola Salas, \'{O}scar and Adzic, Velibor and Shah, Akash and Kalva, Hari},
title = {Assessing internet video quality using crowdsourcing},
year = {2013},
isbn = {9781450323963},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2506364.2506366},
doi = {10.1145/2506364.2506366},
abstract = {In this paper, we present a subjective video quality evaluation system that has been integrated with different crowdsourcing platforms. We try to evaluate the feasibility of replacing the time consuming and expensive traditional tests with a faster and less expensive crowdsourcing alternative. CrowdFlower and Amazon's Mechanical Turk were used as the crowdsourcing platforms to collect data. The data was compared with the formal subjective tests conducted by MPEG as part of the video standardization process, as well as with previous results from a study we ran at the university level. High quality compressed videos with known Mean Opinion Score (MOS) are used as references instead of the original lossless videos in order to overcome intrinsic bandwidth limitations. The bitrates chosen for the experiment were selected targeting Internet use, since this is the environment in which users were going to be evaluating the videos. Evaluations showed that the results are consistent with formal subjective evaluation scores, and can be reproduced across different crowds with low variability, which makes this type of test setting very promising.},
booktitle = {Proceedings of the 2nd ACM International Workshop on Crowdsourcing for Multimedia},
pages = {23–28},
numpages = {6},
keywords = {subjective quality, quality assessment, mos, mean opinion score, internet video quality, crowdsourcing},
location = {Barcelona, Spain},
series = {CrowdMM '13}
}

@inproceedings{10.1145/2962132.2962134,
author = {Conboy, Kieran and Cullina, Eoin and Morgan, Lorraine},
title = {A Crowdsourcing Practices Framework for Public Scientific Research Funding Agencies},
year = {2016},
isbn = {9781450344814},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2962132.2962134},
doi = {10.1145/2962132.2962134},
booktitle = {Proceedings of the 12th International Symposium on Open Collaboration Companion},
articleno = {2},
numpages = {4},
keywords = {practices, framework, Crowdsourcing},
location = {Berlin, Germany},
series = {OpenSym '16}
}

@inproceedings{10.5555/2021773.2021817,
author = {Sakamoto, Yasuaki and Tanaka, Yuko and Yu, Lixiu and Nickerson, Jeffrey V.},
title = {The crowdsourcing design space},
year = {2011},
isbn = {9783642218514},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {Crowdsourcing is a new kind of organizational structure, one that is conducive to large amounts of short parallel work: thousands of individuals may work for several minutes on tasks, their outputs aggregated into a useful product or service. The dimensions of this new organizational form are described. Areas for future research are identified, focusing on open-ended tasks and the coordination structures that might foster collective creativity.},
booktitle = {Proceedings of the 6th International Conference on Foundations of Augmented Cognition: Directing the Future of Adaptive Systems},
pages = {346–355},
numpages = {10},
keywords = {peer production, organizational design, human computation, distributed cognition, crowdsourcing, collective creativity},
location = {Orlando, FL},
series = {FAC'11}
}

@inproceedings{10.1007/978-3-642-31095-9_30,
author = {Khazankin, Roman and Schall, Daniel and Dustdar, Schahram},
title = {Predicting qos in scheduled crowdsourcing},
year = {2012},
isbn = {9783642310942},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-642-31095-9_30},
doi = {10.1007/978-3-642-31095-9_30},
abstract = {Crowdsourcing has emerged as a new paradigm for outsourcing simple for humans yet hard to automate tasks to an undefined network of people. Crowdsourcing platforms like Amazon Mechanical Turk provide scalability and flexibility for customers that need to get manifold similar independent jobs done. However, such platforms do not provide certain guarantees for their services regarding the expected job quality and the time of processing, although such guarantees are advantageous from the perspective of Business Process Management. In this paper, we consider an alternative architecture of a crowdsourcing platform, where the workers are assigned to tasks by the platform according to their availability and skills. We propose the technique for estimating accomplishable guarantees and negotiating Service Level Agreements in such an environment.},
booktitle = {Proceedings of the 24th International Conference on Advanced Information Systems Engineering},
pages = {460–472},
numpages = {13},
keywords = {scheduling, negotiation, crowdsourcing, SLA, QoS},
location = {Gda\'{n}sk, Poland},
series = {CAiSE'12}
}

@inproceedings{10.1109/IECON.2019.8927134,
author = {Nie, Yan and Xu, Kun and Chen, Haoyan and Peng, Lei},
title = {Crowd-parking: A New Idea of Parking Guidance Based on Crowdsourcing of Parking Location Information from Automobiles},
year = {2019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/IECON.2019.8927134},
doi = {10.1109/IECON.2019.8927134},
abstract = {City-wide parking guidance system (CPGS) is the emerging infrastructure of intelligent transportation systems in China. The most difficulty in CPGS development is the lack of parking lots data. Without these data, the traditional well-designed parking guidance algorithms will fail. Meanwhile, the real-time location information of automobiles is increasing and can be accessed easily thanks to the popularity of smartphone and smart vehicular devices. In this paper, we propose a new idea of guiding automobiles to proper parking lots by using the data collected from the vehicular Global Positioning System (GPS) device. Then we build a spatial-temporal classifier based on convolutional neural network (CNN) with long-short term memory (LSTM) to learn the parking experience from the automobiles uploading the parking locations. Finally, we use the classifier to recommend the proper parking lots nearby to vehicles, considering the related driving context. The experiment shows the proposed method can guide automobiles to the target parking lots accurately even if we do not know the real-time data of the target parking lots. Moreover, the classifier shows the obvious spatial characteristics and preference for the first time when guiding vehicles. This method is a kind of crowdsourcing and encourages automobiles to share their parking experience, to help park easier for themselves eventually. So, we think the method, crowd-parking, is very novel and competitive in the current state of endless business and technique negotiation among parking lots.},
booktitle = {IECON 2019 - 45th Annual Conference of the IEEE Industrial Electronics Society},
pages = {2779–2784},
numpages = {6},
location = {Lisbon, Portugal}
}

@inproceedings{10.1145/2348543.2348567,
author = {Yang, Dejun and Xue, Guoliang and Fang, Xi and Tang, Jian},
title = {Crowdsourcing to smartphones: incentive mechanism design for mobile phone sensing},
year = {2012},
isbn = {9781450311595},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2348543.2348567},
doi = {10.1145/2348543.2348567},
abstract = {Mobile phone sensing is a new paradigm which takes advantage of the pervasive smartphones to collect and analyze data beyond the scale of what was previously possible. In a mobile phone sensing system, the platform recruits smartphone users to provide sensing service. Existing mobile phone sensing applications and systems lack good incentive mechanisms that can attract more user participation. To address this issue, we design incentive mechanisms for mobile phone sensing. We consider two system models: the platform-centric model where the platform provides a reward shared by participating users, and the user-centric model where users have more control over the payment they will receive. For the platform-centric model, we design an incentive mechanism using a Stackelberg game, where the platform is the leader while the users are the followers. We show how to compute the unique Stackelberg Equilibrium, at which the utility of the platform is maximized, and none of the users can improve its utility by unilaterally deviating from its current strategy. For the user-centric model, we design an auction-based incentive mechanism, which is computationally efficient, individually rational, profitable, and truthful. Through extensive simulations, we evaluate the performance and validate the theoretical properties of our incentive mechanisms.},
booktitle = {Proceedings of the 18th Annual International Conference on Mobile Computing and Networking},
pages = {173–184},
numpages = {12},
keywords = {mobile phone sensing, incentive mechanism design, crowdsourcing},
location = {Istanbul, Turkey},
series = {Mobicom '12}
}

@inproceedings{10.1145/3244827,
author = {Kumara, Soundar},
title = {Session details: Crowdsourcing landscape},
year = {2011},
isbn = {9781450309271},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3244827},
doi = {10.1145/3244827},
booktitle = {Proceedings of the 2nd International Workshop on Ubiquitous Crowdsouring},
location = {Beijing, China},
series = {UbiCrowd '11}
}

@inproceedings{10.1145/2806416.2806460,
author = {Davtyan, Martin and Eickhoff, Carsten and Hofmann, Thomas},
title = {Exploiting Document Content for Efficient Aggregation of Crowdsourcing Votes},
year = {2015},
isbn = {9781450337946},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2806416.2806460},
doi = {10.1145/2806416.2806460},
abstract = {The use of crowdsourcing for document relevance assessment has been found to be a viable alternative to corpus annotation by highly trained experts. The question of quality control is a recurring challenge that is often addressed by aggregating multiple individual assessments of the same topic-document pair from independent workers. In the past, such aggregation schemes have been weighted or filtered by estimates of worker reliability based on a multitude of behavioral features. In this paper, we propose an alternative approach by relying on document information. Inspired by the clustering hypothesis of information retrieval, we assume textually similar documents to show similar degrees of relevance towards a given topic. Following up on this intuition, we propagate crowd-generated relevance judgments to similar documents, effectively smoothing the distribution of relevance labels across the similarity space.Our experiments are based on TREC Crowdsourcing Track data and show that even simple aggregation methods utilizing document similarity information significantly improve over majority voting in terms of accuracy as well as cost efficiency.},
booktitle = {Proceedings of the 24th ACM International on Conference on Information and Knowledge Management},
pages = {783–790},
numpages = {8},
keywords = {relevance assessment, crowdsourcing, clustering hypothesis},
location = {Melbourne, Australia},
series = {CIKM '15}
}

@inproceedings{10.5555/2018966.2018975,
author = {Rumshisky, Anna},
title = {Crowdsourcing word sense definition},
year = {2011},
isbn = {9781932432930},
publisher = {Association for Computational Linguistics},
address = {USA},
abstract = {In this paper, we propose a crowdsourcing methodology for a single-step construction of both an empirically-derived sense inventory and the corresponding sense-annotated corpus. The methodology taps the intuitions of non-expert native speakers to create an expert-quality resource, and natively lends itself to supplementing such a resource with additional information about the structure and reliability of the produced sense inventories. The resulting resource will provide several ways to empirically measure distances between related word senses, and will explicitly address the question of fuzzy boundaries between them.},
booktitle = {Proceedings of the 5th Linguistic Annotation Workshop},
pages = {74–81},
numpages = {8},
location = {Portland, Oregon},
series = {LAW V '11}
}

@inproceedings{10.1145/2465529.2465761,
author = {Karger, David R. and Oh, Sewoong and Shah, Devavrat},
title = {Efficient crowdsourcing for multi-class labeling},
year = {2013},
isbn = {9781450319003},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2465529.2465761},
doi = {10.1145/2465529.2465761},
abstract = {Crowdsourcing systems like Amazon's Mechanical Turk have emerged as an effective large-scale human-powered platform for performing tasks in domains such as image classification, data entry, recommendation, and proofreading. Since workers are low-paid (a few cents per task) and tasks performed are monotonous, the answers obtained are noisy and hence unreliable. To obtain reliable estimates, it is essential to utilize appropriate inference algorithms (e.g. Majority voting) coupled with structured redundancy through task assignment. Our goal is to obtain the best possible trade-off between reliability and redundancy. In this paper, we consider a general probabilistic model for noisy observations for crowd-sourcing systems and pose the problem of minimizing the total price (i.e. redundancy) that must be paid to achieve a target overall reliability. Concretely, we show that it is possible to obtain an answer to each task correctly with probability 1-ε as long as the redundancy per task is O((K/q) log (K/ε)), where each task can have any of the $K$ distinct answers equally likely, q is the crowd-quality parameter that is defined through a probabilistic model. Further, effectively this is the best possible redundancy-accuracy trade-off any system design can achieve. Such a single-parameter crisp characterization of the (order-)optimal trade-off between redundancy and reliability has various useful operational consequences. Further, we analyze the robustness of our approach in the presence of adversarial workers and provide a bound on their influence on the redundancy-accuracy trade-off.Unlike recent prior work [GKM11, KOS11, KOS11], our result applies to non-binary (i.e. K&gt;2) tasks. In effect, we utilize algorithms for binary tasks (with inhomogeneous error model unlike that in [GKM11, KOS11, KOS11]) as key subroutine to obtain answers for K-ary tasks. Technically, the algorithm is based on low-rank approximation of weighted adjacency matrix for a random regular bipartite graph, weighted according to the answers provided by the workers.},
booktitle = {Proceedings of the ACM SIGMETRICS/International Conference on Measurement and Modeling of Computer Systems},
pages = {81–92},
numpages = {12},
keywords = {random graphs, low-rank matrix, human computation, crowdsourcing},
location = {Pittsburgh, PA, USA},
series = {SIGMETRICS '13}
}

@article{10.1145/2494232.2465761,
author = {Karger, David R. and Oh, Sewoong and Shah, Devavrat},
title = {Efficient crowdsourcing for multi-class labeling},
year = {2013},
issue_date = {June 2013},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {41},
number = {1},
issn = {0163-5999},
url = {https://doi.org/10.1145/2494232.2465761},
doi = {10.1145/2494232.2465761},
abstract = {Crowdsourcing systems like Amazon's Mechanical Turk have emerged as an effective large-scale human-powered platform for performing tasks in domains such as image classification, data entry, recommendation, and proofreading. Since workers are low-paid (a few cents per task) and tasks performed are monotonous, the answers obtained are noisy and hence unreliable. To obtain reliable estimates, it is essential to utilize appropriate inference algorithms (e.g. Majority voting) coupled with structured redundancy through task assignment. Our goal is to obtain the best possible trade-off between reliability and redundancy. In this paper, we consider a general probabilistic model for noisy observations for crowd-sourcing systems and pose the problem of minimizing the total price (i.e. redundancy) that must be paid to achieve a target overall reliability. Concretely, we show that it is possible to obtain an answer to each task correctly with probability 1-ε as long as the redundancy per task is O((K/q) log (K/ε)), where each task can have any of the $K$ distinct answers equally likely, q is the crowd-quality parameter that is defined through a probabilistic model. Further, effectively this is the best possible redundancy-accuracy trade-off any system design can achieve. Such a single-parameter crisp characterization of the (order-)optimal trade-off between redundancy and reliability has various useful operational consequences. Further, we analyze the robustness of our approach in the presence of adversarial workers and provide a bound on their influence on the redundancy-accuracy trade-off.Unlike recent prior work [GKM11, KOS11, KOS11], our result applies to non-binary (i.e. K&gt;2) tasks. In effect, we utilize algorithms for binary tasks (with inhomogeneous error model unlike that in [GKM11, KOS11, KOS11]) as key subroutine to obtain answers for K-ary tasks. Technically, the algorithm is based on low-rank approximation of weighted adjacency matrix for a random regular bipartite graph, weighted according to the answers provided by the workers.},
journal = {SIGMETRICS Perform. Eval. Rev.},
month = jun,
pages = {81–92},
numpages = {12},
keywords = {random graphs, low-rank matrix, human computation, crowdsourcing}
}

@inproceedings{10.1145/3058555.3058586,
author = {Guo, Anhong and Bigham, Jeffrey P.},
title = {Making Real-World Interfaces Accessible Through Crowdsourcing, Computer Vision, and Fabrication},
year = {2017},
isbn = {9781450349000},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3058555.3058586},
doi = {10.1145/3058555.3058586},
abstract = {The world is full of physical interfaces that are inaccessible to blind people, from microwaves and information kiosks to thermostats and checkout terminals. Blind people cannot independently use such devices without at least first learning their layout, and usually only after labeling them with sighted assistance. To address this problem, we introduce VizLens---a robust and interactive screen reader for interfaces in the real world. VizLens users take a picture of an interface they would like to use, it is interpreted quickly and robustly by multiple crowd workers in parallel, and then computer vision is able to give interactive feedback and guidance to users to help them use the interface in real time. Built on top of VizLens, we developed automatically generating tactile overlays to physical interfaces to provide blind people with a permanent static solution. We introduce Facade---a crowdsourced fabrication pipeline to help blind people independently make physical interfaces accessible by adding a 3D printed augmentation of tactile buttons overlaying the original panel.},
booktitle = {Proceedings of the 14th International Web for All Conference},
articleno = {29},
numpages = {2},
keywords = {visually impaired, mobile, fabrication, crowdsourcing, computer vision, blind, accessibility, Non-visual interfaces, 3D printing},
location = {Perth, Western Australia, Australia},
series = {W4A '17}
}

@inproceedings{10.1145/3058555.3058573,
author = {Li, Liangcheng and Wang, Can and Song, Shuyi and Yu, Zhi and Zhou, Fenqin and Bu, Jiajun},
title = {A task assignment strategy for crowdsourcing-based web accessibility evaluation system},
year = {2017},
isbn = {9781450349000},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3058555.3058573},
doi = {10.1145/3058555.3058573},
abstract = {Web accessibility evaluation aims to find the interactive barrier for people with disabilities in accessing the contents on the Web. As some of the checkpoints require human inspection for conformance evaluation, evaluating a website will usually incur an expensive cost. To address this issue, crowdsourcing-based system is used in web accessibility evaluation to elicit contributions from volunteer participants. However, some of accessibility evaluation tasks are complicated and require a certain level of expertise in evaluation. This makes the task assignment in crowdsourcing a challenging problem in that poor evaluation accuracy will be resulted when complicated tasks are assigned to inexperienced participants. To address this issue, we propose in this paper a novel task assignment strategy called Evaluator-Decision-Based Assignment (EDBA) to better leverage the participation and expertise of the volunteers. Using evaluators' historical evaluation records and experts' review, we train a minimum cost model via machine learning methods to obtain an optimal task assignment map. Experiments on Chinese Web Accessibility Evaluation System show that our method achieves high accuracy in website accessibility evaluation. Meanwhile, the balanced assignments from EDBA also enable both novices and old hands effective participation in accessibility evaluation.},
booktitle = {Proceedings of the 14th International Web for All Conference},
articleno = {18},
numpages = {4},
keywords = {Web Accessibility Evaluation, Task Assignment, Crowdsourcing System},
location = {Perth, Western Australia, Australia},
series = {W4A '17}
}

@inproceedings{10.1145/2970276.2970334,
author = {Peng, Xin and Gu, Jingxiao and Tan, Tian Huat and Sun, Jun and Yu, Yijun and Nuseibeh, Bashar and Zhao, Wenyun},
title = {CrowdService: serving the individuals through mobile crowdsourcing and service composition},
year = {2016},
isbn = {9781450338455},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2970276.2970334},
doi = {10.1145/2970276.2970334},
abstract = {Some user needs in real life can only be accomplished by leveraging the intelligence and labor of other people via crowdsourcing tasks. For example, one may want to confirm the validity of the description of a secondhand laptop by asking someone else to inspect the laptop on site. To integrate these crowdsourcing tasks into user applications, it is required that crowd intelligence and labor be provided as easily accessible services (e.g., Web services), which can be called crowd services. In this paper, we develop a framework named CROWDSERVICE which supplies crowd intelligence and labor as publicly accessible crowd services via mobile crowdsourcing. We implement the proposed framework on the Android platform and evaluate the usability of the framework with a user study.},
booktitle = {Proceedings of the 31st IEEE/ACM International Conference on Automated Software Engineering},
pages = {214–219},
numpages = {6},
keywords = {service composition, reliability, mobile crowdsourcing},
location = {Singapore, Singapore},
series = {ASE '16}
}

@inproceedings{10.4108/icst.collaboratecom.2012.250499,
author = {Allahbakhsh, M. and Ignjatovic, A. and Benatallah, B. and Beheshti, S. and Bertino, E. and Foo, N.},
title = {Reputation management in crowdsourcing systems},
year = {2012},
isbn = {9781467327404},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.4108/icst.collaboratecom.2012.250499},
doi = {10.4108/icst.collaboratecom.2012.250499},
abstract = {Worker selection is a significant and challenging issue in crowdsourcing systems. Such selection is usually based on an assessment of the reputation of the individual workers participating in such systems. However, assessing the credibility and adequacy of such calculated reputation is a real challenge. In this paper, we propose a reputation management model which leverages the values of the tasks completed, the credibility of the evaluators of the results of the tasks and time of evaluation of the results of these tasks in order to calculate more dependable quality metrics for workers and evaluators. The model has been implemented and experimentally validated.},
booktitle = {Proceedings of the 2012 8th International Conference on Collaborative Computing: Networking, Applications and Worksharing (CollaborateCom 2012)},
pages = {664–671},
numpages = {8},
series = {COLLABORATECOM '12}
}

@inproceedings{10.1109/SOSE.2014.79,
author = {Hu, Zhenghui and Wu, Wenjun},
title = {A Game Theoretic Model of Software Crowdsourcing},
year = {2014},
isbn = {9781479936168},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/SOSE.2014.79},
doi = {10.1109/SOSE.2014.79},
abstract = {With the rapid development of crowdsourcing all over the world, crowdsourcing for software engineering begins to attract more and more considerable attentions from software developers, coders and researchers. And many successful online platforms such as TopCoder has demonstrated crowdsourcing's capability and potential for supporting various software development activities. In order to study the competitive behaviours for software crowdsourcing, we apply the famous game theory to model the 2-player algorithm challenges on TopCoder. And as all the participants are aware of other players' information and they make decisions almost simultaneously, this article adopts the theory of static games with complete information. Through Nash equilibria computing, we find that the value of successfully challenging probability can be used to deduce specific competitive decisions of coders in the algorithm challenge stage. Specifically, if a coder's probability to make a successful challenge exceeds some certain value, then he will always choose to challenge. The paper provides a new research perspective for software engineering crowdsourcing, and empirical research will be done in the next step of work.},
booktitle = {Proceedings of the 2014 IEEE 8th International Symposium on Service Oriented System Engineering},
pages = {446–453},
numpages = {8},
series = {SOSE '14}
}

@inproceedings{10.1145/1880071.1880139,
author = {Wiggins, Andrea},
title = {Crowdsourcing science: organizing virtual participation in knowledge production},
year = {2010},
isbn = {9781450303873},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1880071.1880139},
doi = {10.1145/1880071.1880139},
abstract = {Citizen science is a form of research collaboration that involves the public in scientific research to address real-world problems. Virtual citizen science projects, entirely mediated by information and communication technologies (ICTs), are often considered a form of crowdsourcing applied to science. The use of ICTs to support citizen science has already yielded significant impacts on the scale and scope of participation and research, but there is little guidance to help projects choose and implement appropriate technologies to support research and participation goals. This dissertation study employs a comparative case study methodology to examine how virtuality and technology shape processes of organizing and participation in citizen science, and how these processes influence scientific outcomes. The goal of the study is to conceptualize virtual participation by examining the relationship between ICT and practice in order to inform design and management of cyberinfrastructure for citizen science.},
booktitle = {Proceedings of the 2010 ACM International Conference on Supporting Group Work},
pages = {337–338},
numpages = {2},
keywords = {virtuality, participation, organizing, citizen science},
location = {Sanibel Island, Florida, USA},
series = {GROUP '10}
}

@inproceedings{10.5555/2760440.2760728,
author = {Mahalingam, Usha and Manju, P. Leela},
title = {A Crowdsourcing Framework for Toll Plazas: Optimizing Delay at Toll Plazas},
year = {2014},
isbn = {9781479970025},
publisher = {IEEE Computer Society},
address = {USA},
abstract = {The aim of this paper is to propose an ideal and time saving method for vehicles to pass through toll plazas. Choosing the best tollbooth to move out of the toll plaza quickly, has become a tough task and is error prone. The proposed system uses crowdsourcing to maintain waiting time for each tollbooth, as a result travelers are provided with the best tollbooth in a toll plaza. This scheme reduces the waiting time in the queue, as a result, people can drive through toll plazas swiftly. Travelers passing the toll plaza act as the crowd that cooperate in choosing the best lane and help maintain the waiting time of each tollbooth and adjust queue parameters.},
booktitle = {Proceedings of the 2014 3rd International Conference on Eco-Friendly Computing and Communication Systems},
pages = {19–22},
numpages = {4},
keywords = {waiting time, toll plaza, sustainable growth, smart transport, quality of experience, optimization, crowdsourcing},
series = {ICECCS '14}
}

@inproceedings{10.1145/2566486.2567989,
author = {Venanzi, Matteo and Guiver, John and Kazai, Gabriella and Kohli, Pushmeet and Shokouhi, Milad},
title = {Community-based bayesian aggregation models for crowdsourcing},
year = {2014},
isbn = {9781450327442},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2566486.2567989},
doi = {10.1145/2566486.2567989},
abstract = {This paper addresses the problem of extracting accurate labels from crowdsourced datasets, a key challenge in crowdsourcing. Prior work has focused on modeling the reliability of individual workers, for instance, by way of confusion matrices, and using these latent traits to estimate the true labels more accurately. However, this strategy becomes ineffective when there are too few labels per worker to reliably estimate their quality. To mitigate this issue, we propose a novel community-based Bayesian label aggregation model, CommunityBCC, which assumes that crowd workers conform to a few different types, where each type represents a group of workers with similar confusion matrices. We assume that each worker belongs to a certain community, where the worker's confusion matrix is similar to (a perturbation of) the community's confusion matrix. Our model can then learn a set of key latent features: (i) the confusion matrix of each community, (ii) the community membership of each user, and (iii) the aggregated label of each item. We compare the performance of our model against established aggregation methods on a number of large-scale, real-world crowdsourcing datasets. Our experimental results show that our CommunityBCC model consistently outperforms state-of-the-art label aggregation methods, requiring, on average, 50\% less data to pass the 90\% accuracy mark.},
booktitle = {Proceedings of the 23rd International Conference on World Wide Web},
pages = {155–164},
numpages = {10},
keywords = {crowdsourcing, community detection, bayesian inference},
location = {Seoul, Korea},
series = {WWW '14}
}

@inproceedings{10.1145/3250113,
author = {Parikh, Tapan},
title = {Session details: Papers: crowdsourcing: people power},
year = {2013},
isbn = {9781450318990},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3250113},
doi = {10.1145/3250113},
booktitle = {Proceedings of the SIGCHI Conference on Human Factors in Computing Systems},
location = {Paris, France},
series = {CHI '13}
}

@inproceedings{10.1007/978-3-319-64248-2_14,
author = {Sakamoto, Mizuki and Gushima, Kota and Alexandrova, Todorka and Nakajima, Tatsuo},
title = {Designing Human Behavior Through Social Influence in Mobile Crowdsourcing with Micro-communities},
year = {2017},
isbn = {978-3-319-64247-5},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-319-64248-2_14},
doi = {10.1007/978-3-319-64248-2_14},
abstract = {This paper proposes a new mobile social media infrastructure for motivating collective people to participate in flourishing our society. For motivating them, designing human behavior through social influence within communities is one of the most important issues to make their lifestyle better, but it is not easy to promote the entire collective people’s activities towards achieving a common goal. Our proposal is to use a layered approach where an entire community consists of many micro-communities; if each independent community is encouraged to contribute to its society, the entire community will be finally motivated to achieve a flourishing society. Our approach adopts a virtual currency and a crowdfunding concept to encourage members in a micro-community; we analyze the effect of social influence on human behavior from the experiment-based analysis. The analysis shows that the proposed approach might work well within a community of well-known people. We finally suggest a possible solution to overcome potential limitations as a future direction.},
booktitle = {Electronic Government and the Information Systems Perspective: 6th International Conference, EGOVIS 2017, Lyon, France, August 28-31, 2017, Proceedings},
pages = {189–205},
numpages = {17},
keywords = {Human behavior, Social influence, Mobile crowdsourcing, Micro-level crowdfunding},
location = {Lyon, France}
}

@inproceedings{10.1109/HICSS.2013.143,
author = {Pedersen, Jay and Kocsis, David and Tripathi, Abhishek and Tarrell, Alvin and Weerakoon, Aruna and Tahmasbi, Nargess and Xiong, Jie and Deng, Wei and Oh, Onook and de Vreede, Gert-Jan},
title = {Conceptual Foundations of Crowdsourcing: A Review of IS Research},
year = {2013},
isbn = {9780769548920},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/HICSS.2013.143},
doi = {10.1109/HICSS.2013.143},
abstract = {Crowd sourcing is a collaboration model enabled by people-centric web technologies to solve individual, organizational, and societal problems using a dynamically formed crowd of people who respond to an open call for participation. We report on a literature survey of crowd sourcing research, focusing on top journals and conferences in the Information Systems (IS) field. To our knowledge, ours is the first effort of this type in the IS discipline. Contributions include providing a synopsis of crowd sourcing research to date, a common definition for crowd sourcing, and a conceptual model for guiding future studies of crowd sourcing. We show how existing IS literature applies to the elements of that conceptual model: Problem, People (Problem Owner, Individual, and Crowd), Governance, Process, Technology, and Outcome. We close with suggestions for future research.},
booktitle = {Proceedings of the 2013 46th Hawaii International Conference on System Sciences},
pages = {579–588},
numpages = {10},
series = {HICSS '13}
}

@inproceedings{10.5555/2486788.2486963,
author = {Mao, Ke and Yang, Ye and Li, Mingshu and Harman, Mark},
title = {Pricing crowdsourcing-based software development tasks},
year = {2013},
isbn = {9781467330763},
publisher = {IEEE Press},
abstract = {Many organisations have turned to crowdsource their software development projects. This raises important pricing questions, a problem that has not previously been addressed for the emerging crowdsourcing development paradigm. We address this problem by introducing 16 cost drivers for crowdsourced development activities and evaluate 12 predictive pricing models using 4 popular performance measures. We evaluate our predictive models on TopCoder, the largest current crowdsourcing platform for software development. We analyse all 5,910 software development tasks (for which partial data is available), using these to extract our proposed cost drivers. We evaluate our predictive models using the 490 completed projects (for which full details are available). Our results provide evidence to support our primary finding that useful prediction quality is achievable (Pred(30)&gt;0.8). We also show that simple actionable advice can be extracted from our models to assist the 430,000 developers who are members of the TopCoder software development market.},
booktitle = {Proceedings of the 2013 International Conference on Software Engineering},
pages = {1205–1208},
numpages = {4},
location = {San Francisco, CA, USA},
series = {ICSE '13}
}

@inproceedings{10.1145/3406865.3418572,
author = {Qiu, Sihang and Gadiraju, Ujwal and Bozzon, Alessandro},
title = {TickTalkTurk: Conversational Crowdsourcing Made Easy},
year = {2020},
isbn = {9781450380591},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3406865.3418572},
doi = {10.1145/3406865.3418572},
abstract = {This demo presents TickTalkTurk, a tool that can assist task requesters in quickly deploying crowdsourcing tasks in a customizable conversational worker interface. The conversational worker interface can convey task instructions, deploy microtasks, and gather worker input in a dialogue-based workflow. The interface is implemented as a Web-based application, which makes it compatible with popular crowdsourcing platforms. The tool we developed is demonstrated through two microtask crowdsourcing examples with different task types. Results reveal that our conversational worker interface is capable of better engaging workers and analyzing workers performance.},
booktitle = {Companion Publication of the 2020 Conference on Computer Supported Cooperative Work and Social Computing},
pages = {53–57},
numpages = {5},
keywords = {microtask crowdsourcing, conversational interface, conversational agent, chatbot},
location = {Virtual Event, USA},
series = {CSCW '20 Companion}
}

@inproceedings{10.1109/ICDMW.2015.134,
author = {Liang, Tingting and Chen, Liang and Ying, Haochao and Zheng, Zibin and Wu, Jian},
title = {Crowdsourcing Based API Search via Leveraging Twitter Lists Information},
year = {2015},
isbn = {9781467384933},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/ICDMW.2015.134},
doi = {10.1109/ICDMW.2015.134},
abstract = {With the rapid growth of open APIs on the Internet, searching appropriate APIs for a given query becomes a challenging problem. General API search systems, such as ProgrammableWeb, usually can not provide satisfactory results of API search due to the simple keywords matching between queries and API information offered by providers (e.g. name and description). In this paper, we propose a crowdsourcing based search approach named CrowdAPS to effectively find the appropriate APIs. Specifically, CrowdAPS leverages Twitter lists, which is a tool used by individual users to organize accounts that interest them on semantics. List meta-data, including list name and description, is generated from collective intelligence and can be used by Latent Semantic Indexing (LSI) model to acquire semantic similarity between APIs and queries. Furthermore, CrowdAPS exploits list number to infer the popularity of APIs. The final search result relies on the integration of semantic similarity and popularity. Comprehensive experiment based on real-world datasets crawled from ProgrammableWeb and Twitter demonstrates the effectiveness of CrowdAPS.},
booktitle = {Proceedings of the 2015 IEEE International Conference on Data Mining Workshop (ICDMW)},
pages = {1540–1547},
numpages = {8},
series = {ICDMW '15}
}

@inproceedings{10.1145/1979742.1979803,
author = {Wu, Shao-Yu and Thawonmas, Ruck and Chen, Kuan-Ta},
title = {Video summarization via crowdsourcing},
year = {2011},
isbn = {9781450302685},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1979742.1979803},
doi = {10.1145/1979742.1979803},
abstract = {Although video summarization has been studied extensively, existing schemes are neither lightweight nor generalizable to all types of video content. To generate accurate abstractions of all types of video, we propose a framework called Click2SMRY, which leverages the wisdom of the crowd to generate video summaries with a low workload for workers. The framework is lightweight because workers only need to click a dedicated key when they feel that the video being played is reaching a highlight. One unique feature of the framework is that it can generate different abstraction levels of video summaries according to viewers' preferences in real time. The results of experiments conducted to evaluate the framework demonstrate that it can generate satisfactory summaries for different types of video clips.},
booktitle = {CHI '11 Extended Abstracts on Human Factors in Computing Systems},
pages = {1531–1536},
numpages = {6},
keywords = {video summarization, video skimming, human computation, crowdsourcing},
location = {Vancouver, BC, Canada},
series = {CHI EA '11}
}

@inproceedings{10.5555/2936924.2937172,
author = {Bhat, Satyanath and Padmanabhan, Divya and Jain, Shweta and Narahari, Yadati},
title = {A Truthful Mechanism with Biparameter Learning for Online Crowdsourcing: (Extended Abstract)},
year = {2016},
isbn = {9781450342391},
publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
address = {Richland, SC},
booktitle = {Proceedings of the 2016 International Conference on Autonomous Agents \&amp; Multiagent Systems},
pages = {1385–1386},
numpages = {2},
keywords = {strategic agents, mechanism design},
location = {Singapore, Singapore},
series = {AAMAS '16}
}

@inproceedings{10.1109/HotWeb.2015.9,
author = {Jarrett, Julian and Silva, Larissa Ferreira da and Mello, Laerte and Andere, Sadallo and Cruz, Gustavo and Blake, M. Brian},
title = {Self-Generating a Labor Force for Crowdsourcing: Is Worker Confidence a Predictor of Quality?},
year = {2015},
isbn = {9781467396882},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/HotWeb.2015.9},
doi = {10.1109/HotWeb.2015.9},
abstract = {When leveraging the crowd to perform complex tasks, it is imperative to identify the most effective worker for a particular job. Demographic profiles provided by workers, skill self-assessments by workers, and past performance as captured by employers all represent viable data points available within labor markets. Employers often question the validity of a worker's self-assessment of skills and expertise level when selecting workers in context of other information. More specifically, employers would like to answer the question, "Is worker confidence a predictor of quality?" In this paper, we discuss the state-of-the-art in recommending crowd workers based on assessment information. A major contribution of our work is an architecture, platform, and push/pull process for categorizing and recommending workers based on available self-assessment information. We present a study exploring the validity of skills input by workers in light of their actual performance and other metrics captured by employers. A further contribution of this approach is the extrapolation of a body of workers to describe the nature of the community more broadly. Through experimentation, within the language-processing domain, we demonstrate a new capability of deriving trends that might help future employers to select appropriate workers.},
booktitle = {Proceedings of the 2015 Third IEEE Workshop on Hot Topics in Web Systems and Technologies (HotWeb)},
pages = {85–90},
numpages = {6},
series = {HOTWEB '15}
}

@inproceedings{10.1109/IRI.2016.15,
author = {Jarrett, Julian and Blake, M. Brian},
title = {Reusable Meta-Models for Crowdsourcing Driven Elastic Systems (Invited Paper)},
year = {2016},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/IRI.2016.15},
doi = {10.1109/IRI.2016.15},
abstract = {Elastic systems utilize both human and machine working units to accomplish tasks that are eligible for crowdsourcing. The quality in the results of work completed by either type of computing unit is tantamount on the characteristics they bear. In this paper we draw parallels from our previous work into looking at the suitability of working units in completing viable tasks in crowdsourcing. We seek to understand characteristics for modeling tasks and workers within these types of systems. Based on our experiments and lessons learned in related literature, we propose a dynamic worker-task information meta-model with a corresponding operational workflow model that can be used in a variety of problem domains involving crowdsourced tasks to provide support in making this decision.},
booktitle = {2016 IEEE 17th International Conference on Information Reuse and Integration (IRI)},
pages = {50–57},
numpages = {8},
location = {Pittsburgh, PA, USA}
}

@inproceedings{10.1145/2213836.2213951,
author = {Van Pelt, Chris and Sorokin, Alex},
title = {Designing a scalable crowdsourcing platform},
year = {2012},
isbn = {9781450312479},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2213836.2213951},
doi = {10.1145/2213836.2213951},
abstract = {Computers are extremely efficient at crawling, storing and processing huge volumes of structured data. They are great at exploiting link structures to generate valuable knowledge. Yet there are plenty of data processing tasks that are difficult today. Labeling sentiment, moderating images, and mining structured content from the web are still too hard for computers. Automated techniques can get us a long way in some of those, but human inteligence is required when an accurate decision is ultimately important. In many cases that decision is easy for people and can be made quickly - in a few seconds to few minutes.By creating millions of simple online tasks we create a distributed computing machine. By shipping the tasks to millions of contributers around the globe, we make this human computer available 24/7 to make important decisions about your data. In this talk, I will describe our approach to designing CrowdFlower - a scalable crowdsourcing platform - as it evolved over the last 4 years.We think about crowdsourcing in terms of Quality, Cost and Speed. They are the ultimate design objectives of a human computer. Unfortunately, we can't have all 3. A general price-constrained task requiring 99.9\% accuracy and 10 minute turnaround is not possible today. I will discuss design decisions behind CrowdFlower that allow us to pursue any two of these objectives.I will briefly present examples of common crowdsourced tasks and tools built into the platform to make the design of complex tasks easy, tools such as CrowdFlower Markup Language(CML).Quality control is the single most important challenge in Crowdsourcing. To enable an unidentified crowd of people to produce meaningful work, we must be certain that we can filter out bad contributors and produce high quality output. Initially we only used consensus. As the diversity and size of our crowd grew, so did the number of people attempting fraud. CrowdFlower developed "Gold standard" to block attempts of fraud. The use of gold allowed us to train contributors for the details of specific domains. By defining expected responses for a subset of the work and providing explanations of why a given response was expected, we are able distribute tasks to an ever-expanding anonymous workforce without sacrificing quality.},
booktitle = {Proceedings of the 2012 ACM SIGMOD International Conference on Management of Data},
pages = {765–766},
numpages = {2},
keywords = {workforce, data, crowdsourcing},
location = {Scottsdale, Arizona, USA},
series = {SIGMOD '12}
}

@inproceedings{10.1109/SOSE.2015.53,
author = {Aldhahri, Eman and Shandilya, Vivek and Shiva, Sajjan},
title = {Towards an Effective Crowdsourcing Recommendation System: A Survey of the State-of-the-Art},
year = {2015},
isbn = {9781479983568},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/SOSE.2015.53},
doi = {10.1109/SOSE.2015.53},
abstract = {Crowdsourcing is an approach where requesters can call for workers with different capabilities to process a task for monetary reward. With the vast amount of tasks posted every day, satisfying workers, requesters, and service providers--who are the stakeholders of any crowdsourcing system--is critical to its success. To achieve this, the system should address three objectives: (1) match the worker with a suitable task that fits the worker's interests and skills, and raise the worker's rewards; (2) give requesters more qualified solutions with lower cost and time; and (3) raise the accepted tasks rate which will raise the aggregated commissions accordingly. For these objectives, we present a critical study of the state-of-the-art in recommendation systems that are ubiquitous among crowdsourcing and other online systems to highlight the potential of the best approaches which could be applied in a crowdsourcing system, and highlight the shortcomings in the existing crowdsourcing recommendation systems that should be addressed.},
booktitle = {Proceedings of the 2015 IEEE Symposium on Service-Oriented System Engineering},
pages = {372–377},
numpages = {6},
keywords = {task matching, recommendation, Survey, Crowdsourcing},
series = {SOSE '15}
}

@inproceedings{10.1109/DySPAN.2015.7343932,
author = {Ying, Xuhang and Roy, Sumit and Poovendran, Radha},
title = {Incentivizing crowdsourcing for radio environment mapping with statistical interpolation},
year = {2015},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/DySPAN.2015.7343932},
doi = {10.1109/DySPAN.2015.7343932},
abstract = {White Space Networking crucially relies on the active monitoring of spectrum usage (to identify white space opportunities) in both space and time. One way to achieve this is wide-area deployment of spectrum sensors to gather spatio-temporal spectrum data, and use them to construct better Radio Environment Maps (REMs) via suitable statistical interpolation techniques (i.e., Kriging). Cost of such large-scale sensor deployment can be reduced via crowdsourcing, i.e., outsourcing the sensing task to mobile users equipped with sensorized high-end client devices (e.g., tablets or smartphones), and success of such crowdsourced sensing presumes some incentive mechanisms to attract user participation. In this work, we present an incentivized crowdsourcing system architecture that (periodically) acquires spectrum data from users, so as to optimize the resulting radio environment map (i.e., minimizing the average prediction-error variance) for a given data acquisition budget. First, we introduce an auction-based incentive mechanism that is computationally efficient, individually rational and truthful, and prove that the total payment of the proposed mechanism is a monotonically increasing function of the cardinality of the winner set. Then we propose a budget-feasible version and through extensive simulations, we evaluate the performance of proposed mechanisms for comparison to a baseline to demonstrate its significantly superior performance in crowdsourced radio mapping.},
booktitle = {2015 IEEE International Symposium on Dynamic Spectrum Access Networks (DySPAN)},
pages = {365–374},
numpages = {10},
location = {Stockholm, Sweden}
}

@inproceedings{10.1109/UCC.2014.95,
author = {Smith, Ross and Kilty, Lori Ada},
title = {Crowdsourcing and Gamification of Enterprise Meeting Software Quality},
year = {2014},
isbn = {9781479978816},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/UCC.2014.95},
doi = {10.1109/UCC.2014.95},
abstract = {Microsoft has a 10+ year history with the application of gamification techniques to help crowd source quality related efforts. While this paper includes a brief review of previously published material about past success stories and lessons learned, the focus is on an autumn 2014 project designed to encourage employees to respond to enterprise online meeting software quality survey requests. The product team and IT department have come together to help improve meetings and improve the lives of people around the world. "Meeting Needs" is a proposed experiment that allows you to help us improve your Lync online meeting. At the same time, "Meeting Needs" builds in a charitable feature where participants are able to raise funds on behalf of a set of humanitarian agencies. Enterprise meeting quality is difficult to measure and there are many factors that make up a good meeting. Since the number of employees who typically reply to survey requests is generally low (often &lt; 30\%) there is a unique opportunity to create a feedback loop that will ensure that high quality and a higher rate of survey completion through the use of game mechanics and charitable giving elements.},
booktitle = {Proceedings of the 2014 IEEE/ACM 7th International Conference on Utility and Cloud Computing},
pages = {611–613},
numpages = {3},
keywords = {Productivity Games, Gamification, Enterprise software},
series = {UCC '14}
}

@inproceedings{10.1145/2534142.2534151,
author = {Yen, Yu-Chuan and Chu, Cing-Yu and Yeh, Su-Ling and Chu, Hao-Hua and Huang, Polly},
title = {Lab experiment vs. crowdsourcing: a comparative user study on Skype call quality},
year = {2013},
isbn = {9781450324519},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2534142.2534151},
doi = {10.1145/2534142.2534151},
abstract = {To deliver voice over the Internet in a cost-effective way, it is essential to quantify the quality of user experience (i.e., QoE) of a voice service at various provisioning levels. Conducting user studies is an inevitable step facilitating quantitative studies of QoE. The two experimental methods -- lab experiment vs. crowdsourcing via Amazon Mechanical Turk [1] -- are compared in this study. We find that, for the study of Skype call quality, the crowdsourcing approach stands out in terms of efficiency and user diversity, which in turn strengthens the robustness and the depth of the analysis.},
booktitle = {Proceedings of the 9th Asian Internet Engineering Conference},
pages = {65–72},
numpages = {8},
keywords = {rate control, mechanical turk, VoIP, Skype, QoE},
location = {Chiang Mai, Thailand},
series = {AINTEC '13}
}

@inproceedings{10.1007/978-3-319-49004-5_44,
author = {ul Hassan, Umair and Zaveri, Amrapali and Marx, Edgard and Curry, Edward and Lehmann, Jens},
title = {ACRyLIQ: Leveraging DBpedia for Adaptive Crowdsourcing in Linked Data Quality Assessment},
year = {2016},
isbn = {978-3-319-49003-8},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-319-49004-5_44},
doi = {10.1007/978-3-319-49004-5_44},
abstract = {Crowdsourcing has emerged as a powerful paradigm for quality assessment and improvement of Linked Data. A major challenge of employing crowdsourcing, for quality assessment in Linked Data, is the cold-start problem: how to estimate the reliability of crowd workers and assign the most reliable workers to tasks? We address this challenge by proposing a novel approach for generating test questions from DBpedia based on the topics associated with quality assessment tasks. These test questions are used to estimate the reliability of the new workers. Subsequently, the tasks are dynamically assigned to reliable workers to help improve the accuracy of collected responses. Our proposed approach, ACRyLIQ, is evaluated using workers hired from Amazon Mechanical Turk, on two real-world Linked Data datasets. We validate the proposed approach in terms of accuracy and compare it against the baseline approach of reliability estimate using gold-standard task. The results demonstrate that our proposed approach achieves high accuracy without using gold-standard task.},
booktitle = {Knowledge Engineering and Knowledge Management: 20th International Conference, EKAW 2016, Bologna, Italy, November 19-23, 2016, Proceedings},
pages = {681–696},
numpages = {16},
keywords = {Link Data, Task Assignment, Test Question, Overhead Cost, Assignment Algorithm},
location = {Bologna, Italy}
}

@inproceedings{10.1109/SMC.2019.8914041,
author = {Murata, Shingo and Yanagida, Hikaru and Katahira, Kentaro and Suzuki, Shinsuke and Ogata, Tetsuya and Yamashita, Yuichi},
title = {Large-scale Data Collection for Goal-directed Drawing Task with Self-report Psychiatric Symptom Questionnaires via Crowdsourcing},
year = {2019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/SMC.2019.8914041},
doi = {10.1109/SMC.2019.8914041},
abstract = {Drawing is a representative human cognitive ability and may mirror cognitive characteristics including those associated with psychiatric symptoms. Therefore, analysis of drawing data collected from various populations such as healthy people and psychiatric patients may be beneficial for better understanding human cognition. However, collecting such large-scale data about the relationship between drawing and cognitive/personality traits offline—in a laboratory—is a difficult issue. To overcome this issue, we devised a novel experimental paradigm involving a goal-directed drawing task conducted online—on the eb—with participants recruited via a crowdsourcing platform. With the assistance of 1155 participants with differing levels of psychiatric symptoms, we collected a total of 194,040 trajectory data and answers to seven different self-report psychiatric symptom questionnaires comprising 181 items. We visualized the collected trajectory data and performed an exploratory factor analysis on the correlation matrix of the psychiatric symptom questionnaire items. Our results suggest that there were associations between psychiatric symptoms represented by specific psychiatric factors and atypical behavior observed while performing the goal-directed drawing task. This indicates the efficacy of a dimensional approach to large-scale online experiments with respect to clinical psychiatry.},
booktitle = {2019 IEEE International Conference on Systems, Man and Cybernetics (SMC)},
pages = {3859–3865},
numpages = {7},
location = {Bari, Italy}
}

@inproceedings{10.5555/2540128.2540538,
author = {Le Bras, Ronan and Bernstein, Richard and Gomes, Carla P. and Selman, Bart and Van Dover, R. Bruce},
title = {Crowdsourcing backdoor identification for combinatorial optimization},
year = {2013},
isbn = {9781577356332},
publisher = {AAAI Press},
abstract = {We will show how human computation insights can be key to identifying so-called backdoor variables in combinatorial optimization problems. Backdoor variables can be used to obtain dramatic speedups in combinatorial search. Our approach leverages the complementary strength of human input, based on a visual identification of problem structure, crowdsourcing, and the power of combinatorial solvers to exploit complex constraints. We describe our work in the context of the domain of materials discovery. The motivation for considering the materials discovery domain comes from the fact that new materials can provide solutions for key challenges in sustainability, e.g., in energy, new catalysts for more efficient fuel cell technology.},
booktitle = {Proceedings of the Twenty-Third International Joint Conference on Artificial Intelligence},
pages = {2840–2847},
numpages = {8},
location = {Beijing, China},
series = {IJCAI '13}
}

@inproceedings{10.1145/2661829.2661918,
author = {Nguyen, Dong and Trieschnigg, Dolf and Theune, Mari\"{e}t},
title = {Using Crowdsourcing to Investigate Perception of Narrative Similarity},
year = {2014},
isbn = {9781450325981},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2661829.2661918},
doi = {10.1145/2661829.2661918},
abstract = {For many applications measuring the similarity between documents is essential. However, little is known about how users perceive similarity between documents. This paper presents the first large-scale empirical study that investigates perception of narrative similarity using crowdsourcing. As a dataset we use a large collection of Dutch folk narratives. We study the perception of narrative similarity by both experts and non-experts by analyzing their similarity ratings and motivations for these ratings. While experts focus mostly on the plot, characters and themes of narratives, non-experts also pay attention to dimensions such as genre and style. Our results show that a more nuanced view is needed of narrative similarity than captured by story types, a concept used by scholars to group similar folk narratives. We also evaluate to what extent unsupervised and supervised models correspond with how humans perceive narrative similarity.},
booktitle = {Proceedings of the 23rd ACM International Conference on Conference on Information and Knowledge Management},
pages = {321–330},
numpages = {10},
keywords = {crowdsourcing, folktales, narratives, similarity},
location = {Shanghai, China},
series = {CIKM '14}
}

@inproceedings{10.1109/MASS.2015.76,
author = {Armengol, Patrick and Tobkes, Rachelle and Akkaya, Kemal and Ciftler, Bekir S. and Guvenc, Ismail},
title = {Efficient Privacy-Preserving Fingerprint-Based Indoor Localization Using Crowdsourcing},
year = {2015},
isbn = {9781467391016},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/MASS.2015.76},
doi = {10.1109/MASS.2015.76},
abstract = {Indoor localization has been widely studied due to the inability of GPS to function indoors. Numerous approaches have been proposed in the past and a number of these approaches are currently being used commercially. However, little attention was paid to the privacy of the users especially in the commercial products. Malicious individuals can determine a client's daily habits and activities by simply analyzing their WiFi signals and tracking information. In this paper, we implemented a privacy-preserving indoor localization scheme that is based on a fingerprinting approach to analyze the performance issues in terms of accuracy, complexity, scalability and privacy. We developed an Android app and collected a large number of data on the third floor of the FIU Engineering Center. The analysis of data provided excellent opportunities for performance improvement which have been incorporated to the privacy-preserving localization scheme.},
booktitle = {Proceedings of the 2015 IEEE 12th International Conference on Mobile Ad Hoc and Sensor Systems (MASS)},
pages = {549–554},
numpages = {6},
series = {MASS '15}
}

@inproceedings{10.1109/INFOCOM.2016.7524615,
author = {Chatterjee, Avhishek and Borokhovich, Michael and Varshney, Lav R. and Vishwanath, Sriram},
title = {Efficient and flexible crowdsourcing of specialized tasks with precedence constraints},
year = {2016},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/INFOCOM.2016.7524615},
doi = {10.1109/INFOCOM.2016.7524615},
abstract = {Many companies now use crowdsourcing to leverage external (as well as internal) crowds to perform specialized work, and so methods of improving efficiency are critical. Tasks in crowdsourcing systems with specialized work have multiple steps and each step requires multiple skills. Steps may have different flexibilities in terms of obtaining service from one or multiple agents, due to varying levels of dependency among parts of steps. Steps of a task may have precedence constraints among them. Moreover, there are variations in loads of different types of tasks requiring different skill-sets and availabilities of different types of agents with different skill-sets. Considering these constraints together necessitates the design of novel schemes to allocate steps to agents. In addition, large crowdsourcing systems require allocation schemes that are simple, fast, decentralized and offer customers (task requesters) the freedom to choose agents. In this work we study the performance limits of such crowdsourcing systems and propose efficient allocation schemes that provably meet the performance limits under these additional requirements. We demonstrate our algorithms on data from a crowdsourcing platform run by a non-profit company and show significant improvements over current practice.},
booktitle = {IEEE INFOCOM 2016 - The 35th Annual IEEE International Conference on Computer Communications},
pages = {1–9},
numpages = {9},
location = {San Francisco, CA, USA}
}

@inproceedings{10.1145/2187836.2187969,
author = {Venetis, Petros and Garcia-Molina, Hector and Huang, Kerui and Polyzotis, Neoklis},
title = {Max algorithms in crowdsourcing environments},
year = {2012},
isbn = {9781450312295},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2187836.2187969},
doi = {10.1145/2187836.2187969},
abstract = {Our work investigates the problem of retrieving the maximum item from a set in crowdsourcing environments. We first develop parameterized families of max algorithms, that take as input a set of items and output an item from the set that is believed to be the maximum. Such max algorithms could, for instance, select the best Facebook profile that matches a given person or the best photo that describes a given restaurant. Then, we propose strategies that select appropriate max algorithm parameters. Our framework supports various human error and cost models and we consider many of them for our experiments. We evaluate under many metrics, both analytically and via simulations, the tradeoff between three quantities: (1) quality, (2) monetary cost, and (3) execution time. Also, we provide insights on the effectiveness of the strategies in selecting appropriate max algorithm parameters and guidelines for choosing max algorithms and strategies for each application.},
booktitle = {Proceedings of the 21st International Conference on World Wide Web},
pages = {989–998},
numpages = {10},
keywords = {vote aggregation, worker models, crowdsourcing, human computation, max algorithms, plurality voting},
location = {Lyon, France},
series = {WWW '12}
}

@inproceedings{10.5555/2857070.2857138,
author = {Erez, Eden S. and Zhitomirsky-Geffet, Maayan and Bar-Ilan, Judit},
title = {Subjective vs. objective evaluation of ontological statements with crowdsourcing},
year = {2015},
isbn = {087715547X},
publisher = {American Society for Information Science},
address = {USA},
abstract = {In this paper we propose and test a methodology for evaluation of statements of a multi-viewpoint ontology by crowdsourcing. The task for the workers was to assess each of the given statement as true statements, controversial viewpoint statement or error. Typically, in crowdsourcing experiments the workers are asked for their personal opinions on the given subject. However, in our case their ability to objectively assess others' opinions is examined as well. We conducted two large-scale crowdsourcing experiments with about 750 ontological statements originating from diverse single-viewpoint ontologies. Our results show substantially higher accuracy in evaluation for the objective assessment approach compared to the experiment based on personal opinions.},
booktitle = {Proceedings of the 78th ASIS&amp;T Annual Meeting: Information Science with Impact: Research in and for the Community},
articleno = {68},
numpages = {4},
keywords = {crowdsourcing, multi-viewpoint ontology, ontology statement classification},
location = {St. Louis, Missouri},
series = {ASIST '15}
}

@inproceedings{10.5555/2878453.2878517,
author = {Hanika, Florian and Wohlgenannt, Gerhard and Sabou, Marta},
title = {The uComp protege plugin for crowdsourcing ontology validation},
year = {2014},
publisher = {CEUR-WS.org},
address = {Aachen, DEU},
abstract = {The validation of ontologies using domain experts is expensive. Crowdsourcing has been shown a viable alternative for many knowledge acquisition tasks. We present a Prot\'{e}g\'{e} plugin and a workflow for outsourcing a number of ontology validation tasks to Games with a Purpose and paid micro-task crowdsourcing.},
booktitle = {Proceedings of the 2014 International Conference on Posters \&amp; Demonstrations Track - Volume 1272},
pages = {253–256},
numpages = {4},
keywords = {crowdsourcing, human computation, ontology engineering, prot\'{e}g\'{e} plugin},
location = {Riva del Garda, Italy},
series = {ISWC-PD'14}
}

@inproceedings{10.1109/PIMRC.2015.7343555,
author = {Zhang, Bo and Liu, Chi Harold and Ren, Ziyu and Ma, Jian and Wang, Wendong},
title = {Crowdsourcing energy-efficient participants to ensure quality-of-information},
year = {2015},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/PIMRC.2015.7343555},
doi = {10.1109/PIMRC.2015.7343555},
abstract = {Crowdsourcing systems, by using smart devices like smartphones and iPad, have been widely used in various domains, but are currently facing new challenges. On one hand, different tasks offer different amount of incentive budgets in multitask systems, thus those tasks pay more should be satisfied preferentially. On the other hand, even if there are not enough budget for a sensing task, the system should also try to provide as much sensory data as possible to obtain potential clients. Furthermore, it is challenging to select an “optimal” set of participants as data contributors due to the above two points. In this paper, first, we introduce a metric to describe how the collected sensory data can quantify task's the multi-dimensional data requirements, in terms of data distribution in spatiotemporal domains. Second, we propose a “task priority model” based on incentive budget, to explicitly quantify the relationship between the incentive budget usage and task priority. Then, we present a quality of information (QoI) aware participant selection approach as a suboptimal solution to the defined optimization problem. Finally, we compare our proposed scheme with existing methods via extensive simulations based on the real movement traces of ordinary citizens in Beijing. Extensive simulation results well justify the effectiveness and robustness of our approach.},
booktitle = {2015 IEEE 26th Annual International Symposium on Personal, Indoor, and Mobile Radio Communications (PIMRC)},
pages = {1606–1610},
numpages = {5},
location = {Hong Kong, China}
}

@inproceedings{10.1145/3079628.3079702,
author = {Lowe, Ryan and Steichen, Ben},
title = {Multilingual Search User Behaviors -- Exploring Multilingual Querying and Result Selection Through Crowdsourcing},
year = {2017},
isbn = {9781450346351},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3079628.3079702},
doi = {10.1145/3079628.3079702},
abstract = {The unprecedented increase in online search user diversity across the globe has led to new challenges for search engine providers. In particular, among these challenges is the need to better support individuals who are proficient in multiple languages. To investigate this particular user characteristic, this paper presents an analysis of multilingual search user behaviors through a series of large-scale studies using crowdsourcing. Results show that multilingual users make significant use of each of their languages when searching, and that there are significant differences in behaviors between querying and result selection. In addition, results show that language use strongly depends on a number of task factors and individual user characteristics. These results are discussed in terms of building novel adaptive multilingual search solutions that better support and adapt to users who have multiple language abilities.},
booktitle = {Proceedings of the 25th Conference on User Modeling, Adaptation and Personalization},
pages = {303–307},
numpages = {5},
keywords = {crowd-sourcing, multilingual search, multilingual user characteristics, personalized search, user study},
location = {Bratislava, Slovakia},
series = {UMAP '17}
}

@inproceedings{10.1145/1868914.1868976,
author = {Wightman, Doug},
title = {Crowdsourcing human-based computation},
year = {2010},
isbn = {9781605589343},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1868914.1868976},
doi = {10.1145/1868914.1868976},
abstract = {Thousands of websites have been created to crowdsource tasks. In this paper, systems that crowdsource human-based computations are organized into four distinct classes using two factors: the users' motivation for completing the task (direct or indirect) and whether task completion is competitive. These classes are described and compared. Considerations and selection criteria for systems designers are presented. This investigation also identified several opportunities for further research. For example, existing systems might benefit from the integration of methods for transforming complex tasks into many simple tasks.},
booktitle = {Proceedings of the 6th Nordic Conference on Human-Computer Interaction: Extending Boundaries},
pages = {551–560},
numpages = {10},
keywords = {crowdsourcing, distributed knowledge acquisition, human-based computation},
location = {Reykjavik, Iceland},
series = {NordiCHI '10}
}

@inproceedings{10.1145/3025453.3026032,
author = {Huang, Yun and Huang, Yifeng and Xue, Na and Bigham, Jeffrey P.},
title = {Leveraging Complementary Contributions of Different Workers for Efficient Crowdsourcing of Video Captions},
year = {2017},
isbn = {9781450346559},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3025453.3026032},
doi = {10.1145/3025453.3026032},
abstract = {Hearing-impaired people and non-native speakers rely on captions for access to video content, yet most videos remain uncaptioned or have machine-generated captions with high error rates. In this paper, we present the design, implementation and evaluation of BandCaption, a system that combines automatic speech recognition with input from crowd workers to provide a cost-efficient captioning solution for accessible online videos. We consider four stakeholder groups as our source of crowd workers: (i) individuals with hearing impairments, (ii) second-language speakers with low proficiency, (iii) second-language speakers with high proficiency, and (iv) native speakers. Each group has different abilities and incentives, which our workflow leverages. Our findings show that BandCaption enables crowd workers who have different needs and strengths to accomplish micro-tasks and make complementary contributions. Based on our results, we outline opportunities for future research and provide design suggestions to deliver cost-efficient captioning solutions.},
booktitle = {Proceedings of the 2017 CHI Conference on Human Factors in Computing Systems},
pages = {4617–4626},
numpages = {10},
keywords = {complementary contributions, crowdsourcing, video caption},
location = {Denver, Colorado, USA},
series = {CHI '17}
}

@inproceedings{10.1145/2757757.2757758,
author = {Huo, Zhiqiang and Shu, Lei and Zhou, Zhangbing and Chen, Yuanfang and Li, Kailiang and Zeng, Junlin},
title = {Data Collection Middleware for Crowdsourcing-based Industrial Sensing Intelligence},
year = {2015},
isbn = {9781450335140},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2757757.2757758},
doi = {10.1145/2757757.2757758},
abstract = {In this paper, crowdsourcing-based industrial sensing intelligence (CISI) is proposed as a collaborative approach for large-scale monitoring in modern industrial plants, targeting at improved productivity and increased workplace safety. The proposed approach focuses on middleware, which considers both application and industry-grade requirements. Through embedding crowdsourcing knowledge at different levels and supporting QoS services, systems based on CISI can perform effective work assignment and flexible configuration of wireless sensor networks (WSNs). This paper presents a middleware that addresses these characteristics, which is an extension of GSN, our earlier work on middleware for rapid deployment and integration of heterogeneous sensor networks. Wireless sensor devices and wearable equipment are employed as modeling tools for the middleware implementation.},
booktitle = {Proceedings of the ACM International Workshop on Mobility and MiddleWare Management in HetNets},
pages = {3–8},
numpages = {6},
keywords = {cisi, middleware, wearable equipment, wsns},
location = {Hangzhou, China},
series = {MobiMWareHN '15}
}

@inproceedings{10.1145/2987386.2987413,
author = {Alabduljabbar, Reham and Al-Dossari, Hmood},
title = {A Task Ontology-based Model for Quality Control in Crowdsourcing Systems},
year = {2016},
isbn = {9781450344555},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2987386.2987413},
doi = {10.1145/2987386.2987413},
abstract = {In the era of big data, a vast amount of data is created every day. Crowdsourcing systems have recently gained significance as an interesting practice in managing and performing big data operations. Crowdsourcing has facilitated the process of performing tasks that cannot be adequately solved by machines including image labeling, transcriptions, data validation and sentiment analysis. However, quality control remains one of the biggest challenges for crowdsourcing. Current crowdsourcing systems use the same quality control mechanism for evaluating different types of tasks. In this paper, we argue that quality mechanisms vary by task type. We propose a task ontology-based model to identify the most appropriate quality mechanism for a given task. The proposed model has been enriched by a reputation system to collect requesters' feedback on quality mechanisms. Accordingly, the reputation of each mechanism can be established and used for mapping between tasks and mechanisms. Description of the model's framework, algorithms, and its components' interaction are presented.},
booktitle = {Proceedings of the International Conference on Research in Adaptive and Convergent Systems},
pages = {22–28},
numpages = {7},
keywords = {Big Data, Crowd Computing, Crowdsourcing, HITs, Human Computation, MTurk, Ontology, Quality Control, Reputation},
location = {Odense, Denmark},
series = {RACS '16}
}

@inproceedings{10.1145/2254129.2254133,
author = {Simperl, Elena},
title = {Crowdsourcing semantic data management: challenges and opportunities},
year = {2012},
isbn = {9781450309158},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2254129.2254133},
doi = {10.1145/2254129.2254133},
abstract = {Linked Data refers to a set of guidelines and best practices for publishing and accessing structured data on the Web. It builds upon established Web technologies, in particular HTTP and URIs, extended with Semantic Web representation formats and protocols such as RDF, RDFS, OWL and SPARQL, by which data from different sources can be shared, interconnected and used beyond the application scenarios for which it was originally created. RDF is a central building block of the Linked Data technology stack. It is a graph-based data model based on the idea of making statements about (information and non-information) resources on the Web in terms of triples of the form subject predicate object. The object of any RDF triple may be used in the subject position in other triples, leading to a directed, labeled graph typically referred to as an 'RDF graph'. Both nodes and edges in such graphs are identified via URIs; nodes represent Web resources, while edges stand for attributes of such resources or properties connecting them. Schema information can be expressed using languages such as RDFS and OWL, by which resources can be typed as classes described in terms of domain-specific attributes, properties and constraints. RDF graphs can be natively queried using the query language SPARQL. A SPARQL query is composed of graph patterns and can be stored as RDF triples together with any RDF domain model using SPIN to facilitate the definition of constraints and inference rules in ontologies.},
booktitle = {Proceedings of the 2nd International Conference on Web Intelligence, Mining and Semantics},
articleno = {1},
numpages = {3},
location = {Craiova, Romania},
series = {WIMS '12}
}

@inproceedings{10.1109/HICSS.2015.646,
author = {Dissanayake, Indika and Zhang, Jie and Yuan, Feirong and Wang, Jingguo},
title = {Peer-recognition and Performance in Online Crowdsourcing Communities},
year = {2015},
isbn = {9781479973675},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/HICSS.2015.646},
doi = {10.1109/HICSS.2015.646},
abstract = {Recent advances in information technology bring significant changes to the nature of social interactions and information exchange. Physical face-to-face communications are slowly replaced by online virtual communities. Motivated by this phenomenon, this research investigates how voluntary community involvement and self-disclosure behavior impact a member's peer-recognition and task performance within a virtual crowd sourcing competition community. We collected secondary data from the discussion forums of a specialized crowd sourcing platform that focuses on data analytics projects. Our results reveal that a member's community involvement improves both the peer-recognition of the member in the community and his/her performance ranking. Our findings have strategic implications to participants of virtual crowd sourcing communities and other professional online communities.},
booktitle = {Proceedings of the 2015 48th Hawaii International Conference on System Sciences},
pages = {4262–4265},
numpages = {4},
series = {HICSS '15}
}

@inproceedings{10.1145/2493432.2493481,
author = {Goncalves, Jorge and Ferreira, Denzil and Hosio, Simo and Liu, Yong and Rogstadius, Jakob and Kukka, Hannu and Kostakos, Vassilis},
title = {Crowdsourcing on the spot: altruistic use of public displays, feasibility, performance, and behaviours},
year = {2013},
isbn = {9781450317702},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2493432.2493481},
doi = {10.1145/2493432.2493481},
abstract = {This study is the first attempt to investigate altruistic use of interactive public displays in natural usage settings as a crowdsourcing mechanism. We test a non-paid crowdsourcing service on public displays with eight different motivation settings and analyse users' behavioural patterns and crowdsourcing performance (e.g., accuracy, time spent, tasks completed). The results show that altruistic use, such as for crowdsourcing, is feasible on public displays, and through the controlled use of motivational design and validation check mechanisms, performance can be improved. The results shed insights on three research challenges in the field: i) how does crowdsourcing performance on public displays compare to that of online crowdsourcing, ii) how to improve the quality of feedback collected from public displays which tends to be noisy, and iii) identify users' behavioural patterns towards crowdsourcing on public displays in natural usage settings.},
booktitle = {Proceedings of the 2013 ACM International Joint Conference on Pervasive and Ubiquitous Computing},
pages = {753–762},
numpages = {10},
keywords = {altruism, crowdsourcing, motivation, public displays},
location = {Zurich, Switzerland},
series = {UbiComp '13}
}

@inproceedings{10.1109/SERVICES-I.2009.56,
author = {Vukovic, Maja},
title = {Crowdsourcing for Enterprises},
year = {2009},
isbn = {9780769537085},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/SERVICES-I.2009.56},
doi = {10.1109/SERVICES-I.2009.56},
abstract = {Crowdsourcing is emerging as the new on-line distributed problem solving and production model in which networked people collaborate to complete a task.Enterprises are increasingly employing crowdsourcing to access scalable workforce on-line. In parallel, cloud computing has emerged as a new paradigm for delivering computational services, which seamlessly interweave physical and digital worlds through a common infrastructure.This paper presents a sample crowdsourcing scenario in software development domain to derive the requirements for delivering a general-purpose crowdsourcing service in the Cloud. It proposes taxonomy for categorization of crowdsourcing platforms, and evaluates a number of existing systems against the set of identified features. Finally, the paper outlines a research agenda for enhancing crowdsourcing capabilities, with focus on virtual team building and task-based service provisioning, whose lack has been a barrier to the realization of a peer-production model that engages providers from around the world.},
booktitle = {Proceedings of the 2009 Congress on Services - I},
pages = {686–692},
numpages = {7},
keywords = {cloud, crowdsourcing, people services},
series = {SERVICES '09}
}

@inproceedings{10.5555/2888116.2888384,
author = {Bigham, Jeffrey P.},
title = {What's hot in crowdsourcing and human computation},
year = {2015},
isbn = {0262511290},
publisher = {AAAI Press},
abstract = {The focus of HCOMP 2014 was the crowd worker. While crowdsourcing is motivated by the promise of leveraging people's intelligence and diverse skillsets in computational processes, the human aspects of this workforce are all too often overlooked. Instead, workers are frequently viewed as interchangeable components that can be statistically managed to eek out reasonable outputs. We are quickly moving past and rejecting these notions, and beginning to understand that it is sometimes the very abstractions that we introduce to make human computation feasible, e.g., abstracting humans behind APIs or isolating workers from others in order to ensure independent input, that can lead to the problems that we then set about trying to solve, e.g., poor or inconsistent quality work. Creating a brighter future for crowd work will require new socio-technical systems that not only decompose tasks, recruit and coordinate workers, and make sense of results, but also find interesting tasks for people to contribute to, structure tasks so that workers learn from them as they go, and eventually automate mundane parts of work. Research in artificial intelligence will be vital for achieving this future.},
booktitle = {Proceedings of the Twenty-Ninth AAAI Conference on Artificial Intelligence},
pages = {4318–4319},
numpages = {2},
location = {Austin, Texas},
series = {AAAI'15}
}

@inproceedings{10.1007/978-3-319-91563-0_9,
author = {Suzuki, Rikuya and Sakaguchi, Tetsuo and Matsubara, Masaki and Kitagawa, Hiroyuki and Morishima, Atsuyuki},
title = {CrowdSheet: An Easy-To-Use One-Stop Tool for Writing and Executing Complex Crowdsourcing},
year = {2018},
isbn = {978-3-319-91562-3},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-319-91563-0_9},
doi = {10.1007/978-3-319-91563-0_9},
abstract = {Developing crowdsourcing applications with dataflows among tasks requires requesters to submit tasks to crowdsourcing services, obtain results, write programs to process the results, and often repeat this process. This paper proposes CrowdSheet, an application that provides a spreadsheet interface to easily write and execute such complex crowdsourcing applications. We prove that a natural extension to existing spreadsheets, with only two types of new spreadsheet functions, allows us to write a fairly wide range of real-world applications. Our experimental results indicate that many spreadsheet users can easily write complex crowdsourcing applications with CrowdSheet.},
booktitle = {Advanced Information Systems Engineering: 30th International Conference, CAiSE 2018, Tallinn, Estonia, June 11-15, 2018, Proceedings},
pages = {137–153},
numpages = {17},
keywords = {Rapid development, Complex crowdsourcing, Expressive power analysis},
location = {Tallinn, Estonia}
}

@inproceedings{10.1145/2386958.2386960,
author = {Chen, Xiao and Santos-Neto, Elizeu and Ripeanu, Matei},
title = {Crowdsourcing for on-street smart parking},
year = {2012},
isbn = {9781450316255},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2386958.2386960},
doi = {10.1145/2386958.2386960},
abstract = {Crowdsourcing has inspired a variety of novel mobile applications. However, identifying common practices across different applications is still challenging. In this paper, we use smart parking as a case study to investigate features of crowdsourcing that may apply to other mobile applications. Based on this we derive principles for efficiently harnessing crowdsourcing. We draw three key guidelines: First, we suggest that that the organizer can play an important role in coordinating participants', a key factor to successful crowdsourcing experience. Second, we suggest that the expected participation rate is a key factor when designing the crowdsourcing system: a system with a lower expected participation rate will place a higher burden in individual participants (e.g., through more complex interfaces that aim to improve the accuracy of the collected data). Finally, we suggest that not only above certain threshold of contributors, a crowdsourcing-based system is resilient to freeriding but, surprisingly, that including freeriders (i.e., actors that do not participate in system effort but share its benefits in terms of coordination) benefits the entire system.},
booktitle = {Proceedings of the Second ACM International Symposium on Design and Analysis of Intelligent Vehicular Networks and Applications},
pages = {1–8},
numpages = {8},
keywords = {collaborative sensing, mobile crowdsourcing, smart parking},
location = {Paphos, Cyprus},
series = {DIVANet '12}
}

@inproceedings{10.1145/2470654.2470684,
author = {Komarov, Steven and Reinecke, Katharina and Gajos, Krzysztof Z.},
title = {Crowdsourcing performance evaluations of user interfaces},
year = {2013},
isbn = {9781450318990},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2470654.2470684},
doi = {10.1145/2470654.2470684},
abstract = {Online labor markets, such as Amazon's Mechanical Turk (MTurk), provide an attractive platform for conducting human subjects experiments because the relative ease of recruitment, low cost, and a diverse pool of potential participants enable larger-scale experimentation and faster experimental revision cycle compared to lab-based settings. However, because the experimenter gives up the direct control over the participants' environments and behavior, concerns about the quality of the data collected in online settings are pervasive. In this paper, we investigate the feasibility of conducting online performance evaluations of user interfaces with anonymous, unsupervised, paid participants recruited via MTurk. We implemented three performance experiments to re-evaluate three previously well-studied user interface designs. We conducted each experiment both in lab and online with participants recruited via MTurk. The analysis of our results did not yield any evidence of significant or substantial differences in the data collected in the two settings: All statistically significant differences detected in lab were also present on MTurk and the effect sizes were similar. In addition, there were no significant differences between the two settings in the raw task completion times, error rates, consistency, or the rates of utilization of the novel interaction mechanisms introduced in the experiments. These results suggest that MTurk may be a productive setting for conducting performance evaluations of user interfaces providing a complementary approach to existing methodologies.},
booktitle = {Proceedings of the SIGCHI Conference on Human Factors in Computing Systems},
pages = {207–216},
numpages = {10},
keywords = {crowdsourcing, mechanical turk, user interface evaluation},
location = {Paris, France},
series = {CHI '13}
}

@inproceedings{10.1145/2145204.2145354,
author = {Kulkarni, Anand and Can, Matthew and Hartmann, Bj\"{o}rn},
title = {Collaboratively crowdsourcing workflows with turkomatic},
year = {2012},
isbn = {9781450310864},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2145204.2145354},
doi = {10.1145/2145204.2145354},
abstract = {Preparing complex jobs for crowdsourcing marketplaces requires careful attention to workflow design, the process of decomposing jobs into multiple tasks, which are solved by multiple workers. Can the crowd help design such workflows? This paper presents Turkomatic, a tool that recruits crowd workers to aid requesters in planning and solving complex jobs. While workers decompose and solve tasks, requesters can view the status of worker-designed workflows in real time; intervene to change tasks and solutions; and request new solutions to subtasks from the crowd. These features lower the threshold for crowd employers to request complex work. During two evaluations, we found that allowing the crowd to plan without requester supervision is partially successful, but that requester intervention during workflow planning and execution improves quality substantially. We argue that Turkomatic's collaborative approach can be more successful than the conventional workflow design process and discuss implications for the design of collaborative crowd planning systems.},
booktitle = {Proceedings of the ACM 2012 Conference on Computer Supported Cooperative Work},
pages = {1003–1012},
numpages = {10},
keywords = {crowdsourcing, task decomposition, workflows},
location = {Seattle, Washington, USA},
series = {CSCW '12}
}

@inproceedings{10.1145/2660114.2660118,
author = {Redi, Judith and Povoa, Isabel},
title = {Crowdsourcing for Rating Image Aesthetic Appeal: Better a Paid or a Volunteer Crowd?},
year = {2014},
isbn = {9781450331289},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2660114.2660118},
doi = {10.1145/2660114.2660118},
abstract = {Crowdsourcing has the potential to become a preferred tool to study image aesthetic appeal preferences of users. Nevertheless, some reliability issues still exist, partially due to the sometimes doubtful commitment of paid workers to perform the rating task properly. In this paper we compare the reliability in scoring image aesthetic appeal of both a paid and a volunteer crowd. We recruit our volunteers through Facebook and our paid users via Microworkers. We conclude that, whereas volunteer participants are more likely to leave the rating task unfinished, when they complete it they do so more reliably than paid users.},
booktitle = {Proceedings of the 2014 International ACM Workshop on Crowdsourcing for Multimedia},
pages = {25–30},
numpages = {6},
keywords = {aesthetics, crowdsourcing, photography, qoe},
location = {Orlando, Florida, USA},
series = {CrowdMM '14}
}

@inproceedings{10.1109/HICSS.2013.537,
author = {Simula, Henri},
title = {The Rise and Fall of Crowdsourcing?},
year = {2013},
isbn = {9780769548920},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/HICSS.2013.537},
doi = {10.1109/HICSS.2013.537},
abstract = {Crowd sourcing has been discussed both in academic and managerial articles in recent years. Despite some critical voices, the overall attitude towards crowd sourcing has been quite positive in extant literature. In this paper we want to address potential drawbacks and issue that create shadows on top of crowd sourcing. The overall purpose of this paper is to discuss the reasons why crowd sourcing initiatives may not always live up to the expectations placed upon them. Despite some seemingly successful case examples, not every crowd sourcing initiative has taken off. While some of the barriers are case or industry specific, there are also certain overall reasons hindering crowd sourcing from reaching de facto modus operandi, especially in the innovation creation context. This paper is intentionally written through a critical lens by design and hopefully provides a constructive balance for those with an overly positive approach towards crowd sourcing.},
booktitle = {Proceedings of the 2013 46th Hawaii International Conference on System Sciences},
pages = {2783–2791},
numpages = {9},
keywords = {crowdsourcing, future directions, issues, problems},
series = {HICSS '13}
}

@inproceedings{10.1145/2740908.2743052,
author = {Zubiaga, Arkaitz and Liakata, Maria and Procter, Rob and Bontcheva, Kalina and Tolmie, Peter},
title = {Crowdsourcing the Annotation of Rumourous Conversations in Social Media},
year = {2015},
isbn = {9781450334730},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2740908.2743052},
doi = {10.1145/2740908.2743052},
abstract = {Social media are frequently rife with rumours, and the study of rumour conversational aspects can provide valuable knowledge about how rumours evolve over time and are discussed by others who support or deny them. In this work, we present a new annotation scheme for capturing rumour-bearing conversational threads, as well as the crowdsourcing methodology used to create high quality, human annotated datasets of rumourous conversations from social media. The rumour annotation scheme is validated through comparison between crowdsourced and reference annotations. We also found that only a third of the tweets in rumourous conversations contribute towards determining the veracity of rumours, which reinforces the need for developing methods to extract the relevant pieces of information automatically.},
booktitle = {Proceedings of the 24th International Conference on World Wide Web},
pages = {347–353},
numpages = {7},
keywords = {annotation, crowdsourcing, rumors, rumours, social media},
location = {Florence, Italy},
series = {WWW '15 Companion}
}

@inproceedings{10.5555/2772879.2773387,
author = {Yu, Han and Miao, Chunyan and Shen, Zhiqi and Leung, Cyril},
title = {Quality and Budget Aware Task Allocation for Spatial Crowdsourcing},
year = {2015},
isbn = {9781450334136},
publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
address = {Richland, SC},
abstract = {A major research challenge for spatial crowdsourcing is to improve the expected quality of the results. However, existing research in this field mostly focuses on achieving this objective in volunteer-based spatial crowdsourcing. In this paper, we introduce the budget limitations into the above problem and consider realistic cases where workers are paid unequally based on their trustworthiness. We propose a novel quality and budget aware spatial task allocation approach which jointly considers the workers' reputation and proximity to the task locations to maximize the expected quality of the results while staying within a limited budget.},
booktitle = {Proceedings of the 2015 International Conference on Autonomous Agents and Multiagent Systems},
pages = {1689–1690},
numpages = {2},
keywords = {reputation, spatial crowdsourcing, task allocation, trust},
location = {Istanbul, Turkey},
series = {AAMAS '15}
}

@inproceedings{10.1145/1935826.1935831,
author = {Alonso, Omar and Lease, Matthew},
title = {Crowdsourcing 101: putting the WSDM of crowds to work for you},
year = {2011},
isbn = {9781450304931},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1935826.1935831},
doi = {10.1145/1935826.1935831},
abstract = {Crowdsourcing has emerged in recent years as an exciting new avenue for leveraging the tremendous potential and resources of today's digitally-connected, diverse, distributed workforce. Generally speaking, crowdsourcing describes outsourcing of tasks to a large group of people instead of assigning such tasks to an in-house employee or contractor. Crowdsourcing platforms such as Amazon Mechanical Turk and CrowdFlower have gained particular attention as active online market places for reaching and tapping into this glut of a still largely under-utilized workforce.Crowdsourcing offers intriguing new opportunities for accomplishing different kinds of tasks or achieving broader participation than previously possible, as well as completing standard tasks more accurately in less time and at lower cost. Unlocking the potential of crowdsourcing in practice, however, requires a tri-partite understanding of principles, platforms, and best practices. This tutorial will introduce the opportunities and challenges of crowdsourcing while discussing the three issues above. This will provide attendees with a basic foundation to begin applying crowdsourcing in the context of their own particular tasks.},
booktitle = {Proceedings of the Fourth ACM International Conference on Web Search and Data Mining},
pages = {1–2},
numpages = {2},
keywords = {crowdsourcing, human computation},
location = {Hong Kong, China},
series = {WSDM '11}
}

@inproceedings{10.1145/2506364.2506369,
author = {Tavares, Gon\c{c}alo and Mour\~{a}o, Andr\'{e} and Magalhaes, Jo\~{a}o},
title = {Crowdsourcing for affective-interaction in computer games},
year = {2013},
isbn = {9781450323963},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2506364.2506369},
doi = {10.1145/2506364.2506369},
abstract = {Affective-interaction in computer games is a novel area with several new challenges, such as detecting players' facial expressions (e.g., happy, sad, surprise) in a robust manner. In this paper we describe a crowdsourcing effort for creating the ground-truth of a large-scale dataset of images capturing users playing a computer game. The computer game is designed to elicit a particular facial expressions and the game will score the player according to the detected expression. For designing the crowdsourcing task, some of the examined variables include: reward, tagging limits, golden questions, workers' location. In the end, we designed a large tagging job to maximize workers agreement. Each image with a facial expressions is tagged with one of the following expressions labels: happy, anger, disgust, contempt, sad, fear, surprise, and neutral. The dataset included over 40,000 images, the workers' judgments, the game's detected facial expression and what facial expression the player should be performing.},
booktitle = {Proceedings of the 2nd ACM International Workshop on Crowdsourcing for Multimedia},
pages = {7–12},
numpages = {6},
keywords = {affective-interaction, crowdsourcing, facial expressions},
location = {Barcelona, Spain},
series = {CrowdMM '13}
}

@inproceedings{10.1109/CVPR.2013.81,
author = {Deng, Jia and Krause, Jonathan and Fei-Fei, Li},
title = {Fine-Grained Crowdsourcing for Fine-Grained Recognition},
year = {2013},
isbn = {9780769549897},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/CVPR.2013.81},
doi = {10.1109/CVPR.2013.81},
abstract = {Fine-grained recognition concerns categorization at sub-ordinate levels, where the distinction between object classes is highly local. Compared to basic level recognition, fine-grained categorization can be more challenging as there are in general less data and fewer discriminative features. This necessitates the use of stronger prior for feature selection. In this work, we include humans in the loop to help computers select discriminative features. We introduce a novel online game called "Bubbles" that reveals discriminative features humans use. The player's goal is to identify the category of a heavily blurred image. During the game, the player can choose to reveal full details of circular regions ("bubbles"), with a certain penalty. With proper setup the game generates discriminative bubbles with assured quality. We next propose the "Bubble Bank" algorithm that uses the human selected bubbles to improve machine recognition performance. Experiments demonstrate that our approach yields large improvements over the previous state of the art on challenging benchmarks.},
booktitle = {Proceedings of the 2013 IEEE Conference on Computer Vision and Pattern Recognition},
pages = {580–587},
numpages = {8},
series = {CVPR '13}
}

@inproceedings{10.1109/CGAMES.2011.6000339,
author = {van 't Woud, J. S. S. and Sandberg, J. A. C. and Wielinga, B. J.},
title = {The Mars crowdsourcing experiment: Is crowdsourcing in the form of a serious game applicable for annotation in a semantically-rich research domain?},
year = {2011},
isbn = {9781457714511},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/CGAMES.2011.6000339},
doi = {10.1109/CGAMES.2011.6000339},
abstract = {This study investigates crowdsourcing using a serious game concerning the annotation of semantically rich features of Mars. Photographic data is used transmitted from the Mars Reconnaissance Orbiter recording the surface. A computer game, called Cerberus, was developed allowing players to tag features of the Mars surface. The game investigated in four different conditions what the effects were of different help levels to support knowledge transfer and different levels of game features to provide the players with a stimulating game experience. The performance of the participating players was measured in terms of precision and motivation. Precision reflects the quality of the work done and motivation is represented by the amount of work done by the players. The four game conditions varied in an implicit and an explicit level of help and poor and rich game features. The game condition with the explicit help function combined with the rich game experience showed significantly more motivation among the players than the game condition with the implicit help function combined with the poor gaming experience. Precision did not show any significant difference between the game conditions, but was high enough to generate Mars maps exposing aeolian processes, surface layering, river meanders and other concepts. Apparently the players were capable of acquiring deeper concepts about Mars's geology and the results were of such a high quality that they could be used as input for and to reinforce scientific research.},
booktitle = {Proceedings of the 2011 16th International Conference on Computer Games},
pages = {201–208},
numpages = {8},
keywords = {Cerberus, Mars crowdsourcing experiment, Mars reconnaissance orbiter, Mars's geology, aeolian processes, computer game, knowledge transfer, motivation, photographic data, river meanders, semantically rich features, surface layering},
series = {CGAMES '11}
}

@inproceedings{10.1145/2837689.2837705,
author = {Keles, Ilkcan and Saltenis, Simonas and Jensen, Christian S.},
title = {Synthesis of partial rankings of points of interest using crowdsourcing},
year = {2015},
isbn = {9781450339377},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2837689.2837705},
doi = {10.1145/2837689.2837705},
abstract = {The web is increasingly being accessed from mobile devices, and studies suggest that a large fraction of keyword-based search engine queries have local intent, meaning that users are interested in local content and that the underlying ranking function should take into account both relevance to the query keywords and the query location. A key challenge in being able to make progress on the design of ranking functions is to be able to assess the quality of the results returned by ranking functions. We propose a model that synthesizes a ranking of points of interest from answers to crowdsourced pairwise relevance questions. To evaluate the model, we propose an innovative methodology that enables evaluation of the quality of synthesized rankings in a simulated setting. We report on an experimental evaluation based on the methodology that shows that the proposed model produces promising results in pertinent settings and that it is capable of outperforming an approach based on majority voting.},
booktitle = {Proceedings of the 9th Workshop on Geographic Information Retrieval},
articleno = {15},
numpages = {10},
keywords = {crowdsourcing, pairwise relevance, points of interest, ranking},
location = {Paris, France},
series = {GIR '15}
}

@inproceedings{10.1145/3051457.3053973,
author = {Whitehill, Jacob and Seltzer, Margo},
title = {A Crowdsourcing Approach to Collecting Tutorial Videos -- Toward Personalized Learning-at-Scale},
year = {2017},
isbn = {9781450344500},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3051457.3053973},
doi = {10.1145/3051457.3053973},
abstract = {We investigated the feasibility of crowdsourcing full- fledged tutorial videos from ordinary people on the Web on how to solve math problems related to logarithms. This kind of approach (a form of learnersourcing [9, 11]) to efficiently collecting tutorial videos and other learning resources could be useful for realizing personalized learning-at-scale, whereby students receive specific learning resources -- drawn from a large and diverse set -- that are tailored to their individual and time-varying needs. Results of our study, in which we collected 399 videos from 66 unique "teachers" on Mechanical Turk, suggest that (1) approximately 100 videos -- over 80\% of which are mathematically fully correct -- can be crowdsourced per week for $5/video; (2) the average learning gains (posttest minus pretest score) associated with watching the videos was stat. sig. higher than for a control video (0.105 versus 0.045); and (3) the average learning gains (0.1416) from watching the best tested crowdsourced videos was comparable to the learning gains (0.1506) from watching a popular Khan Academy video on logarithms.},
booktitle = {Proceedings of the Fourth (2017) ACM Conference on Learning @ Scale},
pages = {157–160},
numpages = {4},
keywords = {crowdsourcing, personalized learning},
location = {Cambridge, Massachusetts, USA},
series = {L@S '17}
}

@inproceedings{10.1145/3206129.3239431,
author = {Roy, Debopriyo},
title = {A Model for Language Learning with Crowdsourcing and Social Network Analysis for Community Decision-Making},
year = {2018},
isbn = {9781450365253},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3206129.3239431},
doi = {10.1145/3206129.3239431},
abstract = {Based on the fundamentals of content and language integrated learning (CLIL), this article provides a holistic overview of how different technological applications such as Google Maps, social network analysis (SNA), and in general, crowdsourcing of spatial and location-specific data could help identify poverty, and local socio-economic and lifestyle-oriented problems, and trigger a discussion about community decision making. Such use of technology could potentially help make a convincing case for the type of poverty; including exact issues in the area, proximity to resource hubs, lack of basic facilities, and employment and so on. The primary focus in this article is on how content language integrated learning (CLIL) combines content areas such as mechanism and technology for poverty identification and analysis on the way to learning the target language. Use of such technological applications in a foreign language-learning course for policy decision-making and community engagement is rather unique. We need for students to have a rich experience with different combinations of text-graphics-video modality including hands-on engagement, and language acquisition is expected to happen as a result. Technical communication could be an important focus for such courses with report writing, feasibility and recommendation studies, email communication, writing commentaries, chats, text captions, interviewing etc. With such use of technology and documentation, the aim is to empower students in revitalization efforts in the community.},
booktitle = {Proceedings of the 2nd International Conference on Education and Multimedia Technology},
pages = {14–19},
numpages = {6},
keywords = {Maps, crowdsourcing, design, language, poverty, social networks},
location = {Okinawa, Japan},
series = {ICEMT '18}
}

@inproceedings{10.1145/2145204.2145382,
author = {Ambati, Vamshi and Vogel, Stephan and Carbonell, Jaime},
title = {Collaborative workflow for crowdsourcing translation},
year = {2012},
isbn = {9781450310864},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2145204.2145382},
doi = {10.1145/2145204.2145382},
abstract = {In this paper we explore the challenges in crowdsourcing the task of translation over the web in which remotely located translators work on providing translations independent of each other. We then propose a collaborative workflow for crowdsourcing translation to address some of these challenges. In our pipeline model, the translators are working in phases where output from earlier phases can be enhanced in the subsequent phases. We also highlight some of the novel contributions of the pipeline model like assistive translation and translation synthesis that can leverage monolingual and bilingual speakers alike. We evaluate our approach by eliciting translations for both a minority-to-majority language pair and a minority-to-minority language pair. We observe that in both scenarios, our workflow produces better quality translations in a cost-effective manner, when compared to the traditional crowdsourcing workflow.},
booktitle = {Proceedings of the ACM 2012 Conference on Computer Supported Cooperative Work},
pages = {1191–1194},
numpages = {4},
keywords = {amazon mechanical turk, collaborative workflow, crowdsourcing, language translation},
location = {Seattle, Washington, USA},
series = {CSCW '12}
}

@inproceedings{10.1109/eScience.2015.63,
author = {Martinez-Ortiz, Carlos and Aroyo, Lora and Inel, Oana and Champilomatis, Stavros and Dumitrache, Anca and Timmermans, Benjamin},
title = {Provenance-driven Representation of Crowdsourcing Data for Efficient Data Analysis},
year = {2015},
isbn = {9781467393256},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/eScience.2015.63},
doi = {10.1109/eScience.2015.63},
abstract = {Crowdsourcing has proved to be a feasible way of harnessing human computation for solving complex problems. However, crowdsourcing frequently faces various challenges: data handling, task reusability, and platform selection. Domain scientists rely on eScientists to find solutions for these challenges. CrowdTruth is a framework that builds on existing crowdsourcing platforms and provides an enhanced way to manage crowdsourcing tasks across platforms, offering solutions to commonly faced challenges. Provenance modeling proves means for documenting and examining scientific workflows. CrowdTruth keeps a provenance trace of the data flow through the framework, thus allowing to trace how data was transformed and by whom to reach its final state. In this way, eScientists have a tool to determine the impact that crowdsourcing has on enhancing their data.},
booktitle = {Proceedings of the 2015 IEEE 11th International Conference on E-Science},
pages = {300–303},
numpages = {4},
keywords = {Crowdsourcing, provenance},
series = {E-SCIENCE '15}
}

@inproceedings{10.1609/aaai.v37i13.27045,
author = {Xu, Ruoyu and Li, Gaoxiang and Jin, Wei and Chen, Austin and Sheng, Victor S.},
title = {ACCD: an adaptive clustering-based collusion detector in crowdsourcing (student abstract)},
year = {2023},
isbn = {978-1-57735-880-0},
publisher = {AAAI Press},
url = {https://doi.org/10.1609/aaai.v37i13.27045},
doi = {10.1609/aaai.v37i13.27045},
abstract = {Crowdsourcing is a popular method for crowd workers to collaborate on tasks. However, workers coordinate and share answers during the crowdsourcing process. The term for this is "collusion". Copies from others and repeated submissions are detrimental to the quality of the assignments. The majority of the existing research on collusion detection is limited to ground truth problems (e.g., labeling tasks) and requires a predetermined threshold to be established in advance. In this paper, we aim to detect collusion behavior of workers in an adaptive way, and propose an Adaptive Clustering Based Collusion Detection approach (ACCD) for a broad range of task types and data types solved via crowdsourcing (e.g., continuous rating with or without distributions). Extensive experiments on both real-world and synthetic datasets show the superiority of ACCD over state-of-the-art approaches.},
booktitle = {Proceedings of the Thirty-Seventh AAAI Conference on Artificial Intelligence and Thirty-Fifth Conference on Innovative Applications of Artificial Intelligence and Thirteenth Symposium on Educational Advances in Artificial Intelligence},
articleno = {1976},
numpages = {2},
series = {AAAI'23/IAAI'23/EAAI'23}
}

@inproceedings{10.1109/DASC.2011.60,
author = {Afridi, Ahmad Hassan},
title = {Crowdsourcing in Mobile: A Three Stage Context Based Process},
year = {2011},
isbn = {9780769546124},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/DASC.2011.60},
doi = {10.1109/DASC.2011.60},
abstract = {Crowdsourcing is a new paradigm that has influenced the businesses, emergency management, collaboration and online interaction. The term crowd source means tapping the power of crowd. Research in crowd sourcing involves issues such as quality of data, incentives, working modal and contracts. This research deals with the issues of uncertainty and quality of social mobile computing systems. It suggests the relationship between context and crowd sourcing activity. This understanding can enhance the future and existing applications in a way that it can optimize systems response, adapt the components and decrease the uncertainty.},
booktitle = {Proceedings of the 2011 IEEE Ninth International Conference on Dependable, Autonomic and Secure Computing},
pages = {242–245},
numpages = {4},
keywords = {Crowdsourcing, Mobile Context Aware Systems},
series = {DASC '11}
}

@inproceedings{10.1145/2390803.2390820,
author = {Nghiem, Thi Phuong and Carlier, Axel and Morin, Geraldine and Charvillat, Vincent},
title = {Enhancing online 3D products through crowdsourcing},
year = {2012},
isbn = {9781450315890},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2390803.2390820},
doi = {10.1145/2390803.2390820},
abstract = {In this paper, we propose to build semantic links between a product's textual description and its corresponding 3D visualization. These links help gathering knowledge about a product and ease browsing its 3D model. Our goal is to support the common behavior that when reading a textual information of a product, users naturally imagine how it looks like in real life. We generate the association between a textual description and a 3D feature from crowdsourcing. A user study of 82 people assesses the usefulness of the association for subsequent users, both for correctness and efficiency. Users are asked to perform the identification of features on 3D models; from the traces, associations leading to recommended views are derived. This information (recommended view) is proposed to subsequent users for performing the same task. Whereas the associations could be simply given by an expert, crowdsourcing offers advantages: we have inexpensive experts in the crowd as well as a natural access to users' (eg. customers') preferences and opinions.},
booktitle = {Proceedings of the ACM Multimedia 2012 Workshop on Crowdsourcing for Multimedia},
pages = {47–52},
numpages = {6},
keywords = {crowdsourcing, web semantics, web3d technologies},
location = {Nara, Japan},
series = {CrowdMM '12}
}

@inproceedings{10.1007/978-3-319-25639-9_20,
author = {Morbidoni, Christian and Piccioli, Alessio},
title = {Curating a Document Collection via Crowdsourcing with Pundit 2.0},
year = {2015},
isbn = {978-3-319-25638-2},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-319-25639-9_20},
doi = {10.1007/978-3-319-25639-9_20},
abstract = {Pundit 2.0 is a semantic web annotation system that supports users in creating structured data on top of web pages. Annotations in Pundit are RDF triples that users build starting from web page elements, as text or images. Annotations can be made public and developers can access and combine them into RDF knowledge graphs, while authorship of each triple is always retrievable. In this demo we showcase Pundit 2.0 and demonstrate how it can be used to enhance a digital library, by providing a data crowdsourcing platform. Pundit enables users to annotate different kind of entities and to contribute to the collaborative creation of a knowledge graph. This, in turn, refines in real-time the exploration functionalities of the library’s faceted search, providing an immediate added value out of the annotation effort. Ad-hoc configurations can be used to drive specific visualisations, like the timeline-map shown in this demo.},
booktitle = {The Semantic Web: ESWC 2015 Satellite Events: ESWC 2015 Satellite Events, Portoro\v{z}, Slovenia, May 31 – June 4, 2015, Revised Selected Papers},
pages = {102–106},
numpages = {5},
keywords = {Semantic annotation, Linked data, Faceted browsing, Digital humanities, Pundit},
location = {Portoro\v{z}, Slovenia}
}

@inproceedings{10.1145/2818052.2869074,
author = {Saito, Susumu and Kobayashi, Tetsunori and Nakano, Teppei},
title = {Towards a Framework for Collaborative Video Surveillance System Using Crowdsourcing},
year = {2016},
isbn = {9781450339506},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2818052.2869074},
doi = {10.1145/2818052.2869074},
abstract = {This paper proposes a new framework for video surveillance systems for crime prevention. The main purpose of this framework is to help provide reasonable and stable solutions for automated video surveillance systems in a collaborative way. This framework is characterized by a verification process using crowdsourcing after the image analysis process: automated image analyzer detects as many suspicious events as possible followed by filtering process using human intelligence, to achieve both high re-call and high precision rates. Here we describe the basic mechanisms for collaboration between camera devices, data stores, image analyzers and surveillance crowds.},
booktitle = {Proceedings of the 19th ACM Conference on Computer Supported Cooperative Work and Social Computing Companion},
pages = {393–396},
numpages = {4},
keywords = {Collaborative video surveillance, crowdsourcing, framework},
location = {San Francisco, California, USA},
series = {CSCW '16 Companion}
}

@inproceedings{10.1109/IMIS.2014.49,
author = {Barr\'{o}n, Jos\'{e} Pablo G\'{o}mez and Manso, Miguel \'{A}ngel and Alcarria, Ram\'{o}n and G\'{o}mez, Rufino P\'{e}rez},
title = {A Mobile Crowdsourcing Platform for Urban Infrastructure Maintenance},
year = {2014},
isbn = {9781479943319},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/IMIS.2014.49},
doi = {10.1109/IMIS.2014.49},
abstract = {Geospatial mobile applications interact with local and remote hardware to connect various data sources. These applications are currently evolving to include crowd sourcing functionalities, for citizen participation. In this paper we propose the identification, collection and information exchange of smart objects their geographic information, facilitating connectivity and communication between citizens and local organizations, and building interoperable services with data from sensors, smart physical objects and social media. We propose to identify spatial patterns, report object locations to identify problems and improve maintenance strategies of a city. As a validation of this approach, we develop a mobile mapping and data hub platform to visualize, monitor, and assist urban maintenance planning that enables better interaction between citizens of Smart Cities.},
booktitle = {Proceedings of the 2014 Eighth International Conference on Innovative Mobile and Internet Services in Ubiquitous Computing},
pages = {358–363},
numpages = {6},
keywords = {NFC, crowdsourcing, geo-information, geographical characterization, social media},
series = {IMIS '14}
}

@inproceedings{10.1109/ICDCS.2014.9,
author = {Boutsis, Ioannis and Kalogeraki, Vana},
title = {On Task Assignment for Real-Time Reliable Crowdsourcing},
year = {2014},
isbn = {9781479951697},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/ICDCS.2014.9},
doi = {10.1109/ICDCS.2014.9},
abstract = {With the rapid growth of mobile smartphone users, several commercial mobile companies have exploited crowd sourcing as an effective approach to collect and analyze data, to improve their services. In a crowd sourcing system, "human workers" are enlisted to perform small tasks, that are difficult to be automated, in return for some monetary compensation. This paper presents our crowd sourcing system that seeks to address the challenge of determining the most efficient allocation of tasks to the human crowd. The goal of our algorithm is to efficiently determine the most appropriate set of workers to assign to each incoming task, so that the real-time demands are met and high quality results are returned. We empirically evaluate our approach and show that our system effectively meets the requested demands, has low overhead and can improve the number of tasks processed under the defined constraints over 71\% compared to traditional approaches.},
booktitle = {Proceedings of the 2014 IEEE 34th International Conference on Distributed Computing Systems},
pages = {1–10},
numpages = {10},
keywords = {distributed systems, crowdsourcing, real-time},
series = {ICDCS '14}
}

@inproceedings{10.1109/SOLI.2016.7551657,
author = {Meht\"{a}l\"{a}, Joanna and Kauranen, Ilkka and Karjalainen, Jesse and Nyberg, Timo},
title = {A crowdsourcing model for new idea development in the fashion industry},
year = {2016},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/SOLI.2016.7551657},
doi = {10.1109/SOLI.2016.7551657},
abstract = {Crowdsourcing is a powerful tool for new product development; companies create online platforms where customers can contribute their own ideas to design tasks, often motivated by either a monetary prize or gaining social status. The phenomenon can help companies gain significant competitive advantage due to e.g. reduced R&amp;D costs and increased customer satisfaction, but to be successful, crowdsourcing needs to be implemented efficiently. The crowdsourcing platform and the practices related to it need to be built to enable effective data management and evaluation. Fashion is a very promising area for crowdsourcing due to its rapidly evolving nature. Styles can change overnight, which means that brands need to rely more and more on the knowledge of their customers to be able to provide them with the newest trends. This article first provides an overview on current crowdsourcing practices and crowdsourcing platforms used by companies in general, and then discusses special requirements that the fashion industry sets for crowdsourcing practices and crowdsourcing platforms. At the end, the article presents a new model of crowdsourcing processes that are especially suitable for the fashion industry in the design phase of new fashion garments.},
booktitle = {2016 IEEE International Conference on Service Operations and Logistics, and Informatics (SOLI)},
pages = {29–36},
numpages = {8},
location = {Beijing, China}
}

@inproceedings{10.1145/3127404.3127408,
author = {Fang, Yili and Chen, Pengpeng and Sun, Kai and Sun, Hailong},
title = {A Decision Tree Based Quality Control Framework for Multi-phase Tasks in Crowdsourcing},
year = {2017},
isbn = {9781450353526},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3127404.3127408},
doi = {10.1145/3127404.3127408},
abstract = {In crowdsourcing, there exists an important category of tasks that comprise an ordered sequence of subtasks, which we refer to as Multi-phase Tasks (MPTs) - e.g. travel planning, translation and micro-writing. Existing result inference methods are ineffective for processing MPTs. The constrained relationships among phase-level subtasks of MPT cannot be ignored for two reasons. First, it is ineffective to conduct a MPT without phase-processing, e.g. for travel planning, recommending a complete route of travel planning, and using existing methods to infer the final result generated by an individual worker can hardly meet various requirements due to the lack of flexibility. Second, although a MPT consists of a set of phase-level subtasks, it is unsuitable to simply split a MPT into subtasks and use top-k methods to recommend final results; because this will not only increase costs but also lose the constrained relationships among the phases. Thus it calls for a new approach to handle MPTs. This research first introduces the concept of MPT to identify these special tasks. Second, a decision tree based framework is provided to control task generation and final result combination in the crowdsourcing cooperative workflow for MPTs. Third, a probabilistic graphical model is proposed to characterize the subtasks of each MPT phase and a maximum likelihood based method is designed for result inference. Finally, extensive experiments were conducted based on real-world travel planning tasks and experimental results demonstrate the superiority of this approach in comparison with the state-of-the-art methods.},
booktitle = {Proceedings of the 12th Chinese Conference on Computer Supported Cooperative Work and Social Computing},
pages = {10–17},
numpages = {8},
keywords = {Crowdsourcing, multiphase tasks, planning, quality control, result inference},
location = {Chongqing, China},
series = {ChineseCSCW '17}
}

@inproceedings{10.1109/ACII.2013.18,
author = {Morris, Robert R. and Dontcheva, Mira and Finkelstein, Adam and Gerber, Elizabeth},
title = {Affect and Creative Performance on Crowdsourcing Platforms},
year = {2013},
isbn = {9780769550480},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/ACII.2013.18},
doi = {10.1109/ACII.2013.18},
abstract = {Performance on crowd sourcing platforms varies greatly, especially for tasks requiring significant cognitive effort or creative insight. Researchers have proposed several techniques to address these problems, yet few have considered the role of affect, despite the well-established link between positive affect and creative performance. In this paper, we examine two affective techniques to boost creativity on crowd sourcing platforms - affective priming and affective pre-screening. Across three experiments, we find divergent results, depending on which technique is used. We find that not all happy crowd workers are alike. Those that are primed to feel happy exhibit enhanced creative performance, whereas those that merely report feeling happy exhibit impaired creative performance. We examine these findings in light of preexisting research on creativity, affect, and mood saliency. Lastly, we show how our findings have implications not only for crowd sourcing platforms, but also for other human-computer interaction scenarios that involve affect and creative performance.},
booktitle = {Proceedings of the 2013 Humaine Association Conference on Affective Computing and Intelligent Interaction},
pages = {67–72},
numpages = {6},
keywords = {affective computing, affective priming, affective self-report, creativity, crowdsourcing},
series = {ACII '13}
}

@inproceedings{10.5555/2343896.2343988,
author = {Kamar, Ece and Horvitz, Eric},
title = {Incentives for truthful reporting in crowdsourcing},
year = {2012},
isbn = {0981738133},
publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
address = {Richland, SC},
abstract = {A challenge with the programmatic access of human talent via crowdsourcing platforms is the specification of incentives and the checking of the quality of contributions. Methodologies for checking quality include providing a payment if the work is approved by the task owner and hiring additional workers to evaluate contributors' work. Both of these approaches place a burden on people and on the organizations commissioning tasks, and may be susceptible to manipulation by workers and task owners. Moreover, neither a task owner nor the task market may know the task well enough to be able to evaluate worker reports. Methodologies for incentivizing workers without external quality checking include rewards based on agreement with a peer worker or with the final output of the system. These approaches are vulnerable to strategic manipulations by workers. Recent experiments on Mechanical Turk have demonstrated the negative influence of manipulations by workers and task owners on crowdsourcing systems [3]. We address this central challenge by introducing incentive mechanisms that promote truthful reporting in crowdsourcing and discourage manipulation by workers and task owners without introducing additional overhead.},
booktitle = {Proceedings of the 11th International Conference on Autonomous Agents and Multiagent Systems - Volume 3},
pages = {1329–1330},
numpages = {2},
keywords = {crowdsourcing systems, peer-prediction rules},
location = {Valencia, Spain},
series = {AAMAS '12}
}

@inproceedings{10.1145/2595188.2595199,
author = {Forn\'{e}s, Alicia and Llad\'{o}s, Josep and Mas, Joan and Pujades, Joana Maria and Cabr\'{e}, Anna},
title = {A bimodal crowdsourcing platform for demographic historical manuscripts},
year = {2014},
isbn = {9781450325882},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2595188.2595199},
doi = {10.1145/2595188.2595199},
abstract = {In this paper we present a crowdsourcing web-based application for extracting information from demographic handwritten document images. The proposed application integrates two points of view: the semantic information for demographic research, and the ground-truthing for document analysis research. Concretely, the application has the contents view, where the information is recorded into forms, and the labeling view, with the word labels for evaluating document analysis techniques. The crowdsourcing architecture allows to accelerate the information extraction (many users can work simultaneously), validate the information, and easily provide feedback to the users. We finally show how the proposed application can be extended to other kind of demographic historical manuscripts.},
booktitle = {Proceedings of the First International Conference on Digital Access to Textual Cultural Heritage},
pages = {103–108},
numpages = {6},
keywords = {crowdsourcing, document image analysis, ground-truth generation, historical documents},
location = {Madrid, Spain},
series = {DATeCH '14}
}

@inproceedings{10.1007/978-3-662-45960-7_19,
author = {Poblet, Marta and Garc\'{\i}a-Cuesta, Esteban and Casanovas, Pompeu},
title = {Crowdsourcing Tools for Disaster Management: A Review of Platforms and Methods},
year = {2013},
isbn = {9783662459591},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-662-45960-7_19},
doi = {10.1007/978-3-662-45960-7_19},
abstract = {Recent advances on information technologies and communications, coupled with the advent of the social media applications have fuelled a new landscape of emergency and disaster response systems by enabling affected citizens to generate georeferenced real time information on critical events. The identification and analysis of such events is not straightforward and the application of crowdsourcing methods or automatic tools is needed for that purpose. Whereas crowdsourcing makes emphasis on the resources of people to produce, aggregate, or filter original data, automatic tools make use of information retrieval techniques to analyze publicly available information. This paper reviews a set of online tools and platforms implemented in recent years which are currently being applied in the area of emergency management and proposes a taxonomy for its categorization.},
booktitle = {Revised Selected Papers of the AICOL 2013 International Workshops on AI Approaches to the Complexity of Legal Systems - Volume 8929},
pages = {261–274},
numpages = {14},
keywords = {crowdsensing, crowdsourcing, disaster management, emergency management, micro-tasking, mobile apps, platforms}
}

@proceedings{10.1145/2810188,
title = {CrowdMM '15: Proceedings of the Fourth International Workshop on Crowdsourcing for Multimedia},
year = {2015},
isbn = {9781450337465},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {Crowdsourcing has the potential to address key challenges in multimedia research. Multimedia evaluation, annotation, retrieval and creation can be obtained at a low time and monetary cost from the contribution of large crowds and by leveraging human computation. In fact, the applicative frontiers of this potential are yet to be discovered. And yet, challenges already arise as to how to cautiously exploit it. The crowd, as a users' (workers) community, is a complex and dynamic system highly sensitive to changes in the form and the parameterization of their activities. Issues concerning motivation, reliability, and engagement are being more and more often documented, and need to be addressed.Since 2012, the International ACM Workshop on Crowdsourcing for Multimedia (CrowdMM) has welcomed new insights on the effective deployment of crowdsourcing towards boosting Multimedia research. On its fourth year, CrowdMM15 focuses on contributions addressing the key challenges that still hinder widespread adoption of crowdsourcing paradigms in the multimedia research community: remote monitoring of the user behavior, effective test design, controlling noise and quality in the results, designing incentive structures that do not breed cheating, and effective ways of keeping the user (the crowd!) in the loop to boost multimedia applications.The call for papers attracted a good number of international submissions, two of which short papers. Of these, three were accepted as oral presentations and four as posters. All papers received at least three double blind reviews, and 3.5 reviews on average.CrowdMM15 also proudly features the keynote talk of Prof. Shih-Fu Chang (Columbia University), addressing Crowdsourcing in video event detection, sentiment analysis and user intent modelling. Furthermore, for the second year this year CrowdMM proposes the Crowdkeynote: a crowd-sourced keynote, during which all members of the CrowdMM community give their view on the future and the Challenges that Crowdsourcing has still ahead. The slides of the Crowdkeynote can be found at https://goo.gl/Xlur2E.},
location = {Brisbane, Australia}
}

@inproceedings{10.1145/2487575.2487600,
author = {Baba, Yukino and Kashima, Hisashi},
title = {Statistical quality estimation for general crowdsourcing tasks},
year = {2013},
isbn = {9781450321747},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2487575.2487600},
doi = {10.1145/2487575.2487600},
abstract = {One of the biggest challenges for requesters and platform providers of crowdsourcing is quality control, which is to expect high-quality results from crowd workers who are neither necessarily very capable nor motivated. A common approach to tackle this problem is to introduce redundancy, that is, to request multiple workers to work on the same tasks. For simple multiple-choice tasks, several statistical methods to aggregate the multiple answers have been proposed. However, these methods cannot always be applied to more general tasks with unstructured response formats such as article writing, program coding, and logo designing, which occupy the majority on most crowdsourcing marketplaces. In this paper, we propose an unsupervised statistical quality estimation method for such general crowdsourcing tasks. Our method is based on the two-stage procedure; multiple workers are first requested to work on the same tasks in the creation stage, and then another set of workers review and grade each artifact in the review stage. We model the ability of each author and the bias of each reviewer, and propose a two-stage probabilistic generative model using the graded response model in the item response theory. Experiments using several general crowdsourcing tasks show that our method outperforms popular vote aggregation methods, which implies that our method can deliver high quality results with lower costs.},
booktitle = {Proceedings of the 19th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {554–562},
numpages = {9},
keywords = {crowdsourcing, human computation, quality control},
location = {Chicago, Illinois, USA},
series = {KDD '13}
}

@inproceedings{10.1145/3578503.3583622,
author = {Marshall, Catherine C. and Goguladinne, Partha S.R. and Maheshwari, Mudit and Sathe, Apoorva and Shipman, Frank M.},
title = {Who Broke Amazon Mechanical Turk? An Analysis of Crowdsourcing Data Quality over Time},
year = {2023},
isbn = {9798400700897},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3578503.3583622},
doi = {10.1145/3578503.3583622},
abstract = {We present the results of a survey fielded in June of 2022 as a lens to examine recent data reliability issues on Amazon Mechanical Turk. We contrast bad data from this survey with bad data from the same survey fielded among US workers in October 2013, April 2018, and February 2019. Application of an established data cleaning scheme reveals that unusable data has risen from a little over 2\% in 2013 to almost 90\% in 2022. Through symptomatic diagnosis, we attribute the data reliability drop not to an increase in bad faith work, but rather to a continuum of English proficiency levels. A qualitative analysis of workers’ responses to open-ended questions allows us to distinguish between low fluency workers, ultra-low fluency workers, satisficers, and bad faith workers. We go on to show the effects of the new low fluency work on Likert scale data and on the study's qualitative results. Attention checks are shown to be much less effective than they once were at identifying survey responses that should be discarded.},
booktitle = {Proceedings of the 15th ACM Web Science Conference 2023},
pages = {335–345},
numpages = {11},
keywords = {Crowdsourcing, Mechanical Turk, data cleaning, data quality},
location = {Austin, TX, USA},
series = {WebSci '23}
}

@inproceedings{10.5555/2986459.2986660,
author = {Wauthier, Fabian L. and Jordan, Michael I.},
title = {Bayesian Bias Mitigation for Crowdsourcing},
year = {2011},
isbn = {9781618395993},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Biased labelers are a systemic problem in crowdsourcing, and a comprehensive toolbox for handling their responses is still being developed. A typical crowdsourcing application can be divided into three steps: data collection, data curation, and learning. At present these steps are often treated separately. We present Bayesian Bias Mitigation for Crowdsourcing (BBMC), a Bayesian model to unify all three. Most data curation methods account for the effects of labeler bias by modeling all labels as coming from a single latent truth. Our model captures the sources of bias by describing labelers as influenced by shared random effects. This approach can account for more complex bias patterns that arise in ambiguous or hard labeling tasks and allows us to merge data curation and learning into a single computation. Active learning integrates data collection with learning, but is commonly considered infeasible with Gibbs sampling inference. We propose a general approximation strategy for Markov chains to efficiently quantify the effect of a perturbation on the stationary distribution and specialize this approach to active learning. Experiments show BBMC to outperform many common heuristics.},
booktitle = {Proceedings of the 24th International Conference on Neural Information Processing Systems},
pages = {1800–1808},
numpages = {9},
location = {Granada, Spain},
series = {NIPS'11}
}

@inproceedings{10.1145/3277139.3277177,
author = {Zhang, Xi-zheng and Zhang, Leshao and Luo, Wen},
title = {A task assignment model and its application for crowdsourcing project factored multi-objective and risks},
year = {2018},
isbn = {9781450364867},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3277139.3277177},
doi = {10.1145/3277139.3277177},
abstract = {Assigning tasks to workers for a crowdsourcing project is challenging and needs to consider worker capacity, project duration, total cost and some operational constraints. We formulate the problem as a tri-objective optimisation model, which minimises project duration while simultaneously achieving lower total cost and fewer workers under the considering the capacity risk of contractor. A heuristic algorithm is developed to obtain Pareto efficient solutions. we used the fuzzy AHP(analytic hierarchy process) to confirm the priority among the time cost and the number of selected workers in the process of collaborative manufacturing task allocation. Finally, we designed the heuristic algorithm to solve the multi- objectives model and verified the validity superiority of the algorithm through a specific example.},
booktitle = {Proceedings of the 1st International Conference on Information Management and Management Science},
pages = {174–179},
numpages = {6},
keywords = {crowdsourcing, heuristic algorithm, multi-objective, task assignment},
location = {Chengdu, China},
series = {IMMS '18}
}

@inproceedings{10.1109/CCNC.2019.8651850,
author = {Blanc, Nicolas and Liu, Zhan and Ertz, Olivier and Rojas, Diego and Sandoz, Romain and Sokhn, Maria and Ingensand, Jens and Loubier, Jean-Christophe},
title = {Building a Crowdsourcing based Disabled Pedestrian Level of Service routing application using Computer Vision and Machine Learning},
year = {2019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/CCNC.2019.8651850},
doi = {10.1109/CCNC.2019.8651850},
abstract = {The availability of global and scalable tools to assess disabled pedestrian level of service (DPLoS) is a real need, yet still a challenge in today’s world. This is due to the lack of tools that can ease the measurement of a level of service adapted to disabled people, and also to the limitation concerns about the availability of information regarding the existing level of service, especially in real time. This paper describes preliminary results to progress on those needs. It also includes a design for a navigation tool that can help a disabled person move around a city by suggesting the most adapted routes according to the person’s disabilities. The main topics are how to use advanced computer vision technologies, and how to benefit from the prevalence of handheld devices. Our approach intends to show how crowdsourcing techniques can improve data quality by gathering and combining up-to-date data with valuable field observations.},
booktitle = {2019 16th IEEE Annual Consumer Communications \&amp; Networking Conference (CCNC)},
pages = {1–5},
numpages = {5},
location = {Las Vegas, NV, USA}
}

@inproceedings{10.1145/3019612.3019923,
author = {Faisal, Mohammad Imran},
title = {Predicting the quality of contests on crowdsourcing-based software development platforms: student research abstract},
year = {2017},
isbn = {9781450344869},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3019612.3019923},
doi = {10.1145/3019612.3019923},
abstract = {As an emerging and promising approach, crowdsourcing-based software development becomes popular in many domains due to the participation of talented pool of developers in the contests, and to promote the ability of requesters (or customers) to choose the 'wining' solution with respect to their desire quality levels. However, due to lack of a central mechanism for team formation, continuity in the developer's work on consecutive tasks and risk of noise in submissions of a contest, requesters of a domain have quality concerns to adopt a crowdsourcing-based software development platform. In order to address this concern, we proposed a measure Quality of Contest (QoC) to analyze and predict the quality of a crowdsourcing-based platform through historical information on its completed tasks. We evaluate the capacity of QoC as assessor to predict the quality. Subsequently, we implement a crawler to mine the information of completed development tasks from the TopCoder platform of Tech Platform Inc (TPI) to empirically investigate the proposed measures. The promising results of QoC measure suggest the applicability of the proposed measure across the other crowdsourcing-based platforms.},
booktitle = {Proceedings of the Symposium on Applied Computing},
pages = {1305–1306},
numpages = {2},
keywords = {crowdsource, development tasks, measure, quality, regression, topcoder},
location = {Marrakech, Morocco},
series = {SAC '17}
}

@inproceedings{10.1109/WAINA.2014.28,
author = {Sharifi, Mahdi and Manaf, Azizah Abdul and Memariani, Ali and Movahednejad, Homa and Sarkan, Haslina Md and Dastjerdi, Amir Vahid},
title = {Multi-criteria Consensus-Based Service Selection Using Crowdsourcing},
year = {2014},
isbn = {9781479926534},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/WAINA.2014.28},
doi = {10.1109/WAINA.2014.28},
abstract = {Different evaluator entities, either human agents (e.g., experts) or software agents (e.g., monitoring services), are involved to assess QoS parameters of candidate services, which leads to diversity in service assessments. This diversity makes the service selection a challenging task, especially when numerous qualities of service criteria and range of providers are considered. To address this problem, this study as first step presents a consensus-based service assessment methodology that utilizes consensus theory to evaluate the service behavior for single QoS criteria using the power of crowd sourcing. For this purpose, trust level metrics are introduced to measure the strength of a consensus based on the trustworthiness levels of crowd members. The peers converged to the most trustworthy evaluation. Next, the fuzzy inference engine was used to aggregate each obtained assessed QoS value based on user preferences because we address multiple QoS criteria in real life scenarios.},
booktitle = {Proceedings of the 2014 28th International Conference on Advanced Information Networking and Applications Workshops},
pages = {114–120},
numpages = {7},
keywords = {Web service, Consensus, Trust, Service selection, Fuzzy aggregation},
series = {WAINA '14}
}

@proceedings{10.1145/2897659,
title = {CSI-SE '16: Proceedings of the 3rd International Workshop on CrowdSourcing in Software Engineering},
year = {2016},
isbn = {9781450341585},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {It is our pleasure to welcome the reader to the (pre-workshop) proceedings of the 3rd International Workshop on CrowdSourcing in Software Engineering (CSI-SE 2016), co-located with the 38th International Conference on Software Engineering (ICSE 2016) held in Austin, TX, USA on May 16, 2016.A number of trends under the broad banner of crowdsourcing are beginning to fundamentally disrupt the way in which software is engineered. Programmers increasingly rely on crowdsourced knowledge and code, as they look at Q&amp;A websites for answers or use code from publicly posted snippets. Programmers play, compete, and learn with the crowd, engaging in programming competitions and puzzles with crowds of programmers. Online IDEs make radically new forms of collaboration possible, allowing developers to synchronously program with crowds of distributed programmers. Programmer reputation is increasingly visible on Q&amp;A sites and public code repositories, opening new possibilities in how developers find jobs and companies identify talent. Crowds of non-programmers increasingly participate in development, usability testing software or even constructing specifications while playing games. Developers can take feedback from a crowd of users to guide further evolution of their applications. Crowdfunding democratizes choices about which software is built, broadening the software which might be feasibly constructed. Approaches for crowd development seek to microtask software development, dramatically increasing participation in open source by enabling software projects to be built through casual, transient work.},
location = {Austin, Texas}
}

@inproceedings{10.5555/2887007.2887181,
author = {Chandra, Praphul and Narahari, Yadati and Mandal, Debmalya and Dey, Prasenjit},
title = {Novel mechanisms for online crowdsourcing with unreliable, strategic agents},
year = {2015},
isbn = {0262511290},
publisher = {AAAI Press},
abstract = {Motivated by current day crowdsourcing platforms and emergence of online labor markets, this work addresses the problem of task allocation and payment decisions when unreliable and strategic workers arrive over time to work on tasks which must be completed within a deadline. We consider the following scenario: a requester has a set of tasks that must be completed before a deadline; agents (aka crowd workers) arrive over time and it is required to make sequential decisions regarding task allocation and pricing. Agents may have different costs for providing service and these costs are private information of the agents. We assume that agents are not strategic about their arrival times but could be strategic about their costs of service. In addition, agents could be unreliable in the sense of not being able to complete the assigned tasks within the allocated time; these tasks must then be reallocated to other agents to ensure on-time completion of the set of tasks by the deadline. For this setting, we propose two mechanisms: a DPM (Dynamic Price Mechanism) and an ABM (Auction Based Mechanism). Both mechanisms are dominant strategy incentive compatible, budget feasible, and also satisfy ex-post individual rationality for agents who complete the allocated tasks. These mechanisms can be implemented in current day crowdsourcing platforms with minimal changes to the current interaction model.},
booktitle = {Proceedings of the Twenty-Ninth AAAI Conference on Artificial Intelligence},
pages = {1256–1262},
numpages = {7},
location = {Austin, Texas},
series = {AAAI'15}
}

@inproceedings{10.1145/2800835.2800967,
author = {Konomi, Shin'ichi and Sasao, Tomoyo},
title = {The use of colocation and flow networks in mobile crowdsourcing},
year = {2015},
isbn = {9781450335751},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2800835.2800967},
doi = {10.1145/2800835.2800967},
abstract = {Requesting relevant tasks to mobile crowds is extremely difficult without considering their movements. We propose an approach to geo-cast crowdsourcing tasks based on networks of human flows, and show that our approach can achieve higher geographical relevance than simple proximity-based approaches.},
booktitle = {Adjunct Proceedings of the 2015 ACM International Joint Conference on Pervasive and Ubiquitous Computing and Proceedings of the 2015 ACM International Symposium on Wearable Computers},
pages = {1343–1348},
numpages = {6},
keywords = {community detection, complex networks, mobile crowdsourcing, mobility, situated crowdsourcing},
location = {Osaka, Japan},
series = {UbiComp/ISWC'15 Adjunct}
}

@inproceedings{10.5555/3054117.3054120,
author = {Xiang, Qikun and Nevat, Ido and Zhang, Pengfei and Zhang, Jie},
title = {Collusion-resistant spatial phenomena crowdsourcing via mixture of Gaussian processes regression},
year = {2016},
publisher = {CEUR-WS.org},
address = {Aachen, DEU},
abstract = {With the rapid development of mobile devices, spatial location-based crowdsourcing applications have attracted much attention. These applications also introduce new security risks due to untrustworthy data sources. In the context of crowdsourcing applications for spatial interpolation (i.e. spatial regression) using crowdsourced data, the results can be seriously affected if malicious data sources initiate a colluding (collaborate) attacks which purposely alter some of the measurements. To combat this serious detrimental effect, and to mitigate such attacks, we develop a robust version via a Gaussian Process mixture model and develop a computationally efficient algorithm which utilises a Markov chain Monte Carlo (MCMC)-based methodology to produce an accurate predictive inference in the presence of collusion attacks. The algorithm is fully Bayesian and produces posterior predictive distribution for any point-of-interest in the input space. It also assesses the trustworthiness of each worker, i.e. the probability of each worker being honest (trustworthy). Simulation results demonstrate the accuracy of this algorithm.},
booktitle = {Proceedings of the 18th International Conference on Trust in Agent Societies - Volume 1578},
pages = {30–41},
numpages = {12},
location = {Singapore, Singapore},
series = {TRUST'16}
}

@inproceedings{10.1007/978-3-642-39209-2_26,
author = {Nakatsu, Robbie and Grossman, Elissa},
title = {Designing effective user interfaces for crowdsourcing: an exploratory study},
year = {2013},
isbn = {9783642392085},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-642-39209-2_26},
doi = {10.1007/978-3-642-39209-2_26},
abstract = {We investigate characteristics of the technology platform for different types of crowdsourcing initatives, as characterized by their task type--specifically we classify crowdsourcing applications by task structure, task interdependence, and task commitment. The method employed is to examine best practices of well-known crowdsourcing applications, investigating their user interface features, and characteristics that make them successful examples of crowdsourcing. Among the best practices uncovered were the following: easy searching for information; adaptive user interfaces that learned from the crowd; easy-to-use mobile interfaces; the ability to vote ideas up or down; credentialing; and creating sticky user interfaces that engaged the user. Finally, we consider issues for further study and investigation.},
booktitle = {Proceedings of the 15th International Conference on Human Interface and the Management of Information: Information and Interaction Design - Volume Part I},
pages = {221–229},
numpages = {9},
keywords = {crowdsourcing, distributed knowledge gathering, online problem-solving platforms, open source design, user interface design, wisdom of the crowds},
location = {Las Vegas, NV},
series = {HCI International'13}
}

@inproceedings{10.1145/2556288.2557241,
author = {Kong, Nicholas and Hearst, Marti A. and Agrawala, Maneesh},
title = {Extracting references between text and charts via crowdsourcing},
year = {2014},
isbn = {9781450324731},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2556288.2557241},
doi = {10.1145/2556288.2557241},
abstract = {News articles, reports, blog posts and academic papers often include graphical charts that serve to visually reinforce arguments presented in the text. To help readers better understand the relation between the text and the chart, we present a crowdsourcing pipeline to extract the references between them. Specifically, we give crowd workers paragraph-chart pairs and ask them to select text phrases as well as the corresponding visual marks in the chart. We then apply automated clustering and merging techniques to unify the references generated by multiple workers into a single set. Comparing the crowdsourced references to a set of gold standard references using a distance measure based on the F1 score, we find that the average distance between the raw set of references produced by a single worker and the gold standard is 0.54 (out of a max of 1.0). When we apply clustering and merging techniques the average distance between the unified set of references and the gold standard reduces to 0.39; an improvement of 27\%. We conclude with an interactive document viewing application that uses the extracted references; readers can select phrases in the text and the system highlights the related marks in the chart.},
booktitle = {Proceedings of the SIGCHI Conference on Human Factors in Computing Systems},
pages = {31–40},
numpages = {10},
keywords = {crowdsourcing, interactive documents, visualization},
location = {Toronto, Ontario, Canada},
series = {CHI '14}
}

@inproceedings{10.1145/2393347.2393400,
author = {Xu, Qianqian and Huang, Qingming and Yao, Yuan},
title = {Online crowdsourcing subjective image quality assessment},
year = {2012},
isbn = {9781450310895},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2393347.2393400},
doi = {10.1145/2393347.2393400},
abstract = {Recently, HodgeRank on random graphs has been proposed as an effective framework for multimedia quality assessment problem based on paired comparison method. With the random design on large graphs, it is particularly suitable for large scale crowdsourcing experiments on Internet. However, to make it more practical toward this purpose, it is necessary to develop online algorithms to deal with sequential or streaming data. In this paper, we propose an online rating scheme based on HodgeRank on random graphs, to assess image quality when assessors and image pairs enter the system in a sequential way in a crowdsourceable scenario. The scheme is shown in both theory and experiments to be effective by exhibiting similar performance to batch learning under the Erd\"{o}s-R\'{e}nyi random graph model for sampling. It enables us to derive global rating and monitor intrinsic inconsistency in the real time. We demonstrate the effectiveness of the proposed framework on LIVE and IVC databases.},
booktitle = {Proceedings of the 20th ACM International Conference on Multimedia},
pages = {359–368},
numpages = {10},
keywords = {crowdsourcing, hodgerank, online, paired comparison, persistent homology, random graphs, subjective image quality assessment, topology evolution, triangular curl},
location = {Nara, Japan},
series = {MM '12}
}

@inproceedings{10.5555/3304652.3304764,
author = {Salisbury, Elliot and Kamar, Ece and Morris, Meredith Ringel},
title = {Evaluating and complementing vision-to-language technology for people who are blind with conversational crowdsourcing},
year = {2018},
isbn = {9780999241127},
publisher = {AAAI Press},
abstract = {We study how real-time crowdsourcing can be used both for evaluating the value provided by existing automated approaches and for enabling workflows that provide scalable and useful alt text to blind users. We show that the shortcomings of existing AI image captioning systems frequently hinder a user's understanding of an image they cannot see to a degree that even clarifying conversations with sighted assistants cannot correct. Based on analysis of clarifying conversations collected from our studies, we design experiences that can effectively assist users in a scalable way without the need for real-time interaction. Our results provide lessons and guidelines that the designers of future AI captioning systems can use to improve labeling of social media imagery for blind users.},
booktitle = {Proceedings of the 27th International Joint Conference on Artificial Intelligence},
pages = {5349–5353},
numpages = {5},
location = {Stockholm, Sweden},
series = {IJCAI'18}
}

@inproceedings{10.1145/2600057.2602880,
author = {Ho, Chien-Ju and Slivkins, Aleksandrs and Vaughan, Jennifer Wortman},
title = {Adaptive contract design for crowdsourcing markets: bandit algorithms for repeated principal-agent problems},
year = {2014},
isbn = {9781450325653},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2600057.2602880},
doi = {10.1145/2600057.2602880},
abstract = {Crowdsourcing markets have emerged as a popular platform for matching available workers with tasks to complete. The payment for a particular task is typically set by the task's requester, and may be adjusted based on the quality of the completed work, for example, through the use of 'bonus' payments. In this paper, we study the requester's problem of dynamically adjusting quality-contingent payments for tasks. We consider a multi-round version of the well-known principal-agent model, whereby in each round a worker makes a strategic choice of the effort level which is not directly observable by the requester. In particular, our formulation significantly generalizes the budget-free online task pricing problems studied in prior work.We treat this problem as a multi-armed bandit problem, with each 'arm' representing a potential contract. To cope with the large (and in fact, infinite) number of arms, we propose a new algorithm, AgnosticZooming, which discretizes the contract space into a finite number of regions, effectively treating each region as a single arm. This discretization is adaptively refined, so that more promising regions of the contract space are eventually discretized more finely. We provide a full analysis of this algorithm, showing that it achieves regret sublinear in the time horizon and substantially improves over non-adaptive discretization (which is the only competing approach in the literature).},
booktitle = {Proceedings of the Fifteenth ACM Conference on Economics and Computation},
pages = {359–376},
numpages = {18},
keywords = {crowdsourcing, dynamic pricing, multi-armed bandits, principal-agent, regret},
location = {Palo Alto, California, USA},
series = {EC '14}
}

@inproceedings{10.1109/ASWEC.2014.11,
author = {Xiao, Lu and Paik, Hye-Young},
title = {Supporting Complex Work in Crowdsourcing Platforms: A View from Service-Oriented Computing},
year = {2014},
isbn = {9781479931491},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/ASWEC.2014.11},
doi = {10.1109/ASWEC.2014.11},
abstract = {Crowd sourcing is changing the way people work and solve problems from "in-house working" to "public out-sourcing". Most online crowd sourcing platforms perform two main functions: (i) allowing users to advertise their tasks and (ii) helping them find candidate workers. However, they do not support crowd sourcing of complex work consisting of interdependent tasks. Those tasks require not only having simple task/worker pairs, but also coordinating multiple workers together for completion of the crowd work. In this paper, we propose a conceptual framework to bring the coordination support into current online crowd sourcing platforms. In our framework, each crowd worker is modeled as a service that can be self-described, dynamically discovered and assembled into the complex crowd work, meanwhile, we define a workflow-based schema to structure and represent the complex crowd work. Then the crowd sourcing performance is initiated and managed by our coordination protocol.},
booktitle = {Proceedings of the 2014 23rd Australian Software Engineering Conference},
pages = {11–14},
numpages = {4},
series = {ASWEC '14}
}

@inproceedings{10.1007/978-3-642-34630-9_20,
author = {Hardas, Manas S. and Purvis, Lisa},
title = {Bayesian vote weighting in crowdsourcing systems},
year = {2012},
isbn = {9783642346293},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-642-34630-9_20},
doi = {10.1007/978-3-642-34630-9_20},
abstract = {In social collaborative crowdsourcing platforms, the votes which people give on the content generated by others is a very important component of the system which seeks to find the best content through collaborative action. In a crowdsourced innovation platform, people vote on innovations/ideas generated by others which enables the system to synthesize the view of the crowd about an idea. However, in many such systems gaming or vote spamming as it is commonly known is prevalent. In this paper we present a Bayesian mechanism for weighting the actual vote given by a user to compute an effective vote which incorporates the voters history of voting and also what the crowd is thinking about the value of the innovation. The model results into some interesting insights about social voting systems and new avenues for gamification.},
booktitle = {Proceedings of the 4th International Conference on Computational Collective Intelligence: Technologies and Applications - Volume Part I},
pages = {194–203},
numpages = {10},
location = {Ho Chi Minh City, Vietnam},
series = {ICCCI'12}
}

@inproceedings{10.1007/978-3-319-15168-7_50,
author = {Novak, Jasminko and Bozzon, Alessandro and Fraternali, Piero and Daras, Petros and Chrons, Otto and Nardi, Bonnie and Jaimes, Alejandro},
title = {SoHuman 2014 – 3rd International Workshop on Social Media in Crowdsourcing and Human Computation - Introduction: Theme: Socially-Aware Crowdsourcing – The Value of the Human Touch},
year = {2015},
isbn = {978-3-319-15167-0},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-319-15168-7_50},
doi = {10.1007/978-3-319-15168-7_50},
abstract = {This workshop aims at bringing together researchers and practitioners from different disciplines to explore the challenges and opportunities of novel approaches to collective intelligence, crowdsourcing and human computation that address social aspects as a core element of their design principles, implementations or scientific investigation.},
booktitle = {Social Informatics: SocInfo 2014 International Workshops, Barcelona, Spain, November 11, 2014, Revised Selected Papers},
pages = {417–420},
numpages = {4},
keywords = {Crowdsourcing, Human computation, Collective intelligence, Social media, Collaborative systems, AI, Multimedia information retrieval, HCI, Socio-technical systems},
location = {Barcelona, Spain}
}

@inproceedings{10.5555/3172077.3172302,
author = {Wang, Wei and Guo, Xiang-Yu and Li, Shao-Yuan and Jiang, Yuan and Zhou, Zhi-Hua},
title = {Obtaining high-quality label by distinguishing between easy and hard items in crowdsourcing},
year = {2017},
isbn = {9780999241103},
publisher = {AAAI Press},
abstract = {Crowdsourcing systems make it possible to hire voluntary workers to label large-scale data by offering them small monetary payments. Usually, the taskmaster requires to collect high-quality labels, while the quality of labels obtained from the crowd may not satisfy this requirement. In this paper, we study the problem of obtaining high-quality labels from the crowd and present an approach of learning the difficulty of items in crowdsourcing, in which we construct a small training set of items with estimated difficulty and then learn a model to predict the difficulty of future items. With the predicted difficulty, we can distinguish between  easy and  hard  items to obtain high-quality labels. For  easy  items, the quality of their labels inferred from the crowd could be high enough to satisfy the requirement; while for  hard  items, the crowd could not provide high-quality labels, it is better to choose a more knowledgable crowd or employ specialized workers to label them. The experimental results demonstrate that the proposed approach by learning to distinguish between  easy  and  hard  items can significantly improve the label quality.},
booktitle = {Proceedings of the 26th International Joint Conference on Artificial Intelligence},
pages = {2964–2970},
numpages = {7},
location = {Melbourne, Australia},
series = {IJCAI'17}
}

@inproceedings{10.1007/978-3-319-91125-0_14,
author = {Aihara, Kenro and Bin, Piao and Imura, Hajime and Takasu, Atsuhiro and Tanaka, Yuzuru},
title = {Collecting Bus Locations by Users: A Crowdsourcing Model to Estimate Operation Status of Bus Transit Service},
year = {2018},
isbn = {978-3-319-91124-3},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-319-91125-0_14},
doi = {10.1007/978-3-319-91125-0_14},
abstract = {This paper describes a crowdsourcing model to collect bus locations from onboard passengers.Bus location service, or realtime bus tracking service, is getting more and more general these days. Some models are proposed to build such services. One approach is facilitated every vehicle has a function to position itself with location sensor, such as GPS receiver, and transmits its own location with time to the server. Another is an environmental approach that bus detectors are deployed along the route to detect ids of nearby buses and transmit to the server. These models are well-established and practical. However, it is not easy to install such services especially for small operators because costs on devices and data transmission are relatively high.This paper proposes that a sustainable model even for small operators to provide bus locations to passengers. The key idea of the proposal is that collecting bus locations is not by bus operators but by onboard passengers. To collect them, a smartphone application of bus tracker is provided to public. The application shows current locations of buses in operation on bus transit services, while it detects nearby buses around users and transmits bus ids with time and location of detection to the service platform. That is, locations of buses are collected by users.},
booktitle = {Distributed, Ambient and Pervasive Interactions: Understanding Humans: 6th International Conference, DAPI 2018, Held as Part of HCI International 2018, Las Vegas, NV, USA, July 15–20, 2018, Proceedings, Part I},
pages = {171–180},
numpages = {10},
keywords = {Internet of Things, Smart and hybrid cities, Crowdsensing, Crowdsourcing},
location = {Las Vegas, NV, USA}
}

@inproceedings{10.1007/978-3-319-23063-4_23,
author = {Tranquillini, Stefano and Daniel, Florian and Kucherbaev, Pavel and Casati, Fabio},
title = {BPMN Task Instance Streaming for Efficient Micro-task Crowdsourcing Processes},
year = {2015},
isbn = {9783319230627},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-319-23063-4_23},
doi = {10.1007/978-3-319-23063-4_23},
abstract = {The Business Process Model and Notation BPMN is a standard for modeling and executing business processes with human or machine tasks. The semantics of tasks is usually discrete: a task has exactly one start event and one end event; for multi-instance tasks, all instances must complete before an end event is emitted. We propose a new task type and streaming connector for crowdsourcing able to run hundreds or thousands of micro-task instances in parallel. The two constructs provide for task streaming semantics that is new to BPMN, enable the modeling and efficient enactment of complex crowdsourcing scenarios, and are applicable also beyond the special case of crowdsourcing. We implement the necessary design and runtime support on top of CrowdFlower, demonstrate the viability of the approach via a case study, and report on a set of runtime performance experiments.},
booktitle = {Proceedings of the 13th International Conference on Business Process Management - Volume 9253},
pages = {333–349},
numpages = {17},
keywords = {BPMN, Crowdsourcing processes, Task instance streaming}
}

@inproceedings{10.1145/2820783.2820831,
author = {Deng, Dingxiong and Shahabi, Cyrus and Zhu, Linhong},
title = {Task matching and scheduling for multiple workers in spatial crowdsourcing},
year = {2015},
isbn = {9781450339674},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2820783.2820831},
doi = {10.1145/2820783.2820831},
abstract = {A new platform, termed spatial crowdsourcing, is emerging which enables a requester to commission workers to physically travel to some specified locations to perform a set of spatial tasks (i.e., tasks related to a geographical location and time). The current approach is to formulate spatial crowdsourcing as a matching problem between tasks and workers; hence the primary objective of the existing solutions is to maximize the number of matched tasks. Our goal is to solve the spatial crowdsourcing problem in the presence of multiple workers where we optimize for both travel cost and the number of completed tasks, while taking the tasks' expiration times into consideration. The challenge is that the solution should be a mixture of task-matching and task-scheduling, which are fundamentally different. In this paper, we show that a baseline approach that performs a task-matching first, and subsequently schedules the tasks assigned per worker in a following phase, does not perform well. Hence, we add a third phase in which we iterate back to the matching phase to improve the assignment per the output of the scheduling phase, and thus further improves the quality of matching and scheduling. Even though this 3-phase approach generates high quality results, it is very slow and does not scale. Hence, to scale our algorithm to large number of workers and tasks, we propose a Bisection-based framework which recursively divides all the workers and tasks into different partitions such that assignment and scheduling can be performed locally in a much smaller and promising space. Our experiments show that this approach is three orders of magnitude faster than the 3-phase approach while it only sacrifices 4\% of the results' quality.},
booktitle = {Proceedings of the 23rd SIGSPATIAL International Conference on Advances in Geographic Information Systems},
articleno = {21},
numpages = {10},
keywords = {scalability, spatial crowdsourcing, task matching and scheduling},
location = {Seattle, Washington},
series = {SIGSPATIAL '15}
}

@inproceedings{10.1109/CSI-SE.2015.16,
author = {Zhao, Mengyao and Hoek, Andr\'{e} van der},
title = {A Brief Perspective on Microtask Crowdsourcing Workflows for Interface Design},
year = {2015},
isbn = {9781467370400},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/CSI-SE.2015.16},
doi = {10.1109/CSI-SE.2015.16},
abstract = {User interface design, as a crucial part of software design, is complex. Current micro task crowd sourcing workflows do not support its complexity well. The difficulty particularly relates to the process to decompose an interface design task into micro tasks. In order to make micro task crowd sourcing more supportive for interface design, we need a workflow that can help task owners to break down interface design tasks more easily. This paper briefly describes three experiments that help inform various aspects of workflow design for interface design through micro task crowd sourcing.},
booktitle = {Proceedings of the 2015 IEEE/ACM 2nd International Workshop on CrowdSourcing in Software Engineering},
pages = {45–46},
numpages = {2},
keywords = {complex work, crowdsourcing, interface design},
series = {CSI-SE '15}
}

@inproceedings{10.1145/2872427.2883070,
author = {Mavridis, Panagiotis and Gross-Amblard, David and Mikl\'{o}s, Zolt\'{a}n},
title = {Using Hierarchical Skills for Optimized Task Assignment in Knowledge-Intensive Crowdsourcing},
year = {2016},
isbn = {9781450341431},
publisher = {International World Wide Web Conferences Steering Committee},
address = {Republic and Canton of Geneva, CHE},
url = {https://doi.org/10.1145/2872427.2883070},
doi = {10.1145/2872427.2883070},
abstract = {Besides the simple human intelligence tasks such as image labeling, crowdsourcing platforms propose more and more tasks that require very specific skills, especially in participative science projects. In this context, there is a need to reason about the required skills for a task and the set of available skills in the crowd, in order to increase the resulting quality. Most of the existing solutions rely on unstructured tags to model skills (vector of skills). In this paper we propose to finely model tasks and participants using a skill tree, that is a taxonomy of skills equipped with a similarity distance within skills. This model of skills enables to map participants to tasks in a way that exploits the natural hierarchy among the skills. We illustrate the effectiveness of our model and algorithms through extensive experimentation with synthetic and real data sets.},
booktitle = {Proceedings of the 25th International Conference on World Wide Web},
pages = {843–853},
numpages = {11},
keywords = {crowdsourcing, skill modeling, task mapping},
location = {Montr\'{e}al, Qu\'{e}bec, Canada},
series = {WWW '16}
}

@inproceedings{10.1145/2487294.2487332,
author = {Tajedin, Hamed and Nevo, Dorit},
title = {Determinants of success in crowdsourcing software development},
year = {2013},
isbn = {9781450319751},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2487294.2487332},
doi = {10.1145/2487294.2487332},
abstract = {With the advent of digitization, recent years have witnessed a surge toward collective undertaking of production process different from traditional ways of organizing. In this vein, crowdsourcing has lent itself into a successful emerging mode of organizing and firms are increasingly using it in their value creation activities. However, despite popularity in practice, crowdsourcing has received little attention from IS scholars. Specifically, what the determinants of success in this model are remains an unexplored area of research that we strive to address in this paper. We focus on software development via crowdsourcing and drawing on studies from IS success, OSS and software development, we build a model of success that has three determinants: the characteristics of the project, the composition of the crowd and the relationship among key players. Finally, we describe our research methodology and conclude with potential contributions of our work.},
booktitle = {Proceedings of the 2013 Annual Conference on Computers and People Research},
pages = {173–178},
numpages = {6},
keywords = {crowdsourcing, opensource software, outsourcing, software development},
location = {Cincinnati, Ohio, USA},
series = {SIGMIS-CPR '13}
}

@inproceedings{10.1109/SEAA.2015.33,
author = {Kilamo, Terhi and Rahikkala, Jurka and Mikkonen, Tommi},
title = {Spicing Up Open Source Development with a Touch of Crowdsourcing},
year = {2015},
isbn = {9781467375856},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/SEAA.2015.33},
doi = {10.1109/SEAA.2015.33},
abstract = {Leveraging the work and innovation of third party developers has risen as a viable business model for software companies. Most obviously, open source software has become an opportune ecosystem for creating innovative products with minimum number of paid developers. Then, having a company core where most of the development is done in-house by developers employed by the company can lead to a situation where the community contributions are not smoothly integrated into the code base of the open source product. Similarly, during the last decade, the use of the specialized workforce available online--so-called crowd sourcing--has received a lot of attention. While tapping into the unknown group of experts differs from the open source community-driven approach, they share certain similarities as well. In this paper, we present results of an initial study on how adopting and utilizing elements from crowd sourcing can help to boost community contributions in company lead development of an open source software product. We further discuss how such activity can be supported by an in-house development model where all contributions whether done by the developers of the company or community participants enter a common, automated integration pipeline.},
booktitle = {Proceedings of the 2015 41st Euromicro Conference on Software Engineering and Advanced Applications},
pages = {390–397},
numpages = {8},
keywords = {crowdsourcing, open source software, software ecosystems},
series = {SEAA '15}
}

@inproceedings{10.1145/2463728.2463819,
author = {Goodspeed, Robert and Spanring, Christian and Reardon, Timothy},
title = {Crowdsourcing as data sharing: a regional web-based real estate development database},
year = {2012},
isbn = {9781450312004},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2463728.2463819},
doi = {10.1145/2463728.2463819},
abstract = {The paper describes a web-based database of residential and commercial real estate development projects, created by a regional urban planning agency in Metropolitan Boston. In Phase I, now complete, the tool is used to facilitate inter-agency information sharing, demonstrating the ability of web-base data collection tools to increase information sharing between government agencies by reducing transaction costs. Phase II, now under development, will expand the functionality of the website to allow the general public to contribute information to the database, as well as view and download its contents. The project is unusual in its integration of a crowdsourcing paradigm with traditional methods of spatial information sharing. The project demonstrates the potential for technology to facilitate data sharing in favorable contexts where interorganizational relationships and sharing norms exist.},
booktitle = {Proceedings of the 6th International Conference on Theory and Practice of Electronic Governance},
pages = {460–463},
numpages = {4},
keywords = {crowdsourcing, spatial data sharing, urban development, web 2.0},
location = {Albany, New York, USA},
series = {ICEGOV '12}
}

@inproceedings{10.1109/SOSE.2015.48,
author = {Fan, Yue and Sun, Hailong and Zhu, Yanmin and Liu, Xudong and Yuan, Ji},
title = {A Truthful Online Auction for Tempo-spatial Crowdsourcing Tasks},
year = {2015},
isbn = {9781479983568},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/SOSE.2015.48},
doi = {10.1109/SOSE.2015.48},
abstract = {Mobile crowdsourcing is an emerging paradigm which utilizes the distributed smartphones to monitor diverse phenomena about human activities and surrounding environment, enabling a large number of mobile crowdsourcing applications. For those applications to collect sufficient data, motivating smartphone users to be interested in mobile crowdsourcing campaign becomes very significant. Most of the incentive mechanisms assume that tasks are static in mobile crowdsourcing systems. Even for those studies that take the uncertain arrival of tasks into consideration, they always ignore the important geographic location information of the tasks. In the paper, we propose a near-optimal online incentive mechanism based on a more realistic scenario in which crowdsourcing tasks and users both arrive dynamically with tempo-spatial constraints. Through adequate simulations and rigorous theoretical analysis, the online mechanism is proved to satisfy the properties of truthfulness, computational efficiency, individual rationality, and achieve high social welfare and low total payment.},
booktitle = {Proceedings of the 2015 IEEE Symposium on Service-Oriented System Engineering},
pages = {332–338},
numpages = {7},
series = {SOSE '15}
}

@inproceedings{10.1145/2442882.2442909,
author = {Ei Chew, Han and Sort, Borort and Haddawy, Peter},
title = {Building a crowdsourcing community: how online social learning helps in poverty reduction},
year = {2013},
isbn = {9781450318563},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2442882.2442909},
doi = {10.1145/2442882.2442909},
abstract = {In this paper, we describe the design and use of a knowledge sharing network that has recently been deployed for agricultural extension work in the Lao People's Democratic Republic (Lao PDR). The system, Poverty Reduction and Agricultural Management -- Knowledge Sharing Network (PRAM-KSN), was built using a collaborative design process that involved both experts and ministerial agricultural extension workers who are also the current users of this web-based platform. This paper also discusses the relevance of the PRAM-KSN for agricultural extension work, how the principles of crowdsourcing apply to the system, and how social learning occurs for the benefit of agricultural extension work. Suggestions for impact assessment of the PRAM-KSN at different time-frames are offered.},
booktitle = {Proceedings of the 3rd ACM Symposium on Computing for Development},
articleno = {21},
numpages = {2},
keywords = {crowdsourcing, evaluation, participatory design, poverty reduction, social cognitive theory},
location = {Bangalore, India},
series = {ACM DEV '13}
}

@inproceedings{10.1145/2501105.2501113,
author = {Di Salvo, R. and Giordano, D. and Kavasidis, I.},
title = {A crowdsourcing approach to support video annotation},
year = {2013},
isbn = {9781450321693},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2501105.2501113},
doi = {10.1145/2501105.2501113},
abstract = {In this paper we present an innovative approach to support efficient large scale video annotation by exploiting the crowdsourcing. In particular, we collect big noisy annotations by an on-line Flash game which aims at taking photos of objects appearing through the game levels. The data gathered (suitably processed) from the game is then used to drive image segmentation approaches, namely the Region Growing and Grab Cut, which allow us to derive meaningful annotations. A comparison against hand-labeled ground truth data showed that the proposed approach constitutes a valid alternative to the existing video annotation approaches and allow a reliable and fast collection of large scale ground truth data for performance evaluation in computer vision.},
booktitle = {Proceedings of the International Workshop on Video and Image Ground Truth in Computer Vision Applications},
articleno = {8},
numpages = {6},
keywords = {ground truth generation, image segmentation, online game, seed positioning},
location = {St. Petersburg, Russia},
series = {VIGTA '13}
}

@inproceedings{10.1109/FIE.2017.8190648,
author = {de Deus, William Sim\~{a}o and Machado, Heydi Miura and Barros, Renata Marques and Fabri, Jos\'{e} Augusto and L'Erario, Alexandre},
title = {Enhancing collaboration among undergraduates in informatics: A teaching and learning process based on crowdsourcing},
year = {2017},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/FIE.2017.8190648},
doi = {10.1109/FIE.2017.8190648},
abstract = {Crowdsourcing (CS) is a development model in which small activities are carried out through the collaboration of participants. Many organizations are developing their products through CS to parallel activities, reduce costs, and employ specialists. These factors have boosted the area of informatics, creating a new paradigm for the development of software. However, despite the widespread application of CS in informatics, the literature on teaching and learning CS for undergraduates is still very incipient, and the informatics courses aimed to maximize only the teaching of traditional development processes (Agile, distributed, etc). With this in mind, this study was developed to present the following contributions: (i) a personalized process of teaching and learning CS to the undergraduates in computing courses, and (ii) demonstrates the configuration that a classroom and/or computer lab should possess to generate a CS teaching and learning environment. To accomplishment this study, experimental trials were conducted with undergraduates in different courses in informatics. In conducting the trials, classrooms and computer labs at a university were set up simulating barriers found in CS. As results, all undergraduates have accomplished and reached the ultimate goal through CS, and the process of teaching and learning allowed for the enhancement of several factors, such as teamwork, integration, and support.},
booktitle = {2017 IEEE Frontiers in Education Conference (FIE)},
pages = {1–8},
numpages = {8},
location = {Indianapolis, IN, USA}
}

@inproceedings{10.1145/2506364.2506368,
author = {Redi, Judith Alice and Ho\ss{}feld, Tobias and Korshunov, Pavel and Mazza, Filippo and Povoa, Isabel and Keimel, Christian},
title = {Crowdsourcing-based multimedia subjective evaluations: a case study on image recognizability and aesthetic appeal},
year = {2013},
isbn = {9781450323963},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2506364.2506368},
doi = {10.1145/2506364.2506368},
abstract = {Research on Quality of Experience (QoE) heavily relies on subjective evaluations of media. An important aspect of QoE concerns modeling and quantifying the subjective notions of 'beauty' (aesthetic appeal) and 'something well-known' (content recognizability), which are both subject to cultural and social effects. Crowdsourcing, which allows employing people worldwide to perform short and simple tasks via online platforms, can be a great tool for performing subjective studies in a time and cost-effective way. On the other hand, the crowdsourcing environment does not allow for the degree of experimental control which is necessary to guarantee reliable subjective data. To validate the use of crowdsourcing for QoE assessments, in this paper, we evaluate aesthetic appeal and recognizability of images using the Microworkers crowdsourcing platform and compare the outcomes with more conventional evaluations conducted in a controlled lab environment. We find high correlation between crowdsourcing and lab scores for recognizability but not for aesthetic appeal, indicating that crowdsourcing can be used for QoE subjective assessments as long as the workers' tasks are designed with extreme care to avoid misinterpretations.},
booktitle = {Proceedings of the 2nd ACM International Workshop on Crowdsourcing for Multimedia},
pages = {29–34},
numpages = {6},
keywords = {aesthetics, crowdsourcing, qoe, subjective evaluations},
location = {Barcelona, Spain},
series = {CrowdMM '13}
}

@inproceedings{10.1145/2818052.2869098,
author = {Wasik, Szymon and Antczak, Maciej and Badura, Jan and Laskowski, Artur and Sternal, Tomasz},
title = {Optil.io: Cloud Based Platform For Solving Optimization Problems Using Crowdsourcing Approach},
year = {2016},
isbn = {9781450339506},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2818052.2869098},
doi = {10.1145/2818052.2869098},
abstract = {The main objective of the presented research is to design a platform for continuous evaluation of optimization algorithms using crowdsourcing technique. The resulting platform, called Optil.io, runs in a cloud using platform as a service model and allows researchers from all over the world to collaboratively solve computational problems. This is the approach that has been already proved to be very successful for data mining problems by web services such as Kaggle. During our project we adapted this concept for solving computational problems that require implementation of software. To achieve this we designed the on-line judge system that receives algorithmic solutions in a form of source code from the crowd of programmers, compiles it, executes in a homogeneous run-time environment and objectively evaluates using the set of test cases. It was verified during internal experiments at the Poznan University of Technology and it is now ready to be presented to wider audience.},
booktitle = {Proceedings of the 19th ACM Conference on Computer Supported Cooperative Work and Social Computing Companion},
pages = {433–436},
numpages = {4},
keywords = {algorithms, cloud computing, crowdsourcing, on-line judge, operational research, optimization, platform as a service},
location = {San Francisco, California, USA},
series = {CSCW '16 Companion}
}

@inproceedings{10.1145/2669557.2669561,
author = {Abdul-Rahman, Alfie and Proctor, Karl J. and Duffy, Brian and Chen, Min},
title = {Repeated measures design in crowdsourcing-based experiments for visualization},
year = {2014},
isbn = {9781450332095},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2669557.2669561},
doi = {10.1145/2669557.2669561},
abstract = {Crowdsourcing platforms, such as Amazon's Mechanical Turk (MTurk), are providing visualization researchers with a new avenue for conducting empirical studies. While such platforms offer several advantages over lab-based studies, they also feature some "unknown" or "uncontrolled" variables, which could potentially introduce serious confounding effects in the resultant measurement data. In this paper, we present our experience of using repeated measures in three empirical studies using MTurk. Each study presented participants with a set of stimuli, each featuring a condition of an independent variable. Participants were exposed to stimuli repeatedly in a pseudo-random order through four trials and their responses were measured digitally. Only a small portion of the participants were able to perform with absolute consistency for all stimuli throughout each experiment. This suggests that a repeated measures design is highly desirable (if not essential) when designing empirical studies for crowdsourcing platforms. Additionally, the majority of participants performed their tasks with reasonable consistency when all stimuli in an experiment are considered collectively. In other words, to most participants, inconsistency occurred occasionally. This suggests that crowdsourcing remains a valid experimental environment, provided that one can integrate the means to observe and alleviate the potential confounding effects of "unknown" or "uncontrolled" variables in the design of the experiment.},
booktitle = {Proceedings of the Fifth Workshop on Beyond Time and Errors: Novel Evaluation Methods for Visualization},
pages = {95–102},
numpages = {8},
keywords = {empirical studies, mechanical turk},
location = {Paris, France},
series = {BELIV '14}
}

@inproceedings{10.1109/HICSS.2015.196,
author = {Jackson, Corey Brian and \O{}sterlund, Carsten and Mugar, Gabriel and Hassman, Katie DeVries and Crowston, Kevin},
title = {Motivations for Sustained Participation in Crowdsourcing: Case Studies of Citizen Science on the Role of Talk},
year = {2015},
isbn = {9781479973675},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/HICSS.2015.196},
doi = {10.1109/HICSS.2015.196},
abstract = {The paper explores the motivations of volunteers in a large crowd sourcing project and contributes to our understanding of the motivational factors that lead to deeper engagement beyond initial participation. Drawing on the theory of legitimate peripheral participation (LPP) and the literature on motivation in crowd sourcing, we analyze interview and trace data from a large citizen science project. The analyses identify ways in which the technical features of the projects may serve as motivational factors leading participants towards sustained participation. The results suggest volunteers first engage in activities to support knowledge acquisition and later share knowledge with other volunteers and finally increase participation in Talk through a punctuated process of role discovery.},
booktitle = {Proceedings of the 2015 48th Hawaii International Conference on System Sciences},
pages = {1624–1634},
numpages = {11},
keywords = {Citizen Science, Crowdsourcing, Legitimate peripheral participation, Motivation},
series = {HICSS '15}
}

@inproceedings{10.1145/2461466.2461508,
author = {Nguyen-Dinh, Long-Van and Waldburger, C\'{e}dric and Roggen, Daniel and Tr\"{o}ster, Gerhard},
title = {Tagging human activities in video by crowdsourcing},
year = {2013},
isbn = {9781450320337},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2461466.2461508},
doi = {10.1145/2461466.2461508},
abstract = {Activity annotation in videos is necessary to create a training dataset for most of activity recognition systems. This is a very time consuming and repetitive task. Crowdsourcing gains popularity to distribute annotation tasks to a large pool of taggers. We present for the first time an approach to achieve good quality for activity annotation in videos through crowdsourcing on the AmazonMechanical Turk platform (AMT). Taggers must annotate the start, end boundaries and the label of all occurrences of activities in videos. Two strategies to detect non-serious taggers according to temporal annotated results are presented. Individual filtering checks the consistence in the answers of each tagger with the characteristic of dataset to identify and remove nonserious taggers. Collaborative filtering checks the agreement in annotations among taggers. The filtering techniques detect and remove non-serious taggers and finally, the majority voting applied to AMT temporal tags to generate one final AMT activity annotation set. We conduct the experiments to get activity annotation from AMT on a subset of two rich datasets frequently used in activity recognition. The results show that our proposed filtering strategies can increase the accuracy by up to 40\%. The final annotation set is of comparable quality of the annotation of experts with high accuracy (76\% to 92\%).},
booktitle = {Proceedings of the 3rd ACM Conference on International Conference on Multimedia Retrieval},
pages = {263–270},
numpages = {8},
keywords = {activity recognition, amazon mechanical turk, crowdsourcing, human computation, video annotation},
location = {Dallas, Texas, USA},
series = {ICMR '13}
}

@inproceedings{10.5555/3367032.3367230,
author = {Ko, Ching-Yun and Lin, Rui and Li, Shu and Wong, Ngai},
title = {MiSC: mixed strategies crowdsourcing},
year = {2019},
isbn = {9780999241141},
publisher = {AAAI Press},
abstract = {Popular crowdsourcing techniques mostly focus on evaluating workers' labeling quality before adjusting their weights during label aggregation. Recently, another cohort of models regard crowdsourced annotations as incomplete tensors and recover unfilled labels by tensor completion. However, mixed strategies of the two methodologies have never been comprehensively investigated, leaving them as rather independent approaches. In this work, we propose MiSC (Mixed Strategies Crowdsourcing), a versatile framework integrating arbitrary conventional crowdsourcing and tensor completion techniques. In particular, we propose a novel iterative Tucker label aggregation algorithm that outperforms state-of-the-art methods in extensive experiments.},
booktitle = {Proceedings of the 28th International Joint Conference on Artificial Intelligence},
pages = {1394–1400},
numpages = {7},
location = {Macao, China},
series = {IJCAI'19}
}

@inproceedings{10.1145/2647868.2647871,
author = {Redi, Judith A. and Lux, Mathias},
title = {CrowdMM14 - 2014 International ACM Workshop on Crowdsourcing for Multimedia},
year = {2014},
isbn = {9781450330633},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2647868.2647871},
doi = {10.1145/2647868.2647871},
abstract = {The power of crowds, leveraging a large number of human contributors and the capabilities of human computation, has enormous potential to address key challenges in the area of multimedia research. This power is, however, of difficult exploitation: challenges arise from the fact that a community of users or workers is a complex and dynamic system highly sensitive to changes in the form and the parameterization of their activities. Since 2012, the International ACM Workshop on Crowdsourcing for Multimedia emph{CrowdMM} has been the venue for collecting new insights on the effective deployment of crowdsourcing towards boosting Multimedia research. In its third edition, CrowdMM14 especially focuses on contributions that propose solutions for the key challenges that face widespread adoption of crowdsourcing paradigms in the multimedia research community. These include: identification of optimal crowd members (e.g., user expertise, worker reliability), providing effective explanations (i.e., good task design), controlling noise and quality in the results, designing incentive structures that do not breed cheating, adversarial environments, gathering necessary background information about crowd members without violating privacy, controlling descriptions of task.},
booktitle = {Proceedings of the 22nd ACM International Conference on Multimedia},
pages = {1247–1248},
numpages = {2},
keywords = {crowdsourcing, human computation, human factors, multimedia annotation, multimedia evaluation},
location = {Orlando, Florida, USA},
series = {MM '14}
}

@inproceedings{10.1007/978-3-030-67835-7_18,
author = {Nguyen, Dang-Hieu and Nguyen-Tai, Tan-Loc and Nguyen, Minh-Tam and Nguyen, Thanh-Binh and Dao, Minh-Son},
title = {MNR-Air: An Economic and Dynamic Crowdsourcing Mechanism to Collect Personal Lifelog and Surrounding Environment Dataset. A Case Study in Ho Chi Minh City, Vietnam},
year = {2021},
isbn = {978-3-030-67834-0},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-67835-7_18},
doi = {10.1007/978-3-030-67835-7_18},
abstract = {This paper introduces an economical and dynamic crowdsourcing mechanism to collect personal lifelog associated environment datasets, namely MNR-Air. This mechanism’s significant advantage is to use personal sensor boxes that can be carried on citizens (and their vehicles) to collect data. The MNR-HCM dataset is also introduced in this paper as the output of MNR-Air and collected in Ho Chi Minh City, Vietnam. The MNR-HCM dataset contains weather data, air pollution data, GPS data, lifelog images, and citizens’ cognition of urban nature on a personal scale. We also introduce AQI-T-RM, an application that can help people plan their travel to avoid as much air pollution as possible while still saving time on travel. Besides, we discuss how useful MNR-Air is when contributing to the open data science community and other communities that benefit citizens living in urban areas.},
booktitle = {MultiMedia Modeling: 27th International Conference, MMM 2021, Prague, Czech Republic, June 22–24, 2021, Proceedings, Part II},
pages = {206–217},
numpages = {12},
keywords = {Particulate matter, PM2.5, AQI, Sensors, Sensor data, Smart navigation, Crowdsourcing},
location = {Prague, Czech Republic}
}

@inproceedings{10.5555/1920331.1920416,
author = {Giudice, Katherine Del},
title = {Crowdsourcing credibility: the impact of audience feedback on web page credibility},
year = {2010},
publisher = {American Society for Information Science},
address = {USA},
abstract = {Social feedback in the form of audience ratings, community tags, recommendations, and text comments is becoming increasingly commonplace on the Web. Prior research has uncovered a number of Web site features that can impact its perceived credibility. However, to date research has not investigated whether social feedback on a Web page can influence the perceived credibility of the information on the page or increase or decrease the likelihood that an individual will subsequently use the information contained within it. This paper describes a study investigating whether one type of social feedback, audience ratings, can influence perceptions of credibility. The results of the study suggest that the type of audience feedback, positive, mixed, or negative, can influence perceptions of credibility while the size of the audience giving feedback does not. Also, audience feedback does not appear to increase the likelihood of use of the information on a web page.},
booktitle = {Proceedings of the 73rd ASIS&amp;T Annual Meeting on Navigating Streams in an Information Ecosystem - Volume 47},
articleno = {59},
numpages = {9},
keywords = {Web, credibility, social information use},
location = {Pittsburgh, Pennsylvania},
series = {ASIS&amp;T '10}
}

@inproceedings{10.1145/2567948.2578841,
author = {Auer, S\"{o}ren and Kontokostas, Dimitris},
title = {Towards web intelligence through the crowdsourcing of semantics},
year = {2014},
isbn = {9781450327459},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2567948.2578841},
doi = {10.1145/2567948.2578841},
abstract = {A key success factor for the Web as a whole was and is its participatory nature. We discuss strategies for engaging human-intelligence to make the Web more semantic.},
booktitle = {Proceedings of the 23rd International Conference on World Wide Web},
pages = {991–992},
numpages = {2},
keywords = {crowdsourcing, semantic web},
location = {Seoul, Korea},
series = {WWW '14 Companion}
}

@inproceedings{10.5555/2900728.2900735,
author = {Ho, Chien-Ju and Vaughan, Jennifer Wortman},
title = {Online task assignment in crowdsourcing markets},
year = {2012},
publisher = {AAAI Press},
abstract = {We explore the problem of assigning heterogeneous tasks to workers with different, unknown skill sets in crowdsourcing markets such as Amazon Mechanical Turk. We first formalize the online task assignment problem, in which a requester has a fixed set of tasks and a budget that specifies how many times he would like each task completed. Workers arrive one at a time (with the same worker potentially arriving multiple times), and must be assigned to a task upon arrival. The goal is to allocate workers to tasks in a way that maximizes the total benefit that the requester obtains from the completed work. Inspired by recent research on the online adwords problem, we present a two-phase exploration-exploitation assignment algorithm and prove that it is competitive with respect to the optimal offline algorithm which has access to the unknown skill levels of each worker. We empirically evaluate this algorithm using data collected on Mechanical Turk and show that it performs better than random assignment or greedy algorithms. To our knowledge, this is the first work to extend the online primal-dual technique used in the online adwords problem to a scenario with unknown parameters, and the first to offer an empirical validation of an online primal-dual algorithm.},
booktitle = {Proceedings of the Twenty-Sixth AAAI Conference on Artificial Intelligence},
pages = {45–51},
numpages = {7},
location = {Toronto, Ontario, Canada},
series = {AAAI'12}
}

@inproceedings{10.1145/2645791.2645807,
author = {Souliotis, Nikos and Tsadimas, Anargyros and Nikolaidou, Mara},
title = {Real-time information about public transport's position using crowdsourcing},
year = {2014},
isbn = {9781450328975},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2645791.2645807},
doi = {10.1145/2645791.2645807},
abstract = {Nowadays there is a multitude of mobile and tablet applications being developed in order to facilitate or disrupt every day tasks. Many of these are location based. A technique to serve in providing information and content is crowdsourcing. This technique is based on the public contributing information or resources giving them the opportunity to become both service providers and recipients at the same time.Taking into account the above and after observing passengers using the public transport system, we came to the conclusion that it would be useful to be able to determine which transport medium (i.e which bus line out of a number running concurrently) is nearer at any given moment. This information allows for better decision making and choice of transportation.For this we propose the development of an application to show the position of a selected transport vehicle. The position will be calculated based on geo-tracking provided by passengers boarded on a vehicle. This will allow for real time information to the application users in order to be able to determine their optimal route.},
booktitle = {Proceedings of the 18th Panhellenic Conference on Informatics},
pages = {1–6},
numpages = {6},
keywords = {Mobile Application, Transportations},
location = {Athens, Greece},
series = {PCI '14}
}

@inproceedings{10.1145/3250552,
author = {Poole, Erika},
title = {Session details: Crowdsourcing \&amp; peer production II},
year = {2012},
isbn = {9781450310154},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3250552},
doi = {10.1145/3250552},
booktitle = {Proceedings of the SIGCHI Conference on Human Factors in Computing Systems},
location = {Austin, Texas, USA},
series = {CHI '12}
}

@inproceedings{10.1145/2968219.2968586,
author = {Luo, Chu and Kuutila, Miikka and Klakegg, Simon and Ferreira, Denzil and Flores, Huber and Goncalves, Jorge and Kostakos, Vassilis and M\"{a}ntyl\"{a}, Mika},
title = {How to validate mobile crowdsourcing design? leveraging data integration in prototype testing},
year = {2016},
isbn = {9781450344623},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2968219.2968586},
doi = {10.1145/2968219.2968586},
abstract = {Mobile crowdsourcing applications often run in dynamic environments. Due to limited time and budget, developers of mobile crowdsourcing applications sometimes cannot completely test their prototypes in real world situations. We describe a data integration technique for developers to validate their design in prototype testing. Our approach constructs the intended context by combining real-time, historical and simulated data. With correct context-aware design, mobile crowdsourcing applications presenting crowdsourcing questions in relevant context to users are likely to obtain high response quality.},
booktitle = {Proceedings of the 2016 ACM International Joint Conference on Pervasive and Ubiquitous Computing: Adjunct},
pages = {1448–1453},
numpages = {6},
keywords = {ambient intelligence, mobile crowdsourcing, smartphones, software testing and debugging, ubiquitous computing},
location = {Heidelberg, Germany},
series = {UbiComp '16}
}

@inproceedings{10.5555/1866696.1866710,
author = {Higgins, Chiara and McGrath, Elizabeth and Moretto, Lailla},
title = {MTurk crowdsourcing: a viable method for rapid discovery of Arabic nicknames?},
year = {2010},
publisher = {Association for Computational Linguistics},
address = {USA},
abstract = {This paper presents findings on using crowdsourcing via Amazon Mechanical Turk (MTurk) to obtain Arabic nicknames as a contribution to exiting Named Entity (NE) lexicons. It demonstrates a strategy for increasing MTurk participation from Arab countries. The researchers validate the nicknames using experts, MTurk workers, and Google search and then compare them against the Database of Arabic Names (DAN). Additionally, the experiment looks at the effect of pay rate on speed of nickname collection and documents an advertising effect where MTurk workers respond to existing work batches, called Human Intelligence Tasks (HITs), more quickly once similar higher paying HITs are posted.},
booktitle = {Proceedings of the NAACL HLT 2010 Workshop on Creating Speech and Language Data with Amazon's Mechanical Turk},
pages = {89–92},
numpages = {4},
location = {Los Angeles, California},
series = {CSLDAMT '10}
}

@inproceedings{10.1109/BIGCOMP.2016.7425956,
author = {Jae-ho Shin and Gyoung-Don Joo and Chulyun Kim},
title = {XPath based crawling method with crowdsourcing for targeted online market places},
year = {2016},
isbn = {9781467387965},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/BIGCOMP.2016.7425956},
doi = {10.1109/BIGCOMP.2016.7425956},
abstract = {An increasing number of online market places have emerged as online shopping becomes more popular for a couple of decades. During that time, technologies to construct web sites have been evolved as well and, currently, AJAX is a representative technique to construct dynamic web pages. Crawling is a basic tool to collect information in the internet, and traditional crawling techniques randomly choose and follow links represented by the anchor tag in order to navigate the Word-Wide-Web. However, when a traditional crawler is applied for gathering information from a targeted up-to-date online market place, there are some critical problems. The first issue is that there are too many links, among which only few are enough to navigate all web pages in the site. The second issue is that most links are given by JavaScript but not by the anchor tags, which cannot be followed by the traditional web crawlers. Therefore, to overcome these issues, we suggest a webpage crawling method which can extract only necessary and sufficient links by adopting crowdsourcing approach and can follow JavaScript links by using a navigating information represented by XPaths.},
booktitle = {Proceedings of the 2016 International Conference on Big Data and Smart Computing (BigComp)},
pages = {395–397},
numpages = {3},
series = {BIGCOMP '16}
}

@inproceedings{10.1007/978-3-319-21410-8_43,
author = {Tiwari, Sunita and Kaushik, Saroj},
title = {Crowdsourcing Based Fuzzy Information Enrichment of Tourist Spot Recommender Systems},
year = {2015},
isbn = {9783319214092},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-319-21410-8_43},
doi = {10.1007/978-3-319-21410-8_43},
abstract = {Tourist Spot Recommender Systems TSRS help users to find the interesting locations/spots in vicinity based on their preferences. Enriching the list of recommended spots with contextual information such as right time to visit, weather conditions, traffic condition, right mode of transport, crowdedness, security alerts etc. may further add value to the systems. This paper proposes the concept of information enrichment for a tourist spot recommender system. Proposed system works in collaboration with a Tourist Spot Recommender System, takes the list of spots to be recommended to the current user and collects the current contextual information for those spots. A new score/rank is computed for each spot to be recommender based on the recommender's rank and current context and sent back to the user. Contextual information may be collected by several techniques such as sensors, collaborative tagging folksonomy, crowdsourcing etc. This paper proposes an approach for information enrichment using just in time location aware crowdsourcing. Location aware crowdsourcing is used to get current contextual information about a spot from the crowd currently available at that spot. Most of the contextual parameters such as traffic conditions, weather conditions, crowdedness etc. are fuzzy in nature and therefore, fuzzy inference is proposed to compute a new score/rank, with each recommended spot. The proposed system may be used with any spot recommender system, however, in this work a personalized tourist spot recommender system is considered as a case for study and evaluation. A prototype system has been implemented and is evaluated by 104 real users.},
booktitle = {Proceedings, Part IV, of the 15th International Conference on Computational Science and Its Applications -- ICCSA 2015 - Volume 9158},
pages = {559–574},
numpages = {16},
keywords = {Crowdsourcing, Fuzzy inference, Information enrichment, Recommender systems, Tourism}
}

@inproceedings{10.1007/978-3-319-47099-3_14,
author = {An, Jian and Wu, Ruobiao and Xiang, Lele and Gui, Xiaolin and Peng, Zhenlong},
title = {FCM: A Fine-Grained Crowdsourcing Model Based on Ontology in Crowd-Sensing},
year = {2016},
isbn = {978-3-319-47098-6},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-319-47099-3_14},
doi = {10.1007/978-3-319-47099-3_14},
abstract = {Crowd sensing between users with smart mobile devices is a new trend of development in Internet. In order to recommend the suitable service providers for crowd sensing requests, this paper presents a Fine-grained Crowdsourcing Model (FCM) based on Ontology theory that helps users to select appropriate service providers. First, the characteristic properties which extracted from the service request will be compared with the service provider based on ontology triple. Second, recommendation index of each service provider is calculated through similarity analysis and cluster analysis. Finally, the service decision tree is proposed to predict and recommend appropriate candidate users to participate in crowd sensing service. Experimental results show that this method provides more accurate recommendation than present recommendation systems and consumes less time to find the service provider through clustering algorithm.},
booktitle = {Network and Parallel Computing: 13th IFIP WG 10.3 International Conference, NPC 2016, Xi'an, China, October 28-29, 2016, Proceedings},
pages = {172–179},
numpages = {8},
keywords = {Service Provider, Service Request, Service Type, Service Recommendation, Ontology Theory},
location = {Xi'an, China}
}

@inproceedings{10.1007/978-3-319-48496-9_70,
author = {Inzerillo, Laura and Santagati, Cettina},
title = {Crowdsourcing Cultural Heritage: From 3D Modeling to the Engagement of Young Generations},
year = {2016},
isbn = {978-3-319-48495-2},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-319-48496-9_70},
doi = {10.1007/978-3-319-48496-9_70},
abstract = {Monitoring, digitizing and archiving museum artworks represent an important socio-cultural accomplishment and an overcoming in digital preservation today. Cultural heritage is constantly under threat of terrorist attacks and natural disaster. The high costs related to documentation task have prevented a constantly and massive survey activity. The low cost 3D image based acquisition and elaboration techniques of an object, allow to carry out a 3D photorealistic model in a short time. Therefore, a lot of museum adopted these techniques for the artworks archiving. Crowdsourcing activities can significantly speed up survey and elaboration procedures. If, on the one hand, these initiatives can have a positive impact, on the other hand involve the online user with a marginal role. In this paper we demonstrate how it is appropriate thinking the museum visitor as “museum operator/maker” of the digital model overstepping the outcomes achieved so far.},
booktitle = {Digital Heritage. Progress in Cultural Heritage: Documentation, Preservation, and Protection: 6th International Conference, EuroMed 2016, Nicosia, Cyprus, October 31 – November 5, 2016, Proceedings, Part I},
pages = {869–879},
numpages = {11},
keywords = {Cultural heritage, 3D modeling, Structure from Motion (SfM), Museum collections, Crowdsourcing},
location = {Nicosia, Cyprus}
}

@inproceedings{10.1145/2557500.2557512,
author = {Park, Sunghyun and Shoemark, Philippa and Morency, Louis-Philippe},
title = {Toward crowdsourcing micro-level behavior annotations: the challenges of interface, training, and generalization},
year = {2014},
isbn = {9781450321846},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2557500.2557512},
doi = {10.1145/2557500.2557512},
abstract = {Research that involves human behavior analysis usually requires laborious and costly efforts for obtaining micro-level behavior annotations on a large video corpus. With the emerging paradigm of crowdsourcing however, these efforts can be considerably reduced. We first present OCTAB (Online Crowdsourcing Tool for Annotations of Behaviors), a web-based annotation tool that allows precise and convenient behavior annotations in videos, directly portable to popular crowdsourcing platforms. As part of OCTAB, we introduce a training module with specialized visualizations. The training module's design was inspired by an observational study of local experienced coders, and it enables an iterative procedure for effectively training crowd workers online. Finally, we present an extensive set of experiments that evaluates the feasibility of our crowdsourcing approach for obtaining micro-level behavior annotations in videos, showing the reliability improvement in annotation accuracy when properly training online crowd workers. We also show the generalization of our training approach to a new independent video corpus.},
booktitle = {Proceedings of the 19th International Conference on Intelligent User Interfaces},
pages = {37–46},
numpages = {10},
keywords = {behavior annotations, crowdsourcing, inter-rater reliability, micro-level annotations, training crowd workers},
location = {Haifa, Israel},
series = {IUI '14}
}

@inproceedings{10.1145/2487788.2488079,
author = {Singh, Priyanka and Shadbolt, Nigel},
title = {Linked data in crowdsourcing purposive social network},
year = {2013},
isbn = {9781450320382},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2487788.2488079},
doi = {10.1145/2487788.2488079},
abstract = {Internet is an easy medium for people to collaborate and crowdsourcing is an efficient feature of social web where people with common interest and expertise come together to solve specific problems by collective thinking and create a community. It can also be used to filter out important information from large data, remove spams, and gamification techniques are used to reward the users for their contribution and keep a sustainable environment for the growth of the community. Semantic web technologies can be used to structure the community data so it can be combined, decentralized and be used across platform. Using such tools knowledge can be enhanced and easily discovered and merged together. This paper discusses the concept of a purposive social network where people with similar interest and varied expertise come together, use crowdsourcing technique to solve a common problem and build tools for common purpose. The StackOverflow website is chosen to study the purposive network, different network ties and roles of user is studied. Linked Data is used for name disambiguation of keywords and topics for easier search and discovery of experts in a field and provide useful information that is otherwise unavailable in the website.},
booktitle = {Proceedings of the 22nd International Conference on World Wide Web},
pages = {913–918},
numpages = {6},
keywords = {crowdsourcing, linked data, name entity disambiguation, q&amp;a, social machine, social media},
location = {Rio de Janeiro, Brazil},
series = {WWW '13 Companion}
}

@inproceedings{10.1145/3136014.3136033,
author = {Brambilla, Marco and Cabot, Jordi and C\'{a}novas Izquierdo, Javier Luis and Mauri, Andrea},
title = {Better call the crowd: using crowdsourcing to shape the notation of domain-specific languages},
year = {2017},
isbn = {9781450355254},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3136014.3136033},
doi = {10.1145/3136014.3136033},
abstract = {Crowdsourcing has emerged as a novel paradigm where humans are employed to perform computational tasks. In the context of Domain-Specific Modeling Language (DSML) development, where the involvement of end-users is crucial to assure that the resulting language satisfies their needs, crowdsourcing tasks could be defined to assist in the language definition process. By relying on the crowd, it is possible to show an early version of the language to a wider spectrum of users, thus increasing the validation scope and eventually promoting its acceptance and adoption. We propose a systematic method for creating crowdsourcing campaigns aimed at refining the graphical notation of DSMLs. The method defines a set of steps to identify, create and order the questions for the crowd. As a result, developers are provided with a set of notation choices that best fit end-users' needs. We also report on an experiment validating the approach.},
booktitle = {Proceedings of the 10th ACM SIGPLAN International Conference on Software Language Engineering},
pages = {129–138},
numpages = {10},
keywords = {crowdsourcing, domain-specific languages, model-driven development},
location = {Vancouver, BC, Canada},
series = {SLE 2017}
}

@inproceedings{10.1145/2800835.2800969,
author = {Minoda, Yuki and Ohama, Iku and Muramoto, Eiichi},
title = {A machine learning approach for lighting perception analysis via crowdsourcing},
year = {2015},
isbn = {9781450335751},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2800835.2800969},
doi = {10.1145/2800835.2800969},
abstract = {In this paper, a new analytical scheme designed for crowd sourced subjective lighting evaluation system is proposed. Participants are gathered through crowdsourcing and evaluations are done based on an online system that shows CG (Computer Graphics) on a display. Data about preferable space brightness which is collected by the system is analyzed by machine learning techniques, the Bradley-Terry Mixture (BTM) model and the logistic regression. The results show that applying machine learning techniques enable us to extract important relationships between lighting preference and participants' attributes such as age.},
booktitle = {Adjunct Proceedings of the 2015 ACM International Joint Conference on Pervasive and Ubiquitous Computing and Proceedings of the 2015 ACM International Symposium on Wearable Computers},
pages = {1355–1360},
numpages = {6},
keywords = {crowdsourcing, human factor, lighting perception, machine learning},
location = {Osaka, Japan},
series = {UbiComp/ISWC'15 Adjunct}
}

@inproceedings{10.1145/2316936.2316940,
author = {Aparicio, Manuela and Costa, Carlos J. and Braga, Andrew Simoes},
title = {Proposing a system to support crowdsourcing},
year = {2012},
isbn = {9781450315258},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2316936.2316940},
doi = {10.1145/2316936.2316940},
abstract = {In this paper, a conceptual framework is proposed, supported in the literature review, derived by identifying the main concepts related to crowdsourcing, as well as ways of improving group participation. We also propose a software solution that may be used to support the crowdsourcing process. This software solution is inspired by the conceptual framework.},
booktitle = {Proceedings of the Workshop on Open Source and Design of Communication},
pages = {13–17},
numpages = {5},
keywords = {collaborative systems, collective intelligence, crowdsourcing, web application, web based systems},
location = {Lisboa, Portugal},
series = {OSDOC '12}
}

@inproceedings{10.5555/2936924.2936941,
author = {Jain, Shweta and Ghalme, Ganesh and Bhat, Satyanath and Gujar, Sujit and Narahari, Y.},
title = {A Deterministic MAB Mechanism for Crowdsourcing with Logarithmic Regret and Immediate Payments},
year = {2016},
isbn = {9781450342391},
publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
address = {Richland, SC},
abstract = {We consider a general crowdsourcing setting with strategic workers whose qualities are unknown and design a multi-armed bandit (MAB) mechanism, CrowdUCB, which is deterministic, regret minimizing, and offers immediate payments to the workers. The problem involves sequentially selecting workers to process tasks in order to maximize the social welfare while learning the qualities of the strategic workers (strategic about their costs). Existing MAB mechanisms are either: (a) deterministic which potentially cause significant loss in social welfare, or (b) randomized which typically lead to high variance in payments. CrowdUCB completely addresses the above problems with the following features: (i) offers deterministic payments, (ii) achieves logarithmic regret in social welfare, (iii) renders allocations more effective by allocating blocks of tasks to a worker instead of a single task, and (iv) offers payment to a worker immediately upon completion of an assigned block of tasks. CrowdUCB is a mechanism with learning that learns the qualities of the workers while eliciting their true costs, irrespective of whether or not the workers know their own qualities. We show that CrowdUCB is ex-post individually rational (EPIR) and ex-post incentive compatible (EPIC) when the workers do not know their own qualities and when they update their beliefs in sync with the requester. When the workers know their own qualities, CrowdUCB is EPIR and εEPIC where ε is sub-linear in terms of the number of tasks.},
booktitle = {Proceedings of the 2016 International Conference on Autonomous Agents \&amp; Multiagent Systems},
pages = {86–94},
numpages = {9},
keywords = {crowdsourcing, mechanism design, multi-armed bandit},
location = {Singapore, Singapore},
series = {AAMAS '16}
}

@inproceedings{10.1145/2009916.2010170,
author = {Alonso, Omar and Lease, Matthew},
title = {Crowdsourcing for information retrieval: principles, methods, and applications},
year = {2011},
isbn = {9781450307574},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2009916.2010170},
doi = {10.1145/2009916.2010170},
abstract = {Crowdsourcing has emerged in recent years as a promising new avenue for leveraging today's digitally-connected, diverse, distributed workforce. Generally speaking, crowdsourcing describes outsourcing of tasks to a large group of people instead of assigning such tasks to an in-house employee or contractor. Crowdsourcing platforms such as Amazon Mechanical Turk and CrowdFlower have gained particular attention as active online market places for reaching and tapping into this still largely under-utilized workforce. Crowdsourcing also offers intriguing new opportunities for accomplishing different kinds of tasks or achieving broader participation than previously possible, as well as completing standard tasks more accurately in less time and at lower cost. Unlocking the potential of crowdsourcing in practice, however, requires a tri-partite understanding of principles, platforms, and best practices. We will introduce the opportunities and challenges of crowdsourcing while discussing the three issues above. This will provide a basic foundation to begin crowdsourcing in the context of one's own particular tasks},
booktitle = {Proceedings of the 34th International ACM SIGIR Conference on Research and Development in Information Retrieval},
pages = {1299–1300},
numpages = {2},
keywords = {crowdsourcing, human computation},
location = {Beijing, China},
series = {SIGIR '11}
}

@inproceedings{10.1145/2987386.2987416,
author = {Lazarova-Molnar, Sanja and Logason, Halld\'{o}r \TH{}\'{o}r and Andersen, Peter Gr\o{}nb\ae{}k and Kj\ae{}rgaard, Mikkel Baun},
title = {Mobile Crowdsourcing of Data for Fault Detection and Diagnosis in Smart Buildings},
year = {2016},
isbn = {9781450344555},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2987386.2987416},
doi = {10.1145/2987386.2987416},
abstract = {Energy use of buildings represents roughly 40\% of the overall energy consumption. Most of the national agendas contain goals related to reducing the energy consumption and carbon footprint. Timely and accurate fault detection and diagnosis (FDD) in building management systems (BMS) have the potential to reduce energy consumption cost by approximately 15-30\%. Most of the FDD methods are data-based, meaning that their performance is tightly linked to the quality and availability of relevant data. Based on our experience, faults and relevant events data is very sparse and inadequate, mostly because of the lack of will and incentive for those that would need to keep track of faults. In this paper we introduce the idea of using crowdsourcing to support FDD data collection processes, and illustrate our idea through a mobile application that has been implemented for this purpose. Furthermore, we propose a strategy of how to successfully deploy this building occupants' crowdsourcing application.},
booktitle = {Proceedings of the International Conference on Research in Adaptive and Convergent Systems},
pages = {12–17},
numpages = {6},
keywords = {Crowdsourcing, buildings, data collection, energy performance, fault detection and diagnosis, occupants},
location = {Odense, Denmark},
series = {RACS '16}
}

@inproceedings{10.1007/978-3-662-44845-8_49,
author = {Schnitzler, Fran\c{c}ois and Artikis, Alexander and Weidlich, Matthias and Boutsis, Ioannis and Liebig, Thomas and Piatkowski, Nico and Bockermann, Christian and Morik, Katharina and Kalogeraki, Vana and Marecek, Jakub and Gal, Avigdor and Mannor, Shie and Kinane, Dermot and Gunopulos, Dimitrios},
title = {Heterogeneous Stream Processing and Crowdsourcing for Traffic Monitoring: Highlights},
year = {2014},
isbn = {978-3-662-44844-1},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-662-44845-8_49},
doi = {10.1007/978-3-662-44845-8_49},
abstract = {We give an overview of an intelligent urban traffic management system. Complex events related to congestions are detected from heterogeneous sources involving fixed sensors mounted on intersections and mobile sensors mounted on public transport vehicles. To deal with data veracity, sensor disagreements are resolved by crowdsourcing. To deal with data sparsity, a traffic model offers information in areas with low sensor coverage. We apply the system to a real-world use-case.},
booktitle = {Machine Learning and Knowledge Discovery in Databases},
pages = {520–523},
numpages = {4},
keywords = {smart cities, crowdsourcing, event pattern matching, traffic, stream processing, big data},
location = {Nancy
France}
}

@inproceedings{10.1109/CIT.2014.126,
author = {Trow, Josh and Liu, Lu and Li, Zhiyuan},
title = {An Investigation Into Internet Crowdsourcing for Enterprise Software Development},
year = {2014},
isbn = {9781479962396},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/CIT.2014.126},
doi = {10.1109/CIT.2014.126},
abstract = {Recent years have witnessed the continuing development of the Internet from its original communication purpose (e.g., email), content provision (e.g., Web) and software deployment platform (e.g. SaaS) to a software development platform for enterprise. This paper investigated crowd sourcing, in particular its application to the software development sector. By making use of this method developers can be recruited from around the world, resulting in a wide range of expertise and more manageable workload. This paper considers a number of ways which crowd sourcing can be applied to existing software development methods and techniques, as well as exploring and evaluating the benefits and constraints of such methods. The paper used data gathered from the Top Coder platform to assess the skill levels and demographics of the current crowd sourcing user base. This paper also examined a wider range of crowd sourcing platforms to discover which features makes platforms successful and how they can be applied to a software engineering platform. Recommendations are then made as to what should constitute a software engineering specific crowd sourcing platform.},
booktitle = {Proceedings of the 2014 IEEE International Conference on Computer and Information Technology},
pages = {474–481},
numpages = {8},
keywords = {Internet, crowdsourcing, software development},
series = {CIT '14}
}

@inproceedings{10.5555/2820116.2820125,
author = {Zhao, Mengyao and van der Hoek, Andr\'{e}},
title = {A brief perspective on microtask crowdsourcing workflows for interface design},
year = {2015},
publisher = {IEEE Press},
abstract = {User interface design, as a crucial part of software design, is complex. Current microtask crowdsourcing workflows do not support its complexity well. The difficulty particularly relates to the process to decompose an interface design task into microtasks. In order to make microtask crowdsourcing more supportive for interface design, we need a workflow that can help task owners to break down interface design tasks more easily. This paper briefly describes three experiments that help inform various aspects of workflow design for interface design through microtask crowdsourcing.},
booktitle = {Proceedings of the Second International Workshop on CrowdSourcing in Software Engineering},
pages = {45–46},
numpages = {2},
keywords = {complex work, crowdsourcing, interface design},
location = {Florence, Italy},
series = {CSI-SE '15}
}

@inproceedings{10.1145/2787394.2790462,
author = {Clark, David and Texeira, Renata and Mellia, Macro},
title = {What has Worked and What Won't Work in Crowdsourcing},
year = {2015},
isbn = {9781450335393},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2787394.2790462},
doi = {10.1145/2787394.2790462},
abstract = {An interactive discussion around the questions/issues raised in the CFP including: Are there successful crowdsourcing ideas that have not come out today? Are there specific projects that you think the community should know more about? How should we enlist vantage points in the right locations? How do these platforms differ from/extend human-entered crowdsourcing systems? What kinds of experiments are technically and ethically viable? What is the right programming interface for the experimenter? Given the limited control we have on these platforms, what is the right experimental model? Could we build a federation of platforms and how would that work?},
booktitle = {Proceedings of the 2015 ACM SIGCOMM Workshop on Crowdsourcing and Crowdsharing of Big (Internet) Data},
pages = {51},
numpages = {1},
location = {London, United Kingdom},
series = {C2B(1)D '15}
}

@inproceedings{10.5555/2888116.2888337,
author = {Wu, Heting and Sun, Hailong and Fang, Yili and Hu, Kefan and Xie, Yongqing and Song, Yangqiu and Liu, Xudong},
title = {Combining machine learning and crowdsourcing for better understanding commodity reviews},
year = {2015},
isbn = {0262511290},
publisher = {AAAI Press},
abstract = {In e-commerce systems, customer reviews are important information for understanding market feedbacks on certain commodities. However, accurate analyzing reviews is challenging due to the complexity of natural language processing and informal descriptions in reviews. Existing methods mainly focus on studying efficient algorithms that cannot guarantee the accuracy for review analysis. Crowdsourcing can improve the accuracy of review analysis while it is subject to extra costs and low response time. In this work, we combine machine learning and crowdsourcing together for better understanding customer reviews. First, we collectively use multiple machine learning algorithms to pre-process review classification. Second, we select the reviews on which all machine learning algorithms cannot agree and assign them to humans to process. Third, the results from machine learning and crowdsourcing are aggregated to be the final analysis results. Finally, we perform real experiments with practical review data to confirm the effectiveness of our method.},
booktitle = {Proceedings of the Twenty-Ninth AAAI Conference on Artificial Intelligence},
pages = {4220–4221},
numpages = {2},
location = {Austin, Texas},
series = {AAAI'15}
}

@inproceedings{10.1145/2441776.2441848,
author = {Hansen, Derek L. and Schone, Patrick J. and Corey, Douglas and Reid, Matthew and Gehring, Jake},
title = {Quality control mechanisms for crowdsourcing: peer review, arbitration, \&amp; expertise at familysearch indexing},
year = {2013},
isbn = {9781450313315},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2441776.2441848},
doi = {10.1145/2441776.2441848},
abstract = {The FamilySearch Indexing project has enabled hundreds of thousands of volunteers to transcribe billions of records, making it one of the largest crowdsourcing initiatives in the world. Assuring high quality transcriptions (i.e., indexes) with a reasonable amount of volunteer effort is essential to keep pace with the mounds of newly digitized documents. Using historical data, we show the relationship between prior experience and native language on transcriber agreement. We then present a field experiment comparing the effectiveness (accuracy) and efficiency (time) of two quality control mechanisms: (1) Arbitration -- the existing mechanism wherein two volunteers independently transcribe records and disagreements go to an arbitrator, and (2) Peer Review -- a mechanism wherein one volunteer's work is reviewed by another volunteer. Peer Review is significantly more efficient, though not as effective for certain fields as Arbitration. Design suggestions for FamilySearch Indexing and related crowdsourcing initiatives are provided.},
booktitle = {Proceedings of the 2013 Conference on Computer Supported Cooperative Work},
pages = {649–660},
numpages = {12},
keywords = {crowdsourcing, familysearch indexing, genealogy, historical documents, peer review, quality control, transcription},
location = {San Antonio, Texas, USA},
series = {CSCW '13}
}

@inproceedings{10.5555/2772879.2773400,
author = {Chen, Cen and Cheng, Shih-Fen and Misra, Archan and Lau, Hoong Chuin},
title = {Multi-Agent Task Assignment for Mobile Crowdsourcing under Trajectory Uncertainties},
year = {2015},
isbn = {9781450334136},
publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
address = {Richland, SC},
abstract = {In this work, we investigate the problem of mobile crowdsourcing, where workers are financially motivated to perform location-based tasks physically. Unlike current industry practice that relies on workers to manually browse and filter tasks to perform, we intend to automatically make task recommendations based on workers' historical trajectories and desired time budgets. However, predicting workers' trajectories is inevitably faced with uncertainties, as no one will take exactly the same route every day; yet such uncertainties are oftentimes abstracted away in the known literature. In this work, we depart from the deterministic modeling and study the stochastic task recommendation problem where each worker is associated with several predicted routine routes with probabilities. We formulate this problem as a stochastic integer linear program whose goal is to maximize the expected total utility achieved by all workers. We further exploit the separable structure of the formulation and apply the Lagrangian relaxation technique to scale up the solution approach. Experiments have been performed over the instances generated using the real Singapore transportation network. The results show that we can find significantly better solutions than the deterministic formulation.},
booktitle = {Proceedings of the 2015 International Conference on Autonomous Agents and Multiagent Systems},
pages = {1715–1716},
numpages = {2},
keywords = {mobile crowdsourcing, multiagent planning},
location = {Istanbul, Turkey},
series = {AAMAS '15}
}

@inproceedings{10.1109/MASS.2014.66,
author = {Luo, Tie and Kanhere, Salil S. and Tan, Hwee-Pink},
title = {Optimal Prizes for All-Pay Contests in Heterogeneous Crowdsourcing},
year = {2014},
isbn = {9781479960361},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/MASS.2014.66},
doi = {10.1109/MASS.2014.66},
abstract = {Incentive is key to the success of crowd sourcing which heavily depends on the level of user participation. This paper designs an incentive mechanism to motivate a heterogeneous crowd of users to actively participate in crowd sourcing campaigns. We cast the problem in a new, asymmetric all-pay contest model with incomplete information, where an arbitrary n of users exert irrevocable effort to compete for a prize tuple. The prize tuple is an array of prize functions as opposed to a single constant prize typically used by conventional contests. We design an optimal contest that (a) induces the maximum profit -- total user effort minus the prize payout -- for the crowdsourcer, and (b) ensures users to strictly have incentive to participate. In stark contrast to intuition and prior related work, our mechanism induces an equilibrium in which heterogeneous users behave independently of one another as if they were in a homogeneous setting. This newly discovered property, which we coin as strategy autonomy (SA), is of practical significance: it (a) reduces computational and storage complexity by n-fold for each user, (b) increases the crowdsourcer's revenue by counteracting an effort reservation effect existing in asymmetric contests, and (c) neutralizes the (almost universal) law of diminishing marginal returns (DMR). Through an extensive numerical case study, we demonstrate and scrutinize the superior profitability of our mechanism, as well as draw insights into the SA property.},
booktitle = {Proceedings of the 2014 IEEE 11th International Conference on Mobile Ad Hoc and Sensor Systems},
pages = {136–144},
numpages = {9},
keywords = {Incentive mechanism, all-pay auction, asymmetric contest, network economics, participatory sensing, strategy autonomy},
series = {MASS '14}
}

@inproceedings{10.1145/3397166.3409142,
author = {Sarkar, Shamik and Baset, Aniqua and Singh, Harsimran and Smith, Phillip and Patwari, Neal and Kasera, Sneha and Derr, Kurt and Ramirez, Samuel},
title = {LLOCUS: learning-based localization using crowdsourcing},
year = {2020},
isbn = {9781450380157},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3397166.3409142},
doi = {10.1145/3397166.3409142},
abstract = {We present LLOCUS, a novel learning-based system that uses mobile crowdsourced RF sensing to estimate the location and power of unknown mobile transmitters in real time, while allowing unrestricted mobility of the crowdsourcing participants. We carefully identify and tackle several challenges in learning and localizing, based on RSS, in such a dynamic environment. We decouple the problem of localizing a transmitter with unknown transmit power into two problems, 1) predicting the power of a transmitter at an unknown location, and 2) localizing a transmitter with known transmit power. LLOCUS first estimates the power of the unknown transmitter and then scales the reported RSS values such that the unknown transmit power problem is transparent to the method of localization. We evaluate LLOCUS using three experiments in different indoor and outdoor environments. We find that LLOCUS reduces the localization error by 17-68\% compared to several non-learning methods.},
booktitle = {Proceedings of the Twenty-First International Symposium on Theory, Algorithmic Foundations, and Protocol Design for Mobile Networks and Mobile Computing},
pages = {201–210},
numpages = {10},
location = {Virtual Event, USA},
series = {Mobihoc '20}
}

@inproceedings{10.1145/2207676.2207709,
author = {Willett, Wesley and Heer, Jeffrey and Agrawala, Maneesh},
title = {Strategies for crowdsourcing social data analysis},
year = {2012},
isbn = {9781450310154},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2207676.2207709},
doi = {10.1145/2207676.2207709},
abstract = {Web-based social data analysis tools that rely on public discussion to produce hypotheses or explanations of the patterns and trends in data, rarely yield high-quality results in practice. Crowdsourcing offers an alternative approach in which an analyst pays workers to generate such explanations. Yet, asking workers with varying skills, backgrounds and motivations to simply "Explain why a chart is interesting" can result in irrelevant, unclear or speculative explanations of variable quality. To address these problems, we contribute seven strategies for improving the quality and diversity of worker-generated explanations. Our experiments show that using (S1) feature-oriented prompts, providing (S2) good examples, and including (S3) reference gathering, (S4) chart reading, and (S5) annotation subtasks increases the quality of responses by 28\% for US workers and 196\% for non-US workers. Feature-oriented prompts improve explanation quality by 69\% to 236\% depending on the prompt. We also show that (S6) pre-annotating charts can focus workers' attention on relevant details, and demonstrate that (S7) generating explanations iteratively increases explanation diversity without increasing worker attrition. We used our techniques to generate 910 explanations for 16 datasets, and found that 63\% were of high quality. These results demonstrate that paid crowd workers can reliably generate diverse, high-quality explanations that support the analysis of specific datasets.},
booktitle = {Proceedings of the SIGCHI Conference on Human Factors in Computing Systems},
pages = {227–236},
numpages = {10},
keywords = {crowdsourcing, information visualization, social data analysis},
location = {Austin, Texas, USA},
series = {CHI '12}
}

@inproceedings{10.1145/3338147.3338159,
author = {Guo, Dongpo and Li, Qing and Liu, Sanya and Chai, Huanyou},
title = {Research on the Ecology Model of Crowdfunding and Crowdsourcing for Digital Education Service and its Applications in China},
year = {2019},
isbn = {9781450362658},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3338147.3338159},
doi = {10.1145/3338147.3338159},
abstract = {Crowdfunding and crowdsourcing in digital education service has become an important content in educational informatization. Different personalized demands from learners for digital education service are increasing day by day, which leads to the asymmetry between supply and demand. This paper aims to solve the asymmetry of supply and demand in education sector under the SISC ecology model based on value chain theory, and also to elaborate the ecology environment in digital education service, as well as the subject and content of crowdfunding and crowdsourcing. Block chain is the protection mechanism for the rights of subject, which improves the development of personalized education. Empirical study and qualitative analysis are used to fulfill the effectiveness of SISC ecology model of CFCS in DES. A case of "Parallel" successfully recommends 10 resources that suitable for the students, which proves the feasibility of the SISC ecology model so as to improve the motivations of CFCS subjects and student' satisfaction. This paper also supports future research by developing a framework for crowdfunding and crowdsourcing in digital education service.},
booktitle = {Proceedings of the 2019 4th International Conference on Distance Education and Learning},
pages = {147–152},
numpages = {6},
keywords = {Block chain, Crowdfunding and Crowdsourcing, Digital education service, Ecology model},
location = {Shanghai, China},
series = {ICDEL '19}
}

@inproceedings{10.1109/ICPADS.2015.28,
author = {Hao Wang and Dong Zhao and Huadong Ma and Huaiyu Xu and Xiabing Hou},
title = {Crowdsourcing Based Mobile Location Recognition with Richer Fingerprints from Smartphone Sensors},
year = {2015},
isbn = {9780769557854},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/ICPADS.2015.28},
doi = {10.1109/ICPADS.2015.28},
abstract = {With the rapid advancements of mobile computing, mobile location recognition is becoming an important and useful service, which recognizes the logical locations of places/scenes that users are interested in, instead of physical coordinates. Most of the existing mobile location recognition systems utilize the image as visual fingerprint of a place, and need to construct a large-scale visual fingerprint database in advance. However, collecting visual fingerprints is a labor-intensive and time-consuming procedure. In order to address this problem, we propose a novel crowdsourcing-based framework, and leverage a variety of sensors embedded in smartphones to collect richer location fingerprints for exploring their positive effects. To achieve higher recognition accuracy, we propose an object-centric fingerprint searching which can sufficiently take advantage of smartphone sensors and determine more accurate searching space than the traditional user-centric method. We build a crowdsourcing-based database with richer fingerprints and implement a location recognition system, called CrowdLR. Extensive experiments verify that our object-centric method can achieve promising results maintaining around 10\% precision higher than the user-centric method.},
booktitle = {Proceedings of the 2015 IEEE 21st International Conference on Parallel and Distributed Systems (ICPADS)},
pages = {156–163},
numpages = {8},
series = {ICPADS '15}
}

@inproceedings{10.5555/2050843.2050864,
author = {Viappiani, Paolo and Zilles, Sandra and Hamilton, Howard J. and Boutilier, Craig},
title = {Learning complex concepts using crowdsourcing: a Bayesian approach},
year = {2011},
isbn = {9783642248726},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {We develop a Bayesian approach to concept learning for crowdsourcing applications. A probabilistic belief over possible concept definitions is maintained and updated according to (noisy) observations from experts, whose behaviors are modeled using discrete types. We propose recommendation techniques, inference methods, and query selection strategies to assist a user charged with choosing a configuration that satisfies some (partially known) concept. Our model is able to simultaneously learn the concept definition and the types of the experts. We evaluate our model with simulations, showing that our Bayesian strategies are effective even in large concept spaces with many uninformative experts.},
booktitle = {Proceedings of the Second International Conference on Algorithmic Decision Theory},
pages = {277–291},
numpages = {15},
location = {Piscataway, NJ},
series = {ADT'11}
}

@inproceedings{10.1109/CCNC.2018.8319312,
author = {Mangiatordi, Andrea and Lazzari, Marco},
title = {Combined use of artificial intelligence and crowdsourcing to provide alternative content for images on websites},
year = {2018},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/CCNC.2018.8319312},
doi = {10.1109/CCNC.2018.8319312},
abstract = {Web accessibility can be efficiently evaluated in both automated and manual ways, but fixing accessibility issues on live websites is still a major challenge, as it requires attention, dedication and knowledge of Assistive Technology requirements. A website revision system is thus proposed, relying on Artificial Intelligence to produce alternative text, and on crowdsourcing to correct it, for images or other generic types of resources. This solution is integrated in a web application called Farfalla, which also offers easy integration of accessibility options in websites. A major advantage offered by this approach is the independence from a specific browser or operating system: the proposed architecture is highly portable and platform-agnostic. The basic idea that will be put forward here is that Artificial Intelligence can be used to generate basic information about images in websites, and crowdsourcing would serve as a way for refining that information. As the development of the project is still in progress, the most important implementation issues are analyzed: they include context-aware alternatives, localization, scalable deployment and input validation.},
booktitle = {2018 15th IEEE Annual Consumer Communications \&amp; Networking Conference (CCNC)},
pages = {1–6},
numpages = {6},
location = {Las Vegas, NV, USA}
}

@inproceedings{10.1007/978-3-319-40216-1_22,
author = {Cucari, Giovanni and Leotta, Francesco and Mecella, Massimo and Vassos, Stavros},
title = {Collecting Human Habit Datasets for Smart Spaces Through Gamification and Crowdsourcing},
year = {2015},
isbn = {9783319402154},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-319-40216-1_22},
doi = {10.1007/978-3-319-40216-1_22},
abstract = {A lot of research in the last years has focused on smart spaces, covering aspects related to ambient intelligence, activity monitoring and mining, etc. All these efforts require datasets to be used for experimental purposes and as benchmarks for novel techniques. Such datasets are today difficult to obtain as, on the one hand, building smart facilities is expensive, requiring considerable costs for maintenance and extension, and, on the other hand, freely available datasets are scarce, not continuously updated and contain a limited set of sensors, thus not allowing the evaluation of algorithms that require the availability of specific categories of sensors. To this aim, we have built a prototype smart virtual environment producing sensor logs on the basis of activities performed by users as if they were really acting in a physical smart space.},
booktitle = {Revised Selected Papers of the 4th International Conference on Games and Learning Alliance - Volume 9599},
pages = {208–217},
numpages = {10},
location = {Rome, Italy},
series = {GALA 2015}
}

@inproceedings{10.1007/978-3-642-41347-6_8,
author = {Vreede, Triparna and Nguyen, Cuong and Vreede, Gert-Jan and Boughzala, Imed and Oh, Onook and Reiter-Palmon, Roni},
title = {A Theoretical Model of User Engagement in Crowdsourcing},
year = {2013},
isbn = {9783642413469},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-642-41347-6_8},
doi = {10.1007/978-3-642-41347-6_8},
abstract = {Social media technology has enabled virtual collaborative environments where people actively interact, share knowledge, coordinate activities, solve problems, co-create value, and innovate. Organizations have begun to leverage approaches and technologies to involve numerous people from outside their boundaries to perform organizational tasks. Despite the success and popularity of this 'crowdsourcing' phenomenon, there appears to be a distinct gap in the literature regarding the empirical evaluation of the factors involved in a crowdsourcing user experience. This paper aims to fill this void by proposing a theoretical model of the antecedents and their relationships for crowdsourcing user engagement. It is defined as the quality of effort online users devote to collaboration activities that contribute directly to desired outcomes. Drawing from research in psychology and IS, we identify three critical elements that precede crowdsourcing user engagement: personal interest in topic, goal clarity, and motivation to contribute. This paper examines the theoretical basis of these variables of interest in detail, derives a causal model of their interrelationships, and identifies future plans for model testing.},
booktitle = {Proceedings of the 19th International Conference on Collaboration and Technology - Volume 8224},
pages = {94–109},
numpages = {16},
keywords = {Crowdsourcing, engagement, motivation, open collaboration, social media}
}

@inproceedings{10.5555/2893873.2893955,
author = {Sina, Sigal and Rosenfeld, Avi and Kraus, Sarit},
title = {Generating content for scenario-based serious-games using crowdsourcing},
year = {2014},
publisher = {AAAI Press},
abstract = {Scenario-based serious-games have become an important tool for teaching new skills and capabilities. An important factor in the development of such systems is reducing the time and cost overheads in manually creating content for these scenarios. To address this challenge, we present Scenario-Gen, an automatic method for generating content about everyday activities through combining computer science techniques with the crowd. ScenarioGen uses the crowd in three different ways: to capture a database of scenarios of everyday activities, to generate a database of likely replacements for specific events within that scenario, and to evaluate the resulting scenarios. We evaluated ScenarioGen in 6 different content domains and found that it was consistently rated as coherent and consistent as the originally captured content. We also compared ScenarioGen's content to that created by traditional planning techniques. We found that both methods were equally effective in generating coherent and consistent scenarios, yet ScenarioGen's content was found to be more varied and easier to create.},
booktitle = {Proceedings of the Twenty-Eighth AAAI Conference on Artificial Intelligence},
pages = {522–529},
numpages = {8},
location = {Qu\'{e}bec City, Qu\'{e}bec, Canada},
series = {AAAI'14}
}

@inproceedings{10.1145/2835596.2835610,
author = {Xu, Zheng and Sugumaran, Vijayan and Zhang, Hui},
title = {Crowdsourcing based spatial mining of urban emergency events using social media},
year = {2015},
isbn = {9781450339704},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2835596.2835610},
doi = {10.1145/2835596.2835610},
abstract = {With the advances of information communication technologies, it is critical to improve the efficiency and accuracy of emergency management systems through modern data processing techniques. The past decade has witnessed the tremendous technical advances in Sensor Networks, Internet/Web of Things, Cloud Computing, Mobile/Embedded Computing, Spatial/Temporal Data Processing, and Big Data, and these technologies have provided new opportunities and solutions to emergency management. GIS models and simulation capabilities are used to exercise response and recovery plans during non-disaster times. They help the decision-makers understand near real-time possibilities during an event. In this paper, a crowdsourcing based model for mining spatial information of urban emergency events is introduced. Firstly, basic definitions of the proposed method are given. Secondly, positive samples are selected to mine the spatial information of urban emergency events. Thirdly, location and GIS information are extracted from positive samples. At last, the real spatial information is determined based on address and GIS information. At last, a case study on an urban emergency event is given.},
booktitle = {Proceedings of the 1st ACM SIGSPATIAL International Workshop on the Use of GIS in Emergency Management},
articleno = {9},
numpages = {4},
keywords = {crowdsourcing, social media, urban computing, urban emergency events},
location = {Bellevue, Washington},
series = {EM-GIS '15}
}

@inproceedings{10.1145/2502081.2502083,
author = {Xu, Qianqian and Xiong, Jiechao and Huang, Qingming and Yao, Yuan},
title = {Robust evaluation for quality of experience in crowdsourcing},
year = {2013},
isbn = {9781450324045},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2502081.2502083},
doi = {10.1145/2502081.2502083},
abstract = {Strategies exploiting crowdsourcing are increasingly being applied in the area of Quality of Experience (QoE) for multimedia. They enable researchers to conduct experiments with a more diverse set of participants and at a lower economic cost than conventional laboratory studies. However, a major challenge for crowdsourcing tests is the detection and control of outliers, which may arise due to different test conditions, human errors or abnormal variations in context. For this purpose, it is desired to develop a robust evaluation methodology to deal with crowdsourceable data, which are possibly incomplete, imbalanced, and distributed on a graph. In this paper, we propose a robust rating scheme based on robust regression and Hodge Decomposition on graphs, to assess QoE using crowdsourcing. The scheme shows that the removal of outliers in crowdsourcing experiments would be helpful for purifying data and could provide us with more reliable results. The effectiveness of the proposed scheme is further confirmed by experimental studies on both simulated examples and real-world data.},
booktitle = {Proceedings of the 21st ACM International Conference on Multimedia},
pages = {43–52},
numpages = {10},
keywords = {crowdsourcing, hodge decomposition, lasso, outlier detection, paired comparison, quality of experience (QoE), random graph, robust evaluation},
location = {Barcelona, Spain},
series = {MM '13}
}

@inproceedings{10.5555/2892753.2893000,
author = {Fang, Yili and Sun, Hailong and Zhang, Richong and Huai, Jinpeng and Mao, Yongyi},
title = {A model for aggregating contributions of synergistic crowdsourcing workflows},
year = {2014},
publisher = {AAAI Press},
abstract = {One of the most important crowdsourcing topics is to study the effective quality control methods so as to reduce the cost and to guarantee the quality of task processing. As an effective approach, iterative improvement workflow is known to choose the best result from multiple workflows. However, for complex crowdsourcing tasks that consists of a certain number of subtasks under some specific constraints, but cannot be split into subtasks to be crowdsourced, the approach merely considers the best workflow without integrating the contributions of all workflows, which potentially results in extra costs for more iterations. In this paper, we propose an assembly model to integrate the best output of subtasks from different workflows. Moreover, we devise an efficient iterative method based on POMDP to improve the quality of assembled output. Empirical studies confirms the superiority of our proposed model.},
booktitle = {Proceedings of the Twenty-Eighth AAAI Conference on Artificial Intelligence},
pages = {3102–3103},
numpages = {2},
location = {Qu\'{e}bec City, Qu\'{e}bec, Canada},
series = {AAAI'14}
}

@inproceedings{10.1109/SCC.2013.58,
author = {Vukovic, Maja and Natarajan, Arjun},
title = {Operational Excellence in IT Services Using Enterprise Crowdsourcing},
year = {2013},
isbn = {9780769550268},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/SCC.2013.58},
doi = {10.1109/SCC.2013.58},
abstract = {The IT services industry has been undergoing a significant transformation over the last decade primarily driven by the global delivery model - services are provided from delivery centers across the globe based on skill and cost. While extremely effective a key challenge is being able to harness distributed knowledge, especially to drive operational and business process optimizations and a superior client experience. The knowledge about service requirements, client experience and delivery quality is in collective possession of different communities, such as clients, service designers and delivery teams. Current practices to discovering this distributed and unstructured knowledge are semi-automated, and as such they fail to scale and provide accurate insights on demand. Enterprise crowdsourcing provides a mechanism to harness the tacit knowledge from a large group of network-connected humans. In this paper we describe a novel approach to improving operational excellence, with focus on compliance posture, using enterprise crowdsourcing. We demonstrate how enterprise crowdsourcing accelerated deployment of a novel identity access management capability in a global IT service delivery center, reducing the time to discover the required knowledge by 80\%. We discuss how the uncovered knowledge networks can be engaged for various on-going operational activities as well as large-scale business transformation initiatives.},
booktitle = {Proceedings of the 2013 IEEE International Conference on Services Computing},
pages = {494–501},
numpages = {8},
keywords = {IT services, component, crowdsourcing, operational excellence},
series = {SCC '13}
}

@inproceedings{10.1145/2502081.2503828,
author = {Chen, Kuan-Ta and Chu, Wei-Ta and Larson, Martha},
title = {ACM multimedia 2013 workshop on crowdsourcing for multimedia},
year = {2013},
isbn = {9781450324045},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2502081.2503828},
doi = {10.1145/2502081.2503828},
abstract = {The topic "Crowdsourcing for Multimedia" encompasses the full range of techniques that combine human intelligence and a large number of individual contributors to advance the state of the art in multimedia research. The ACM Multimedia 2013 Workshop on Crowdsourcing for Multimedia (CrowdMM 2013) provided a forum for presenting new crowdsourcing techniques, exchanging innovative crowdsourcing ideas, and discussing crowdsourcing best practices for multimedia. The workshop program consisted of presented papers, a keynote speech and a panel discussion. A special feature of this year's workshop was the "Crowdsourcing for Multimedia Ideas Competition", the results of which were presented at the workshop.},
booktitle = {Proceedings of the 21st ACM International Conference on Multimedia},
pages = {1089–1090},
numpages = {2},
keywords = {crowdsourcing, human computation, multimedia},
location = {Barcelona, Spain},
series = {MM '13}
}

@inproceedings{10.1109/MobServ.2014.30,
author = {Jarrett, Julian and Saleh, Iman and Blake, M. Brian and Thorpe, Sean and Grandison, Tyrone and Malcolm, Rohan},
title = {Mobile Services for Enhancing Human Crowdsourcing with Computing Elements},
year = {2014},
isbn = {9781479950607},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/MobServ.2014.30},
doi = {10.1109/MobServ.2014.30},
abstract = {Crowdsourcing enables one to leverage the power of the crowd. Normally, it involves utilizing humans for tasks that machines have difficulty performing. We propose a system, delivered as a mobile service, which dynamically adapts to the application domain and selects a combination of human and machine crowdsourcing components. Our work is towards the design of elastic systems that adaptively optimizes the use of human and automated software resources in order to maximize overall performance. We propose a performance model that predicts both human and machine outcomes for a certain task and then optimizes task assignment accordingly. Our experimentation shows that our proposed system significantly enhances the outcome precision of a crowdsourced task.},
booktitle = {Proceedings of the 2014 IEEE International Conference on Mobile Services},
pages = {149–152},
numpages = {4},
keywords = {Crowdsourcing, Mobile, Services},
series = {MS '14}
}

@inproceedings{10.1109/ITNG.2015.78,
author = {Brito, Jailson and Vieira, Vaninha and Duran, Adolfo},
title = {Towards a Framework for Gamification Design on Crowdsourcing Systems: The G.A.M.E. Approach},
year = {2015},
isbn = {9781479988280},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/ITNG.2015.78},
doi = {10.1109/ITNG.2015.78},
abstract = {Crowd sourcing systems call a crowd of users to collaborate on solving real-life problems. One key issue for the success of such systems is to guarantee users' participation. A strategy that has been used to promote user participation is the use of game design techniques, since games have successful strategies to grant enjoyable user experience. However, most gamification methods and guidelines are too generic, do not emphasize the collaboration aspects and focus on introducing rewarding elements into the application, instead of designing player-centric applications. Reward-based design is dangerous, specially for collaborative systems, because it may put points gathering as the primary purpose of the application instead of the collaboration goal. This paper presents G.A.M.E., a conceptual framework to guide the design of gamification in crowd sourcing-based systems. The framework provides a flexible step-by-step guideline that combines knowledge from software engineering, collaborative systems, game design and interaction design. To evaluate our proposal, we instantiated G.A.M.E. Into two applications in the domain of public transportation. The influence of gamification in those applications were evaluated through controlled navigation tests in a crowd sourcing usability testing platform. Our findings showed us that gamification improved user interfaces of collaboration activities by 16\% on usability and were more trustworthy in 80\% the cases.},
booktitle = {Proceedings of the 2015 12th International Conference on Information Technology - New Generations},
pages = {445–450},
numpages = {6},
keywords = {Conceptual Framework, Crowdsourcing Systems, End-user software engineering, Gamification, Interaction Design},
series = {ITNG '15}
}

@inproceedings{10.5555/3044805.3044917,
author = {Zhou, Yuan and Chen, Xi and Li, Jian},
title = {Optimal PAC multiple arm identification with applications to crowdsourcing},
year = {2014},
publisher = {JMLR.org},
abstract = {We study the problem of selecting K arms with the highest expected rewards in a stochastic n-armed bandit game. Instead of using existing evaluation metrics (e.g., misidentification probability (Bubeck et al., 2013) or the metric in EXPLORE-K (Kalyanakrishnan \&amp; Stone, 2010)), we propose to use the aggregate regret, which is defined as the gap between the average reward of the optimal solution and that of our solution. Besides being a natural metric by itself, we argue that in many applications, such as our motivating example from crowdsourcing, the aggregate regret bound is more suitable. We propose a new PAC algorithm, which, with probability at least 1 - δ, identifies a set of K arms with regret at most ε. We provide the sample complexity bound of our algorithm. To complement, we establish the lower bound and show that the sample complexity of our algorithm matches the lower bound. Finally, we report experimental results on both synthetic and real data sets, which demonstrates the superior performance of the proposed algorithm.},
booktitle = {Proceedings of the 31st International Conference on International Conference on Machine Learning - Volume 32},
pages = {II–217–II–225},
location = {Beijing, China},
series = {ICML'14}
}

@inproceedings{10.1145/3250538,
author = {Dontcheva, Mira},
title = {Session details: Crowdsourcing \&amp; peer production I},
year = {2012},
isbn = {9781450310154},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3250538},
doi = {10.1145/3250538},
booktitle = {Proceedings of the SIGCHI Conference on Human Factors in Computing Systems},
location = {Austin, Texas, USA},
series = {CHI '12}
}

@inproceedings{10.1145/2908131.2908209,
author = {Gadiraju, Ujwal and Siehndel, Patrick and Dietze, Stefan},
title = {Estimating domain specificity for effective crowdsourcing of link prediction and schema mapping},
year = {2016},
isbn = {9781450342087},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2908131.2908209},
doi = {10.1145/2908131.2908209},
abstract = {Crowdsourcing has been widely adopted in research and practice over the last decade. In this work, we first investigate the extent to which crowd workers can substitute expert-based judgments in the task of link prediction and schema mapping, which is the creation of explicit links between resources on the Semantic Web at the instance and schema level. This is important since human input is required to evaluate and improve automated approaches for these tasks. We present a novel method to assess the inherent specificity of the link prediction task, and the impact of task specificity on quality of the results. We propose a Wikipedia-based mechanism to estimate specificity and show the influence of concept familiarity in producing high quality link prediction. Our findings indicate that the effectiveness of crowdsourcing the task of link prediction can improve by estimating the specificity.},
booktitle = {Proceedings of the 8th ACM Conference on Web Science},
pages = {323–324},
numpages = {2},
keywords = {crowd workers, crowdsourcing, experts, link prediction, nichesourcing, schema mapping},
location = {Hannover, Germany},
series = {WebSci '16}
}

@inproceedings{10.1109/WI-IAT.2012.86,
author = {Jiang, Huan and Matsubara, Shigeo},
title = {Improving Crowdsourcing Efficiency Based on Division Strategy},
year = {2012},
isbn = {9780769548807},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/WI-IAT.2012.86},
doi = {10.1109/WI-IAT.2012.86},
abstract = {This paper examines the efficiency in crowd sourcing, especially crowd sourcing for the software bug detection. Crowd sourcing has recently emerged as a lucrative paradigm for leveraging the collective intelligence of crowds. However, it has inherent weakness that a simple reward setting causes an uneven distribution of workers on each task, which reduces the efficiency of solving the tasks. A challenge is that the system designer is not allowed to set the reward to the arbitrary value because so-called "market wages" exist and if the reward is set to the value lower than the market wage, such a task fails to attract the sufficient number of workers. To solve this problem, we focus on the division strategy that divides the crowds into different groups and the workers compete with each other among the same group. We have developed a model that crowds write their codes independently and then try to find bugs in the codes written by the others. Next, we examine two division strategy, random grouping and ability grouping by analyzing the equilibrium behavior of each worker and carrying out simulations. The results show that the division strategy affect the efficiency of crowd sourcing bug detection and the random grouping leads to a higher efficiency compared to the ability grouping.},
booktitle = {Proceedings of the The 2012 IEEE/WIC/ACM International Joint Conferences on Web Intelligence and Intelligent Agent Technology - Volume 02},
pages = {425–429},
numpages = {5},
keywords = {Crowdsourcing efficiency, division strategy, software bug detection},
series = {WI-IAT '12}
}

@inproceedings{10.5555/3620237.3620297,
author = {Qiu, Wenjun and Lie, David and Austin, Lisa},
title = {Calpric: inclusive and fine-grained labeling of privacy policies with crowdsourcing and active learning},
year = {2023},
isbn = {978-1-939133-37-3},
publisher = {USENIX Association},
address = {USA},
abstract = {A significant challenge to training accurate deep learning models on privacy policies is the cost and difficulty of obtaining a large and comprehensive set of training data. To address these challenges, we present Calpric, which combines automatic text selection and segmentation, active learning and the use of crowdsourced annotators to generate a large, balanced training set for Android privacy policies at low cost. Automated text selection and segmentation simplifies the labeling task, enabling untrained annotators from crowdsourcing platforms, like Amazon's Mechanical Turk, to be competitive with trained annotators, such as law students, and also reduces inter-annotator agreement, which decreases labeling cost. Having reliable labels for training enables the use of active learning, which uses fewer training samples to efficiently cover the input space, further reducing cost and improving class and data category balance in the data set.The combination of these techniques allows Calpric to produce models that are accurate over a wider range of data categories, and provide more detailed, fine-grained labels than previous work. Our crowdsourcing process enables Calpric to attain reliable labeled data at a cost of roughly $0.92-$1.71 per labeled text segment. Calpric's training process also generates a labeled data set of 16K privacy policy text segments across 9 data categories with balanced assertions and denials.},
booktitle = {Proceedings of the 32nd USENIX Conference on Security Symposium},
articleno = {60},
numpages = {18},
location = {Anaheim, CA, USA},
series = {SEC '23}
}

@inproceedings{10.1145/2875913.2875945,
author = {He, Meimei and Tang, Hongyin and Wu, Guoquan and Wei, Jun and Zhong, Hua},
title = {A Crowdsourcing framework for Detecting Cross-Browser Issues in Web Application},
year = {2015},
isbn = {9781450336413},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2875913.2875945},
doi = {10.1145/2875913.2875945},
abstract = {With the advent of Web 2.0 application, and the increasing number of browsers and platforms on which the applications can be executed, cross-browser incompatibilities (XBIs) are becoming a serious problem for organizations to develop web-based software with good user experience. Although some techniques and tools have been proposed to identify XBIs, some XBIs are still missed as only partial state space is explored (by the crawler) in the testing environment. To address this limitation, based on record/replay technique, this paper proposed a crowdsourcing framework to detect cross-browser issues for Web application deployed in the field. Our empirical evaluation shows that the proposed technique is effective and efficient, improves on the state of the art.},
booktitle = {Proceedings of the 7th Asia-Pacific Symposium on Internetware},
pages = {239–242},
numpages = {4},
keywords = {Crowdsourcing, Record/Replay, Web Application},
location = {Wuhan, China},
series = {Internetware '15}
}

@inproceedings{10.1109/ICDCS.2014.10,
author = {Zhu, Yanmin and Zhang, Qian and Zhu, Hongzi and Yu, Jiadi and Cao, Jian and Ni, Lionel M.},
title = {Towards Truthful Mechanisms for Mobile Crowdsourcing with Dynamic Smartphones},
year = {2014},
isbn = {9781479951697},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/ICDCS.2014.10},
doi = {10.1109/ICDCS.2014.10},
abstract = {Stimulating participation from smartphone users is of paramount importance to mobile crowd sourcing systems and applications. A few incentive mechanisms have been proposed, but most of them have made the impractical assumption that smartphones remain static in the system and sensing tasks are known in advance. The existing mechanisms fail when being applied to the realistic scenario where smartphones dynamically arrive to the system and sensing tasks are submitted at random. It is particularly challenging to design an incentive mechanism for such a mobile crowd sourcing system, given dynamic smartphones, uncertain arrivals of tasks, strategic behaviors, and private information of smartphones. We propose two truthful auction mechanisms for two different cases of mobile crowd sourcing with dynamic smartphones. For the offline case, we design an optimal truthful mechanism with an optimal task allocation algorithm of polynomial-time computation complexity of O (n+γ)3, where n is the number of smartphones and γ is the number of sensing tasks. For the online case, we design a near-optimal truthful mechanism with an online task allocation algorithm that achieves a constant competitive ratio of 1 2. Rigorous theoretical analysis and extensive simulations have been performed, and the results demonstrate the proposed auction mechanisms achieve truthfulness, individual rationality, computational efficiency, and low overpayment.},
booktitle = {Proceedings of the 2014 IEEE 34th International Conference on Distributed Computing Systems},
pages = {11–20},
numpages = {10},
keywords = {Crowdsourcing, Truthful mechanisms, Online mechanisms},
series = {ICDCS '14}
}

@inproceedings{10.1145/2846661.2846665,
author = {Liang, Guangtai and Li, Shaochun},
title = {An energy-saving framework for mobile devices based on crowdsourcing intelligences},
year = {2015},
isbn = {9781450339063},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2846661.2846665},
doi = {10.1145/2846661.2846665},
abstract = {Rich mobile apps make mobile devices increasingly pervasive in our daily life. However, energy consumptions of mobile devices brings lots of users’ frustrations. To guarantee good experiences of mobile users, we propose an energy-saving framework for mobile devices, which uses a set of coarse-grained and general-purpose energy-waste heuristics as a starting point and then smartly takes advantages of crowdsourcing intelligence to refine energy-waste related knowledge to help detect/resolve energy wastes in mobile devices. In return, summarized energy-waste related knowledge can be presented to the developers of related mobile apps and guide them to identify/fix related energy bugs. Through initial evaluations, we demonstrate the proposed framework is able to extend the lifetime of a mobile device with one full charge to a large degree (e.g., 30\%-70\%).},
booktitle = {Proceedings of the 3rd International Workshop on Mobile Development Lifecycle},
pages = {5–6},
numpages = {2},
keywords = {crowdsourcing intelligence, energy-saving framework, mobile devices},
location = {Pittsburgh, PA, USA},
series = {MobileDeLi 2015}
}

@inproceedings{10.1109/ICCV.2013.373,
author = {Long, Chengjiang and Hua, Gang and Kapoor, Ashish},
title = {Active Visual Recognition with Expertise Estimation in Crowdsourcing},
year = {2013},
isbn = {9781479928408},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/ICCV.2013.373},
doi = {10.1109/ICCV.2013.373},
abstract = {We present a noise resilient probabilistic model for active learning of a Gaussian process classifier from crowds, i.e., a set of noisy labelers. It explicitly models both the overall label noises and the expertise level of each individual labeler in two levels of flip models. Expectation propagation is adopted for efficient approximate Bayesian inference of our probabilistic model for classification, based on which, a generalized EM algorithm is derived to estimate both the global label noise and the expertise of each individual labeler. The probabilistic nature of our model immediately allows the adoption of the prediction entropy and estimated expertise for active selection of data sample to be labeled, and active selection of high quality labelers to label the data, respectively. We apply the proposed model for three visual recognition tasks, i.e., object category recognition, gender recognition, and multi-modal activity recognition, on three datasets with real crowd-sourced labels from Amazon Mechanical Turk. The experiments clearly demonstrated the efficacy of the proposed model.},
booktitle = {Proceedings of the 2013 IEEE International Conference on Computer Vision},
pages = {3000–3007},
numpages = {8},
keywords = {Active Learning, Crowdsourcing, Expectation Propagation, Gaussian Processes, Visual Recognition},
series = {ICCV '13}
}

@inproceedings{10.5555/2986459.2986677,
author = {Karger, David R. and Oh, Sewoong and Shah, Devavrat},
title = {Iterative learning for reliable crowdsourcing systems},
year = {2011},
isbn = {9781618395993},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Crowdsourcing systems, in which tasks are electronically distributed to numerous "information piece-workers", have emerged as an effective paradigm for human-powered solving of large scale problems in domains such as image classification, data entry, optical character recognition, recommendation, and proofreading. Because these low-paid workers can be unreliable, nearly all crowdsourcers must devise schemes to increase confidence in their answers, typically by assigning each task multiple times and combining the answers in some way such as majority voting. In this paper, we consider a general model of such crowdsourcing tasks, and pose the problem of minimizing the total price (i.e., number of task assignments) that must be paid to achieve a target overall reliability. We give a new algorithm for deciding which tasks to assign to which workers and for inferring correct answers from the workers' answers. We show that our algorithm significantly outperforms majority voting and, in fact, is asymptotically optimal through comparison to an oracle that knows the reliability of every worker.},
booktitle = {Proceedings of the 24th International Conference on Neural Information Processing Systems},
pages = {1953–1961},
numpages = {9},
location = {Granada, Spain},
series = {NIPS'11}
}

@inproceedings{10.1007/978-3-319-07626-3_64,
author = {Choi, Joohee and Choi, Heejin and So, Woonsub and Lee, Jaeki and You, Jongjun},
title = {A Study about Designing Reward for Gamified Crowdsourcing System},
year = {2014},
isbn = {9783319076256},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-319-07626-3_64},
doi = {10.1007/978-3-319-07626-3_64},
abstract = {The goal of this study is to understand the mechanism of gamification in crowdsourcing by investigating the ways of giving rewards. Perceived reward diversity is proposed as a construct to induce fun experience from participants based on previous studies about gamified crowdsourcing. With respect to system manipulation, explicating the anticipated level of rewards before task phase is conducted. The effect of explication on task outcome and psychological outcome is compared with control group. As a result, both perceived reward diversity and explicating the anticipated level of rewards significantly affect both quality and quantity of submitted answers, as well as feeling of fun during the task phase. The limitation and implication of the study is stated in the end.},
booktitle = {Proceedings of the Third International Conference on Design, User Experience, and Usability. User Experience Design for Diverse Interaction Platforms and Environments - Volume 8518},
pages = {678–687},
numpages = {10},
keywords = {crowdsourcing, fun experience, gamification, perceived diversity, reward}
}

@inproceedings{10.1145/2424321.2424383,
author = {Efentakis, Alexandros and Theodorakis, Dimitris and Pfoser, Dieter},
title = {Crowdsourcing computing resources for shortest-path computation},
year = {2012},
isbn = {9781450316910},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2424321.2424383},
doi = {10.1145/2424321.2424383},
abstract = {Crowdsourcing road network data, i.e., involving users to collect data including the detection and assessment of changes to the road network graph, poses a challenge to shortest-path algorithms that rely on preprocessing. Hence, current research challenges lie with improving performance by adequately balancing preprocessing with respect to fast-changing road networks. In this work, we take the crowdsourcing approach further in that we solicit the help of users not only for data collection, but also to provide us their computing resources. A promising approach is parallelization, which splits the graph into chunks of data that may be processed separately. This work extends this approach in that small-enough chunks allow us to use browser-based computing to solve the pre-computation problem. Essentially, we aim for a Web-based navigation service that whenever users request a route, the service uses their browsers for partially preprocessing a large, but changing road network. The paper gives performance studies that highlight the potential of the browser as a computing platform and showcases a scalable approach, which almost eliminates the computing load on the server.},
booktitle = {Proceedings of the 20th International Conference on Advances in Geographic Information Systems},
pages = {434–437},
numpages = {4},
keywords = {crowdsourcing, graph separators, preprocessing, shortest path},
location = {Redondo Beach, California},
series = {SIGSPATIAL '12}
}

@inproceedings{10.1109/ISM.2011.87,
author = {Hoβfeld, Tobias and Seufert, Michael and Hirth, Matthias and Zinner, Thomas and Tran-Gia, Phuoc and Schatz, Raimund},
title = {Quantification of YouTube QoE via Crowdsourcing},
year = {2011},
isbn = {9780769545899},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/ISM.2011.87},
doi = {10.1109/ISM.2011.87},
abstract = {This paper addresses the challenge of assessing and modeling Quality of Experience (QoE) for online video services that are based on TCP-streaming. We present a dedicated QoE model for You Tube that takes into account the key influence factors (such as stalling events caused by network bottlenecks) that shape quality perception of this service. As second contribution, we propose a generic subjective QoE assessment methodology for multimedia applications (like online video) that is based on crowd sourcing - a highly cost-efficient, fast and flexible way of conducting user experiments. We demonstrate how our approach successfully leverages the inherent strengths of crowd sourcing while addressing critical aspects such as the reliability of the experimental data obtained. Our results suggest that, crowd sourcing is a highly effective QoE assessment method not only for online video, but also for a wide range of other current and future Internet applications.},
booktitle = {Proceedings of the 2011 IEEE International Symposium on Multimedia},
pages = {494–499},
numpages = {6},
keywords = {Crowdsourcing, HTTP video streaming, YouTube, reliability},
series = {ISM '11}
}

@inproceedings{10.1145/1982624.1982625,
author = {Ipeirotis, Panos},
title = {Crowdsourcing using Mechanical Turk: quality management and scalability},
year = {2011},
isbn = {9781450306201},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1982624.1982625},
doi = {10.1145/1982624.1982625},
abstract = {I will discuss the repeated acquisition of "labels" for data items when the labeling is imperfect. Labels are values provided by humans for specified variables on data items, such as "PG-13" for "Adult Content Rating on this Web Page." With the increasing popularity of micro-outsourcing systems, such as Amazon's Mechanical Turk, it often is possible to obtain less-than-expert labeling at low cost. We examine the improvement (or lack thereof) in data quality via repeated labeling, and focus especially on the improvement of training labels for supervised induction.},
booktitle = {Proceedings of the 8th International Workshop on Information Integration on the Web: In Conjunction with WWW 2011},
articleno = {1},
numpages = {1},
keywords = {labeling, spam detection},
location = {Hyderabad, India},
series = {IIWeb '11}
}

@inproceedings{10.1007/978-3-319-27974-9_6,
author = {Goh, Dion Hoe-Lian and Pe-Than, Ei Pa and Lee, Chei Sian},
title = {Investigating the Antecedents of Playing Games for Crowdsourcing Location-Based Content},
year = {2015},
isbn = {9783319279732},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-319-27974-9_6},
doi = {10.1007/978-3-319-27974-9_6},
abstract = {Human Computation Games HCGs are games that harness human intelligence to tackle computational problems. Put differently, they are a means of crowdsourcing via games. Due to this entertainment-output generation duality, perceived enjoyment and perceived quality of outputs are potentially important determinants of HCG usage. This study adopts a multidimensional view of perceived enjoyment and output quality to investigate their influence on intention to use HCGs. This is done using SPLASH, our developed mobile HCG for crowdsourcing location-based content. Since SPLASH comprises various gaming features, we further study how the different dimensions of enjoyment vary across them. Using a survey of 105 undergraduate and graduate students, findings validated the multidimensionality of perceived enjoyment and output quality and showed their differing influence. As well, the different gaming features elicited different perceptions of enjoyment. Our results thus suggest that HCGs can be used for crowdsourcing tasks if they can fulfill enjoyment and assure output quality.},
booktitle = {Proceedings of the 17th International Conference on Asia-Pacific Digital Libraries - Volume 9469},
pages = {52–63},
numpages = {12},
keywords = {Crowdsourcing, Enjoyment, Human computation games, Location-based content, Mobile devices, Output quality}
}

@inproceedings{10.1109/MASCOTS.2014.28,
author = {Amor, Iheb Ben and Ouziri, Mourad and Sahri, Soror and Karam, Naouel},
title = {Be a Collaborator and a Competitor in Crowdsourcing System},
year = {2014},
isbn = {9781479956104},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/MASCOTS.2014.28},
doi = {10.1109/MASCOTS.2014.28},
abstract = {Crowd sourcing is emerging as a powerful paradigm to solve a wide range of tedious and complex problems in various enterprise applications. It spawns the issue of finding the unknown collaborative and competitive group of solvers. The formation of collaborative team should provide the best solution and treat that solution as a trade secret avoiding data leak between competitive teams due to reward behind the outsourcing of the issue. The formation of effective competitive teams not only requires adequate skills between members of each team, but also good team connectivity through social network and to provide the best solution and treat that solution as a trade secret avoiding data leak between teams due to reward behind the outsourcing of the issue. In this paper, we propose a data leak aware crowd sourcing system called Social Crowd. We introduce a clustering algorithm that uses social relationships between crowd workers to discover all possible teams while avoiding inter-team data leakage.},
booktitle = {Proceedings of the 2014 IEEE 22nd International Symposium on Modelling, Analysis \&amp; Simulation of Computer and Telecommunication Systems},
pages = {158–167},
numpages = {10},
series = {MASCOTS '14}
}

@inproceedings{10.1145/1837885.1837895,
author = {Stewart, Osamuyimen and Lubensky, David and Huerta, Juan M.},
title = {Crowdsourcing participation inequality: a SCOUT model for the enterprise domain},
year = {2010},
isbn = {9781450302227},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1837885.1837895},
doi = {10.1145/1837885.1837895},
abstract = {In large scale online multi-user communities, the phenomenon of 'participation inequality,' has been described as generally following a more or less 90-9-1 rule [9]. In this paper, we examine crowdsourcing participation levels inside the enterprise (within a company's firewall) and show that it is possible to achieve a more equitable distribution of 33-66-1. Accordingly, we propose a SCOUT ((S)uper Contributor, (C)ontributor, and (OUT)lier)) model for describing user participation based on quantifiable effort-level metrics. In support of this framework, we present an analysis that measures the quantity of contributions correlated with responses to motivation and incentives. In conclusion, SCOUT provides the task-based categories to characterize participation inequality that is evident in online communities, and crucially, also demonstrates the inequality curve (and associated characteristics) in the enterprise domain.},
booktitle = {Proceedings of the ACM SIGKDD Workshop on Human Computation},
pages = {30–33},
numpages = {4},
keywords = {crowdsourcing, incentives, motivation, online community},
location = {Washington DC},
series = {HCOMP '10}
}

@inproceedings{10.1145/2384916.2384941,
author = {Burton, Michele A. and Brady, Erin and Brewer, Robin and Neylan, Callie and Bigham, Jeffrey P. and Hurst, Amy},
title = {Crowdsourcing subjective fashion advice using VizWiz: challenges and opportunities},
year = {2012},
isbn = {9781450313216},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2384916.2384941},
doi = {10.1145/2384916.2384941},
abstract = {Fashion is a language. How we dress signals to others who we are and how we want to be perceived. However, this language is primarily visual, making it inaccessible to people with vision impairments. Someone who is low-vision or completely blind cannot see what others are wearing or readily know what constitutes the norms and extremes of fashion, but most everyone they encounter can see (and judge) their fashion choices. We describe our findings of a diary study with people with vision impairments that revealed the many accessibility barriers fashion presents, and how an online survey revealed that clothing decisions are often made collaboratively, regardless of visual ability. Based on these findings, we identified a need for a collaborative and real-time environment for fashion advice. We have tested the feasibility of providing this advice through crowdsourcing using VizWiz, a mobile phone application where participants receive nearly real-time answers to visual questions. Our pilot study results show that this application has the potential to address a great need within the blind community, but remaining challenges include improving photo capture and assembling a set of crowd workers with the requisite expertise. More broadly our research highlights the feasibility of using crowdsourcing for subjective, opinion-based advice.},
booktitle = {Proceedings of the 14th International ACM SIGACCESS Conference on Computers and Accessibility},
pages = {135–142},
numpages = {8},
keywords = {blind users, crowdsourcing, fashion},
location = {Boulder, Colorado, USA},
series = {ASSETS '12}
}

@inproceedings{10.5555/2887352.2887358,
author = {Simperl, Elena and Norton, Barry and Vrande\v{c}i\'{c}, Denny},
title = {Crowdsourcing tasks in linked data management},
year = {2011},
publisher = {CEUR-WS.org},
address = {Aachen, DEU},
abstract = {Many aspects of Linked Data management - including exposing legacy data and applications to semantic formats, designing vocabularies to describe RDF data, identifying links between entities, query processing, and data curation - are necessarily tackled through the combination of human effort with algorithmic techniques. In the literature on traditional data management the theoretical and technical groundwork to realize and manage such combinations is being established. In this paper we build upon and extend these ideas to propose a framework by which human and computational intelligence can co-exist by augmenting existing Linked Data and Linked Service technology with crowdsourcing functionality. Starting from a motivational scenario we introduce a set of generic tasks which may feasibly be approached using crowdsourcing platforms such as Amazon's Mechanical Turk, explain how these tasks can be decomposed and translated into MTurk projects, and roadmap the extensions to SPARQL, D2RQ/R2R and Linked Data browsing that are required to achieve this vision.},
booktitle = {Proceedings of the Second International Conference on Consuming Linked Data - Volume 782},
pages = {61–72},
numpages = {12},
location = {Bonn, Germany},
series = {COLD'11}
}

@inproceedings{10.5555/2615731.2616098,
author = {Bachrach, Yoram and Ceppi, Sofia and Kash, Ian A. and Key, Peter and Radlinski, Filip and Porat, Ely and Armstrong, Michael and Sharma, Vijay},
title = {Building a personalized tourist attraction recommender system using crowdsourcing},
year = {2014},
isbn = {9781450327381},
publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
address = {Richland, SC},
abstract = {We demonstrate how crowdsourcing can be used to automatically build a personalized tourist attraction recommender system, which tailors recommendations to specific individuals, so different people who use the system each get their own list of recommendations, appropriate to their own traits. Recommender systems crucially depend on the availability of reliable and large scale data that allows predicting how a new individual is likely to rate items from the catalog of possible items to recommend. We show how to automate the process of generating this data using crowdsourcing, so that such a system can be built even when such a dataset is not initially available. We first find possible tourist attractions to recommend by scraping such information from Wikipedia. Next, we use crowdsourced workers to filter the data, then provide their opinions regarding these items. Finally, we use machine learning methods to predict how new individuals are likely to rate each attraction, and recommend the items with the highest predicted ratings.},
booktitle = {Proceedings of the 2014 International Conference on Autonomous Agents and Multi-Agent Systems},
pages = {1631–1632},
numpages = {2},
keywords = {algorithms, economics, human factors},
location = {Paris, France},
series = {AAMAS '14}
}

@inproceedings{10.1109/ICTAI.2015.128,
author = {Fan, Yue and Sun, Hailong and Liu, Xudong},
title = {Truthful Incentive Mechanisms for Dynamic and Heterogeneous Tasks in Mobile Crowdsourcing},
year = {2015},
isbn = {9781509001637},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/ICTAI.2015.128},
doi = {10.1109/ICTAI.2015.128},
abstract = {Crowdsourcing has received tremendous attention for collecting various data with the distributed smartphones of people. For the mobile crowdsourcing applications to obtain high-quality data, stimulating user participation is of paramount importance. Although many incentive mechanisms have been designed, most of them ignore the dynamic arrivals and different sensing requirements of tasks. Thus, the existing mechanisms will fail when being applied to the realistic scenario where tasks are publicized dynamically and heterogeneous with different sensing requirements of locations, time durations and sensing times. In this work, we propose two auction-based truthful mechanisms, TRIMS and TRIMG, for realistic mobile crowdsourcing under special user model and more general model, respectively. Through extensive simulations and theoretical analysis, we demonstrate that our mechanisms can satisfy the desired properties of truthfulness, individual rationality, computational efficiency with both low social cost and low total payment.},
booktitle = {Proceedings of the 2015 IEEE 27th International Conference on Tools with Artificial Intelligence (ICTAI)},
pages = {881–888},
numpages = {8},
series = {ICTAI '15}
}

@inproceedings{10.1145/2556288.2557217,
author = {Dontcheva, Mira and Morris, Robert R. and Brandt, Joel R. and Gerber, Elizabeth M.},
title = {Combining crowdsourcing and learning to improve engagement and performance},
year = {2014},
isbn = {9781450324731},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2556288.2557217},
doi = {10.1145/2556288.2557217},
abstract = {Crowdsourcing complex creative tasks remains difficult, in part because these tasks require skilled workers. Most crowdsourcing platforms do not help workers acquire the skills necessary to accomplish complex creative tasks. In this paper, we describe a platform that combines learning and crowdsourcing to benefit both the workers and the requesters. Workers gain new skills through interactive step-by-step tutorials and test their knowledge by improving real-world images submitted by requesters. In a series of three deployments spanning two years, we varied the design of our platform to enhance the learning experience and improve the quality of the crowd work. We tested our approach in the context of LevelUp for Photoshop, which teaches people how to do basic photograph improvement tasks using Adobe Photoshop. We found that by using our system workers gained new skills and produced high-quality edits for requested images, even if they had little prior experience editing images.},
booktitle = {Proceedings of the SIGCHI Conference on Human Factors in Computing Systems},
pages = {3379–3388},
numpages = {10},
keywords = {crowdsourcing, games, training},
location = {Toronto, Ontario, Canada},
series = {CHI '14}
}

@inproceedings{10.1109/INFOCOM.2016.7524547,
author = {Zhuo, Gaoqiang and Jia, Qi and Guo, Linke and Li, Ming and Li, Pan},
title = {Privacy-preserving verifiable data aggregation and analysis for cloud-assisted mobile crowdsourcing},
year = {2016},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/INFOCOM.2016.7524547},
doi = {10.1109/INFOCOM.2016.7524547},
abstract = {Crowdsourcing is a crowd-based outsourcing, where a requester (task owner) can outsource tasks to workers (public crowd). Recently, mobile crowdsourcing, which can leverage workers' data from smartphones for data aggregation and analysis, has attracted much attention. However, when the data volume is getting large, it becomes a difficult problem for a requester to aggregate and analyze the incoming data, especially when the requester is an ordinary smartphone user or a start-up company with limited storage and computation resources. Besides, workers are concerned about their identity and data privacy. To tackle these issues, we introduce a three-party architecture for mobile crowdsourcing, where the cloud is implemented between workers and requesters to ease the storage and computation burden of the resource-limited requester. Identity privacy and data privacy are also achieved. With our scheme, a requester is able to verify the correctness of computation results from the cloud. We also provide several aggregated statistics in our work, together with efficient data update methods. Extensive simulation shows both the feasibility and efficiency of our proposed solution.},
booktitle = {IEEE INFOCOM 2016 - The 35th Annual IEEE International Conference on Computer Communications},
pages = {1–9},
numpages = {9},
location = {San Francisco, CA, USA}
}

@inproceedings{10.1109/SocialCom.2013.71,
author = {Wang, Yang and Huang, Yun and Louis, Claudia},
title = {Towards a Framework for Privacy-Aware Mobile Crowdsourcing},
year = {2013},
isbn = {9780769551371},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/SocialCom.2013.71},
doi = {10.1109/SocialCom.2013.71},
abstract = {The practice of employing "the crowd" to help solve an organization's problems first became popular in the business sector, and has since spread to public and not-for-profit organizations. Input from the crowd can be solicited using different mechanisms involving various types of web-based applications, or the more recent trend of employing mobile phones with sensing capabilities. However, these crowd sourcing systems may lead to various privacy and security risks which can then hinder the adoption of these services. How to identify and address these potential risks in such systems has both research and practical value. This paper presents two aspects of our work in this emerging space. First, we describe a survey of potential privacy and security risks in mobile crowd sourcing systems (MCSS). Second, we describe our PEALS framework to support privacy-aware mobile crowd sourcing.},
booktitle = {Proceedings of the 2013 International Conference on Social Computing},
pages = {454–459},
numpages = {6},
keywords = {crowdsourcing, privacy, security},
series = {SOCIALCOM '13}
}

@inproceedings{10.1145/2908131.2908187,
author = {El Maarry, Kinda and Balke, Wolf-Tilo},
title = {An impact-driven model for quality control in skewed-domain crowdsourcing tasks},
year = {2016},
isbn = {9781450342087},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2908131.2908187},
doi = {10.1145/2908131.2908187},
abstract = {Not only do the highly-distributed digital crowdsourcing solutions surpass both borders and time-zones, but they materialize the vision of impact sourcing, by tapping into new labor markets in developing countries. Unfortunately, crowdsourcing is associated with severe quality issues. To that end, many countermeasures have been designed to detect spammers, except in practice, also honest, yet not perfect workers will often be exposed and deprived of much-needed earnings. Here, we argue for the need of an impact-driven quality control measure, especially for skewed-domain tasks. Such a measure should ensure high quality results, while simultaneously fulfilling the social responsibility aspect of crowdsourcing.},
booktitle = {Proceedings of the 8th ACM Conference on Web Science},
pages = {320–322},
numpages = {3},
keywords = {crowdsourcing, fraud detection, impact sourcing, quality control},
location = {Hannover, Germany},
series = {WebSci '16}
}

@inproceedings{10.1109/SMC.2013.122,
author = {Barbosa, Carlos Eduardo and Epelbaum, Vanessa Janni and Antelio, Marcio and Oliveira, Jonice and Souza, Jano Moreira de},
title = {Crowdsourcing Environments in E-Learning Scenario: A Classification Based on Educational and Collaboration Criteria},
year = {2013},
isbn = {9781479906529},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/SMC.2013.122},
doi = {10.1109/SMC.2013.122},
abstract = {Recent crowd sourcing tools are changing the Education. However, each tool has a specific approach, business model and e-Learning goals. Due the increasing contribution of crowd sourcing in the e-Learning process, it has become important to study it and map the challenges intrinsic to this activity. This work discusses and classifies several crowd sourcing e-Learning tools that can be found in the Internet. We defined and used eleven dimensions to classify the 22 tools found, ranging from online Universities to marketplaces for online courses. The work contributions are: provide the first map the crowd sourcing tools and a framework for classify the future tools.},
booktitle = {Proceedings of the 2013 IEEE International Conference on Systems, Man, and Cybernetics},
pages = {687–692},
numpages = {6},
keywords = {Classification, Crowdsourcing, e-Learning},
series = {SMC '13}
}

@inproceedings{10.5555/2900728.2900741,
author = {Lin, Christopher H. and Mausam, Mausam and Weld, Daniel S.},
title = {Dynamically switching between synergistic workflows for crowdsourcing},
year = {2012},
publisher = {AAAI Press},
abstract = {To ensure quality results from unreliable crowdsourced workers, task designers often construct complex workflows and aggregate worker responses from redundant runs. Frequently, they experiment with several alternative workflows to accomplish the task, and eventually deploy the one that achieves the best performance during early trials.Surprisingly, this seemingly natural design paradigm does not achieve the full potential of crowdsourcing. In particular, using a single workflow (even the best) to accomplish a task is suboptimal. We show that alternative workflows can compose synergistically to yield much higher quality output. We formalize the insight with a novel probabilistic graphical model. Based on this model, we design and implement AGENTHUNT, a POMDP-based controller that dynamically switches between these workflows to achieve higher returns on investment. Additionally, we design offline and online methods for learning model parameters. Live experiments on Amazon Mechanical Turk demonstrate the superiority of AGENTHUNT for the task of generating NLP training data, yielding up to 50\% error reduction and greater net utility compared to previous methods.},
booktitle = {Proceedings of the Twenty-Sixth AAAI Conference on Artificial Intelligence},
pages = {87–93},
numpages = {7},
location = {Toronto, Ontario, Canada},
series = {AAAI'12}
}

@inproceedings{10.1109/MDM.2014.70,
author = {Nandan, Naveen and Pursche, Andreas and Zhe, Xing},
title = {Challenges in Crowdsourcing Real-Time Information for Public Transportation},
year = {2014},
isbn = {9781479957057},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/MDM.2014.70},
doi = {10.1109/MDM.2014.70},
abstract = {Public transportation is a key enabler of mobility in today's urban environment. Transportation service operators are exploring various technologies in order to be able to provide passengers accurate real-time information to plan their journeys. For them, the challenge is often in understanding where, when and how there is a demand. With rapid advancements in mobile technology, crowd sourcing or participatory sensing can be thought of as a medium by which information can be collected, augmented and used for better planning as well as a mode to deliver real-time information to commuters. In this paper, we conduct an extensive review of both literature and applications using mobile crowd sourcing with a focus on the public transportation domain. We identify certain common challenges across various techniques proposed and applied, categorize them and discuss possible future research directions into these areas.},
booktitle = {Proceedings of the 2014 IEEE 15th International Conference on Mobile Data Management - Volume 02},
pages = {67–72},
numpages = {6},
keywords = {Data collection, Mobile crowdsourcing, Participatory sensing, Real-time passenger information systems},
series = {MDM '14}
}

@inproceedings{10.1145/1979742.1979593,
author = {Bernstein, Michael and Chi, Ed H. and Chilton, Lydia and Hartmann, Bj\"{o}rn and Kittur, Aniket and Miller, Robert C.},
title = {Crowdsourcing and human computation: systems, studies and platforms},
year = {2011},
isbn = {9781450302685},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1979742.1979593},
doi = {10.1145/1979742.1979593},
abstract = {Crowdsourcing and human computation are transforming human-computer interaction, and CHI has led the way. The seminal publication in human computation was initially published in CHI in 2004 [1], and the first paper investigating Mechanical Turk as a user study platform has amassed over one hundred citations in two years [5]. However, we are just beginning to stake out a coherent research agenda for the field. This workshop will bring together researchers in the young field of crowdsourcing and human computation and produce three artifacts: a research agenda for the field, a vision for ideal crowdsourcing platforms, and a group-edited bibliography. These resources will be publically disseminated on the web and evolved and maintained by the community.},
booktitle = {CHI '11 Extended Abstracts on Human Factors in Computing Systems},
pages = {53–56},
numpages = {4},
keywords = {bibliography, crowdsourcing, human computation, platform, workshop},
location = {Vancouver, BC, Canada},
series = {CHI EA '11}
}

@inproceedings{10.1109/SOCA.2014.26,
author = {Allahbakhsh, Mohammad and Samimi, Samira and Motahari-Nezhad, Hamid-Reza and Benatallah, Boualem},
title = {Harnessing Implicit Teamwork Knowledge to Improve Quality in Crowdsourcing Processes},
year = {2014},
isbn = {9781479968336},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/SOCA.2014.26},
doi = {10.1109/SOCA.2014.26},
abstract = {Workers in online crowd sourcing systems have different levels of expertise, trustworthiness, incentives and motivations. Therefore, recruiting sufficient number of well-suited workers is always a challenge. Existing methods usually recruit workers through open calls, friendships relations, matching their profiles with task requirements or recruiting teams of workers. But there are still challenges that need more investigations, mainly all existing recruitment methods are highly vulnerable to collaborating misbehaviour, i.e., Collusion. \%These groups are highly vulnerable to collusion attacks. In this paper, we propose a recruitment method which takes into account individual and social attributes of workers to find suitable workers. The method discovers indirect collaborations between workers to harness implicit teamwork knowledge in order to increase the quality of crowd sourcing tasks' outcome and in the same time prevent collusion attacks. The proposed method is implemented and tested using the simulated data, build based on a public data dump from Stack overflow. The evaluation results show the accuracy of the obtained results and superiority of our proposed method over the other related work.},
booktitle = {Proceedings of the 2014 IEEE 7th International Conference on Service-Oriented Computing and Applications},
pages = {17–24},
numpages = {8},
keywords = {Crowdsourcing, Implicit Teamwork, Quality, Recruitment, Suitability},
series = {SOCA '14}
}

@inproceedings{10.1145/2382636.2382642,
author = {Chorianopoulos, Konstantinos},
title = {Crowdsourcing user interactions with the video player},
year = {2012},
isbn = {9781450317061},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2382636.2382642},
doi = {10.1145/2382636.2382642},
abstract = {Every second millions of users enjoy content streaming on diverse video players (e.g., Web, Apps, social networks) and create billions of interactions within online video, such as play, pause, seek/scrub. This collective intelligence of video viewers might be leveraged into useful information for improved video navigation. For example, we can accurately detect and retrieve interesting scenes through the analysis of the aggregated users' replay interactions with the video player. Effective crowdsourcing of video interactions is grounded on previous work in multimedia, user modeling, and controlled user experiments. These research issues are described for the case of user-based detection of video thumbnails that stand for the semantics of the video. Moreover, we demonstrate the respective experimental environment with a focus on educational and user generated (e.g., how-to, lecture) videos.},
booktitle = {Proceedings of the 18th Brazilian Symposium on Multimedia and the Web},
pages = {13–16},
numpages = {4},
keywords = {key-frame, replay, signal processing., user-based, video},
location = {S\~{a}o Paulo/SP, Brazil},
series = {WebMedia '12}
}

@inproceedings{10.1145/2390803.2390816,
author = {Park, Sunghyun and Mohammadi, Gelareh and Artstein, Ron and Morency, Louis-Philippe},
title = {Crowdsourcing micro-level multimedia annotations: the challenges of evaluation and interface},
year = {2012},
isbn = {9781450315890},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2390803.2390816},
doi = {10.1145/2390803.2390816},
abstract = {This paper presents a new evaluation procedure and tool for crowdsourcing micro-level multimedia annotations and shows that such annotations can achieve a quality comparable to that of expert annotations. We propose a new evaluation procedure, called MM-Eval (Micro-level Multimedia Evaluation), which compares fine time-aligned annotations using Krippendorff's alpha metric and introduce two new metrics to evaluate the types of disagreement between coders. We also introduce OCTAB (Online Crowdsourcing Tool for Annotations of Behaviors), a web-based annotation tool that allows precise and convenient multimedia behavior annotations, directly from Amazon Mechanical Turk interface. With an experiment using the above tool and evaluation procedure, we show that a majority vote among annotations from 3 crowdsource workers leads to a quality comparable to that of local expert annotations.},
booktitle = {Proceedings of the ACM Multimedia 2012 Workshop on Crowdsourcing for Multimedia},
pages = {29–34},
numpages = {6},
keywords = {amazon mechanical turk, behavior annotation, crowdsourcing, inter-coder agreement, inter-rater reliability, octab, video annotation},
location = {Nara, Japan},
series = {CrowdMM '12}
}

@inproceedings{10.1145/2661829.2661946,
author = {Rokicki, Markus and Chelaru, Sergiu and Zerr, Sergej and Siersdorfer, Stefan},
title = {Competitive Game Designs for Improving the Cost Effectiveness of Crowdsourcing},
year = {2014},
isbn = {9781450325981},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2661829.2661946},
doi = {10.1145/2661829.2661946},
abstract = {Crowd based online work is leveraged in a variety of applications such as semantic annotation of images, translation of texts in foreign languages, and labeling of training data for machine learning models. However, annotating large amounts of data through crowdsourcing can be slow and costly. In order to improve both cost and time efficiency of crowdsourcing we examine alternative reward mechanisms compared to the "Pay-per-HIT" scheme commonly used in platforms such as Amazon Mechanical Turk. To this end, we explore a wide range of monetary reward schemes that are inspired by the success of competitions, lotteries, and games of luck. Our large-scale experimental evaluation with an overall budget of more than 1,000 USD and with 2,700 hours of work spent by crowd workers demonstrates that our alternative reward mechanisms are well accepted by online workers and lead to substantial performance boosts.},
booktitle = {Proceedings of the 23rd ACM International Conference on Conference on Information and Knowledge Management},
pages = {1469–1478},
numpages = {10},
keywords = {competitions, crowdsourcing, lotteries, reward schemes},
location = {Shanghai, China},
series = {CIKM '14}
}

@inproceedings{10.5555/2900728.2900734,
author = {Gao, Xi Alice and Bachrach, Yoram and Key, Peter and Graepel, Thore},
title = {Quality expectation-variance tradeoffs in crowdsourcing contests},
year = {2012},
publisher = {AAAI Press},
abstract = {We examine designs for crowdsourcing contests, where participants compete for rewards given to superior solutions of a task. We theoretically analyze tradeoffs between the expectation and variance of the principal's utility (i.e. the best solution's quality), and empirically test our theoretical predictions using a controlled experiment on Amazon Mechanical Turk. Our evaluation method is also crowdsourcing based and relies on the peer prediction mechanism. Our theoretical analysis shows an expectation-variance tradeoff of the principal's utility in such contests through a Pareto efficient frontier. In particular, we show that the simple contest with 2 authors and the 2-pair contest have good theoretical properties. In contrast, our empirical results show that the 2-pair contest is the superior design among all designs tested, achieving the highest expectation and lowest variance of the principal's utility.},
booktitle = {Proceedings of the Twenty-Sixth AAAI Conference on Artificial Intelligence},
pages = {38–44},
numpages = {7},
location = {Toronto, Ontario, Canada},
series = {AAAI'12}
}

@inproceedings{10.1145/2967413.2967421,
author = {Zheng, Yu and Chen, Zhenhua(Jimmy) and Velipasalar, Senem and Tang, Jian},
title = {Person Detection and Re-identification Across Multiple Images and Videos Obtained via Crowdsourcing},
year = {2016},
isbn = {9781450347860},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2967413.2967421},
doi = {10.1145/2967413.2967421},
abstract = {Person re-identification is indispensable for consistent labeling across different camera views. Most existing studies use static cameras, apply background subtraction to detect moving people, and then focus on the matching of detection results. However, if cameras are mobile or only single image frames (not videos) are available, then background subtraction cannot be used, and human detection needs to be performed on entire images. In this paper, different from most of the existing work, we focus on a crowdsourcing scenario to find and follow person(s) of interest in the collected images/videos. We propose a novel approach combining R-CNN based person detection with the GPU implementation of color histogram and SURF-based re-identification. Moreover, GeoTags are extracted from the EXIF data of videos captured by smart phones, and are displayed on a map together with the time-stamps. All the processing is performed on a GPU, and the average processing time is 5 ms per frame.},
booktitle = {Proceedings of the 10th International Conference on Distributed Smart Camera},
pages = {178–183},
numpages = {6},
location = {Paris, France},
series = {ICDSC '16}
}

@inproceedings{10.1145/1835449.1835617,
author = {Potthast, Martin},
title = {Crowdsourcing a wikipedia vandalism corpus},
year = {2010},
isbn = {9781450301534},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1835449.1835617},
doi = {10.1145/1835449.1835617},
abstract = {We report on the construction of the PAN Wikipedia vandalism corpus, PAN-WVC-10, using Amazon's Mechanical Turk. The corpus compiles 32452 edits on 28468 Wikipedia articles, among which 2391 vandalism edits have been identified. 753 human annotators cast a total of 193022 votes on the edits, so that each edit was reviewed by at least 3 annotators, whereas the achieved level of agreement was analyzed in order to label an edit as "regular" or "vandalism." The corpus is available free of charge.},
booktitle = {Proceedings of the 33rd International ACM SIGIR Conference on Research and Development in Information Retrieval},
pages = {789–790},
numpages = {2},
keywords = {corpus, evaluation, vandalism detection, wikipedia},
location = {Geneva, Switzerland},
series = {SIGIR '10}
}

@inproceedings{10.1145/2661334.2661401,
author = {Calvo, Roc\'{\i}o and Kane, Shaun K. and Hurst, Amy},
title = {Evaluating the accessibility of crowdsourcing tasks on Amazon's mechanical turk},
year = {2014},
isbn = {9781450327206},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2661334.2661401},
doi = {10.1145/2661334.2661401},
abstract = {Crowd work web sites such as Amazon Mechanical Turk enable individuals to work from home, which may be useful for people with disabilities. However, the web sites for finding and performing crowd work tasks must be accessible if people with disabilities are to use them. We performed a heuristic analysis of one crowd work site, Amazon's Mechanical Turk, using the Web Content Accessibility Guidelines 2.0. This paper presents the accessibility problems identified in our analysis and offers suggestions for making crowd work platforms more accessible},
booktitle = {Proceedings of the 16th International ACM SIGACCESS Conference on Computers \&amp; Accessibility},
pages = {257–258},
numpages = {2},
keywords = {accessibility, crowdsourcing., evaluation, mechanical turk},
location = {Rochester, New York, USA},
series = {ASSETS '14}
}

@inproceedings{10.1145/2072298.2071901,
author = {Freiburg, Bauke and Kamps, Jaap and Snoek, Cees G.M.},
title = {Crowdsourcing visual detectors for video search},
year = {2011},
isbn = {9781450306164},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2072298.2071901},
doi = {10.1145/2072298.2071901},
abstract = {In this paper, we study social tagging at the video fragment-level using a combination of automated content understanding and the wisdom of the crowds. We are interested in the question whether crowdsourcing can be beneficial to a video search engine that automatically recognizes video fragments on a semantic level. To answer this question, we perform a 3-month online field study with a concert video search engine targeted at a dedicated user-community of pop concert enthusiasts. We harvest the feedback of more than 500 active users and perform two experiments. In experiment 1 we measure user incentive to provide feedback, in experiment 2 we determine the tradeoff between feedback quality and quantity when aggregated over multiple users. Results show that users provide sufficient feedback, which becomes highly reliable when a crowd agreement of 67\% is enforced.},
booktitle = {Proceedings of the 19th ACM International Conference on Multimedia},
pages = {913–916},
numpages = {4},
keywords = {information visualization, semantic indexing, video retrieval},
location = {Scottsdale, Arizona, USA},
series = {MM '11}
}

@inproceedings{10.1145/2389176.2389189,
author = {V\"{a}\"{a}t\"{a}j\"{a}, Heli and Vainio, Teija and Sirkkunen, Esa},
title = {Location-based crowdsourcing of hyperlocal news: dimensions of participation preferences},
year = {2012},
isbn = {9781450314862},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2389176.2389189},
doi = {10.1145/2389176.2389189},
abstract = {We studied the mobile users' preferences and concerns of using location-based assignments (LBA) and geotagging in crowdsourced news making. First, nine readers who had submitted reader's photos were interviewed about their perceptions of LBA and geotagging scenarios. Second, a quasi-experiment in field conditions was carried out with nineteen participants. After completing four LBA tasks with a mobile phone, participants were interviewed on their perceptions and asked to complete a questionnaire on their preferences for receiving LBA and usage of geotags. Findings indicate that the perceived benefits of LBA and geotagging are greater than the perceived risks. The task type, temporal context, preciseness of location query, proximity to the reporting location, parallel tasks, social context and incentives affected the participation preferences. We propose a framework for participation preferences to support further studies in location-based crowdsourcing and in the development of crowdsourcing processes and systems.},
booktitle = {Proceedings of the 2012 ACM International Conference on Supporting Group Work},
pages = {85–94},
numpages = {10},
keywords = {assignment, crowdsourcing, location, news, privacy, reader, ugc},
location = {Sanibel Island, Florida, USA},
series = {GROUP '12}
}

@inproceedings{10.1109/SCC.2010.74,
author = {Lopez, Mariana and Vukovic, Maja and Laredo, Jim},
title = {PeopleCloud Service for Enterprise Crowdsourcing},
year = {2010},
isbn = {9780769541266},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/SCC.2010.74},
doi = {10.1109/SCC.2010.74},
abstract = {Web 2.0 provides the technological foundations upon which the crowdsourcing paradigm evolves and operates, enabling enterprises, universities and eGovernments, to access scalable networks of knowledge experts on-line. However, there is no existing practice allowing for coordination of crowdsourcing tasks, and their integration with existing business processes and embedding these services into the Web fabric. In this paper, we examine two applications of enterprise crowdsourcing service in the domain of IT Service Delivery: 1) IT Inventory Management and 2) End-User Support. We illustrate how a) expert discovery mechanism, b) virtual team building capabilities, c) task management and d) provisioning of task-based services, enable enterprises to effectively build knowledge networks, which are able to execute complex and transformative knowledge-intensive tasks. Finally, based on the application analysis, we propose PeopleCloud, an on-demand service system, which spawns and manages scalable virtual teams of knowledge workers by either (1) building on the wisdom of crowds within an enterprise or across a value chain or (2) creating a marketplace for accessing specialists on-line.},
booktitle = {Proceedings of the 2010 IEEE International Conference on Services Computing},
pages = {538–545},
numpages = {8},
keywords = {Collaborative Production, Crowdsourcing Service},
series = {SCC '10}
}

@inproceedings{10.1145/2470654.2466269,
author = {Lasecki, Walter S. and Miller, Christopher D. and Bigham, Jeffrey P.},
title = {Warping time for more effective real-time crowdsourcing},
year = {2013},
isbn = {9781450318990},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2470654.2466269},
doi = {10.1145/2470654.2466269},
abstract = {In this paper, we introduce the idea of "warping time" to improve crowd performance on the difficult task of captioning speech in real-time. Prior work has shown that the crowd can collectively caption speech in real-time by merging the partial results of multiple workers. Because non-expert workers cannot keep up with natural speaking rates, the task is frustrating and prone to errors as workers buffer what they hear to type later. The TimeWarp approach automatically increases and decreases the speed of speech playback systematically across individual workers who caption only the periods played at reduced speed. Studies with 139 remote crowd workers and 24 local participants show that this approach improves median coverage (14.8\%), precision (11.2\%), and per-word latency (19.1\%). Warping time may also help crowds outperform individuals on other difficult real-time performance tasks.},
booktitle = {Proceedings of the SIGCHI Conference on Human Factors in Computing Systems},
pages = {2033–2036},
numpages = {4},
keywords = {captioning, human computation, real-time crowdsourcing},
location = {Paris, France},
series = {CHI '13}
}

@inproceedings{10.1145/2470654.2470708,
author = {Massung, Elaine and Coyle, David and Cater, Kirsten F. and Jay, Marc and Preist, Chris},
title = {Using crowdsourcing to support pro-environmental community activism},
year = {2013},
isbn = {9781450318990},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2470654.2470708},
doi = {10.1145/2470654.2470708},
abstract = {Community activist groups typically rely on core groups of highly motivated members. In this paper we consider how crowdsourcing strategies can be used to supplement the activities of pro-environmental community activists, thus increasing the scalability of their campaigns. We focus on mobile data collection applications and strategies that can be used to engage casual participants in pro-environmental data collection. We report the results of a study that used both quantitative and qualitative methods to investigate the impact of different motivational factors and strategies, including both intrinsic and extrinsic motivators. The study compared and provides empirical evidence for the effectiveness of two extrinsic motivation strategies, pointification - a subset of gamification - and financial incentives. Prior environmental interest is also assessed as an intrinsic motivation factor. In contrast to previous HCI research on pro-environmental technology, much of which has focused on individual behavior change, this paper offers new insights and recommendations on the design of systems that target groups and communities.},
booktitle = {Proceedings of the SIGCHI Conference on Human Factors in Computing Systems},
pages = {371–380},
numpages = {10},
keywords = {community activism, crowdsourcing, gamification, motivation, participatory urbanism, sustainability},
location = {Paris, France},
series = {CHI '13}
}

@inproceedings{10.1145/3251097,
author = {Clarke, Charlie},
title = {Session details: Session 6: Crowdsourcing, Temporal and Location-based mining},
year = {2015},
isbn = {9781450333177},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3251097},
doi = {10.1145/3251097},
booktitle = {Proceedings of the Eighth ACM International Conference on Web Search and Data Mining},
location = {Shanghai, China},
series = {WSDM '15}
}

@inproceedings{10.1145/2785971.2785976,
author = {Marina, Mahesh K. and Radu, Valentin and Balampekos, Konstantinos},
title = {Impact of Indoor-Outdoor Context on Crowdsourcing based Mobile Coverage Analysis},
year = {2015},
isbn = {9781450335386},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2785971.2785976},
doi = {10.1145/2785971.2785976},
abstract = {We consider the crowdsourcing based mobile cellular network measurement paradigm that is becoming increasingly popular. In particular, we aim to study the impact of user indoor/outdoor environment context at time of measurement. Focusing on signal strength as the measurement metric and using a real large crowdsourced measurement dataset for central London area along with estimated environment state (indoor or outdoor), we show that indoor-outdoor context has a significant impact, suggesting that conflating indoor and outdoor measurements can lead to unreliable results. We validate these observations using a set of diverse and controlled measurements with indoor/outdoor ground truth information. We also discuss some opportunities for future work (e.g., accurate and efficient context detection) relevant to crowdsourced mobile network measurement systems.},
booktitle = {Proceedings of the 5th Workshop on All Things Cellular: Operations, Applications and Challenges},
pages = {45–50},
numpages = {6},
keywords = {cellular coverage, crowdsourced mobile network measurement, indoor-outdoor environment context},
location = {London, United Kingdom},
series = {AllThingsCellular '15}
}

@inproceedings{10.1145/3253131,
author = {Jacknis, Norm},
title = {Session details: Social media and crowdsourcing},
year = {2011},
isbn = {9781450307628},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3253131},
doi = {10.1145/3253131},
booktitle = {Proceedings of the 12th Annual International Digital Government Research Conference: Digital Government Innovation in Challenging Times},
location = {College Park, Maryland, USA},
series = {dg.o '11}
}

@inproceedings{10.1145/2486084.2486085,
author = {Shahabi, Cyrus},
title = {Towards a generic framework for trustworthy spatial crowdsourcing},
year = {2013},
isbn = {9781450321976},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2486084.2486085},
doi = {10.1145/2486084.2486085},
abstract = {Many studies foresee significant future growth in the number of mobile smart phone users, the phone's hardware and software features, and the broadband bandwidth. Therefore, a transformative area of research is to fully utilize this new platform for various tasks, among which the most promising is spatial crowdsourcing. Spatial crowdsourcing (SC) engages individuals, groups, and communities in the act of collecting, analyzing, and disseminating urban, social, and other spatiotemporal information. This new paradigm of data collection has shown to be useful when traditional means fail (e.g., due to disaster), are censored or do not scale in time and space.Two major impediments to the success of spatial crowdsourcing in real-world applications are scalability and trust issues. Without scale considerations, it is impossible to develop a generic multi-campaign spatial crowdsourcing system (SC-system) that can efficiently and in real-time match many requesters' tasks to numerous workers. Without trust, the SC-system cannot evaluate the credibility of the contributed data, rendering it ineffective for replacing the traditional data collection means. In this paper, we survey and study both issues of scale and trust in spatial crowdsourcing.},
booktitle = {Proceedings of the 12th International ACM Workshop on Data Engineering for Wireless and Mobile Acess},
pages = {1–4},
numpages = {4},
keywords = {crowdsourcing, mobile applications, spatial crowdsourcing, spatial data management},
location = {New York, New York},
series = {MobiDE '13}
}

@inproceedings{10.1145/3003965.3003973,
author = {Bock, Fabian and Martino, Sergio Di and Sester, Monika},
title = {What are the potentialities of crowdsourcing for dynamic maps of on-street parking spaces?},
year = {2016},
isbn = {9781450345774},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3003965.3003973},
doi = {10.1145/3003965.3003973},
abstract = {Finding a parking space is a crucial mobility problem, which could be mitigated by dynamic maps of parking availability. The creation of these maps requires current information on the state of the parking stalls, which could be obtained by (I) instrumenting the road infrastructure with sensors, (II) using probe vehicles, or (III) using mobile apps.In this paper, we investigate the potential predictive performances of a random forest binary classifier, comparing these three data collection strategies. As for the dataset, we used real infrastructure measurements in San Francisco for solution I. We simulated the crowdsourcing solutions II and III by downsampling that dataset, based on different assumptions.Evaluations show that the instrumented solution is clearly superior over the two crowdsourcing strategies, but with remarkably small differences to the probe vehicle scenario. On the other hand, a mobile app would require a very high penetration rate in order to be used for meaningful predictions.},
booktitle = {Proceedings of the 9th ACM SIGSPATIAL International Workshop on Computational Transportation Science},
pages = {19–24},
numpages = {6},
keywords = {spatial information and society, spatio-temporal data analysis, traffic telematics, transportation},
location = {Burlingame, California},
series = {IWCTS '16}
}

@inproceedings{10.1109/NBiS.2015.74,
author = {Matsushita, Yuichi and Uchiya, Takahiro and Nishimuray, Ryota and Yamamoto, Daisuke and Takumi, Ichi},
title = {Crowdsourcing Environment to Create Voice Interaction Scenario of Spoken Dialogue System},
year = {2015},
isbn = {9781479999422},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/NBiS.2015.74},
doi = {10.1109/NBiS.2015.74},
abstract = {This university has published the "MMDAgent" voice interaction system toolkit as open source software. Voice Interaction Agent, produced using MMDAgent, is active in various scenarios and provides voice interaction capability. Voice interaction agents produced using MMDAgent talk to people by loading a script file that describes a voice interaction scenario. To increase the agent's vocabulary, it is important to describe many voice interaction scenarios. To date, voice interaction scenarios have been described by a small organization. Therefore, they require high costs to describe many voice interaction scenarios. To resolve this problem, we construct a crowdsourcing[1] model for the creation of a voice interaction scenario. In this study, the works of creation of voice interaction scenario are divided and distributed to numerous people via the internet. Furthermore, we provide in this study a tool for creation voice interaction scenario and test operation of voice interaction agent as a web application service. We hereby reduce the cost of installation MMDAgent and provide a comfortable environment for the creation voice interaction scenario.},
booktitle = {Proceedings of the 2015 18th International Conference on Network-Based Information Systems},
pages = {499–504},
numpages = {6},
keywords = {Crowdsourcing, MMDAgent, Voice Interaction System},
series = {NBIS '15}
}

@inproceedings{10.1145/2998476.2998498,
author = {Agarwal, Bhoomika and Ravikumar, Abhiram and Saha, Snehanshu},
title = {A Novel Approach to Big Data Veracity using Crowdsourcing Techniques and Bayesian Predictors},
year = {2016},
isbn = {9781450348089},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2998476.2998498},
doi = {10.1145/2998476.2998498},
abstract = {In today's world data is being generated at a tremendous pace and there have to be enough measures in place to verify the nature of big data. Analysis performed on 'dirty' data may lead to erroneous insights and thereby shaping decisions poorly. The aspect of big data that deals with its correctness is known as big data veracity. Trusting the data acquired goes a long way in implementing decisions from an automated decision-making system and veracity helps to validate the data acquired. In this paper, we present our solution to the big data veracity problem using crowdsourcing techniques. Our solution involves the use of sentiment analysis, which deals with identifying the sentiment expressed in a piece of text. As a proof of concept, we have developed an app that requires users to tag tweets as per the sentiment it evokes in them. Each tweet would therefore get ratified by hundreds of our participants and the sentiment associated to the tweet gets tagged. The tagged emotion was then evaluated against the verified emotion as compared to a verified data set. This analysis was then plotted on a ROC curve and also evaluated against verified data using a Bayesian predictor trained with a trinomial function. As can be seen, an accuracy of 81\% was obtained as displayed by the ROC curve and 89\% through the Bayesian predictor. Also, a MAP analysis of the Bayesian predictor yields neutral sentiment as the most probable hypothesis. By doing this, we have proven that crowdsourcing of sentiment analysis is a viable solution to the problem of big data veracity and therefore an aid in making better decisions.},
booktitle = {Proceedings of the 9th Annual ACM India Conference},
pages = {153–160},
numpages = {8},
keywords = {Bayesian Predictor, Big Data, Crowdsourcing, Machine Learning, Sentiment Analysis, Tweet Mining},
location = {Gandhinagar, India},
series = {COMPUTE '16}
}

@proceedings{10.5555/2820116,
title = {CSI-SE '15: Proceedings of the Second International Workshop on CrowdSourcing in Software Engineering},
year = {2015},
publisher = {IEEE Press},
abstract = {It is our pleasure to welcome the reader to the (pre-workshop) proceedings of the 2nd International Workshop on Crowd Sourcing in Software Engineering (CSI-SE 2015), co-located with the 37th International Conference on Software Engineering (ICSE 2015) held in Florence, Italy, May 19, 2015.A number of trends under the broad banner of crowdsourcing are beginning to fundamentally disrupt the way in which software is engineered. Programmers increasingly rely on crowdsourced knowledge and code, as they look to Q&amp;A sites for answers or use code from publicly posted snippets. Programmers play, compete, and learn with the crowd, engaging in programming competitions and puzzles with crowds of programmers. Online IDEs make radically new forms of collaboration possible, allowing developers to synchronously program with crowds of distributed programmers. Programmer reputation is increasingly visible on Q&amp;A sites and public code repositories, opening new possibilities in how developers find jobs and companies identify talent. Crowds of non-programmers increasingly participate in development, usability testing software or even constructing specifications while playing games. Crowdfunding democratizes choices about which software is built, broadening the software which might be feasibly constructed. Approaches for crowd development seek to microtask software development, dramatically increasing participation in open source by enabling software projects to be built through casual, transient work.},
location = {Florence, Italy}
}

@inproceedings{10.1145/1753326.1753357,
author = {Heer, Jeffrey and Bostock, Michael},
title = {Crowdsourcing graphical perception: using mechanical turk to assess visualization design},
year = {2010},
isbn = {9781605589299},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1753326.1753357},
doi = {10.1145/1753326.1753357},
abstract = {Understanding perception is critical to effective visualization design. With its low cost and scalability, crowdsourcing presents an attractive option for evaluating the large design space of visualizations; however, it first requires validation. In this paper, we assess the viability of Amazon's Mechanical Turk as a platform for graphical perception experiments. We replicate previous studies of spatial encoding and luminance contrast and compare our results. We also conduct new experiments on rectangular area perception (as in treemaps or cartograms) and on chart size and gridline spacing. Our results demonstrate that crowdsourced perception experiments are viable and contribute new insights for visualization design. Lastly, we report cost and performance data from our experiments and distill recommendations for the design of crowdsourced studies.},
booktitle = {Proceedings of the SIGCHI Conference on Human Factors in Computing Systems},
pages = {203–212},
numpages = {10},
keywords = {crowdsourcing, evaluation, experimentation, graphical perception, information visualization, mechanical turk, user study},
location = {Atlanta, Georgia, USA},
series = {CHI '10}
}

@inproceedings{10.4108/icst.urb-iot.2014.257267,
author = {Prandi, Catia and Mirri, Silvia and Salomoni, Paola},
title = {Trustworthiness assessment in mapping urban accessibility via sensing and crowdsourcing},
year = {2014},
isbn = {9781631900372},
publisher = {ICST (Institute for Computer Sciences, Social-Informatics and Telecommunications Engineering)},
address = {Brussels, BEL},
url = {https://doi.org/10.4108/icst.urb-iot.2014.257267},
doi = {10.4108/icst.urb-iot.2014.257267},
abstract = {In this work we present the trustworthiness assessment in mPASS (mobile Pervasive Accessibility Social Sensing), a location and context aware system that collects data from crowdsourcing and sensing in order to map urban and architectural accessibility. The fusion of heterogeneous urban sources allows mPASS to provide users with personalized paths, computed on the basis of their preferences and needs. To perform this task, the system needs a set of georeferenced data that is dense enough and trustworthy enough to avoid false positives and negatives, e.g. to prevent users from encounter on their path an unknown barrier or a non-existing facility. In order to reach this goal, we propose a trustworthiness assessment which combines accuracy of sensors, source credibility of the crowd and the authority of experts. To evaluate the effectiveness of our trustworthiness assessment, we conducted a set of simulations by exploiting OSM data and we have obtained interesting results.},
booktitle = {Proceedings of the First International Conference on IoT in Urban Space},
pages = {108–110},
numpages = {3},
keywords = {crowdsourcing, sensing, trustworthiness, urban accessibility},
location = {Rome, Italy},
series = {URB-IOT '14}
}

@inproceedings{10.1109/SRII.2012.17,
author = {Varshney, Lav R.},
title = {Privacy and Reliability in Crowdsourcing Service Delivery},
year = {2012},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/SRII.2012.17},
doi = {10.1109/SRII.2012.17},
abstract = {Due to the anonymity and low pay of workers in crowd sourcing platforms, there may be concerns regarding reliability and privacy-preservation when using such platforms to deliver services. This paper describes a technique for jointly providing privacy and reliability through stochastic perturbation of micro task definitions and fusion rules to combine the work of several workers. A mathematical model of a crowd sourcing system using this technique is proposed and precise threshold conditions on loss of privacy when workers collude are provided. Tradeoffs between privacy, reliability, and cost are determined.},
booktitle = {Proceedings of the 2012 Annual SRII Global Conference},
pages = {55–60},
numpages = {6},
series = {SRII '12}
}

@inproceedings{10.1145/2468356.2468506,
author = {Nguyen, Phu and Kim, Juho and Miller, Robert C.},
title = {Generating annotations for how-to videos using crowdsourcing},
year = {2013},
isbn = {9781450319522},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2468356.2468506},
doi = {10.1145/2468356.2468506},
abstract = {How-to videos can be valuable for learning, but searching for and following along with them can be difficult. Having labeled events such as the tools used in how-to videos could improve video indexing, searching, and browsing. We introduce a crowdsourcing annotation tool for Photoshop how-to videos with a three-stage method that consists of: (1) gathering timestamps of important events, (2) labeling each event, and (3) capturing how each event affects the task of the tutorial. Our ultimate goal is to generalize our method to be applied to other domains of how-to videos. We evaluate our annotation tool with Amazon Mechanical Turk workers to investigate the accuracy, costs, and feasibility of our three-stage method for annotating large numbers of video tutorials. Improvements can be made for stages 1 and 3, but stage 2 produces accurate labels over 90\% of the time using majority voting. We have observed that changes in the instructions and interfaces of each task can improve the accuracy of the results significantly.},
booktitle = {CHI '13 Extended Abstracts on Human Factors in Computing Systems},
pages = {835–840},
numpages = {6},
keywords = {crowd workers, how-to videos, video tutorials},
location = {Paris, France},
series = {CHI EA '13}
}

@inproceedings{10.1145/3136755.3136767,
author = {Ramanarayanan, Vikram and Leong, Chee Wee and Suendermann-Oeft, David and Evanini, Keelan},
title = {Crowdsourcing ratings of caller engagement in thin-slice videos of human-machine dialog: benefits and pitfalls},
year = {2017},
isbn = {9781450355438},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3136755.3136767},
doi = {10.1145/3136755.3136767},
abstract = {We analyze the efficacy of different crowds of naive human raters in rating engagement during human--machine dialog interactions. Each rater viewed multiple 10 second, thin-slice videos of native and non-native English speakers interacting with a computer-assisted language learning (CALL) system and rated how engaged and disengaged those callers were while interacting with the automated agent. We observe how the crowd's ratings compared to callers' self ratings of engagement, and further study how the distribution of these rating assignments vary as a function of whether the automated system or the caller was speaking. Finally, we discuss the potential applications and pitfalls of such crowdsourced paradigms in designing, developing and analyzing engagement-aware dialog systems.},
booktitle = {Proceedings of the 19th ACM International Conference on Multimodal Interaction},
pages = {281–287},
numpages = {7},
keywords = {crowdsourcing, engagement, multimodal dialog, thin-slicing},
location = {Glasgow, UK},
series = {ICMI '17}
}

@inproceedings{10.1145/3009912.3009916,
author = {Padhariya, Nilesh and Mondal, Anirban and Madria, Sanjay Kumar},
title = {Efficient Processing of Mobile Crowdsourcing Queries with Multiple Sub-tasks for Facilitating Smart Cities},
year = {2016},
isbn = {9781450346672},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3009912.3009916},
doi = {10.1145/3009912.3009916},
abstract = {The proliferation and ever-increasing popularity of mobile devices have dramatically increased the potential for mobile crowdsourcing across a wide gamut of applications that are relevant to smart cities. While existing works have mostly focused on mobile crowdsourcing for queries involving a single given major task, the issue of addressing complex queries with multiple related large sub-tasks (with spatio-temporal dependencies) has received little attention. In this regard, the key contributions of the paper are three-fold. First, we present a scheme, designated as BMS (Broker-based processing of Multiple Sub-tasks), in which a broker coordinates the processing of multiple related large subtasks in a given query among a set of mobile peers. Second, we propose the DMS (Distributed processing of Multiple Sub-tasks) scheme in which the processing of multiple sub-tasks in a query occurs in a distributed manner without the existence of any brokers. Third, the results of our performance evaluation demonstrate the effectiveness of both of the proposed schemes in terms of relatively low query response times, high query success rates and reasonable communication costs.},
booktitle = {Proceedings of the 2nd International Workshop on Smart},
articleno = {8},
numpages = {6},
keywords = {Mobile crowdsourcing, queries, smart cities, sub-tasks},
location = {Trento, Italy},
series = {SmartCities '16}
}

@inproceedings{10.1007/978-3-319-15168-7_54,
author = {Pereira Santos, Carlos and Khan, Vassilis-Javed and Markopoulos, Panos},
title = {On Utilizing Player Models to Predict Behavior in Crowdsourcing Tasks},
year = {2015},
isbn = {978-3-319-15167-0},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-319-15168-7_54},
doi = {10.1007/978-3-319-15168-7_54},
abstract = {Player Modeling is a research field that studies player characteristics by analyzing in-game behavior. We aim to develop independent models, which are transferable and useful beyond a game’s context. We shall demonstrate the feasibility of this approach by applying player models to crowdsourcing to predict workers’ task completion effectiveness. Specifically, we model a user’s Need for Cognition based on in-game behavior, and based on that try to assign appropriate tasks to workers.},
booktitle = {Social Informatics: SocInfo 2014 International Workshops, Barcelona, Spain, November 11, 2014, Revised Selected Papers},
pages = {448–451},
numpages = {4},
keywords = {Personality Trait, Player Skill, Player Experience, Profile Tool, Predict Behavior},
location = {Barcelona, Spain}
}

@inproceedings{10.1145/2658779.2658795,
author = {Embiricos, Alex and Rahmati, Negar and Zhu, Nicole and Bernstein, Michael S.},
title = {Structured handoffs in expert crowdsourcing improve communication and work output},
year = {2014},
isbn = {9781450330688},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2658779.2658795},
doi = {10.1145/2658779.2658795},
abstract = {Expert crowdsourcing allows specialized, remote teams to complete projects, often large and involving multiple stages. Its execution is complicated due to communication difficulties between remote workers. This paper investigates whether structured handoff methods, from one worker to the next, improve final product quality by helping the workers understand the input of their tasks and reduce overall integration cost. We investigate this question through 1) a "live" handoff method where the next worker shadows the former via screen sharing technology and 2) a "recorded" handoff, where workers summarize work done for the next, via a screen capture and narration. We confirm the need for a handoff process. We conclude that structured handoffs result in higher quality work, improved satisfaction (especially for workers with creative tasks), improved communication of non-obvious instructions, and increased adherence to the original intent of the project.},
booktitle = {Adjunct Proceedings of the 27th Annual ACM Symposium on User Interface Software and Technology},
pages = {99–100},
numpages = {2},
keywords = {CSCW, crowdsourcing, expert crowdsourcing},
location = {Honolulu, Hawaii, USA},
series = {UIST '14 Adjunct}
}

@inproceedings{10.1145/2442882.2442904,
author = {Roy, Shourya and Balamurugan, Chithralekha and Gujar, Sujit},
title = {Sustainable employment in India by crowdsourcing enterprise tasks},
year = {2013},
isbn = {9781450318563},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2442882.2442904},
doi = {10.1145/2442882.2442904},
abstract = {This paper explains how crowdsourcing would constitute for a promising and a successful alternate business model in India, especially at the juncture where the advantages of outsourcing is beginning to fade. The success of this alternate business model depends on the solutions for two challenges -- by taking work to people to leverage large educated population in India and making enough work available for the workforce to work and earn. Though solution to the first challenge is implicitly available through increasing penetration of Information and Communication Technologies (ICT) in India, the second challenge requires enough tasks to be available by enabling business organizations to adopt crowdsourcing. Since enterprise tasks are not readily crowdsourcable owing to security, compliance and contractual reasons, this paper proceeds to describe an end to end system which encompasses technical solutions that could help crowdsourcing business tasks, by tactfully overcoming the existing business constraints. For business tasks, we consider Insurance Claim Form Digitization which is one of the most common tasks taken up by outsourcing enterprises.},
booktitle = {Proceedings of the 3rd ACM Symposium on Computing for Development},
articleno = {16},
numpages = {2},
keywords = {Amazon mechanical turk, crowdsourcing, data digitization, employment models, microtasking},
location = {Bangalore, India},
series = {ACM DEV '13}
}

@inproceedings{10.1007/978-3-642-41428-2_14,
author = {Zhang, Gang and Chen, Haopeng},
title = {Quality Control for Crowdsourcing with Spatial and Temporal Distribution},
year = {2013},
isbn = {9783642414275},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-642-41428-2_14},
doi = {10.1007/978-3-642-41428-2_14},
abstract = {In the past decade, crowdsourcing has become a prospective paradigm for commercial purposes, for it brings a lot of benefits such as low cost and high immediacy, particularly in location-based services LBS. On the other side, there also exist many problems need to be solved in crowdsourcing. For example, the quality control for crowdsourcing systems has been identified as a significant challenge, which includes how to handle massive data more efficiently, how to discriminate poor quality content in workers' submissions and so on. In this paper, we put forward an approach to control the crowdsourcing quality from spatial and temporal distribution. Our experiments have demonstrated the effectiveness and efficiency of the approach.},
booktitle = {Proceedings of the 6th International Conference on Internet and Distributed Computing Systems - Volume 8223},
pages = {169–182},
numpages = {14},
keywords = {crowdsourcing, location-based service LBS, quality control, spatial and temporal distribution},
location = {Hangzhou, China},
series = {IDCS 2013}
}

@inproceedings{10.1145/2810188.2810193,
author = {Gardlo, Bruno and Egger, Sebastian and Hossfeld, Tobias},
title = {Do Scale-Design and Training Matter for Video QoE Assessments through Crowdsourcing?},
year = {2015},
isbn = {9781450337465},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2810188.2810193},
doi = {10.1145/2810188.2810193},
abstract = {Crowdsourcing (CS) has evolved into a mature assessment methodology for subjective experiments in diverse scientific fields and in particular for QoE assessment. However, the results acquired for absolute category rating (ACR) scales through CS are often not fully comparable to QoE assessments done in laboratory environments. A possible reason for such differences may be the scale usage heterogeneity problem caused by deviant scale usage of the crowd workers. In this paper, we study different implementations of (quality) rating scales (in terms of design and number of answer categories) in order to identify if certain scales can help to overcome scale usage problems in crowdsourcing. Additionally, training of subjects is well known to enhance result quality for laboratory ACR evaluations. Hence, we analyzed the appropriateness of training conditions to overcome scale usage problems across different samples in crowdsourcing. As major results, we found that filtering of user ratings and different scale designs are not sufficient to overcome scale usage heterogeneity, but training sessions despite their additional costs, enhance result quality in CS and properly counterfeit the identified scale usage heterogeneity problems.},
booktitle = {Proceedings of the Fourth International Workshop on Crowdsourcing for Multimedia},
pages = {15–20},
numpages = {6},
keywords = {QoE, crowdsourcing, methodology, reliability, scales},
location = {Brisbane, Australia},
series = {CrowdMM '15}
}

@inproceedings{10.1145/2037556.2037582,
author = {Warner, Janice},
title = {Next steps in e-government crowdsourcing},
year = {2011},
isbn = {9781450307628},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2037556.2037582},
doi = {10.1145/2037556.2037582},
abstract = {In the US, the Federal Administration's Open Government Initiative has spurred use of crowdsourcing tools for several types of citizen participation. Likewise, US states and municipalities have implemented several forums. A survey of crowdsourcing initiatives as well as a discussion of important characteristics and features are provided in this paper. Most critical to crowdsourcing success is the feeling by participants that their efforts were considered and that results came from the initiative. This requires moderators who are knowledge workers adept at working in a social network environment, flexible crowdsourcing tools that make linking and feedback easy to provide, as well as a change in processes used to develop governmental services.},
booktitle = {Proceedings of the 12th Annual International Digital Government Research Conference: Digital Government Innovation in Challenging Times},
pages = {177–181},
numpages = {5},
keywords = {crowdsourcing, e-government},
location = {College Park, Maryland, USA},
series = {dg.o '11}
}

@inproceedings{10.1145/2536146.2536172,
author = {Dumitrescu, Stefan Daniel and Trausan-Matu, Stefan and Brut, Mihaela and Sedes, Florence},
title = {Ontology-based flexible topic classification of crowdsourcing textual resources},
year = {2013},
isbn = {9781450320047},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2536146.2536172},
doi = {10.1145/2536146.2536172},
abstract = {The paper presents a solution to the problem of capitalizing in different contexts and by different stakeholders the time-stamped new documents produced by social Web sites (including news, blog entries, and uploaded documents). The solution core includes an ontology-based method to express the interest topics and to automatically classify them. For such textual content obtained in real-time, we propose an unsupervised text classification system based on general YAGO ontology, graph algorithms and a custom scoring method. The system shows good performance using only ontology information and the ontology structure itself. We compare our system against a SVM-based (Support Vector Machine) classic text classification approach. For determining the relevance of a specific document for a specific topic, our approach develops and compares the ontology sub graphs corresponding to the query and to the document. It leads to a high flexibility in terms of capitalizing the already classified documents when refining and changing the interest topic: a graph-based matching of the already obtained ontology-based document representation against the new query representation is enough to assess the document relevance.},
booktitle = {Proceedings of the Fifth International Conference on Management of Emergent Digital EcoSystems},
pages = {145–151},
numpages = {7},
keywords = {concept, graph algorithm, knowledge engineering, ontology, text classification},
location = {Luxembourg, Luxembourg},
series = {MEDES '13}
}

@inproceedings{10.1109/BigData.2015.7363814,
author = {To, Hien and Kim, Seon Ho and Shahabi, Cyrus},
title = {Effectively crowdsourcing the acquisition and analysis of visual data for disaster response},
year = {2015},
isbn = {9781479999262},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/BigData.2015.7363814},
doi = {10.1109/BigData.2015.7363814},
abstract = {Efficient and thorough data collection and its timely analysis are critical for disaster response and recovery in order to save peoples lives during disasters. However, access to comprehensive data in disaster areas and their quick analysis to transform the data to actionable knowledge are challenging. With the popularity and pervasiveness of mobile devices, crowdsourcing data collection and analysis has emerged as an effective and scalable solution. This paper addresses the problem of crowdsourcing mobile videos for disasters by identifying two unique challenges of 1) prioritizing visualdata collection and transmission under bandwidth scarcity caused by damaged communication networks and 2) analyzing the acquired data in a timely manner. We introduce a new crowdsourcing framework for acquiring and analyzing the mobile videos utilizing fine granularity spatial metadata of videos for a rapidly changing disaster situation. We also develop an analytical model to quantify the visual awareness of a video based on its metadata and propose the visual awareness maximization problem for acquiring the most relevant data under bandwidth constraints. The collected videos are evenly distributed to off-site analysts to collectively minimize crowdsourcing efforts for analysis. Our simulation results demonstrate the effectiveness and feasibility of the proposed framework.},
booktitle = {Proceedings of the 2015 IEEE International Conference on Big Data (Big Data)},
pages = {697–706},
numpages = {10},
series = {BIG DATA '15}
}

@inproceedings{10.1145/2468356.2468522,
author = {Ha, Seyong and Kim, Dongwhan and Lee, Joonhwan},
title = {Crowdsourcing as a method for indexing digital media},
year = {2013},
isbn = {9781450319522},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2468356.2468522},
doi = {10.1145/2468356.2468522},
abstract = {As people spend more time online, watching YouTube or playing games, a number of research studies arose in ways to make use of the time and energy from the crowd in doing such activities. In this paper, we have explored the possibility of converting the collective resources from the crowd in making useful information back to people. We collected posts from the online forums about soap operas on the air, and extracted instances when the name of characters in the play has been mentioned. These crowdsourced indexes become good search keywords to find the scenes where the characters mentioned in the posts appear.},
booktitle = {CHI '13 Extended Abstracts on Human Factors in Computing Systems},
pages = {931–936},
numpages = {6},
keywords = {collective intelligence., crowdsourcing, digital media index, social media},
location = {Paris, France},
series = {CHI EA '13}
}

@inproceedings{10.1109/HICSS.2011.207,
author = {Wiggins, Andrea and Crowston, Kevin},
title = {From Conservation to Crowdsourcing: A Typology of Citizen Science},
year = {2011},
isbn = {9780769542829},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/HICSS.2011.207},
doi = {10.1109/HICSS.2011.207},
abstract = {Citizen science is a form of research collaboration involving members of the public in scientific research projects to address real-world problems. Often organized as a virtual collaboration, these projects are a type of open movement, with collective goals addressed through open participation in research tasks. Existing typologies of citizen science projects focus primarily on the structure of participation, paying little attention to the organizational and macrostructural properties that are important to designing and managing effective projects and technologies. By examining a variety of project characteristics, we identified five types-Action, Conservation, Investigation, Virtual, and Education- that differ in primary project goals and the importance of physical environment to participation.},
booktitle = {Proceedings of the 2011 44th Hawaii International Conference on System Sciences},
pages = {1–10},
numpages = {10},
series = {HICSS '11}
}

@inproceedings{10.1109/SocialCom.2013.155,
author = {Machedon, Radu and Rand, William and Joshi, Yogesh},
title = {Automatic Crowdsourcing-Based Classification of Marketing Messaging on Twitter},
year = {2013},
isbn = {9780769551371},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/SocialCom.2013.155},
doi = {10.1109/SocialCom.2013.155},
abstract = {As the volume of social media communications grow, many different stakeholders have sought to apply tools and methods for automatic identification of sentiment and topic in social network communications. In the domain of social media marketing it would be useful to automatically classify social media messaging into the classic framework of informative, persuasive and transformative advertising. In this paper we develop and present the construction and evaluation of supervised machine-learning classifiers for these concepts, drawing upon established procedures from the domains of sentiment analysis and crowd sourced text classification. We demonstrate that a reasonably effective classifier can be created to identify the informative nature of Tweets based on crowd sourced training data, we also present results for identifying persuasive and transformative content. We finish by summarizing our findings regarding applying these methods and by discussing recommendations for future work in the area of classifying the marketing content of Tweets.},
booktitle = {Proceedings of the 2013 International Conference on Social Computing},
pages = {975–978},
numpages = {4},
keywords = {advertising, classification, crowdsourcing, machine learning, marketing},
series = {SOCIALCOM '13}
}

@inproceedings{10.1007/978-3-319-02786-9_9,
author = {Kurve, Aditya and Miller, David J. and Kesidis, George},
title = {Defeating Tyranny of the Masses in Crowdsourcing: Accounting for Low-Skilled and Adversarial Workers},
year = {2013},
isbn = {9783319027852},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-319-02786-9_9},
doi = {10.1007/978-3-319-02786-9_9},
abstract = {Crowdsourcing has emerged as a useful learning paradigm which allows us to instantly recruit workers on the web to solve large scale problems, such as quick annotation of image, web page, or document databases. Automated inference engines that fuse the answers or opinions from the crowd to make critical decisions are susceptible to unreliable, low-skilled and malicious workers who tend to mislead the system towards inaccurate inferences. We present a probabilistic generative framework to model worker responses for multicategory crowdsourcing tasks based on two novel paradigms. First, we decompose worker reliability into skill level and intention. Second, we introduce a stochastic model for answer generation that plausibly captures the interplay between worker skills, intentions, and task difficulties. This framework allows us to model and estimate a broad range of worker "types". A generalized Expectation Maximization algorithm is presented to jointly estimate the unknown ground truth answers along with worker and task parameters. As supported experimentally, the proposed scheme de-emphasizes answers from low skilled workers and leverages malicious workers to, in fact, improve crowd aggregation. Moreover, our approach is especially advantageous when there is an (a priori unknown) majority of low-skilled and/or malicious workers in the crowd.},
booktitle = {4th International Conference on Decision and Game Theory for Security - Volume 8252},
pages = {140–153},
numpages = {14},
keywords = {crowd aggregation, information fusion, malicious workers, probabilistic modeling},
location = {Fort Worth, TX, USA},
series = {GameSec 2013}
}

@inproceedings{10.1145/2525194.2525294,
author = {Balamurugan, Chithralekha and Roy, Shourya},
title = {Human computer interaction paradigm for business process task crowdsourcing},
year = {2013},
isbn = {9781450322539},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2525194.2525294},
doi = {10.1145/2525194.2525294},
abstract = {In conventional systems like the web-based systems, device-based systems, or desktop systems, the Human Computer Interaction (HCI) spectrum encompasses of user interaction methods, interaction modes, user interface design, etc. that strive to facilitate a user to seamlessly and intuitively interact with the system to accomplish his system goals. This spectrum usually considers aspects pertaining to bringing the user to initiate interaction with the system and retaining the user's interest to keep interacting with the system as an outcome of the usability/user experience that a user obtains while interacting with the system. In the context of crowdsourcing, these aspects have individual significance and not completely user experience based. Hence, it is essential to widen the spectrum of HCI for crowdsourcing to embrace these additional aspects explicitly and provide for a comprehensive HCI paradigm for crowdsourcingIn this paper, the conceptualization and definition of HCI paradigm for crowdsourcing has been described. The application of every aspect of the proposed paradigm is illustrated with respect to crowdsourcing of a typical form digitization task. The defined aspects could also serve as HCI evaluation parameters for business process task crowdsourcing},
booktitle = {Proceedings of the 11th Asia Pacific Conference on Computer Human Interaction},
pages = {264–273},
numpages = {10},
keywords = {HCI for crowdsourcing, HCI model for crowdsourcing, business process crowdsourcing, crowdsourcing, crowdsourcing HCI paradigm},
location = {Bangalore, India},
series = {APCHI '13}
}

@inproceedings{10.1109/ICGSE.2014.26,
author = {Wang, Hao and Wang, Yasha and Wang, Jiangtao},
title = {A Participant Recruitment Framework for Crowdsourcing Based Software Requirement Acquisition},
year = {2014},
isbn = {9781479943609},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/ICGSE.2014.26},
doi = {10.1109/ICGSE.2014.26},
abstract = {The opportunity to leverage crowd sourcing-based model to facilitate software requirements acquisition has been recognized to maximize the advantages of the diversity of talents and expertise available within the crowd. Identifying well-suited participants is a common issue in crowd sourcing system. Requirements acquisition tasks call for participants with particular kind of domain knowledge. However, current crowd sourcing system failed to provide such kind of identification among participants. We observed that participants with a particular kind of domain knowledge often have the opportunity to cluster in particular spatiotemporal spaces. Based on this observation, we propose a novel opportunistic participant recruitment framework to enable organizers to recruit participants with desired kind of domain knowledge in a more efficient way. We analyzed the feasibility of our opportunistic approach through both theoretic study on analytical model and simulated experiment on real world mobility model. The results showed the feasibility of our approach.},
booktitle = {Proceedings of the 2014 IEEE 9th International Conference on Global Software Engineering},
pages = {65–73},
numpages = {9},
keywords = {Crowdsourcing, Opportunistic Network},
series = {ICGSE '14}
}

@inproceedings{10.1145/1693042.1693093,
author = {Shah, Neeta and Dhanesha, Ashutosh and Seetharam, Dravida},
title = {Crowdsourcing for e-governance: case study},
year = {2009},
isbn = {9781605586632},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1693042.1693093},
doi = {10.1145/1693042.1693093},
abstract = {In this case study, we share our experience with creating and running a large scale initiative in the state of Gujarat in India wherein Crowdsourcing [1] was used to develop e-Governance applications. The case study provides details on the methodology used, lessons learnt, key success factors, challenges faced, and recommendations for future usage of crowdsourcing for building e-Governance applications.In this initiative, named INVITE, students were engaged in e-Governance application design and development for real project scenarios of various government departments. In a span of 2 years over 5000 students pursuing undergraduate and graduate courses from nearly 100 colleges and universities across the state participated. Over 500 faculty members also participated as project guides for the students who worked in teams of 3 or 4 members. Use of open source software tools was made mandatory. This community e-Governance initiative was supported by the three main pillars of the IT ecosystem - Government, Industry and Academia.A nodal IT agency of the state government documented 27 real world e-Governance project scenarios for various departments for the INVITE crowdsourcing experiment. Village kiosks for farmers, land records, animal husbandry, and museum administration are some of the areas for which the students developed applications. A programming contest was announced for attracting students to participate in the initiative.The state department of technical education provided impetus to INVITE by sending official circulars to heads of universities and colleges to encourage the students to take up the projects as part of their course requirements.An industry sponsor funded the basic infrastructure of a website for team registration and project submission, program management agency for mass communications, posters, and prizes for best applications for each project scenario. Provision was made to reward the faculty guides of winning teams and their institutes as well. As the student teams competed, the state government benefited by getting very good e-Governance applications developed for free by the crowdsourcing model.Various government departments nominated "resource persons" who interacted with the project teams to answer their queries about the prevailing manual systems and their vision of e-Governance for those functions.In summary, INVITE became a great platform for crowdsourcing resulting in "e-Governance for the people, by the people".},
booktitle = {Proceedings of the 3rd International Conference on Theory and Practice of Electronic Governance},
pages = {253–258},
numpages = {6},
keywords = {community, crowdsourcing, ecosystem, students},
location = {Bogota, Colombia},
series = {ICEGOV '09}
}

@inproceedings{10.1109/SOSE.2015.52,
author = {Assemi, Behrang and Schlagwein, Daniel and Safi, Hamid and Mesbah, Mahmoud},
title = {Crowdsourcing as a Method for the Collection of Revealed Preferences Data},
year = {2015},
isbn = {9781479983568},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/SOSE.2015.52},
doi = {10.1109/SOSE.2015.52},
abstract = {Crowdsourcing has been used widely for the collection of stated preference data (e.g., responses in a survey) by researchers. However, the use of crowdsourcing for collection of revealed preference data (e.g., real-life data collected in natural experiments) is much less common. The study reported in this short (research-in-progress) paper shows how crowdsourcing can be used as a method for the collection of revealed preference data in the context of transport studies. In transport studies, data is traditionally collected through surveys, diaries or simulations. Here, crowdsourcing could provide an alternative method that provides real-life data very fast and very cheap to researchers. To generate insights on crowdsourcing as an alternative data collection method, we use an open call on a crowdsourcing platform (Amazon Mechanical Turk - AMT), a mobile application (Advanced Travel Logging Application for Smartphones II - ATLAS II) and a participant survey to practically perform such a crowdsourced data collection and evaluate the effectiveness of the method. While the full study is still in progress, the initial results reported in this paper are promising and support the idea that crowdsourcing can indeed be used as an effective method for the collection of revealed preference data.},
booktitle = {Proceedings of the 2015 IEEE Symposium on Service-Oriented System Engineering},
pages = {378–382},
numpages = {5},
keywords = {Advanced Travel Logging Application for Smartphones II (ATLAS II), Amazon Mechanical Turk (AMT), Crowdsourcing, data collection, revealed preference data, transport studies},
series = {SOSE '15}
}

@inproceedings{10.1109/WCNC.2016.7564930,
author = {Lauridsen, Mads and Rodriguez, Ignacio and Mikkelsen, Lars M⊘ller and Gimenez, Lucas Chavarria and Mogensen, Preben},
title = {Verification of 3G and 4G received power measurements in a crowdsourcing Android app},
year = {2016},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/WCNC.2016.7564930},
doi = {10.1109/WCNC.2016.7564930},
abstract = {Many crowdsourcing Android applications are available for measuring network Key Performance Indicators such as received power, latency, and throughput. The data is useful for end-users, researchers, and Mobile Network Operators, but unfortunately the applications' accuracy are rarely verified. In this paper we verify the crowdsourcing Android application NetMap's ability to measure LTE Reference Signal Received Power by analyzing the Root Mean Squared Error, being 2–3 dB, and cross-correlation coefficient, being above 0.8, with measurements obtained by use of a professional radio network scanner and measurement phones. In addition, the application is applicable, but less accurate, for 3G Received Signal Code Power measurements. The studies are made for various device speeds and in different scenarios including indoor, urban, and highway, where the NetMap application is showed to perform well.},
booktitle = {2016 IEEE Wireless Communications and Networking Conference},
pages = {1–6},
numpages = {6},
location = {Doha, Qatar}
}

@inproceedings{10.1145/3240431.3240445,
author = {Alvarez, Julian},
title = {Datagame: Crowdsourcing, Metrics \&amp; Traces},
year = {2018},
isbn = {9781450364386},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3240431.3240445},
doi = {10.1145/3240431.3240445},
abstract = {This paper aims to present the concept of Datagame, a category of Serious Game associated with data exchange. After having exposed the concept of Datagame, presented subcategories and associated titles, reviewed the notion of crowdsourcing, metrics and traces, we will explore if such games relativize the unproductive criteria of Caillois and if direct benefits could be addressed to players.},
booktitle = {Proceedings of the 2nd International Conference on Web Studies},
pages = {72–76},
numpages = {5},
keywords = {Digital tools, digital methodology, tool criticism},
location = {Paris, France},
series = {WS.2 2018}
}

@inproceedings{10.1109/NBiS.2012.130,
author = {Korthaus, Axel and Dai, Wei},
title = {Crowdsourcing in Heterogeneous Networked Environments - Opportunities and Challenges},
year = {2012},
isbn = {9780769547794},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/NBiS.2012.130},
doi = {10.1109/NBiS.2012.130},
abstract = {Crowd sourcing has become a term used to describe a broad variety of effective approaches that engage potentially large and open crowds of participants for the undertaking of a task. The emergence of Heterogeneous Networks (Het Nets) that brings about a significant expansion of existing mobile network capacity will enable and accelerate new forms of mobile and ubiquitous crowd sourcing. This paper aims to explore some of the opportunities and challenges that will face crowd sourcing-based approaches in emerging heterogeneous networked environments. A conceptual architecture for a context-aware mobile crowd sourcing platform is proposed.},
booktitle = {Proceedings of the 2012 15th International Conference on Network-Based Information Systems},
pages = {483–488},
numpages = {6},
keywords = {crowdsourcing, crowdsourcing platform, heterogeneous networks, mobile crowdsourcing},
series = {NBIS '12}
}

@inproceedings{10.4108/icst.collaboratecom.2012.250434,
author = {Khazankin, R. and Satzger, B. and Dustdar, S.},
title = {Optimized execution of business processes on crowdsourcing platforms},
year = {2012},
isbn = {9781467327404},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.4108/icst.collaboratecom.2012.250434},
doi = {10.4108/icst.collaboratecom.2012.250434},
abstract = {Crowdsourcing in enterprises is a promising approach for organizing a flexible workforce. Recent developments show that the idea gains additional momentum. However, an obstacle for widespread adoption is the lack of an integrated way to execute business processes based on a crowdsourcing platform. The main difference compared to traditional approaches in business process execution is that tasks or activities cannot be directly assigned but are posted to the crowdsourcing platform, while people can choose deliberately which tasks to book and work on. In this paper we propose a framework for adaptive execution of business processes on top of a crowdsourcing platform. Based on historical data gathered by the platform we mine the booking behavior of people based on the nature and incentive of the crowdsourced tasks. Using the learned behavior model we derive an incentive management approach based on mathematical optimization that executes business processes in a cost-optimal way considering their deadlines. We evaluate our approach through simulations to prove the feasibility and effectiveness. The experiments verify our assumptions regarding the necessary ingredients of the approach and show the advantage of taking the booking behavior into account compared to the case when it is partially of fully neglected.},
booktitle = {Proceedings of the 2012 8th International Conference on Collaborative Computing: Networking, Applications and Worksharing (CollaborateCom 2012)},
pages = {443–451},
numpages = {9},
series = {COLLABORATECOM '12}
}

@inproceedings{10.1145/2567948.2567951,
author = {Jung, Hyun Joon},
title = {Quality assurance in crowdsourcing via matrix factorization based task routing},
year = {2014},
isbn = {9781450327459},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2567948.2567951},
doi = {10.1145/2567948.2567951},
abstract = {We investigate a method of crowdsourced task routing based on matrix factorization. From a preliminary analysis of a real crowdsourced data, we begin an exploration of how to route crowdsourcing task via Matrix factorization (MF) which efficiently estimate missing values in a worker-task matrix. Our preliminary results show the benefits of task routing over random assignment, the strength of probabilistic MF over baseline methods.},
booktitle = {Proceedings of the 23rd International Conference on World Wide Web},
pages = {3–8},
numpages = {6},
keywords = {collaborative filtering, crowdsourcing, matrix factorization, quality assurance, recommendation, task routing},
location = {Seoul, Korea},
series = {WWW '14 Companion}
}

@inproceedings{10.1145/3014087.3014112,
author = {Nikiforov, Alexander and Singireja, Anastasija},
title = {Open data and crowdsourcing perspectives for smart city in the United States and Russia},
year = {2016},
isbn = {9781450348591},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3014087.3014112},
doi = {10.1145/3014087.3014112},
abstract = {In this research paper we describe the transformation of open data strategy and implementation of crowdsourcing technologies for the city E-government services. Analysis of smart city projects provides the role of open data and crowdsourcing for smart city vision in United States and Russia. We define challenges and perspectives for collaboration of open data and crowdsourcing in smart city projects.},
booktitle = {Proceedings of the International Conference on Electronic Governance and Open Society: Challenges in Eurasia},
pages = {171–177},
numpages = {7},
keywords = {civic issue tracker, crowdsourcing, e-government, government 2.0, open data, open innovations, smart city},
location = {St. Petersburg, Russia},
series = {EGOSE '16}
}

@inproceedings{10.1145/1935826.1935828,
author = {Carvalho, Vitor R. and Lease, Matthew and Yilmaz, Emine},
title = {Crowdsourcing for search and data mining},
year = {2011},
isbn = {9781450304931},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1935826.1935828},
doi = {10.1145/1935826.1935828},
abstract = {The advent of crowdsourcing is revolutionizing data annotation, evaluation, and other traditionally manual-labor intensive processes by dramatically reducing the time, cost, and effort involved. This in turn is driving a disruptive shift in search and data mining methodology in areas such as: Evaluation: the Cranfield paradigm for search evaluation requires manually assessing document relevance to search queries. Recent work on stochastic evaluation has reduced but not removed this need for manual assessment.Supervised Learning: while traditional costs associated with data annotation have driven recent machine learning work (e.g. Learning to Rank) toward greater use of unsupervised and semi-supervised methods, the emergence of crowdsourcing has made labeled data far easier to acquire, thereby driving a potential resurgence in fully-supervised methods.Applications: Crowdsourcing has introduced exciting new opportunities to integrate human labor into automated systems: handling difficult cases where automation fails, exploiting the breadth of backgrounds, geographic dispersion, real-time response, etc.},
booktitle = {Proceedings of the Fourth ACM International Conference on Web Search and Data Mining},
pages = {5–6},
numpages = {2},
keywords = {crowdsourcing, data mining, search},
location = {Hong Kong, China},
series = {WSDM '11}
}

@inproceedings{10.1145/1629255.1629296,
author = {Jagadeesan, A. P. and Lynn, A. and Corney, J. R. and Yan, X. T. and Wenzel, J. and Sherlock, A. and Regli, W.},
title = {Geometric reasoning via internet CrowdSourcing},
year = {2009},
isbn = {9781605587110},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1629255.1629296},
doi = {10.1145/1629255.1629296},
abstract = {The ability to interpret and reason about shapes is a peculiarly human capability that has proven difficult to reproduce algorithmically. So despite the fact that geometric modeling technology has made significant advances in the representation, display and modification of shapes, there have only been incremental advances in geometric reasoning. For example, although today's CAD systems can confidently identify isolated cylindrical holes, they struggle with more ambiguous tasks such as the identification of partial symmetries or similarities in arbitrary geometries. Even well defined problems such as 2D shape nesting or 3D packing generally resist elegant solution and rely instead on brute force explorations of a subset of the many possible solutions.Identifying economic ways to solving such problems would result in significant productivity gains across a wide range of industrial applications. The authors hypothesize that Internet Crowdsourcing might provide a pragmatic way of removing many geometric reasoning bottlenecks.This paper reports the results of experiments conducted with Amazon's mTurk site and designed to determine the feasibility of using Internet Crowdsourcing to carry out geometric reasoning tasks as well as establish some benchmark data for the quality, speed and costs of using this approach.After describing the general architecture and terminology of the mTurk Crowdsourcing system, the paper details the implementation and results of the following three investigations; 1) the identification of "Canonical" viewpoints for individual shapes, 2) the quantification of "similarity" relationships with-in collections of 3D models and 3) the efficient packing of 2D Strips into rectangular areas. The paper concludes with a discussion of the possibilities and limitations of the approach.},
booktitle = {2009 SIAM/ACM Joint Conference on Geometric and Physical Modeling},
pages = {313–318},
numpages = {6},
keywords = {2D strip packing, 3D similarity, canonical view, crowdsourcing, geometric reasoning, mTurk, micro-outsourcing},
location = {San Francisco, California},
series = {SPM '09}
}

@inproceedings{10.5555/2634433.2635066,
author = {Tsai, Wei-Tek and Qi, Guanqiu},
title = {A Cloud-Based Platform for Crowdsourcing and Self-Organizing Learning},
year = {2014},
isbn = {9781479936168},
publisher = {IEEE Computer Society},
address = {USA},
abstract = {This paper explores the application of Self-Organizing Learning (SOL) to software crowdsourcing so that people can learn software design. SOL principles include communication, reflection, collaboration, community, creative tools, and amplification. Based on these principles, this project proposed a cloud-based environment to support people to learn software design based on crowdsourcing including crowdsourcing competitions.},
booktitle = {Proceedings of the 2014 IEEE 8th International Symposium on Service Oriented System Engineering},
pages = {454–458},
numpages = {5},
keywords = {Cloud, Crowdsourcing, SOL},
series = {SOSE '14}
}

@inproceedings{10.1145/2348283.2348530,
author = {Lease, Matthew and Alonso, Omar},
title = {Crowdsourcing for search evaluation and social-algorithmic search},
year = {2012},
isbn = {9781450314725},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2348283.2348530},
doi = {10.1145/2348283.2348530},
abstract = {The first computers were people. Today, Internet-based access to 24/7 online human crowds has led to a renaissance of research in human computation and the advent of crowdsourcing. These new opportunities have brought a disruptive shift to research and practice for how we build intelligent systems today. Not only can labeled data for training and evaluation be collected faster, cheaper, and easier than ever before, but we now see human computation being integrated into the systems themselves, operating in concert with automation. This tutorial introduces opportunities and challenges of human computation and crowdsourcing, particularly for search evaluation and developing hybrid search solutions that integrate human computation with traditional forms of automated search. We review methodology and findings of recent research and survey current generation crowdsourcing platforms now available, analyzing methods, potential, and limitations across platforms.},
booktitle = {Proceedings of the 35th International ACM SIGIR Conference on Research and Development in Information Retrieval},
pages = {1180},
numpages = {1},
keywords = {crowdsourcing, human computation},
location = {Portland, Oregon, USA},
series = {SIGIR '12}
}

@inproceedings{10.1145/2676440.2676441,
author = {Chang, Kyungmin and Han, Dongsoo},
title = {Crowdsourcing-based radio map update automation for wi-fi positioning systems},
year = {2014},
isbn = {9781450331333},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2676440.2676441},
doi = {10.1145/2676440.2676441},
abstract = {With the popularization of smartphones, the needs for indoor location information are rapidly growing these days. Wi-Fi based positioning technique has been widely used to provide positioning information indoors. In particular, fingerprint-based localization is preferred because of its advantage in accuracy. However, the accuracy of localization gradually degrades as the Wi-Fi environment changes. In order to prevent the accuracy degradation, a radio map, which stores the Wi-Fi environment information, should be updated to accommodate the changes. Recalibration is commonly used to update a radio map, but it usually requires considerable time and effort. In this paper, we propose a method that can update the radio map automatically by using fingerprints collected from numerous users. In order to tag the locations of the collected fingerprints more accurately, the data from various sensors such as accelerometer and gyroscope are used. The proposed method also uses optimization algorithms along with a filtering method to remove erroneous data. The evaluation results showed that the accuracy achieved by the method was comparable to that of manual calibration in spite of using user feedback data. This indicates that without recalibration, the fingerprint-based indoor positioning system can keep up its accuracy if it is used by many users.},
booktitle = {Proceedings of the 3rd ACM SIGSPATIAL International Workshop on Crowdsourced and Volunteered Geographic Information},
pages = {24–31},
numpages = {8},
keywords = {auto-update, crowdsourcing, fingerprinting, radio map, wi-fi-based positioning system},
location = {Dallas, Texas},
series = {GeoCrowd '14}
}

@inproceedings{10.1145/1873951.1874278,
author = {Snoek, Cees G.M. and Freiburg, Bauke and Oomen, Johan and Ordelman, Roeland},
title = {Crowdsourcing rock n' roll multimedia retrieval},
year = {2010},
isbn = {9781605589336},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1873951.1874278},
doi = {10.1145/1873951.1874278},
abstract = {In this technical demonstration, we showcase a multimedia search engine that facilitates semantic access to archival rock n' roll concert video. The key novelty is the crowdsourcing mechanism, which relies on online users to improve, extend, and share, automatically detected results in video fragments using an advanced timeline-based video player. The user-feedback serves as valuable input to further improve automated multimedia retrieval results, such as automatically detected concepts and automatically transcribed interviews. The search engine has been operational online to harvest valuable feedback from rock n' roll enthusiasts.},
booktitle = {Proceedings of the 18th ACM International Conference on Multimedia},
pages = {1535–1538},
numpages = {4},
keywords = {information visualization, semantic indexing, video retrieval},
location = {Firenze, Italy},
series = {MM '10}
}

@inproceedings{10.1145/1807342.1807376,
author = {Horton, John Joseph and Chilton, Lydia B.},
title = {The labor economics of paid crowdsourcing},
year = {2010},
isbn = {9781605588223},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1807342.1807376},
doi = {10.1145/1807342.1807376},
abstract = {We present a model of workers supplying labor to paid crowdsourcing projects. We also introduce a novel method for estimating a worker's reservation wage - the key parameter in our labor supply model. We tested our model by presenting experimental subjects with real-effort work scenarios that varied in the offered payment and difficulty. As predicted, subjects worked less when the pay was lower. However, they did not work less when the task was more time-consuming. Interestingly, at least some subjects appear to be "target earners," contrary to the assumptions of the rational model. The strongest evidence for target earning is an observed preference for earning total amounts evenly divisible by 5, presumably because these amounts make good targets. Despite its predictive failures, we calibrate our model with data pooled from both experiments. We find that the reservation wages of our sample are approximately log normally distributed, with a median wage of $1.38/hour. We discuss how to use our calibrated model in applications.},
booktitle = {Proceedings of the 11th ACM Conference on Electronic Commerce},
pages = {209–218},
numpages = {10},
keywords = {amazon's mechanical turk, crowdsourcing, human computation},
location = {Cambridge, Massachusetts, USA},
series = {EC '10}
}

@inproceedings{10.1007/978-3-642-40495-5_10,
author = {Lykourentzou, Ioanna and Vergados, Dimitrios J. and Papadaki, Katerina and Naudet, Yannick},
title = {Guided Crowdsourcing for Collective Work Coordination in Corporate Environments},
year = {2013},
isbn = {9783642404948},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-642-40495-5_10},
doi = {10.1007/978-3-642-40495-5_10},
abstract = {Crowdsourcing is increasingly gaining attention as one of the most promising forms of large-scale dynamic collective work. However current crowdsourcing approaches do not offer guarantees often demanded by consumers, for example regarding minimum quality, maximum cost or job accomplishment time. The problem appears to have a greater impact in corporate environments because in this case the above-mentioned performance guarantees directly affect its viability against competition. Guided crowdsourcing can be an alternative to overcome these issues. Guided crowdsourcing refers to the use of Artificial Intelligence methods to coordinate workers in crowdsourcing settings, in order to ensure collective performance goals such as quality, cost or time. In this paper, we investigate its potential and examine it on an evaluation setting tailored for intra and inter-corporate environments.},
booktitle = {Proceedings of the 5th International Conference on Computational Collective Intelligence. Technologies and Applications - Volume 8083},
pages = {90–99},
numpages = {10},
keywords = {crowd coordination, crowdsourcing, resource allocation},
location = {Craiova, Romania},
series = {ICCCI 2013}
}

@inproceedings{10.1109/IPDPSW.2013.133,
author = {Sistla, AnilKumar and Parde, Natalie and Patel, Krunalkumar and Mehta, Gayatri},
title = {Cross-Architectural Study of Custom Reconfigurable Devices Using Crowdsourcing},
year = {2013},
isbn = {9780769549798},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/IPDPSW.2013.133},
doi = {10.1109/IPDPSW.2013.133},
abstract = {Coarse grained reconfigurable architectures (CGRAs) are promising due to the ability to highly customize such architectures to an application domain. However, good tools and good algorithms to map benchmarks onto these architectures are needed to support design space exploration for CGRAs. In particular, the mapping problem has been difficult to solve in a satisfying and general way. In this paper, we present an architectural design flow using crowd sourcing to provide mappings of benchmarks onto new architectures. We show that the crowd can provide high quality, reliable mappings, outperforming our custom Simulated Annealing algorithm in 37 of 42 trials. We further show that the crowd can provide other types of feedback that are difficult to obtain from an automatic mapping algorithm. Our proof of concept cross-architectural study concludes that a mesh architecture with 8Way connectivity outperforms the other interconnection options tested. A stripe architecture with dedicated vertical routes (StripeDR) performs competitively as well.},
booktitle = {Proceedings of the 2013 IEEE 27th International Symposium on Parallel and Distributed Processing Workshops and PhD Forum},
pages = {222–230},
numpages = {9},
keywords = {coarse grained reconfigurable architectures, cross-architecture, crowdsourcing, design space exploration, domain specific reconfigurable architectures},
series = {IPDPSW '13}
}

@inproceedings{10.1145/2488388.2488490,
author = {Singla, Adish and Krause, Andreas},
title = {Truthful incentives in crowdsourcing tasks using regret minimization mechanisms},
year = {2013},
isbn = {9781450320351},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2488388.2488490},
doi = {10.1145/2488388.2488490},
abstract = {What price should be offered to a worker for a task in an online labor market? How can one enable workers to express the amount they desire to receive for the task completion? Designing optimal pricing policies and determining the right monetary incentives is central to maximizing requester's utility and workers' profits. Yet, current crowdsourcing platforms only offer a limited capability to the requester in designing the pricing policies and often rules of thumb are used to price tasks. This limitation could result in inefficient use of the requester's budget or workers becoming disinterested in the task.In this paper, we address these questions and present mechanisms using the approach of regret minimization in online learning. We exploit a link between procurement auctions and multi-armed bandits to design mechanisms that are budget feasible, achieve near-optimal utility for the requester, are incentive compatible (truthful) for workers and make minimal assumptions about the distribution of workers' true costs. Our main contribution is a novel, no-regret posted price mechanism, BP-UCB, for budgeted procurement in stochastic online settings. We prove strong theoretical guarantees about our mechanism, and extensively evaluate it in simulations as well as on real data from the Mechanical Turk platform. Compared to the state of the art, our approach leads to a 180\% increase in utility.},
booktitle = {Proceedings of the 22nd International Conference on World Wide Web},
pages = {1167–1178},
numpages = {12},
keywords = {crowdsourcing, incentive compatible mechanisms, multi-armed bandits, posted prices, procurement auctions, regret minimization},
location = {Rio de Janeiro, Brazil},
series = {WWW '13}
}

@inproceedings{10.1145/2030100.2030103,
author = {Mashhadi, Afra J. and Capra, Licia},
title = {Quality control for real-time ubiquitous crowdsourcing},
year = {2011},
isbn = {9781450309271},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2030100.2030103},
doi = {10.1145/2030100.2030103},
abstract = {Crowdsourcing has become a successful paradigm in the past decade, as Web 2.0 users have taken a more active role in producing content as well as consuming it. Recently this paradigm has broadened to incorporate ubiquitous applications, in which the smart-phone users contribute information about their surrounding, thus providing a collective knowledge about the physical world. However the acceptance and openness of such applications has made it easy to contribute poor quality content. Various solutions have been proposed for the Web-based domain, to assist with monitoring and filtering poor quality content, but these methods fall short when applied to ubiquitous crowdsourcing, where the task of collecting information has to be performed continuously and in real-time, by an always changing crowd. In this paper we discuss the challenges for quality control in ubiquitous crowdsorucing and propose a novel technique that reasons on users mobility patterns and quality of their past contributions to estimate user's credibility.},
booktitle = {Proceedings of the 2nd International Workshop on Ubiquitous Crowdsouring},
pages = {5–8},
numpages = {4},
keywords = {crowdsourcing, participation, quality assurance},
location = {Beijing, China},
series = {UbiCrowd '11}
}

@inproceedings{10.1145/2141512.2141572,
author = {Noble, Jennifer A.},
title = {Minority voices of crowdsourcing: why we should pay attention to every member of the crowd},
year = {2012},
isbn = {9781450310512},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2141512.2141572},
doi = {10.1145/2141512.2141572},
abstract = {In this paper I look at the dynamics of human behavior in crowds, focusing the role of non-normative voices in current crowdsourcing initiatives. I reiterate the idea that the power of the crowd lies not in the majority but in the collective. Many popular crowdsourcing platforms are designed to disregard outliers and only reward answers that agree with the masses. I use the example of the Long Tail in order to challenge developers to design more crowdsourcing tasks that take advantage of wide variance.},
booktitle = {Proceedings of the ACM 2012 Conference on Computer Supported Cooperative Work Companion},
pages = {179–182},
numpages = {4},
keywords = {business, crowd motivation, crowdsourcing, literature review, social science, the long tail, theory},
location = {Seattle, Washington, USA},
series = {CSCW '12}
}

@inproceedings{10.1109/WI-IAT.2012.104,
author = {Yu, Han and Shen, Zhiqi and Miao, Chunyan and An, Bo},
title = {Challenges and Opportunities for Trust Management in Crowdsourcing},
year = {2012},
isbn = {9780769548807},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/WI-IAT.2012.104},
doi = {10.1109/WI-IAT.2012.104},
abstract = {Crowd sourcing (CS) systems offer a new way for businesses and individuals to leverage on the power of mass collaboration to accomplish complex tasks in a divide-and-conquer manner. In existing CS systems, no facility has been provided for analyzing the trustworthiness of workers and providing decision support for allocating tasks to workers, which leads to high dependency of the quality of work on the behavior of workers in CS systems as shown in this paper. To address this problem, trust management mechanisms are urgently needed. Traditional trust management techniques are focused on identifying the most trustworthy service providers (SPs) as accurately as possible. Little thoughts were given to the question of how to utilize these SPs due to two common assumptions: 1) an SP can serve an unlimited number of requests in one time unit, and 2) a service consumer (SC) only needs to select one SP for interaction to complete a task. However, in CS systems, these two assumptions are no longer valid. Thus, existing models cannot be directly used for trust management in CS systems. This paper takes the first step towards a systematic investigation of trust management in CS systems by extending existing trust management models for CS trust management and conducting extensive experiments to study and analyze the performance of various trust management models in crowd sourcing. In this paper, the following key contributions are made. We 1) propose extensions to existing trust management approaches to enable them to operate in CS systems, 2) design a simulation test-bed based on the system characteristics of Amazon's Mechanical Turk (AMT) to make evaluation close to practical CS systems, 3) discuss the effect of incorporating trust management into CS system on the overall social welfare, and 4) identify the challenges and opportunities for future trust management research in CS systems.},
booktitle = {Proceedings of the The 2012 IEEE/WIC/ACM International Joint Conferences on Web Intelligence and Intelligent Agent Technology - Volume 02},
pages = {486–493},
numpages = {8},
keywords = {crowdsourcing, interaction decision making, reputation, trust},
series = {WI-IAT '12}
}

@proceedings{10.1145/2660114,
title = {CrowdMM '14: Proceedings of the 2014 International ACM Workshop on Crowdsourcing for Multimedia},
year = {2014},
isbn = {9781450331289},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {The power of crowds, leveraging a large number of human contributors and the capabilities of human computation, has enormous potential to address key challenges in the area of multimedia research. This power is, however, of difficult exploitation: challenges arise from the fact that a community of users or workers is a complex and dynamic system highly sensitive to changes in the form and the parameterization of their activities. Since 2012, the International ACM Workshop on Crowdsourcing for Multimedia, CrowdMM, has been the venue for collecting new insights on the effective deployment of crowdsourcing towards boosting Multimedia research.In its third edition, CrowdMM14 especially focuses on contributions that propose solutions for the key challenges that face widespread adoption of crowdsourcing paradigms in the multimedia research community. These include: identification of optimal crowd members (e.g., user expertise, worker reliability), providing effective explanations (i.e., good task design), controlling noise and quality in the results, designing incentive structures that do not breed cheating, adversarial environments, gathering necessary background information about crowd members without violating privacy, controlling descriptions of task.The call for papers attracted 26 international submissions (62\% increase with respect to the 2013 edition), three of which were short paper submissions. Of these, 8 were accepted as oral presentations and 5 as poster presentations. All papers received at least three double blind reviews, and 3.4 reviews on average.For a keynote talk, Nhatvi Nguyen (CEO of Microworkers) talks about Crowdsourcing Challenges from Platform Provider's Point of View. In addition to that CrowdMM features this year a crowd-sourced keynote, during which all CrowdMM14 authors give their view on the future and the Challenges that Crowdsourcing has still ahead.},
location = {Orlando, Florida, USA}
}

@inproceedings{10.1145/2393347.2396539,
author = {Chen, Kuan-Ta and Chu, Wei-Ta and Larson, Martha and Ooi, Wei Tsang},
title = {ACM multimedia 2012 workshop on crowdsourcing for multimedia},
year = {2012},
isbn = {9781450310895},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2393347.2396539},
doi = {10.1145/2393347.2396539},
abstract = {Crowdsourcing for multimedia involves exploiting both human intelligence and the combination of a large number of individual human contributions (i.e., the 'wisdom of the crowd') to develop techniques, systems and data sets that advance the state of the art. The ACM Multimedia 2012 Workshop on Crowdsourcing for Multimedia (CrowdMM 2012) provides a forum presenting crowdsourcing techniques for multimedia, as well as innovative ideas exemplifying how multimedia research can benefit from crowdsourcing. Through presented papers, invited talks and a panel, the workshop will promote interactive discussion on the scope and research potentials of crowdsourcing. The goal is to provide information to the multimedia research community on the principles of crowdsourcing and to inspire researchers to address the limitations of current studies by innovative use of human computation and collective intelligence. The workshop views crowdsourcing in the broad sense: it encompasses both unsolicited human contributions, e.g., tags assigned by users to images, and also solicited contributions, e.g., annotations gathered by making use of crowdsourcing platforms that micro-outsource tasks to a large pool of human workers.},
booktitle = {Proceedings of the 20th ACM International Conference on Multimedia},
pages = {1505–1506},
numpages = {2},
keywords = {crowdsourcing, human computing, multimedia system},
location = {Nara, Japan},
series = {MM '12}
}

@inproceedings{10.5555/2986459.2986749,
author = {Abernethy, Jacob and Frongillo, Rafael M.},
title = {A collaborative mechanism for crowdsourcing prediction problems},
year = {2011},
isbn = {9781618395993},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Machine Learning competitions such as the Netflix Prize have proven reasonably successful as a method of "crowdsourcing" prediction tasks. But these competitions have a number of weaknesses, particularly in the incentive structure they create for the participants. We propose a new approach, called a Crowdsourced Learning Mechanism, in which participants collaboratively "learn" a hypothesis for a given prediction task. The approach draws heavily from the concept of a prediction market, where traders bet on the likelihood of a future event. In our framework, the mechanism continues to publish the current hypothesis, and participants can modify this hypothesis by wagering on an update. The critical incentive property is that a participant will profit an amount that scales according to how much her update improves performance on a released test set.},
booktitle = {Proceedings of the 24th International Conference on Neural Information Processing Systems},
pages = {2600–2608},
numpages = {9},
location = {Granada, Spain},
series = {NIPS'11}
}

@inproceedings{10.4108/icst.collaboratecom.2012.250412,
author = {Westerski, A. and Iglesias, C. A. and Garcia, J. E.},
title = {Idea relationship analysis in open innovation crowdsourcing systems},
year = {2012},
isbn = {9781467327404},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.4108/icst.collaboratecom.2012.250412},
doi = {10.4108/icst.collaboratecom.2012.250412},
abstract = {Idea Management Systems are an implementation of open innovation notion in the Web environment with the use of crowdsourcing techniques. In this area, one of the popular methods for coping with large amounts of data is duplicate detection. With our research, we answer a question if there is room to introduce more relationship types and in what degree would this change affect the amount of idea metadata and its diversity. Furthermore, based on hierarchical dependencies between idea relationships and relationship transitivity we propose a number of methods for dataset summarization. To evaluate our hypotheses we annotate idea datasets with new relationships using the contemporary methods of Idea Management Systems to detect idea similarity. Having datasets with relationship annotations at our disposal, we determine if idea features not related to idea topic (e.g. innovation size) have any relation to how annotators perceive types of idea similarity or dissimilarity.},
booktitle = {Proceedings of the 2012 8th International Conference on Collaborative Computing: Networking, Applications and Worksharing (CollaborateCom 2012)},
pages = {289–296},
numpages = {8},
series = {COLLABORATECOM '12}
}

@inproceedings{10.1145/2393132.2393159,
author = {K\"{a}rkk\"{a}inen, Hannu and Jussila, Jari and Multasuo, Jani},
title = {Can crowdsourcing really be used in B2B innovation?},
year = {2012},
isbn = {9781450316378},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2393132.2393159},
doi = {10.1145/2393132.2393159},
abstract = {The aim of this research is to explore the use of crowdsourcing especially from business-to-business companies' innovation perspective, and to create a more comprehensive picture of the possibilities of crowdsourcing for companies operating in business-to-business markets. Business-to-business context was chosen because it is in many ways a very different environment for crowdsourcing than business-to-consumer context, and we found no academic studies on the topic. A systematic literature review was performed to gain an understanding of the state-of-the-art, and to create a research framework on the concept of crowdsourcing in innovation covering e.g. the type of crowdsourcing used, which crowds were used and in what innovation process phase crowds were utilized. Concerning the current ways of using 'crowds' and crowdsourcing in B2B innovation process, we found evidence of using crowdsourcing in B2B's in all the three innovation process phases: front-end, product development, and commercialization. Furthermore, evidence was found for crowdsourcing to be used in innovation mainly in the manner of crowd creation, crowd wisdom and crowd funding. We found that the role of social media was quite essential in all the found B2B crowdsourcing examples.},
booktitle = {Proceeding of the 16th International Academic MindTrek Conference},
pages = {134–141},
numpages = {8},
keywords = {business-to-business, crowdsourcing, innovation, social media},
location = {Tampere, Finland},
series = {MindTrek '12}
}

@inproceedings{10.1145/2461121.2461129,
author = {Cardonha, Carlos and Gallo, Diego and Avegliano, Priscilla and Herrmann, Ricardo and Koch, Fernando and Borger, Sergio},
title = {A crowdsourcing platform for the construction of accessibility maps},
year = {2013},
isbn = {9781450318440},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2461121.2461129},
doi = {10.1145/2461121.2461129},
abstract = {We present in this article a crowdsourcing platform that enables the collaborative creation of accessibility maps. The platform provides means for integration of different kind of data, collected automatically or with user intervention, to augment standard maps with accessibility information. The article shows the architecture of the platform, dedicating special attention to the smartphone applications we developed for data collection. The article also describes a preliminar experiment conducted on field, showing how the analysis of data produced by our solution can bring novel insights in accessibility challenges that can be found in cities.},
booktitle = {Proceedings of the 10th International Cross-Disciplinary Conference on Web Accessibility},
articleno = {26},
numpages = {4},
keywords = {accessibility, breadcrumb, citizen sensing, crowdsourcing, data collection, mobile, smarter cities},
location = {Rio de Janeiro, Brazil},
series = {W4A '13}
}

@inproceedings{10.1145/2018602.2018617,
author = {Bischof, Zachary S. and Otto, John S. and S\'{a}nchez, Mario A. and Rula, John P. and Choffnes, David R. and Bustamante, Fabi\'{a}n E.},
title = {Crowdsourcing ISP characterization to the network edge},
year = {2011},
isbn = {9781450308007},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2018602.2018617},
doi = {10.1145/2018602.2018617},
abstract = {Evaluating and characterizing Internet Service Providers (ISPs) is critical to subscribers shopping for alternative ISPs, companies providing reliable Internet services, and governments surveying the coverage of broadband services to its citizens. Ideally, ISP characterization should be done at scale, continuously, and from end users. While there has been significant progress toward this end, current approaches exhibit apparently unavoidable tradeoffs between coverage, continuous monitoring and capturing user-perceived performance.In this paper, we argue that network-intensive applications running on end systems avoid these tradeoffs, thereby offering an ideal platform for ISP characterization. Based on data collected from 500,000 peer-to-peer BitTorrent users across 3,150 networks, together with the reported results from the U.K. Ofcom/SamKnows studies, we show the feasibility of this approach to characterize the service that subscribers can expect from a particular ISP. We discuss remaining research challenges and design requirements for a solution that enables efficient and accurate ISP characterization at an Internet scale.},
booktitle = {Proceedings of the First ACM SIGCOMM Workshop on Measurements up the Stack},
pages = {61–66},
numpages = {6},
keywords = {broadband access networks, characterization, isp},
location = {Toronto, Ontario, Canada},
series = {W-MUST '11}
}

@inproceedings{10.1145/2523429.2532331,
author = {Jussila, Jari and Laine, Tom and Rautiainen, Mika and K\"{a}rkk\"{a}inen, Hannu and Ruohisto, Janne and Erkinheimo, Pia and Myhrberg, Markus},
title = {Future of crowdsourcing and value creation in different media environments},
year = {2013},
isbn = {9781450319928},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2523429.2532331},
doi = {10.1145/2523429.2532331},
abstract = {The focal theme of the panel is crowdsourcing and its future. Crowdsourcing is a relatively new concept, meaning broadly the act of outsourcing a job that is traditionally performed by e.g. an employee of a firm to an undefined, generally large group of people. Famous cases of crowdsourcing include Iron Sky the movie, crowdsourcing part of their fund raising and even parts of the actual movie making to movie fans; the intermediary firm InnoCentive offering the opportunity for other firms to crowdsource e.g. parts of their product development to crowds of people, and Canadian GoldCorp mining corporation crowdsourcing gold resource finding to both professionals and amateurs. Other crowdsourcing objectives include various tasks normally held within companies, such as marketing campaign design, product design, software testing, etc. The panel aims to provide fresh views for the opportunities of crowdsourcing from different angles, including various media environments and industry sectors, companies offering novel crowdsourcing services and platforms, as well as the viewpoint of value creation and business.Panel involves several experts on crowdsourcing, having experience from different fields, as well as business-oriented and academic-oriented moderators.},
booktitle = {Proceedings of International Conference on Making Sense of Converging Media},
pages = {339–340},
numpages = {2},
keywords = {Crowdfunding, Crowdsourcing, Value Creation},
location = {Tampere, Finland},
series = {AcademicMindTrek '13}
}

@inproceedings{10.1007/978-3-642-25535-9_20,
author = {Khazankin, Roman and Psaier, Harald and Schall, Daniel and Dustdar, Schahram},
title = {QoS-Based task scheduling in crowdsourcing environments},
year = {2011},
isbn = {9783642255342},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-642-25535-9_20},
doi = {10.1007/978-3-642-25535-9_20},
abstract = {Crowdsourcing has emerged as an important paradigm in human-problem solving techniques on the Web. One application of crowdsourcing is to outsource certain tasks to the crowd that are difficult to implement as solutions based on software services only. Another benefit of crowdsourcing is the on-demand allocation of a flexible workforce. Businesses may outsource certain tasks to the crowd based on workload variations. The paper addresses the monitoring of crowd members' characteristics and the effective use of monitored data to improve the quality of work. Here we propose the extensions of standards such as Web Service Level Agreement (WSLA) to settle quality guarantees between crowd consumers and the crowdsourcing platform. Based on negotiated agreements, we provide a skill-based crowd scheduling algorithm. We evaluate our approach through simulations.},
booktitle = {Proceedings of the 9th International Conference on Service-Oriented Computing},
pages = {297–311},
numpages = {15},
keywords = {QoS agreements, crowdsourcing, scheduling, skill monitoring},
location = {Paphos, Cyprus},
series = {ICSOC'11}
}

@inproceedings{10.1145/1969289.1969318,
author = {Wald, M.},
title = {Crowdsourcing correction of speech recognition captioning errors},
year = {2011},
isbn = {9781450304764},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1969289.1969318},
doi = {10.1145/1969289.1969318},
abstract = {In this paper, we describe a tool that facilitates crowdsourcing correction of speech recognition captioning errors to provide a sustainable method of making videos accessible to people who find it difficult to understand speech through hearing alone.},
booktitle = {Proceedings of the International Cross-Disciplinary Conference on Web Accessibility},
articleno = {22},
numpages = {2},
keywords = {accessibility},
location = {Hyderabad, Andhra Pradesh, India},
series = {W4A '11}
}

@inproceedings{10.1109/HICSS.2014.179,
author = {Fichman, Pnina and Hara, Noriko and Rosenbaum, Howard},
title = {Introduction to Crowdsourcing Content Production and Online Knowledge Repositories Minitrack},
year = {2014},
isbn = {9781479925049},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/HICSS.2014.179},
doi = {10.1109/HICSS.2014.179},
abstract = {As various forms of collaboration are enabled (and constrained) by the affordances available in social media, researchers are investigating a range of issues including: 1) the diverse ways in which people collaborate to create, manage, curate and manipulate online content and how these activities affect digital repositories, 2) how those who manage these repositories are responding to the co-creation of online content 3) the dynamics of crowd sourced online collaborations and online communities of practice, and 4) the ways in which we can best describe the socio-technical interaction networks that facilitate and inhibit mass knowledge production. This minitrack focuses on online interactions for knowledge production on crowd sourced sites.},
booktitle = {Proceedings of the 2014 47th Hawaii International Conference on System Sciences},
pages = {1385},
series = {HICSS '14}
}

@inproceedings{10.5555/2050728.2050764,
author = {Sautter, Guido and B\"{o}hm, Klemens},
title = {High-throughput crowdsourcing mechanisms for complex tasks},
year = {2011},
isbn = {9783642247033},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {Crowdsourcing is popular for large-scale data processing endeavors that require human input. However, working with a large community of users raises new challenges. In particular, both possible misjudgment and dishonesty threaten the quality of the results. Common countermeasures are based on redundancy, giving way to a tradeoff between result quality and throughput. Ideally, measures should (1) maintain high throughput and (2) ensure high result quality at the same time. Existing work on crowdsourcing mostly focuses on result quality, paying little attention to throughput or even to that tradeoff. One reason is that the number of tasks (individual atomic units of work) is usually small. A further problem is that the tasks users work on are small as well. In consequence, existing result-improvement mechanisms do not scale to the number or complexity of tasks that arise, for instance, in proofreading and processing of digitized legacy literature. This paper proposes novel resultimprovement mechanisms that (1) are independent of the size and complexity of tasks and (2) allow to trade result quality for throughput to a significant extent. Both mathematical analyses and extensive simulations show the effectiveness of the proposed mechanisms.},
booktitle = {Proceedings of the Third International Conference on Social Informatics},
pages = {240–254},
numpages = {15},
keywords = {crowdsourcing, data quality, throughput},
location = {Singapore},
series = {SocInfo'11}
}

@inproceedings{10.1145/2818346.2820748,
author = {Misu, Teruhisa},
title = {Visual Saliency and Crowdsourcing-based Priors for an In-car Situated Dialog System},
year = {2015},
isbn = {9781450339124},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2818346.2820748},
doi = {10.1145/2818346.2820748},
abstract = {This paper addresses issues in situated language understanding in a moving car. We propose a reference resolution method to identify user queries about specific target objects in their surroundings. We investigate methods of predicting which target object is likely to be queried given a visual scene and what kind of linguistic cues users naturally provide to describe a given target object in a situated environment. We propose methods to incorporate the visual saliency of the visual scene as a prior. Crowdsourced statistics of how people describe an object are also used as a prior. We have collected situated utterances from drivers using our research system, which was embedded in a real vehicle. We demonstrate that the proposed algorithms improve target identification rate by 15.1\%.},
booktitle = {Proceedings of the 2015 ACM on International Conference on Multimodal Interaction},
pages = {75–82},
numpages = {8},
keywords = {crowdsourcing, in-car interaction, multimodal interaction, situated dialog, visual saliency},
location = {Seattle, Washington, USA},
series = {ICMI '15}
}

@inproceedings{10.1007/978-3-642-39402-7_20,
author = {Donath, Axel and Kondermann, Daniel},
title = {Is crowdsourcing for optical flow ground truth generation feasible?},
year = {2013},
isbn = {9783642394010},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-642-39402-7_20},
doi = {10.1007/978-3-642-39402-7_20},
abstract = {In 2012, three new optical flow reference datasets have been published, two of them containing ground truth [1,2,3]. None of them contains ground truth for real-world, large-scale outdoor scenes with dynamically and independently moving objects. The reason is that no measurement devices exists to record such data with sufficiently high accuracy. Yet, ground truth is needed to assess the safety of e.g. driver assistance systems. To close this gap, based on existing, accurate ground truth, we analyse the performance of uninformed human motion annotators. Feature annotation bias and non-rigid motions are a major concern, limiting our results to pixel-accuracy. Our approach is the only way to create ground truth for dynamic outdoor sequences and feasible whenever pixel-accuracy is sufficient for performance analysis and piecewise rigid motions dominate the scene. Finally, we show that our approach is highly effective with respect to annotation cost per frame compared to our baseline method [4].},
booktitle = {Proceedings of the 9th International Conference on Computer Vision Systems},
pages = {193–202},
numpages = {10},
location = {St. Petersburg, Russia},
series = {ICVS'13}
}

@inproceedings{10.1109/CBI.2013.66,
author = {Tsaplin, Evgeny and Bushelenkova, Svetlana and Puchkova, Alexandra},
title = {Crowdsourcing in Telework as a New Scalable Business Model},
year = {2013},
isbn = {9780769550725},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/CBI.2013.66},
doi = {10.1109/CBI.2013.66},
abstract = {Business processes in our modern world flow with greater speed. This actual trend influences on the business from both sides: positive and negative. Some approaches, such as web 3.0, crowdsourcing have intention to create a comfortable ecosystem for business. In this article, we consider how appropriate data management helps building successful business.},
booktitle = {Proceedings of the 2013 IEEE 15th Conference on Business Informatics},
pages = {412–415},
numpages = {4},
keywords = {crowdsourcing, human recourse management, microwork, scalable business, semantic web, telemarketing, telework, web 3.0, web mining},
series = {CBI '13}
}

@inproceedings{10.1145/3557991.3567779,
author = {Luedemann, Kai and Nascimento, Mario A.},
title = {BikeVibes: An app for crowdsourcing open road quality data from a cyclist perspective},
year = {2022},
isbn = {9781450395397},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3557991.3567779},
doi = {10.1145/3557991.3567779},
abstract = {This paper presents BikeVibes1, an app that cyclists can use to log data regarding the smoothness of their rides. The main goal of BikeVibes is to facilitate the collection of anonymized open data about road quality that others can download and peruse. A few sample scenarios where having this type of crowdsourced data would be useful are as follows. A city can use the gathered data in order to determine which roads need to be maintained/upgraded since the quality of the road can be perceived very differently when riding a bike compared to driving a car. Likewise, a city can determine paths that are more frequently used by cyclists in order to decide where to build or upgrade dedicated bike lanes and/or how to prioritize maintenance. Also, third-party app developers can use the road quality data to suggest paths to cyclists based on smoothness, as this may be an important attribute for some people, e.g., in the case of parents riding bicycles hauling trailers with children. None of these scenarios could be easily contemplated without the availability of data such as that gathered through BikeVibes.},
booktitle = {Proceedings of the 15th ACM SIGSPATIAL International Workshop on Computational Transportation Science},
articleno = {8},
numpages = {4},
keywords = {crowdsensing, mobile app, open cycling data, road quality monitoring},
location = {Seattle, Washington},
series = {IWCTS '22}
}

@inproceedings{10.5555/2045274.2045277,
author = {Alonso, Omar},
title = {Crowdsourcing for information retrieval experimentation and evaluation},
year = {2011},
isbn = {9783642237072},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {Very recently, crowdsourcing has emerged as a viable alternative for conducting different types of experiments in a wide range of areas. Generally speaking and in the context of IR, crowdsourcing involves outsourcing tasks to a large group of people instead of assigning such tasks to an employee or editor. The availability of commercial crowdsourcing platforms offers vast access to an on-demand workforce. This new approach makes possible to conduct experiments extremely fast, with good results at a low cost. However, like in any experiment, there are several implementation details that would make an experiment work or fail. For large scale evaluation, deployment in practice is not that simple. Tasks have to be designed carefully with special emphasis on the user interface, instructions, content, and quality control.In this invited talk, I will explore some directions that may influence the outcome of a task and I will present a framework for conducting crowdsourcing experiments making some emphasis on a number of aspects that should be of importance for all sorts of IR-like tasks. Finally, I will outline research trends around human computation that promise to make this emerging field even more interesting in the near future.},
booktitle = {Proceedings of the Second International Conference on Multilingual and Multimodal Information Access Evaluation},
pages = {2},
numpages = {1},
location = {Amsterdam, The Netherlands},
series = {CLEF'11}
}

@inproceedings{10.1007/978-3-030-29387-1_25,
author = {Skorupska, Kinga and N\'{u}\~{n}ez, Manuel and Kope\'{c}, Wies\l{}aw and Nielek, Rados\l{}aw},
title = {A Comparative Study of Younger and Older Adults’ Interaction with a Crowdsourcing Android TV App for Detecting Errors in TEDx Video Subtitles},
year = {2019},
isbn = {978-3-030-29386-4},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-29387-1_25},
doi = {10.1007/978-3-030-29387-1_25},
abstract = {In this paper we report the results of a pilot study comparing the older and younger adults’ interaction with an Android TV application which enables users to detect errors in video subtitles. Overall, the interaction with the TV-mediated crowdsourcing system relying on language proficiency was seen as intuitive, fun and accessible, but also cognitively demanding; more so for younger adults who focused on the task of detecting errors, than for older adults who concentrated more on the meaning and edutainment aspect of the videos. We also discuss participants’ motivations and preliminary recommendations for the design of TV-enabled crowdsourcing tasks and subtitle QA systems.},
booktitle = {Human-Computer Interaction – INTERACT 2019: 17th IFIP TC 13 International Conference, Paphos, Cyprus, September 2–6, 2019, Proceedings, Part III},
pages = {455–464},
numpages = {10},
keywords = {Crowdsourcing, Smart TV, Android TV, Design evaluation, Subtitles, Older adults, Younger adults},
location = {Paphos, Cyprus}
}

@inproceedings{10.1145/1816123.1816143,
author = {Eckert, Kai and Niepert, Mathias and Niemann, Christof and Buckner, Cameron and Allen, Colin and Stuckenschmidt, Heiner},
title = {Crowdsourcing the assembly of concept hierarchies},
year = {2010},
isbn = {9781450300858},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1816123.1816143},
doi = {10.1145/1816123.1816143},
abstract = {The "wisdom of crowds" is accomplishing tasks that are cumbersome for individuals yet cannot be fully automated by means of specialized computer algorithms. One such task is the construction of thesauri and other types of concept hierarchies. Human expert feedback on the relatedness and relative generality of terms, however, can be aggregated to dynamically construct evolving concept hierarchies. The InPhO (Indiana Philosophy Ontology) project bootstraps feedback from volunteer users unskilled in ontology design into a precise representation of a specific domain. The approach combines statistical text processing methods with expert feedback and logic programming to create a dynamic semantic representation of the discipline of philosophy.In this paper, we show that results of comparable quality can be achieved by leveraging the workforce of crowdsourcing services such as the Amazon Mechanical Turk (AMT). In an extensive empirical study, we compare the feedback obtained from AMT's workers with that from the InPhO volunteer users providing an insight into qualitative differences of the two groups. Furthermore, we present a set of strategies for assessing the quality of different users when gold standards are missing. We finally use these methods to construct a concept hierarchy based on the feedback acquired from AMT workers.},
booktitle = {Proceedings of the 10th Annual Joint Conference on Digital Libraries},
pages = {139–148},
numpages = {10},
keywords = {crowdsourcing, similarity, thesaurus learning},
location = {Gold Coast, Queensland, Australia},
series = {JCDL '10}
}

@inproceedings{10.5555/2040283.2040295,
author = {Satzger, Benjamin and Psaier, Harald and Schall, Daniel and Dustdar, Schahram},
title = {Stimulating skill evolution in market-based crowdsourcing},
year = {2011},
isbn = {9783642230585},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {Crowdsourcing has emerged as an important paradigm in human problem-solving techniques on the Web. One application of crowdsourcing is to outsource certain tasks to the crowd that are difficult to implement in software. Another potential benefit of crowdsourcing is the on-demand allocation of a flexible workforce. Businesses may outsource tasks to the crowd based on temporary workload variations. A major challenge in crowdsourcing is to guarantee high-quality processing of tasks. We present a novel crowdsourcing marketplace that matches tasks to suitable workers based on auctions. The key to ensuring high quality lies in skilled members whose capabilities can be estimated correctly. We present a novel auction mechanism for skill evolution that helps to correctly estimate workers and to evolve skills that are needed. Evaluations show that this leads to improved crowdsourcing.},
booktitle = {Proceedings of the 9th International Conference on Business Process Management},
pages = {66–82},
numpages = {17},
keywords = {auctions, crowdsourcing, human-centric BPM, online communities, skill evolution, task markets},
location = {Clermont-Ferrand, France},
series = {BPM'11}
}

@inproceedings{10.1109/ASONAM.2012.193,
author = {Brambilla, Marco and Bozzon, Alessandro},
title = {Web Data Management through Crowdsourcing Upon Social Networks},
year = {2012},
isbn = {9780769547992},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/ASONAM.2012.193},
doi = {10.1109/ASONAM.2012.193},
abstract = {Retrieval and management of Web data is becoming a more and more complex problem, due to the amount of information to be dealt with, to the diversity of the information sources and of the data formats, and to the evolving expectations of users. In particular, some tasks such as quality assessment, opinion making, and sense extraction cannot be completely delegated to automatic procedures. More and more users are increasingly relying on social interaction to complete and validate the results of their online activities. For instance, scouting "interesting" results, or suggesting new, unexpected search directions in information seeking processes occurs in most times aside of the search systems and processes, possibly instrumented and mediated by a social network. In this paper we propose paradigm that embodies crowds and social network communities as first-class sources for the information management and extraction on the Web. Our approach aims at filling the gap between traditional Web systems (CMS, search engines and others), which operate upon world-wide information, with social systems, capable of interacting with real people, in real time, to capture their opinions, suggestions, and emotions by leveraging crowd sourcing practices and making them viable upon a social network. This enormously enriches the data manipulation experience for the user can be enormously enriched.},
booktitle = {Proceedings of the 2012 International Conference on Advances in Social Networks Analysis and Mining (ASONAM 2012)},
pages = {1123–1127},
numpages = {5},
keywords = {Communities, Data models, Engines, Facebook, Humans, Object oriented modeling, Social network, Web information system, crowdsourcing, semantic Web},
series = {ASONAM '12}
}

@inproceedings{10.1145/2390803.2390812,
author = {Avlonitis, Markos and Chorianopoulos, Konstantinos and Shamma, David Ayman},
title = {Crowdsourcing user interactions within web video through pulse modeling},
year = {2012},
isbn = {9781450315890},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2390803.2390812},
doi = {10.1145/2390803.2390812},
abstract = {Semantic video research has employed crowdsourcing techniques on social web video data sets such as comments, tags, and annotations, but these data sets require an extra effort on behalf of the user. We propose a pulse modeling method, which analyzes implicit user interactions within web video, such as rewind. In particular, we have modeled the user information seeking behavior as a time series and the semantic regions as a discrete pulse of fixed width. We constructed these pulses from user interactions with a documentary video that has a very rich visual style with too many cuts and camera angles/frames for the same scene. Next, we calculated the correlation coefficient between dynamically detected user pulses at the local maximums and the reference pulse. We have found when people are actively seeking for information in a video, their activity (these pulses) significantly matches the semantics of the video. This proposed pulse analysis method complements previous work in content-based information retrieval and provides an additional user-based dimension for modeling the semantics of a web video.},
booktitle = {Proceedings of the ACM Multimedia 2012 Workshop on Crowdsourcing for Multimedia},
pages = {19–20},
numpages = {2},
keywords = {implicit, interaction, pragmatics, user activity, video},
location = {Nara, Japan},
series = {CrowdMM '12}
}

@inproceedings{10.1145/2463728.2463814,
author = {Burov, Vasiliy and Patarakin, Evgeny and Yarmakhov, Boris},
title = {A crowdsourcing model for public consultations on draft laws},
year = {2012},
isbn = {9781450312004},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2463728.2463814},
doi = {10.1145/2463728.2463814},
abstract = {The paper discusses an innovative approach to lawmaking. In the proposed model a draft law is split into segments and improved by a network community which members can vote for the segments and suggest their own versions. Several cases of public consultations of Russian Laws based on Wikivote approach are presented and analyzed.},
booktitle = {Proceedings of the 6th International Conference on Theory and Practice of Electronic Governance},
pages = {450–451},
numpages = {2},
keywords = {collaboration, crowdsourcing, lawmaking, wiki},
location = {Albany, New York, USA},
series = {ICEGOV '12}
}

@inproceedings{10.5555/2484920.2485052,
author = {Venanzi, Matteo and Rogers, Alex and Jennings, Nicholas R.},
title = {Trust-based fusion of untrustworthy information in crowdsourcing applications},
year = {2013},
isbn = {9781450319935},
publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
address = {Richland, SC},
abstract = {In this paper, we address the problem of fusing untrustworthy reports provided from a crowd of observers, while simultaneously learning the trustworthiness of individuals. To achieve this, we construct a likelihood model of the users's trustworthiness by scaling the uncertainty of its multiple estimates with trustworthiness parameters. We incorporate our trust model into a fusion method that merges estimates based on the trust parameters and we provide an inference algorithm that jointly computes the fused output and the individual trustworthiness of the users based on the maximum likelihood framework. We apply our algorithm to cell tower local- isation using real-world data from the OpenSignal project and we show that it outperforms the state-of-the-art methods in both accuracy, by up to 21\%, and consistency, by up to 50\% of its predictions.},
booktitle = {Proceedings of the 2013 International Conference on Autonomous Agents and Multi-Agent Systems},
pages = {829–836},
numpages = {8},
keywords = {crowdsourcing, data fusion, information trustworthiness},
location = {St. Paul, MN, USA},
series = {AAMAS '13}
}

@inproceedings{10.1145/1566374.1566392,
author = {DiPalantino, Dominic and Vojnovic, Milan},
title = {Crowdsourcing and all-pay auctions},
year = {2009},
isbn = {9781605584584},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1566374.1566392},
doi = {10.1145/1566374.1566392},
abstract = {In this paper we present and analyze a model in which users select among, and subsequently compete in, a collection of contests offering various rewards. The objective is to capture the essential features of a crowdsourcing system, an environment in which diverse tasks are presented to a large community. We aim to demonstrate the precise relationship between incentives and participation in such systems.We model contests as all-pay auctions with incomplete information; as a consequence of revenue equivalence, our model may also be interpreted more broadly as one in which users select among auctions of heterogeneous goods. We present two regimes in which we find an explicit correspondence in equilibrium between the offered rewards and the users' participation levels. The regimes respectively model situations in which different contests require similar or unrelated skills. Principally, we find that rewards yield logarithmically diminishing returns with respect to participation levels. We compare these results to empirical data from the crowdsourcing site Taskcn.com; we find that as we condition the data on more experienced users, the model more closely conforms to the empirical data.},
booktitle = {Proceedings of the 10th ACM Conference on Electronic Commerce},
pages = {119–128},
numpages = {10},
keywords = {all-pay auctions, contests, crowdsourcing, game theory},
location = {Stanford, California, USA},
series = {EC '09}
}

@inproceedings{10.5555/2908738.2908747,
author = {Viappiani, Paolo and Zilles, Sandra and Hamilton, Howard J. and Boutilier, Craig},
title = {A Bayesian concept learning approach to crowdsourcing},
year = {2011},
publisher = {AAAI Press},
abstract = {We develop a Bayesian approach to concept learning for crowdsourcing applications. A probabilistic belief over possible concept definitions is maintained and updated according to (noisy) observations from experts, whose behaviors are modeled using discrete types. We propose recommendation techniques, inference methods, and query selection strategies to assist a user charged with choosing a configuration that satisfies some (partially known) concept. Our model is able to simultaneously learn the concept definition and the types of the experts. We evaluate our model with simulations, showing that our Bayesian strategies are effective even in large concept spaces with many uninformative experts.},
booktitle = {Proceedings of the 13th AAAI Conference on Interactive Decision Theory and Game Theory},
pages = {60–67},
numpages = {8},
series = {AAAIWS'11-13}
}

@inproceedings{10.1145/2677832.2677847,
author = {Lin, Zeqi and Zhao, Junfeng and Xie, Bing},
title = {A graph database based crowdsourcing infrastructure for modelling and searching code structure},
year = {2014},
isbn = {9781450333030},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2677832.2677847},
doi = {10.1145/2677832.2677847},
abstract = {Software reuse offers a solution to eliminate repeated work and improve efficiency and quality in the software development. In order to reuse existing software resources, software developers usually need to understand code structure of them. However, code structure is usually too complex to figure out. Therefore, it is helpful to demonstrate software developers the code structure they want to know. This paper presents a graph database based crowdsourcing infrastructure for modelling and searching code structure. In this paper, a graph based modelling paradigm of code structure is provided, which solves the problem that how code structure should be demonstrated. Software developers' search purposes are analyzed by natural language processing technique. A crowdsourcing mechanism is provided to integrate different code structure analysis algorithms for these different search purposes. Our work improves the efficiency of software reuse, and it is validated through an industrial case study.},
booktitle = {Proceedings of the 6th Asia-Pacific Symposium on Internetware},
pages = {15–24},
numpages = {10},
keywords = {Code Structure, Crowdsourcing, Graph Database, Software Reuse},
location = {Hong Kong, China},
series = {Internetware '14}
}

@inproceedings{10.1109/ICECCS.2011.34,
author = {Skopik, Florian and Schall, Daniel and Dustdar, Schahram},
title = {Computational Social Network Management in Crowdsourcing Environments},
year = {2011},
isbn = {9780769543819},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/ICECCS.2011.34},
doi = {10.1109/ICECCS.2011.34},
abstract = {Flexible interactions in complex social and service-oriented collaboration systems increasingly demand for automated adaptation techniques to optimize partner discovery and selection. Today, applications of complex service-oriented systems can be found in crowd sourcing environments. In such environments, collaborations are typically short-lived and strongly influenced by incentives and actor behavior. As actors prove their reliable and dependable behavior in jointly performed activities, they become increasingly considered as invaluable partners. A social network builds a strong basis to enable successful collaborations between crowd members. In order to keep track of the dynamics in such systems, it is inevitable to apply an autonomous approach to manage social network structures automatically using captured interaction data. Thus, we introduce an adaptation concept that accounts for emerging social relations based on varying interaction behavior of collaboration partners. We describe the foundational concepts for dynamic social link management in Web-based collaborations. We highlight major concerns of computational models in highly dynamic networks and deal with temporal aspects such as supporting the emergence of relations, efficient update mechanisms, and aging of relations.},
booktitle = {Proceedings of the 2011 16th IEEE International Conference on Engineering of Complex Computer Systems},
pages = {273–282},
numpages = {10},
keywords = {computational social network management, emergence of social relations, service-oriented crowdsourcing},
series = {ICECCS '11}
}

@inproceedings{10.1145/1851182.1851228,
author = {Choffnes, David R. and Bustamante, Fabi\'{a}n E. and Ge, Zihui},
title = {Crowdsourcing service-level network event monitoring},
year = {2010},
isbn = {9781450302012},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1851182.1851228},
doi = {10.1145/1851182.1851228},
abstract = {The user experience for networked applications is becoming a key benchmark for customers and network providers. Perceived user experience is largely determined by the frequency, duration and severity of network events that impact a service. While today's networks implement sophisticated infrastructure that issues alarms for most failures, there remains a class of silent outages (e.g., caused by configuration errors) that are not detected. Further, existing alarms provide little information to help operators understand the impact of network events on services. Attempts to address this through infrastructure that monitors end-to-end performance for customers have been hampered by the cost of deployment and by the volume of data generated by these solutions.We present an alternative approach that pushes monitoring to applications on end systems and uses their collective view to detect network events and their impact on services - an approach we call Crowdsourcing Event Monitoring (CEM). This paper presents a general framework for CEM systems and demonstrates its effectiveness for a P2P application using a large dataset gathered from BitTorrent users and confirmed network events from two ISPs. We discuss how we designed and deployed a prototype CEM implementation as an extension to BitTorrent. This system performs online service-level network event detection through passive monitoring and correlation of performance in end-users' applications.},
booktitle = {Proceedings of the ACM SIGCOMM 2010 Conference},
pages = {387–398},
numpages = {12},
keywords = {P2P, anomaly detection, crowdsourcing, service-level network events},
location = {New Delhi, India},
series = {SIGCOMM '10}
}

@article{10.1145/1851275.1851228,
author = {Choffnes, David R. and Bustamante, Fabi\'{a}n E. and Ge, Zihui},
title = {Crowdsourcing service-level network event monitoring},
year = {2010},
issue_date = {October 2010},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {40},
number = {4},
issn = {0146-4833},
url = {https://doi.org/10.1145/1851275.1851228},
doi = {10.1145/1851275.1851228},
abstract = {The user experience for networked applications is becoming a key benchmark for customers and network providers. Perceived user experience is largely determined by the frequency, duration and severity of network events that impact a service. While today's networks implement sophisticated infrastructure that issues alarms for most failures, there remains a class of silent outages (e.g., caused by configuration errors) that are not detected. Further, existing alarms provide little information to help operators understand the impact of network events on services. Attempts to address this through infrastructure that monitors end-to-end performance for customers have been hampered by the cost of deployment and by the volume of data generated by these solutions.We present an alternative approach that pushes monitoring to applications on end systems and uses their collective view to detect network events and their impact on services - an approach we call Crowdsourcing Event Monitoring (CEM). This paper presents a general framework for CEM systems and demonstrates its effectiveness for a P2P application using a large dataset gathered from BitTorrent users and confirmed network events from two ISPs. We discuss how we designed and deployed a prototype CEM implementation as an extension to BitTorrent. This system performs online service-level network event detection through passive monitoring and correlation of performance in end-users' applications.},
journal = {SIGCOMM Comput. Commun. Rev.},
month = aug,
pages = {387–398},
numpages = {12},
keywords = {P2P, anomaly detection, crowdsourcing, service-level network events}
}

@inproceedings{10.1109/SCC.2014.12,
author = {Sharifi, Mahdi and Manaf, Azizah Abdul and Memariani, Ali and Movahednejad, Homa and Dastjerdi, Amir Vahid},
title = {Consensus-Based Service Selection Using Crowdsourcing Under Fuzzy Preferences of Users},
year = {2014},
isbn = {9781479950669},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/SCC.2014.12},
doi = {10.1109/SCC.2014.12},
abstract = {Different evaluator entities, either human agents (e.g., experts) or software agents (e.g., monitoring services), are involved in the assessment of QoS parameters of candidate services, which leads to diversity in service assessments. This diversity makes the service selection a challenging task, especially when numerous qualities of service criteria and range of providers are considered. To address this problem, this study first presents a consensus-based service assessment methodology that utilizes consensus theory to evaluate the service behavior for single QoS criteria using the power of crowdsourcing. To this end, trust level metrics are introduced to measure the strength of a consensus based on the trustworthiness levels of crowd members. The peers converged to the most trustworthy evaluation. Next, the fuzzy inference engine was used to aggregate each obtained assessed QoS value based on user preferences because we address multiple QoS criteria in real life scenarios. The proposed approach was tested and illustrated via two case studies that prove its applicability.},
booktitle = {Proceedings of the 2014 IEEE International Conference on Services Computing},
pages = {17–26},
numpages = {10},
keywords = {Consensus, Fuzzy aggregation, Service selection, Trust, Web service},
series = {SCC '14}
}

@inproceedings{10.1109/DASC.2014.54,
author = {Yue, Dejun and Yu, Ge and Shen, Derong and Yu, Xiaocong},
title = {A Weighted Aggregation Rule in Crowdsourcing Systems for High Result Accuracy},
year = {2014},
isbn = {9781479950799},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/DASC.2014.54},
doi = {10.1109/DASC.2014.54},
abstract = {Many challenging problems could be better solved by exploiting crowdsourcing platforms than traditional machine-based methods. However, data quality in crowdsourcing applications has become a crucial aspect since crowdsourcing workers may have different capabilities. In this paper, we propose a novel weighted aggregation rule (WAR) to improve the result accuracy in crowdsourcing systems. According to the agreement of answers given by the workers, we classify all the tasks into the high-agreement tasks and low-agreement tasks. For the high-agreement tasks, we use simple majority voting to select the correct answer while ensuring the result accuracy. For the low-agreement tasks, we adopt weighted majority voting strategy, which assigns a weight for each worker according to his performance on the high-agreement tasks. We evaluate the effectiveness of our proposed method using three real-world datasets on AMT. The experimental results show that our method achieves excellent result accuracy.},
booktitle = {Proceedings of the 2014 IEEE 12th International Conference on Dependable, Autonomic and Secure Computing},
pages = {265–270},
numpages = {6},
keywords = {aggregation rule, agreement, crowdsourcing, majority voting},
series = {DASC '14}
}

@proceedings{10.1145/2593728,
title = {CSI-SE 2014: Proceedings of the 1st International Workshop on CrowdSourcing in Software Engineering},
year = {2014},
isbn = {9781450328579},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Hyderabad, India}
}

@inproceedings{10.5555/1866696.1866715,
author = {Munro, Robert and Bethard, Steven and Kuperman, Victor and Lai, Vicky Tzuyin and Melnick, Robin and Potts, Christopher and Schnoebelen, Tyler and Tily, Harry},
title = {Crowdsourcing and language studies: the new generation of linguistic data},
year = {2010},
publisher = {Association for Computational Linguistics},
address = {USA},
abstract = {We present a compendium of recent and current projects that utilize crowdsourcing technologies for language studies, finding that the quality is comparable to controlled laboratory experiments, and in some cases superior. While crowdsourcing has primarily been used for annotation in recent language studies, the results here demonstrate that far richer data may be generated in a range of linguistic disciplines from semantics to psycholinguistics. For these, we report a number of successful methods for evaluating data quality in the absence of a 'correct' response for any given data point.},
booktitle = {Proceedings of the NAACL HLT 2010 Workshop on Creating Speech and Language Data with Amazon's Mechanical Turk},
pages = {122–130},
numpages = {9},
location = {Los Angeles, California},
series = {CSLDAMT '10}
}

@inproceedings{10.1109/ICComm.2018.8430109,
author = {Greu, Victor and Ciot\^{\I}rnae, Petric\u{a} and Tuundefined\u{A}, Leontin and Popescu, Florin Gabriel},
title = {Human and Artificial Intelligence Driven Incentive-Operation Model and Algorithms for a Multi-Purpose Integrated Crowdsensing-Crowdsourcing Scalable System},
year = {2018},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICComm.2018.8430109},
doi = {10.1109/ICComm.2018.8430109},
abstract = {The future sensing systems seem to need more performant crowdsensing, using highest technologies as artificial intelligence, but being also more complex by volunteer participation and progressively changing from crowdsensing to crowdsourcing. Our work main idea is to use human/artificial intelligence in order to provide highest incentives arguments and commitments for participants and users, transforming data into information and eventually in knowledge. The human/artificial intelligence support is used first to find the most desired/used tasks/targets/questions/issues, then to control the crowdsensing/crowdsourcing operation with learned artificial intelligence rules, based on two algorithms, first for implementing an optimal efficiency tasks covering strategy as reference and second for attracting participants to enlarge/improve accuracy of service by extending crowdsensing-crowdsourcing with correlation incentive/operation added features.},
booktitle = {2018 International Conference on Communications (COMM)},
pages = {213–218},
numpages = {6},
location = {Bucharest}
}

@inproceedings{10.1145/2598153.2602225,
author = {Bozzon, Alessandro and Aroyo, Lora and Cremonesi, Paolo},
title = {First International Workshop on User Interfaces for Crowdsourcing and Human Computation},
year = {2014},
isbn = {9781450327756},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2598153.2602225},
doi = {10.1145/2598153.2602225},
abstract = {Recent years witnessed an explosion in the number and variety of data crowdsourcing initiatives. From OpenStreetMap to Amazon Mechanical Turk, developers and practitioners have been striving to create user interfaces able to effectively and efficiently support the creation, exploration, and analysis of crowdsourced information.The extensive usage of crowdsourcing techniques brings a major change of paradigm with respect to traditional user interface for data collection and exploration, as effectiveness, speed, and interaction quality concerns play a central role in supporting very demanding incentives, including monetary ones.The First International Workshop on User Interfaces for Crowdsourcing and Human Computation (CrowdUI 2014), co-located with the AVI 2014 conference, brought together researchers and practitioners from a wide range of areas interested in discussing the user interaction challenges posed by crowdsourcing systems.},
booktitle = {Proceedings of the 2014 International Working Conference on Advanced Visual Interfaces},
pages = {398–400},
numpages = {3},
keywords = {crowdsourcing, human computation, user incentives, user interfaces},
location = {Como, Italy},
series = {AVI '14}
}

@inproceedings{10.5555/1927229.1927272,
author = {Karnin, Ehud D. and Walach, Eugene and Drory, Tal},
title = {Crowdsourcing in the document processing practice},
year = {2010},
isbn = {3642169848},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {The processing of scanned documents calls for automatic recognition of the text by OCR (Optical Character Recognition) computer programs, followed by human validation and correction. Crowdsourcing of these essential manual tasks is a good option, provided one can take care of some key challenges, so that the quality level expected by the customer is met. We show how tools for efficient validation and correction are adapted and enhanced to address issues associated with crowdsourcing, such as data privacy, quality control, crowd monitoring, and job quality assurance. We started to implement these ideas and technologies in our COoperative eNgine for Correction of ExtRacted Text (CONCERT), which is used in book digitization projects.},
booktitle = {Proceedings of the 10th International Conference on Current Trends in Web Engineering},
pages = {408–411},
numpages = {4},
keywords = {documents processing, enterprise crowdsourcing, productivity tools, quality assurance, quality control},
location = {Vienna, Austria},
series = {ICWE'10}
}

@inproceedings{10.1145/1979742.1979745,
author = {Xu, Anbang and Bailey, Brian P.},
title = {A crowdsourcing model for receiving design critique},
year = {2011},
isbn = {9781450302685},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1979742.1979745},
doi = {10.1145/1979742.1979745},
abstract = {Designers in many domains are increasingly turning to online communities to receive critiques of early design ideas. However, members of these communities may not contribute an effective critique due to limited skills, motivation, or time, and therefore many critiques may not go beyond "I (don't) like it". We propose a new approach for designers to receive online critique. Our approach is novel because it adopts a theoretical framework for effective critique and implements the framework on a popular crowdsourcing platform. Preliminary results show that our approach allows designers to acquire quality critiques in a timely manner that compare favorably with critiques produced from a well-known online community.},
booktitle = {CHI '11 Extended Abstracts on Human Factors in Computing Systems},
pages = {1183–1188},
numpages = {6},
keywords = {critique, crowdsourcing, design},
location = {Vancouver, BC, Canada},
series = {CHI EA '11}
}

@inproceedings{10.1145/2103354.2103373,
author = {Oomen, Johan and Aroyo, Lora},
title = {Crowdsourcing in the cultural heritage domain: opportunities and challenges},
year = {2011},
isbn = {9781450308243},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2103354.2103373},
doi = {10.1145/2103354.2103373},
abstract = {Galleries, Libraries, Archives and Museums (short: GLAMs) around the globe are beginning to explore the potential of crowdsourcing, i. e. outsourcing specific activities to a community though an open call. In this paper, we propose a typology of these activities, based on an empirical study of a substantial amount of projects initiated by relevant cultural heritage institutions. We use the Digital Content Life Cycle model to study the relation between the different types of crowdsourcing and the core activities of heritage organizations. Finally, we focus on two critical challenges that will define the success of these collaborations between amateurs and professionals: (1) finding sufficient knowledgeable, and loyal users; (2) maintaining a reasonable level of quality. We thus show the path towards a more open, connected and smart cultural heritage: open (the data is open, shared and accessible), connected (the use of linked data allows for interoperable infrastructures, with users and providers getting more and more connected), and smart (the use of knowledge and web technologies allows us to provide interesting data to the right users, in the right context, anytime, anywhere -- both with involved users/consumers and providers). It leads to a future cultural heritage that is open, has intelligent infrastructures and has involved users, consumers and providers.},
booktitle = {Proceedings of the 5th International Conference on Communities and Technologies},
pages = {138–149},
numpages = {12},
keywords = {crowdsourcing, heritage, lifecycle model, metadata, tagging},
location = {Brisbane, Australia},
series = {C&amp;T '11}
}

@inproceedings{10.1109/SMC.2013.85,
author = {Karam, Roula and Melchiori, Michele},
title = {A Crowdsourcing-Based Framework for Improving Geo-spatial Open Data},
year = {2013},
isbn = {9781479906529},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/SMC.2013.85},
doi = {10.1109/SMC.2013.85},
abstract = {Nowadays, more and more people rely on freely available user-generated spatial content, known as Volunteered Geographic Information (VGI). Therefore, spatial data creation is no more exclusively in the hands of professionals and Linked Open Data (LOD) techniques have a role in promoting such online and freely accessible spatial information. However, the volume of VGI data is constantly growing so evaluating its quality is a basic need, especially in urban environments where Points of Interest change frequently and human feedbacks are crucial to get frequent updates of their descriptions. In this context, we propose a crowd sourcing-based framework, relying on linked data principles and devised to collect, organize and rank user-generated content in order to improve accuracy and completeness of geo-spatial LOD. To this purpose, metrics have been defined for evaluating both users and contents.},
booktitle = {Proceedings of the 2013 IEEE International Conference on Systems, Man, and Cybernetics},
pages = {468–473},
numpages = {6},
keywords = {Crowdsourcing, Linked Open Data, Location Based Services, Spatial Databases, Trust Metrics},
series = {SMC '13}
}

@inproceedings{10.1007/978-3-642-34222-6_13,
author = {Ebden, Mark and Huynh, Trung Dong and Moreau, Luc and Ramchurn, Sarvapali and Roberts, Stephen},
title = {Network analysis on provenance graphs from a crowdsourcing application},
year = {2012},
isbn = {9783642342219},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-642-34222-6_13},
doi = {10.1007/978-3-642-34222-6_13},
abstract = {Crowdsourcing has become a popular means for quickly achieving various tasks in large quantities. CollabMap is an online mapping application in which we crowdsource the identification of evacuation routes in residential areas to be used for planning large-scale evacuations. So far, approximately 38,000 micro-tasks have been completed by over 100 contributors. In order to assist with data verification, we introduced provenance tracking into the application, and approximately 5,000 provenance graphs have been generated. They have provided us various insights into the typical characteristics of provenance graphs in the crowdsourcing context. In particular, we have estimated probability distribution functions over three selected characteristics of these provenance graphs: the node degree, the graph diameter, and the densification exponent. We describe methods to define these three characteristics across specific combinations of node types and edge types, and present our findings in this paper. Applications of our methods include rapid comparison of one provenance graph versus another, or of one style of provenance database versus another. Our results also indicate that provenance graphs represent a suitable area of exploitation for existing network analysis tools concerned with modelling, prediction, and the inference of missing nodes and edges.},
booktitle = {Proceedings of the 4th International Conference on Provenance and Annotation of Data and Processes},
pages = {168–182},
numpages = {15},
location = {Santa Barbara, CA},
series = {IPAW'12}
}

@inproceedings{10.1007/978-3-642-12275-0_57,
author = {Alonso, Omar and Schenkel, Ralf and Theobald, Martin},
title = {Crowdsourcing assessments for XML ranked retrieval},
year = {2010},
isbn = {3642122744},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-642-12275-0_57},
doi = {10.1007/978-3-642-12275-0_57},
abstract = {Crowdsourcing has gained a lot of attention as a viable approach for conducting IR evaluations. This paper shows through a series of experiments on INEX data that crowdsourcing can be a good alternative for relevance assessment in the context of XML retrieval.},
booktitle = {Proceedings of the 32nd European Conference on Advances in Information Retrieval},
pages = {602–606},
numpages = {5},
location = {Milton Keynes, UK},
series = {ECIR'2010}
}

@inproceedings{10.1145/2801694.2801705,
author = {Zhao, Cong and Shi, Fengrui and Huang, Ran and Yang, Xinyu and McCann, Julie},
title = {Trustworthy Device Pairing for Opportunistic Device-to-Device Communications in Mobile Crowdsourcing Systems},
year = {2015},
isbn = {9781450337014},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2801694.2801705},
doi = {10.1145/2801694.2801705},
abstract = {Mobile Crowdsourcing System is an emerging service paradigm base on numerous personal smart devices, where the Device-to-Device communication among opportunistically encountered participating devices is an indispensable part of task allocation, file transmission and data collaboration. Considering that participating devices are privately held and opportunistically encountered, we design the Trustworthy Device Pairing (TDP) scheme that realizes user-transparent sharing secret key negotiation and reliable peer device determination for trustworthy spontaneous D2D transactions. TDP is demonstrated to be effective based on our proof-of-concept implementation, and a further evaluation on efficiency will be conducted.},
booktitle = {Proceedings of the 2015 Workshop on Wireless of the Students, by the Students, \&amp; for the Students},
pages = {4–6},
numpages = {3},
keywords = {device pairing, device-to-device communication, mobile crowdsourcing system, trustworthy},
location = {Paris, France},
series = {S3 '15}
}

@inproceedings{10.1145/3406865.3418318,
author = {Ram\'{\i}rez, Jorge and Baez, Marcos and Casati, Fabio and Cernuzzi, Luca and Benatallah, Boualem},
title = {DREC: towards a Datasheet for Reporting Experiments in Crowdsourcing},
year = {2020},
isbn = {9781450380591},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3406865.3418318},
doi = {10.1145/3406865.3418318},
abstract = {Factors such as instructions, payment schemes, platform demographics, along with strategies for mapping studies into crowdsourcing environments, play an important role in the reproducibility of results. However, inferring these details from scientific articles is often a challenging endeavor, calling for the development of proper reporting guidelines. This paper makes the first steps towards this goal, by describing an initial taxonomy of relevant attributes for crowdsourcing experiments, and providing a glimpse into the state of reporting by analyzing a sample of CSCW papers.},
booktitle = {Companion Publication of the 2020 Conference on Computer Supported Cooperative Work and Social Computing},
pages = {377–382},
numpages = {6},
keywords = {crowdsourcing, crowdsourcing experiments, guidelines for reporting experiments, taxonomy},
location = {Virtual Event, USA},
series = {CSCW '20 Companion}
}

@inproceedings{10.1145/3274895.3274902,
author = {Jonathan, Christopher and Mokbel, Mohamed F.},
title = {Stella: geotagging images via crowdsourcing},
year = {2018},
isbn = {9781450358897},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3274895.3274902},
doi = {10.1145/3274895.3274902},
abstract = {Geotagged data (e.g. images or news items) have empowered various important applications, e.g., search engines and news agencies. However, the lack of available geotagged data significantly reduces the impact of such applications. Meanwhile, existing geotagging approaches rely on the existence of prior knowledge, e.g., accurate training dataset for machine learning techniques. This paper presents Stella; a crowdsourcing framework for image geotagging. The high accuracy of Stella is resulted by being able to recruit workers near the image location even without knowing its location. In addition, Stella also return its confidence about the reported location to help users in understanding the result quality. Experimental evaluation shows that Stella consistently geotags an image with an average of 95\% accuracy and 90\% of confidence.},
booktitle = {Proceedings of the 26th ACM SIGSPATIAL International Conference on Advances in Geographic Information Systems},
pages = {169–178},
numpages = {10},
keywords = {crowdsourcing, geotagging framework, spatial crowdsourcing},
location = {Seattle, Washington},
series = {SIGSPATIAL '18}
}

@inproceedings{10.1109/MDM.2014.59,
author = {Tiwari, Sunita and Kaushik, Saroj},
title = {Information Enrichment for Tourist Spot Recommender System Using Location Aware Crowdsourcing},
year = {2014},
isbn = {9781479957057},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/MDM.2014.59},
doi = {10.1109/MDM.2014.59},
abstract = {With the increase in number of available interesting locations, it becomes difficult for users to find interesting ones, thus imposes a need for recommender systems to suggest interesting locations. Further, to ease the user's decision making, the amount of supplementary information, such as right time to visit, weather conditions, traffic condition, right mode of transport, crowdedness, security alerts, etc., may be annotated with the list of recommended locations. This paper explores the possibility of enriching tourist locations using crowd sourcing approach, which can be used by Tourist Spot Recommender System (TSRS) for mobile users. Proposed crowd sourcing system focuses on getting work done from the crowd currently available at the location under consideration. In proposed system, the contributed information is not limited to ones available on blogs, web pages and sensor-readings from the device etc., but includes proactively-generated user's opinions and perspectives, that are processed to offer immediate knowledge. Our system works in collaboration with a TSRS, takes the list of locations to be recommended to the current user and performs just-in-time information enrichment for those selected set of locations. We have implemented a prototype of proposed systems using java android software development toolkit and evaluated this system by 76 real users.},
booktitle = {Proceedings of the 2014 IEEE 15th International Conference on Mobile Data Management - Volume 02},
pages = {11–14},
numpages = {4},
keywords = {Collective Intelligence, Crowdsourcing, Information Enrichment, Pervasive Computing, Tourist Spot Recommender System, Tourist Spots},
series = {MDM '14}
}

@inproceedings{10.1007/978-3-319-22479-4_2,
author = {Panagiotopoulos, Panos and Bowen, Frances},
title = {Conceptualising the Digital Public in Government Crowdsourcing: Social Media and the Imagined Audience},
year = {2015},
isbn = {978-3-319-22478-7},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-319-22479-4_2},
doi = {10.1007/978-3-319-22479-4_2},
abstract = {Public sector organisations seem to be embracing social media for information dissemination and engagement, but less is know about their value as information sources. This paper draws from the notion of the imagined audience to examine how policy teams in the UK Department of Environment, Food and Rural Affairs (DEFRA) conceptualise the value of social media input. Findings from a series of interviews and workshops suggest that policy makers are broadly positive about sourcing useful input from social media in topics like farming and environmental policies, however audience awareness emerges as an important limitation. As different groups of the public use social media for professional activities, policy makers attempt to develop their own capacities to navigate through audiences and understand whom they are listening to. The paper makes suggestions about the technical, methodological and policy challenges of overcoming audience limitations on social media.},
booktitle = {Electronic Government},
pages = {19–30},
numpages = {12},
keywords = {Social media, Policy crowdsourcing, Digital engagement, UK government, Environment and farming, Case study},
location = {Thessaloniki
Greece}
}

@inproceedings{10.5555/2893873.2893985,
author = {Jain, Shweta and Narayanaswamy, Balakrishnan and Narahari, Y.},
title = {A multiarmed bandit incentive mechanism for crowdsourcing demand response in smart grids},
year = {2014},
publisher = {AAAI Press},
abstract = {Demand response is a critical part of renewable integration and energy cost reduction goals across the world. Motivated by the need to reduce costs arising from electricity shortage and renewable energy fluctuations, we propose a novel multiarmed bandit mechanism for demand response (MAB-MDR) which makes monetary offers to strategic consumers who have unknown response characteristics, to incetivize reduction in demand. Our work is inspired by a novel connection we make to crowdsourcing mechanisms. The proposed mechanism incorporates realistic features of the demand response problem including time varying and quadratic cost function. The mechanism marries auctions, that allow users to report their preferences, with online algorithms, that allow distribution companies to learn user-specific parameters. We show that MAB-MDR is dominant strategy incentive compatible, individually rational, and achieves sublinear regret. Such mechanisms can be effectively deployed in smart grids using new information and control architecture innovations and lead to welcome savings in energy costs.},
booktitle = {Proceedings of the Twenty-Eighth AAAI Conference on Artificial Intelligence},
pages = {721–727},
numpages = {7},
location = {Qu\'{e}bec City, Qu\'{e}bec, Canada},
series = {AAAI'14}
}

@inproceedings{10.1109/SocialCom.2013.38,
author = {Besaleva, Liliya I. and Weaver, Alfred C.},
title = {Applications of Social Networks and Crowdsourcing for Disaster Management Improvement},
year = {2013},
isbn = {9780769551371},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/SocialCom.2013.38},
doi = {10.1109/SocialCom.2013.38},
abstract = {Emergency resources are often insufficient to satisfy fully the demands for professional help and supplies after a public disaster. Furthermore, in a mass casualty situation, the emphasis shifts from ensuring the best possible outcome for each individual patient to ensuring the best possible outcome for the greatest number of patients. Historically, various manual and electronic medical triage systems have been used both under civil and military conditions to determine the order and priority of emergency treatment, transport, and best possible destination for the patients. Unfortunately, none of those solutions has proven flexible, accurate, scalable or unobtrusive enough to meet the public's expectations. In this paper, we provide insights into the trends, innovations, and challenges of contemporary crowd sourced e-Health and medical informatics applications in the context of emergency preparedness and response. Additionally, we demonstrate a system, called Crowd Help, for real-time patient assessment which uses mobile electronic triaging accomplished via crowd sourced information. With the use of our system, emergency management professionals receive most of the information they need for preparing themselves to provide timely and accurate treatments of their patients even before dispatching a response team to the event.},
booktitle = {Proceedings of the 2013 International Conference on Social Computing},
pages = {213–219},
numpages = {7},
keywords = {crowdsourcing, e-health, emergency preparedness, mobile technologies, triage},
series = {SOCIALCOM '13}
}

@inproceedings{10.1145/2660114.2660122,
author = {Naderi, Babak and Wechsung, Ina and Polzehl, Tim and M\"{o}ller, Sebastian},
title = {Development and Validation of Extrinsic Motivation Scale for Crowdsourcing Micro-task Platforms},
year = {2014},
isbn = {9781450331289},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2660114.2660122},
doi = {10.1145/2660114.2660122},
abstract = {In this paper, we introduce a scale for measuring the extrinsic motivation of crowd workers. The new questionnaire is strongly based on the Work Extrinsic Intrinsic Motivation Scale (WEIMS) [17] and theoretically follows the Self-Determination Theory (SDT) of motivation. The questionnaire has been applied and validated in a crowdsourcing micro-task platform. This instrument can be used for studying the dynamics of extrinsic motivation by taking into account individual differences and provide meaningful insights which will help to design a proper incentives framework for each crowd worker that eventually leads to a better performance, an increased well-being, and higher overall quality.},
booktitle = {Proceedings of the 2014 International ACM Workshop on Crowdsourcing for Multimedia},
pages = {31–36},
numpages = {6},
keywords = {cfa, crowdsourcing, motivation, sdt},
location = {Orlando, Florida, USA},
series = {CrowdMM '14}
}

@inproceedings{10.1145/3511808.3557297,
author = {Xu, Xiaojia and Liu, An and Liu, Guanfeng and Li, Zhixu and Zhao, Lei},
title = {Drive Less but Finish More: Food Delivery based on Multi-Level Workers in Spatial Crowdsourcing},
year = {2022},
isbn = {9781450392365},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3511808.3557297},
doi = {10.1145/3511808.3557297},
abstract = {In this paper, we study the problem of on-demand food delivery in a new setting where two groups of workers -- riders and taxi drivers (drivers for short) -- cooperate with each other for better service. The riders are responsible for the first and the last mile, and the drivers are in charge of the cross-community transportation. We show this problem is generally NP-hard by a reduction from the well-known 3-dimensional matching (3DM). To tackle with this problem, we first reduce it to the maximum independent set problem and use a simple greedy strategy to design an approximate algorithm which has a polynomial time. Considering the exponents in the polynomial are not very small, we then transform the 3DM into two rounds of 2-dimensional matching and propose a fast algorithm to solve it. Though 3DM problem is NP-hard, we find the cooperation between riders and drivers form a special tripartite graph, based on which we construct a flow network and employ the min-cost max-flow algorithm to efficiently compute the exact solution. We conduct extensive experiments to show the efficiency and the effectiveness of our proposed algorithms.},
booktitle = {Proceedings of the 31st ACM International Conference on Information \&amp; Knowledge Management},
pages = {2331–2340},
numpages = {10},
keywords = {online food delivery, spatial crowdsourcing, task assignment},
location = {Atlanta, GA, USA},
series = {CIKM '22}
}

@inproceedings{10.1145/2030112.2030243,
author = {Vukovic, Maja and Kumara, Soundar},
title = {Second international workshop on ubiquitous crowdsourcing: towards a platform for crowd computing},
year = {2011},
isbn = {9781450306300},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2030112.2030243},
doi = {10.1145/2030112.2030243},
abstract = {With the adoption of mobile, digital and social media networked crowds are reporting and acting upon events in smart environments. Existing platforms for crowdsourcing, support specific activity types, such as micro-tasks on the Amazon's Mechanical Turk; and fall short of facilitating general mechanisms for setting up and maintaining crowd networks easily, flexibly and in a variety of domains. Building upon First International Workshop on Ubiquitous Crowdsourcing, in this edition we challenge researchers and practitioners to identify requirements for a platform for crowd computing, arising from experiences in deployment crowdsourcing applications, which engage crowd members as sensors, controllers and actuators in smart cities and environments. This workshop brings together researchers to produce a vision for the universal crowdsourcing platform, documenting it in a theme publication.},
booktitle = {Proceedings of the 13th International Conference on Ubiquitous Computing},
pages = {617–618},
numpages = {2},
keywords = {crowdsourcing},
location = {Beijing, China},
series = {UbiComp '11}
}

@inproceedings{10.1145/3320435.3320439,
author = {Mavridis, Panagiotis and Huang, Owen and Qiu, Sihang and Gadiraju, Ujwal and Bozzon, Alessandro},
title = {Chatterbox: Conversational Interfaces for Microtask Crowdsourcing},
year = {2019},
isbn = {9781450360210},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3320435.3320439},
doi = {10.1145/3320435.3320439},
abstract = {Conversational interfaces can facilitate human-computer interactions. Whether or not conversational interfaces can improve worker experience and work quality in crowdsourcing marketplaces has remained unanswered. We investigate the suitability of text-based conversational interfaces for microtask crowdsourcing. We designed a rigorous experimental campaign aimed at gauging the interest and acceptance by crowdworkers for this type of work interface. We compared Web and conversational interfaces for five common microtask types and measured the execution time, quality of work, and the perceived satisfaction of 316 workers recruited from the FigureEight platform. We show that conversational interfaces can be used effectively for crowdsourcing microtasks, resulting in a high satisfaction from workers, and without having a negative impact on task execution time or work quality.},
booktitle = {Proceedings of the 27th ACM Conference on User Modeling, Adaptation and Personalization},
pages = {243–251},
numpages = {9},
keywords = {chatbots, conversational agents, microtask crowdsourcing},
location = {Larnaca, Cyprus},
series = {UMAP '19}
}

@inproceedings{10.5555/3042817.3042944,
author = {Chen, Xi and Lin, Qihang and Zhou, Dengyong},
title = {Optimistic knowledge gradient policy for optimal budget allocation in crowdsourcing},
year = {2013},
publisher = {JMLR.org},
abstract = {In real crowdsourcing applications, each label from a crowd usually comes with a certain cost. Given a pre-fixed amount of budget, since different tasks have different ambiguities and different workers have different expertises, we want to find an optimal way to allocate the budget among instance-worker pairs such that the overall label quality can be maximized. To address this issue, we start from the simplest setting in which all workers are assumed to be perfect. We formulate the problem as a Bayesian Markov Decision Process (MDP). Using the dynamic programming (DP) algorithm, one can obtain the optimal allocation policy for a given budget. However, DP is computationally intractable. To solve the computational challenge, we propose a novel approximate policy which is called optimistic knowledge gradient. It is practically efficient while theoretically its consistency can be guaranteed. We then extend the MDP framework to deal with inhomogeneous workers and tasks with contextual information available. The experiments on both simulated and real data demonstrate the superiority of our method.},
booktitle = {Proceedings of the 30th International Conference on International Conference on Machine Learning - Volume 28},
pages = {III–64–III–72},
location = {Atlanta, GA, USA},
series = {ICML'13}
}

@inproceedings{10.1145/2525314.2525370,
author = {Deng, Dingxiong and Shahabi, Cyrus and Demiryurek, Ugur},
title = {Maximizing the number of worker's self-selected tasks in spatial crowdsourcing},
year = {2013},
isbn = {9781450325219},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2525314.2525370},
doi = {10.1145/2525314.2525370},
abstract = {With the progress of mobile devices and wireless broadband, a new eMarket platform, termed spatial crowdsourcing is emerging, which enables workers (aka crowd) to perform a set of spatial tasks (i.e., tasks related to a geographical location and time) posted by a requester. In this paper, we study a version of the spatial crowd-sourcing problem in which the workers autonomously select their tasks, called the worker selected tasks (WST) mode. Towards this end, given a worker, and a set of tasks each of which is associated with a location and an expiration time, we aim to find a schedule for the worker that maximizes the number of performed tasks. We first prove that this problem is NP-hard. Subsequently, for small number of tasks, we propose two exact algorithms based on dynamic programming and branch-and-bound strategies. Since the exact algorithms cannot scale for large number of tasks and/or limited amount of resources on mobile platforms, we also propose approximation and progressive algorithms. We conducted a thorough experimental evaluation on both real-world and synthetic data to compare the performance and accuracy of our proposed approaches.},
booktitle = {Proceedings of the 21st ACM SIGSPATIAL International Conference on Advances in Geographic Information Systems},
pages = {324–333},
numpages = {10},
keywords = {crowdsourcing, spatial crowdsourcing, spatial task assignment},
location = {Orlando, Florida},
series = {SIGSPATIAL'13}
}

@inproceedings{10.5555/3007337.3007471,
author = {Tran-Thanh, Long and Stein, Sebastian and Rogers, Alex and Jennings, Nicholas R.},
title = {Efficient crowdsourcing of unknown experts using multi-armed bandits},
year = {2012},
isbn = {9781614990970},
publisher = {IOS Press},
address = {NLD},
abstract = {We address the expert crowdsourcing problem, in which an employer wishes to assign tasks to a set of available workers with heterogeneous working costs. Critically, as workers produce results of varying quality, the utility of each assigned task is unknown and can vary both between workers and individual tasks. Furthermore, in realistic settings, workers are likely to have limits on the number of tasks they can perform and the employer will have a fixed budget to spend on hiring workers. Given these constraints, the objective of the employer is to assign tasks to workers in order to maximise the overall utility achieved. To achieve this, we introduce a novel multi-armed bandit (MAB) model, the bounded MAB, that naturally captures the problem of expert crowdsourcing. We also propose an algorithm to solve it efficiently, called bounded ε-first, which uses the first εB of its total budget B to derive estimates of the workers' quality characteristics (exploration), while the remaining (1 - ε)B is used to maximise the total utility based on those estimates (exploitation). We show that using this technique allows us to derive an O(B⅔) upper bound on our algorithm's performance regret (i.e. the expected difference in utility between the optimal and our algorithm). In addition, we demonstrate that our algorithm outperforms existing crowdsourcing methods by up to 155\% in experiments based on real-world data from a prominent crowdsourcing site, while achieving up to 75\% of a hypothetical optimal with full information.},
booktitle = {Proceedings of the 20th European Conference on Artificial Intelligence},
pages = {768–773},
numpages = {6},
location = {Montpellier, France},
series = {ECAI'12}
}

@inproceedings{10.5555/2484920.2485027,
author = {Lev, Omer and Polukarov, Maria and Bachrach, Yoram and Rosenschein, Jeffrey S.},
title = {Mergers and collusion in all-pay auctions and crowdsourcing contests},
year = {2013},
isbn = {9781450319935},
publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
address = {Richland, SC},
abstract = {We study the effects of bidder collaboration in all-pay auctions. We analyse both mergers, where the remaining players are aware of the agreement between the cooperating participants, and collusion, where the remaining players are unaware of this agreement. We examine two scenarios: the sum-profit model where the auctioneer obtains the sum of all submitted bids, and the max-profit model of crowdsourcing contests where the auctioneer can only use the best submissions and thus obtains only the winning bid. We show that while mergers do not change the expected utility of the participants, or the principal's utility in the sum-profit model, collusion transfers the utility from the non-colluders to the colluders. Surprisingly, we find that in some cases such collaboration can increase the social welfare. Moreover, mergers and, curiously, also collusion can even be beneficial to the auctioneer under certain conditions.},
booktitle = {Proceedings of the 2013 International Conference on Autonomous Agents and Multi-Agent Systems},
pages = {675–682},
numpages = {8},
keywords = {all-pay auction, collusion, crowdsourcing, mergers},
location = {St. Paul, MN, USA},
series = {AAMAS '13}
}

@inproceedings{10.1145/2593728.2593729,
author = {Prikladnicki, Rafael and Machado, Leticia and Carmel, Erran and de Souza, Cleidson R. B.},
title = {Brazil software crowdsourcing: a first step in a multi-year study},
year = {2014},
isbn = {9781450328579},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2593728.2593729},
doi = {10.1145/2593728.2593729},
abstract = {Crowdsourcing means outsourcing to a large network of people—a crowd. This form of managing work allocation has become much more sophisticated in recent years due to improvements in technology and changes in the work ecosystem. Crowdsourcing portends-- not only the disruption of outsourcing-- but the disruption of the entire global labor market. Small, atomized, tasks that can be completed and paid for in small increments are unprecedented in the history of work. Software has been the pioneer in all the large mega-trends of the last generation: in computer technology, technological entrepreneurship, offshore outsourcing, and now-- in crowdsourcing. This paper describes the starting point of a research project that aims to investigate the Brazilian software labor and industry markets. These markets are being transformed and disrupted as a result of the new phenomena of crowdsourcing. To be more specific, we aim to understand how the three elements of crowdsourcing are emerging in Brazil – the buyers, the platforms, and the crowd. The goal of our project is to identify the challenges faced by Brazilian software developers engaged in crowdsourcing platforms as well as their best practices in order to provide recommendations to the government and support for new developers interested in joining this market.},
booktitle = {Proceedings of the 1st International Workshop on CrowdSourcing in Software Engineering},
pages = {1–4},
numpages = {4},
keywords = {Crowdsourcing, human labor, software development},
location = {Hyderabad, India},
series = {CSI-SE 2014}
}

@inproceedings{10.1145/3246861,
author = {Castillo, Carlos},
title = {Session details: Session 6 -- Big Data Analytics and Crowdsourcing for Public Health 2},
year = {2015},
isbn = {9781450334921},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3246861},
doi = {10.1145/3246861},
booktitle = {Proceedings of the 5th International Conference on Digital Health 2015},
location = {Florence, Italy},
series = {DH '15}
}

@inproceedings{10.5555/2908698.2908701,
author = {Chandler, Dana and Horton, John},
title = {Labor allocation in paid crowdsourcing: experimental evidence on positioning, nudges and prices},
year = {2011},
publisher = {AAAI Press},
abstract = {This paper reports the results of a natural field experiment where workers from a paid crowdsourcing environment selfselect into tasks and are presumed to have limited attention. In our experiment, workers labeled any of six pictures from a 2 \texttimes{} 3 grid of thumbnail images. In the absence of any incentives, workers exhibit a strong default bias and tend to select images from the top-left ("focal") position; the bottomright ("non-focal") position, was the least preferred. We attempted to overcome this bias and increase the rate at which workers selected the least preferred task, by using a combination of monetary and non-monetary incentives. We also varied the saliency of these incentives by placing them in either the focal or non-focal position. Although both incentive types caused workers to re-allocate their labor, monetary incentives were more effective. Most interestingly, both incentive types worked better when they were placed in the focal position and made more salient. In fact, salient non-monetary incentives worked about as well as non-salient monetary ones. Our evidence suggests that user interface and cognitive biases play an important role in online labor markets and that salience can be used by employers as a kind of "incentive multiplier".},
booktitle = {Proceedings of the 11th AAAI Conference on Human Computation},
pages = {14–19},
numpages = {6},
series = {AAAIWS'11-11}
}

@inproceedings{10.1109/NGMAST.2014.59,
author = {Mirri, Silvia and Prandi, Catia and Salomoni, Paola and Callegati, Franco and Campi, Aldo},
title = {On Combining Crowdsourcing, Sensing and Open Data for an Accessible Smart City},
year = {2014},
isbn = {9781479950737},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/NGMAST.2014.59},
doi = {10.1109/NGMAST.2014.59},
abstract = {This work presents a novel geospatial mapping service, based on OpenStreetMap, which has been designed and developed in order to provide personalized path to users with special needs. This system gathers data related to barriers and facilities of the urban environment via crowd sourcing and sensing done by users. It also considers open data provided by bus operating companies to identify the actual accessibility feature and the real time of arrival at the stops of the buses. The resulting service supports citizens with reduced mobility (users with disabilities and/or elderly people) suggesting urban paths accessible to them and providing information related to travelling time, which are tailored to their abilities to move and to the bus arrival time. The manuscript demonstrates the effectiveness of the approach by means of a case study focusing on the differences between the solutions provided by our system and the ones computed by main stream geospatial mapping services.},
booktitle = {Proceedings of the 2014 Eighth International Conference on Next Generation Mobile Apps, Services and Technologies},
pages = {294–299},
numpages = {6},
keywords = {crowdsourcing, geospatial mapping systems, open data, sensing, smart city, urban accessibility},
series = {NGMAST '14}
}

@inproceedings{10.5555/1939281.1939323,
author = {Vukovic, Maja and Bartolini, Claudio},
title = {Towards a research agenda for enterprise crowdsourcing},
year = {2010},
isbn = {3642165575},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {Over the past few years the crowdsourcing paradigm has evolved from its humble beginnings as isolated purpose-built initiatives, such as Wikipedia and Elance and Mechanical Turk to a growth industry employing over 2 million knowledge workers, contributing over half a billion dollars to the digital economy. Web 2.0 provides the technological foundations upon which the crowdsourcing paradigm evolves and operates, enabling networked experts to work collaboratively to complete a specific task. Enterprise crowdsourcing poses interesting challenges for both academic and industrial research along the social, legal, and technological dimensions.In this paper we describe the challenges that researchers and practitioners face when thinking about various aspects of enterprise crowdsourcing. First, to establish technological foundations, what are the interaction models and protocols between the Enterprise and the crowd. Secondly, how is crowdsourcing going to face the challenges in quality assurance, enabling Enterprises to optimally leverage the scalable workforce. Thirdly, what are the novel (Web) applications enabled by Enterprise crowdsourcing.},
booktitle = {Proceedings of the 4th International Conference on Leveraging Applications of Formal Methods, Verification, and Validation - Volume Part I},
pages = {425–434},
numpages = {10},
keywords = {business process modeling, crowdsourcing},
location = {Heraklion, Crete, Greece},
series = {ISoLA'10}
}

@inproceedings{10.1109/UIC-ATC-ScalCom.2014.110,
author = {Jin, Li and Han, Ming and Liu, Gangli and Feng, Ling},
title = {Detecting Cruising Flagged Taxis' Passenger-Refusal Behaviors Using Traffic Data and Crowdsourcing},
year = {2014},
isbn = {9781479976461},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/UIC-ATC-ScalCom.2014.110},
doi = {10.1109/UIC-ATC-ScalCom.2014.110},
abstract = {With the development of urban traffic, taxi accounts for a large proportion to serve people's daily travel. Besides prebooked taxis, flagged taxis also run in many places of the world, allowing passengers to wave at the driver on the side of the road to flag down the taxi as it is approaching. For flagged taxis service, taxi drivers' passenger-refusal (TPR) behaviors have become a serious problem due to rush hours, bad weather and destination choosy, which hurt the living quality and the reputation of the city. With more and more GPS-equipped taxis and smart phones available, detecting such abnormal TPR behaviors from GPS trajectories has become feasible. In this paper, we model the TPR behaviors and develop a system named Crowd TPR to detect TPR behaviors in real time by combining traffic data and crowd sourcing. It efficiently serves real-time monitoring requests from the traffic administrators and generates proper pick-up spots for passengers to select. In our system, we firstly propose a dynamic grid granularity selection method to achieve efficient map-matching and build spatio-temporal index on the road network. Through modeling taxis' routing behaviors, we predict arriving locations of taxis and recommend pick-up spots for passengers to push cluster-based human intelligent tasks (HITs). After passengers submit HIT results to Crowd TPR, we verify TPR behaviors by modeling the anomalous features. We build our system using real trajectory datasets generated by 33,000+ taxis and validate the system with extensive evaluations including over 1-month user study.},
booktitle = {Proceedings of the 2014 IEEE 11th Intl Conf on Ubiquitous Intelligence and Computing and 2014 IEEE 11th Intl Conf on Autonomic and Trusted Computing and 2014 IEEE 14th Intl Conf on Scalable Computing and Communications and Its Associated Workshops (UIC-ATC-ScalCom)},
pages = {18–25},
numpages = {8},
keywords = {crowdsourcing, flagged taxi, gps trajectory, human intelligent task, passenger-refusal},
series = {UIC-ATC-SCALCOM '14}
}

@inproceedings{10.1145/2009916.2009947,
author = {Kazai, Gabriella and Kamps, Jaap and Koolen, Marijn and Milic-Frayling, Natasa},
title = {Crowdsourcing for book search evaluation: impact of hit design on comparative system ranking},
year = {2011},
isbn = {9781450307574},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2009916.2009947},
doi = {10.1145/2009916.2009947},
abstract = {The evaluation of information retrieval (IR) systems over special collections, such as large book repositories, is out of reach of traditional methods that rely upon editorial relevance judgments. Increasingly, the use of crowdsourcing to collect relevance labels has been regarded as a viable alternative that scales with modest costs. However, crowdsourcing suffers from undesirable worker practices and low quality contributions. In this paper we investigate the design and implementation of effective crowdsourcing tasks in the context of book search evaluation. We observe the impact of aspects of the Human Intelligence Task (HIT) design on the quality of relevance labels provided by the crowd. We assess the output in terms of label agreement with a gold standard data set and observe the effect of the crowdsourced relevance judgments on the resulting system rankings. This enables us to observe the effect of crowdsourcing on the entire IR evaluation process. Using the test set and experimental runs from the INEX 2010 Book Track, we find that varying the HIT design, and the pooling and document ordering strategies leads to considerable differences in agreement with the gold set labels. We then observe the impact of the crowdsourced relevance label sets on the relative system rankings using four IR performance metrics. System rankings based on MAP and Bpref remain less affected by different label sets while the Precision@10 and nDCG@10 lead to dramatically different system rankings, especially for labels acquired from HITs with weaker quality controls. Overall, we find that crowdsourcing can be an effective tool for the evaluation of IR systems, provided that care is taken when designing the HITs.},
booktitle = {Proceedings of the 34th International ACM SIGIR Conference on Research and Development in Information Retrieval},
pages = {205–214},
numpages = {10},
keywords = {book search, crowdsourcing quality, prove it},
location = {Beijing, China},
series = {SIGIR '11}
}

@inproceedings{10.1145/2030100.2030108,
author = {Jayakanthan, Ranganathan and Sundararajan, Deepak},
title = {Enterprise crowdsourcing solutions for software development and ideation},
year = {2011},
isbn = {9781450309271},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2030100.2030108},
doi = {10.1145/2030100.2030108},
abstract = {This paper looks into two crowdsourcing applications which attempt to tackle software development and ideation problems in an enterprise environment. The software development crowdsourcing application (Crowdsourcer) is used for internal crowdsourcing of client and internal software development projects across self organizing groups and individuals. Crowdsourcer relies on workflows to identify capable free agents from inside the organization to design, build, test and appraise software projects through a functionality set which rely on crowdsourcing patterns such as virtual marketplaces, intelligent auctions and bidding processes, collaborative project management and reputation systems with virtual currencies and reward systems. The ideation crowdsourcing application features crowdsourced idea creation along with workflows to form actualization groups, which help in bringing an idea to fruition from concept stage. With advanced filter mechanisms, a reputation system and a system of peer based participatory marketplace for evaluation of ideas, this application features sophisticated crowdsourcing patterns to identify and execute ideas through harnessing the wisdom of the crowds.},
booktitle = {Proceedings of the 2nd International Workshop on Ubiquitous Crowdsouring},
pages = {25–28},
numpages = {4},
keywords = {crowd, crowdsourcing, enterprise crowdsourcing, social networks, social software, ubiquitous, web 2.0},
location = {Beijing, China},
series = {UbiCrowd '11}
}

@inproceedings{10.5555/2392701.2392708,
author = {Prabhakaran, Vinodkumar and Bloodgood, Michael and Diab, Mona and Dorr, Bonnie and Levin, Lori and Piatko, Christine D. and Rambow, Owen and Van Durme, Benjamin},
title = {Statistical modality tagging from rule-based annotations and crowdsourcing},
year = {2012},
publisher = {Association for Computational Linguistics},
address = {USA},
abstract = {We explore training an automatic modality tagger. Modality is the attitude that a speaker might have toward an event or state. One of the main hurdles for training a linguistic tagger is gathering training data. This is particularly problematic for training a tagger for modality because modality triggers are sparse for the overwhelming majority of sentences. We investigate an approach to automatically training a modality tagger where we first gathered sentences based on a high-recall simple rule-based modality tagger and then provided these sentences to Mechanical Turk annotators for further annotation. We used the resulting set of training data to train a precise modality tagger using a multi-class SVM that delivers good performance.},
booktitle = {Proceedings of the Workshop on Extra-Propositional Aspects of Meaning in Computational Linguistics},
pages = {57–64},
numpages = {8},
location = {Jeju, Republic of Korea},
series = {ExProM '12}
}

@inproceedings{10.1145/2556288.2557155,
author = {Alagarai Sampath, Harini and Rajeshuni, Rajeev and Indurkhya, Bipin},
title = {Cognitively inspired task design to improve user performance on crowdsourcing platforms},
year = {2014},
isbn = {9781450324731},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2556288.2557155},
doi = {10.1145/2556288.2557155},
abstract = {Recent research in human computation has focused on improving the quality of work done by crowd workers on crowdsourcing platforms. Multiple approaches have been adopted like filtering crowd workers through qualification tasks, and aggregating responses from multiple crowd workers to obtain consensus. We investigate here how improving the presentation of the task itself by using cognitively inspired features affects the performance of crowd workers. We illustrate this with a case-study for the task of extracting text from scanned images. We generated six task-presentation designs by modifying two parameters - visual saliency of the target fields and working memory requirements - and conducted experiments on Amazon Mechanical Turk (AMT) and with an eye-tracker in the lab setting. Our results identify which task-design parameters (e.g. highlighting target fields) result in improved performance, and which ones do not (e.g. reducing the number of distractors). In conclusion, we claim that the use of cognitively inspired features for task design is a powerful technique for maximizing the performance of crowd workers.},
booktitle = {Proceedings of the SIGCHI Conference on Human Factors in Computing Systems},
pages = {3665–3674},
numpages = {10},
keywords = {cognitive psychology, crowdsourcing, eye tracking, mechanical turk, task design, visual saliency, working memory},
location = {Toronto, Ontario, Canada},
series = {CHI '14}
}

@inproceedings{10.5555/1996889.1996910,
author = {Alonso, Omar and Baeza-Yates, Ricardo},
title = {Design and implementation of relevance assessments using crowdsourcing},
year = {2011},
isbn = {9783642201608},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {In the last years crowdsourcing has emerged as a viable platform for conducting relevance assessments. The main reason behind this trend is that makes possible to conduct experiments extremely fast, with good results and at low cost. However, like in any experiment, there are several details that would make an experiment work or fail. To gather useful results, user interface guidelines, inter-agreement metrics, and justification analysis are important aspects of a successful crowdsourcing experiment. In this work we explore the design and execution of relevance judgments using Amazon Mechanical Turk as crowdsourcing platform, introducing a methodology for crowdsourcing relevance assessments and the results of a series of experiments using TREC 8 with a fixed budget. Our findings indicate that workers are as good as TREC experts, even providing detailed feedback for certain query-document pairs. We also explore the importance of document design and presentation when performing relevance assessment tasks. Finally, we show our methodology at work with several examples that are interesting in their own.},
booktitle = {Proceedings of the 33rd European Conference on Advances in Information Retrieval},
pages = {153–164},
numpages = {12},
location = {Dublin, Ireland},
series = {ECIR'11}
}

@inproceedings{10.1007/978-3-642-20161-5_16,
author = {Alonso, Omar and Baeza-Yates, Ricardo},
title = {Design and Implementation of Relevance Assessments Using Crowdsourcing},
year = {2011},
isbn = {9783642201608},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-642-20161-5_16},
doi = {10.1007/978-3-642-20161-5_16},
abstract = {In the last years crowdsourcing has emerged as a viable platform for conducting relevance assessments. The main reason behind this trend is that makes possible to conduct experiments extremely fast, with good results and at low cost. However, like in any experiment, there are several details that would make an experiment work or fail. To gather useful results, user interface guidelines, inter-agreement metrics, and justification analysis are important aspects of a successful crowdsourcing experiment. In this work we explore the design and execution of relevance judgments using Amazon Mechanical Turk as crowdsourcing platform, introducing a methodology for crowdsourcing relevance assessments and the results of a series of experiments using TREC 8 with a fixed budget. Our findings indicate that workers are as good as TREC experts, even providing detailed feedback for certain query-document pairs. We also explore the importance of document design and presentation when performing relevance assessment tasks. Finally, we show our methodology at work with several examples that are interesting in their\"{\i} \'{z}own.},
booktitle = {Proceedings of the 33rd European Conference on Advances in Information Retrieval - Volume 6611},
pages = {153–164},
numpages = {12},
location = {Dublin, Ireland},
series = {ECIR 2011}
}

@inproceedings{10.1145/2009916.2010039,
author = {Blanco, Roi and Halpin, Harry and Herzig, Daniel M. and Mika, Peter and Pound, Jeffrey and Thompson, Henry S. and Tran Duc, Thanh},
title = {Repeatable and reliable search system evaluation using crowdsourcing},
year = {2011},
isbn = {9781450307574},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2009916.2010039},
doi = {10.1145/2009916.2010039},
abstract = {The primary problem confronting any new kind of search task is how to boot-strap a reliable and repeatable evaluation campaign, and a crowd-sourcing approach provides many advantages. However, can these crowd-sourced evaluations be repeated over long periods of time in a reliable manner? To demonstrate, we investigate creating an evaluation campaign for the semantic search task of keyword-based ad-hoc object retrieval. In contrast to traditional search over web-pages, object search aims at the retrieval of information from factual assertions about real-world objects rather than searching over web-pages with textual descriptions. Using the first large-scale evaluation campaign that specifically targets the task of ad-hoc Web object retrieval over a number of deployed systems, we demonstrate that crowd-sourced evaluation campaigns can be repeated over time and still maintain reliable results. Furthermore, we show how these results are comparable to expert judges when ranking systems and that the results hold over different evaluation and relevance metrics. This work provides empirical support for scalable, reliable, and repeatable search system evaluation using crowdsourcing.},
booktitle = {Proceedings of the 34th International ACM SIGIR Conference on Research and Development in Information Retrieval},
pages = {923–932},
numpages = {10},
keywords = {crowdsourcing, evaluation, retrieval, search engines},
location = {Beijing, China},
series = {SIGIR '11}
}

@inproceedings{10.1145/2463676.2465307,
author = {Gao, Jinyang and Liu, Xuan and Ooi, Beng Chin and Wang, Haixun and Chen, Gang},
title = {An online cost sensitive decision-making method in crowdsourcing systems},
year = {2013},
isbn = {9781450320375},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2463676.2465307},
doi = {10.1145/2463676.2465307},
abstract = {Crowdsourcing has created a variety of opportunities for many challenging problems by leveraging human intelligence. For example, applications such as image tagging, natural language processing, and semantic-based information retrieval can exploit crowd-based human computation to supplement existing computational algorithms. Naturally, human workers in crowdsourcing solve problems based on their knowledge, experience, and perception. It is therefore not clear which problems can be better solved by crowdsourcing than solving solely using traditional machine-based methods. Therefore, a cost sensitive quantitative analysis method is needed.In this paper, we design and implement a cost sensitive method for crowdsourcing. We online estimate the profit of the crowdsourcing job so that those questions with no future profit from crowdsourcing can be terminated. Two models are proposed to estimate the profit of crowdsourcing job, namely the linear value model and the generalized non-linear model. Using these models, the expected profit of obtaining new answers for a specific question is computed based on the answers already received. A question is terminated in real time if the marginal expected profit of obtaining more answers is not positive. We extends the method to publish a batch of questions in a HIT. We evaluate the effectiveness of our proposed method using two real world jobs on AMT. The experimental results show that our proposed method outperforms all the state-of-art methods.},
booktitle = {Proceedings of the 2013 ACM SIGMOD International Conference on Management of Data},
pages = {217–228},
numpages = {12},
keywords = {crowdsourcing, decision-making},
location = {New York, New York, USA},
series = {SIGMOD '13}
}

@inproceedings{10.1145/3529372.3533285,
author = {Oelen, Allard and Stocker, Markus and Auer, S\"{o}ren},
title = {TinyGenius: intertwining natural language processing with microtask crowdsourcing for scholarly knowledge graph creation},
year = {2022},
isbn = {9781450393454},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3529372.3533285},
doi = {10.1145/3529372.3533285},
abstract = {As the number of published scholarly articles grows steadily each year, new methods are needed to organize scholarly knowledge so that it can be more efficiently discovered and used. Natural Language Processing (NLP) techniques are able to autonomously process scholarly articles at scale and to create machine readable representations of the article content. However, autonomous NLP methods are by far not sufficiently accurate to create a high-quality knowledge graph. Yet quality is crucial for the graph to be useful in practice. We present TinyGenius, a methodology to validate NLP-extracted scholarly knowledge statements using microtasks performed with crowdsourcing. The scholarly context in which the crowd workers operate has multiple challenges. The explainability of the employed NLP methods is crucial to provide context in order to support the decision process of crowd workers. We employed TinyGenius to populate a paper-centric knowledge graph, using five distinct NLP methods. In the end, the resulting knowledge graph serves as a digital library for scholarly articles.},
booktitle = {Proceedings of the 22nd ACM/IEEE Joint Conference on Digital Libraries},
articleno = {5},
numpages = {5},
keywords = {crowdsourcing microtasks, intelligent user interfaces, knowledge graph validation, scholarly knowledge graphs},
location = {Cologne, Germany},
series = {JCDL '22}
}

@inproceedings{10.1145/3372787.3390435,
author = {Abhinav, Kumar and Bhatia, Gurpriya Kaur and Dubey, Alpana and Jain, Sakshi and Bhardwaj, Nitish},
title = {TasRec: a framework for task recommendation in crowdsourcing},
year = {2020},
isbn = {9781450370936},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3372787.3390435},
doi = {10.1145/3372787.3390435},
abstract = {Lately, crowdsourcing has emerged as a viable option of getting work done by leveraging the collective intelligence of the crowd. With many tasks posted every day, the size of crowdsourcing platforms is growing exponentially. Hence, workers face an important challenge of selecting the right task. Despite the task filtering criteria available on the platform to select the right task, crowd workers find it difficult to choose the most relevant task and must glean through the filtered tasks to find the relevant tasks. In this paper, we propose a framework for recommending tasks to workers. The proposed framework evaluates the worker's fitment over the tasks based on worker's preference, past tasks (s)he has performed, and tasks done by similar workers. We evaluated our approach on the datasets collected from popular crowdsourcing platform. Our experimental results based on 5,000 tasks and 3,000 workers show that the recommendation made by our framework is significantly better as compared to the baseline approach.},
booktitle = {Proceedings of the 15th International Conference on Global Software Engineering},
pages = {86–95},
numpages = {10},
keywords = {crowdsourcing, personalization, recommendation, task selection},
location = {Seoul, Republic of Korea},
series = {ICGSE '20}
}

@inproceedings{10.1007/978-3-642-24873-3_21,
author = {Viappiani, Paolo and Zilles, Sandra and Hamilton, Howard J. and Boutilier, Craig},
title = {Learning Complex Concepts Using Crowdsourcing: A Bayesian Approach},
year = {2011},
isbn = {978-3-642-24872-6},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-642-24873-3_21},
doi = {10.1007/978-3-642-24873-3_21},
abstract = {We develop a Bayesian approach to concept learning for crowdsourcing applications. A probabilistic belief over possible concept definitions is maintained and updated according to (noisy) observations from experts, whose behaviors are modeled using discrete types. We propose recommendation techniques, inference methods, and query selection strategies to assist a user charged with choosing a configuration that satisfies some (partially known) concept. Our model is able to simultaneously learn the concept definition and the types of the experts. We evaluate our model with simulations, showing that our Bayesian strategies are effective even in large concept spaces with many uninformative experts.},
booktitle = {Algorithmic Decision Theory: Second International Conference, ADT 2011, Piscataway, NJ, USA, October 26-28, 2011. Proceedings},
pages = {277–291},
numpages = {15},
keywords = {Bayesian Approach, Recommender System, Latent Concept, Current Belief, True Concept},
location = {Piscataway, NJ, USA}
}

@inproceedings{10.5555/2606265.2606914,
author = {Chaisiri, Sivadon},
title = {Utilizing Human Intelligence in a Crowdsourcing Marketplace for Big Data Processing},
year = {2013},
isbn = {9781479920815},
publisher = {IEEE Computer Society},
address = {USA},
abstract = {Crowd sourcing emerging as a distributed problem-solving model can dispatch tasks to a large number of human workers. The workers' human intelligence can be utilized to execute tasks which cannot be efficiently performed by computer-based systems. A crowd sourcing environment supports various data processing tasks, for example, data collection, data annotation, data validation, data classification, and natural language processing. In particular, the crowd sourcing environment can be applied for big data processing as well. In this paper, we propose Human-Intelligence-as-a-Service (HIaaS) which is a cloud computing service model utilizing human intelligence in a crowd sourcing marketplace to process big data. We also propose an optimization model based on stochastic programming for provisioning human workers in a HIaaS-based marketplace. Numerical studies are performed to evaluate the optimization model. A video rating system handling a big data set is an illustrative example analyzed in the studies. The results show that the model can efficiently reduce the cost to provision workers for processing a big data job.},
booktitle = {Proceedings of the 2013 International Conference on Parallel and Distributed Systems},
pages = {633–638},
numpages = {6},
keywords = {big data, cloud computing, crowdsourcing, stochastic programming},
series = {ICPADS '13}
}

@inproceedings{10.1145/3246859,
author = {Abdelmalik, Philip},
title = {Session details: Session 4 -- Big Data Analytics and Crowdsourcing for Public Health 1},
year = {2015},
isbn = {9781450334921},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3246859},
doi = {10.1145/3246859},
booktitle = {Proceedings of the 5th International Conference on Digital Health 2015},
location = {Florence, Italy},
series = {DH '15}
}

@inproceedings{10.5555/1564131.1564137,
author = {Hsueh, Pei-Yun and Melville, Prem and Sindhwani, Vikas},
title = {Data quality from crowdsourcing: a study of annotation selection criteria},
year = {2009},
publisher = {Association for Computational Linguistics},
address = {USA},
abstract = {Annotation acquisition is an essential step in training supervised classifiers. However, manual annotation is often time-consuming and expensive. The possibility of recruiting annotators through Internet services (e.g., Amazon Mechanic Turk) is an appealing option that allows multiple labeling tasks to be outsourced in bulk, typically with low overall costs and fast completion rates. In this paper, we consider the difficult problem of classifying sentiment in political blog snippets. Annotation data from both expert annotators in a research lab and non-expert annotators recruited from the Internet are examined. Three selection criteria are identified to select high-quality annotations: noise level, sentiment ambiguity, and lexical uncertainty. Analysis confirm the utility of these criteria on improving data quality. We conduct an empirical study to examine the effect of noisy annotations on the performance of sentiment classification models, and evaluate the utility of annotation selection on classification accuracy and efficiency.},
booktitle = {Proceedings of the NAACL HLT 2009 Workshop on Active Learning for Natural Language Processing},
pages = {27–35},
numpages = {9},
location = {Boulder, Colorado},
series = {HLT '09}
}

@inproceedings{10.5555/2772879.2773291,
author = {Biswas, Arpita and Jain, Shweta and Mandal, Debmalya and Narahari, Y.},
title = {A Truthful Budget Feasible Multi-Armed Bandit Mechanism for Crowdsourcing Time Critical Tasks},
year = {2015},
isbn = {9781450334136},
publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
address = {Richland, SC},
abstract = {Motivated by allocation and pricing problems faced by service requesters on modern crowdsourcing platforms, we study a multi-armed bandit (MAB) problem with several real-world features: (a) the requester wishes to crowdsource a number of tasks but has a fixed budget which leads to a trade-off between cost and quality while allocating tasks to workers; (b) each task has a fixed deadline and a worker who is allocated a task is not available until this deadline; (c) the qualities (probability of completing a task successfully within deadline) of crowd workers are not known; and (d) the crowd workers are strategic about their costs. We propose a mechanism that maximizes the expected number of successfully completed tasks, assuring budget feasibility, incentive compatibility, and individual rationality. We establish an upper bound of O(B2/3(K ln(KB))1/3) on the expected regret of the proposed mechanism with respect to an appropriate benchmark algorithm, where B is the total budget and K is the number of workers. Next, we provide a characterization of any deterministic truthful mechanism that solves the above class of problems and use this characterization to establish a lower bound of Ω(B2=3K1=3) on the expected regret for any budgeted MAB mechanism satisfying the above properties.},
booktitle = {Proceedings of the 2015 International Conference on Autonomous Agents and Multiagent Systems},
pages = {1101–1109},
numpages = {9},
keywords = {crowdsourcing, mechanism design, multi-armed bandits, online learning, rational agents, regret bound},
location = {Istanbul, Turkey},
series = {AAMAS '15}
}

@inproceedings{10.1145/1600150.1600168,
author = {Stewart, Osamuyimen and Huerta, Juan M. and Sader, Melissa},
title = {Designing crowdsourcing community for the enterprise},
year = {2009},
isbn = {9781605586724},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1600150.1600168},
doi = {10.1145/1600150.1600168},
abstract = {In this paper, we describe the design principles used for implementing crowdsourcing within the enterprise. This is based on our distinction between two kinds of crowdsourcing: enterprise (inside a firewall) versus the public domain. Whereas public domain crowdsourcing offers monetary rewards in exchange for participation, we show that identifying the right social objects and using these in designing the incentive model is sufficient to incent, motivate, and sustain participation levels in enterprise crowdsourcing. Finally, we show that the systematic integration of the characteristics of levels of participation into the design, e.g., the distinction between direct and indirect crowdsourcing, is sufficient for optimizing users' participation and contributions.},
booktitle = {Proceedings of the ACM SIGKDD Workshop on Human Computation},
pages = {50–53},
numpages = {4},
keywords = {crowdsourcing, design principles, incentives, social objects},
location = {Paris, France},
series = {HCOMP '09}
}

@inproceedings{10.1007/978-3-030-34407-8_6,
author = {Schnitzer, Steffen and Neitzel, Svenja and Schmidt, Sebastian and Rensing, Christoph},
title = {Results of a Survey About the Perceived Task Similarities in Micro Task Crowdsourcing Systems},
year = {2019},
isbn = {978-3-030-33906-7},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-34407-8_6},
doi = {10.1007/978-3-030-34407-8_6},
abstract = {Recommender mechanisms can support the assignment of jobs in crowdsourcing platforms. The use of recommendations can improve the quality and outcome for both worker and requester. Workers expect to get tasks similar to previously finished ones as recommendations, as a preceding study shows. Such similarities between tasks have to be identified and analyzed in order to create task recommendation systems that fulfil the workers’ requirements. How workers characterize task similarity has been left open in the previous study. Therefore, this work provides an empirical study on how workers perceive the similarities between tasks. Different similarity aspects (e.g., the complexity, required action or the requester of the task) are evaluated towards their usefulness and the results are discussed. Worker characteristics, such as age, experience and country of origin are taken into account to determine how different worker groups judge similarity aspects of tasks.},
booktitle = {Behavioral Analytics in Social and Ubiquitous Environments: 6th International Workshop on Mining Ubiquitous and Social Environments, MUSE 2015, Porto, Portugal, September 7, 2015; 6th International Workshop on Modeling Social Media, MSM 2015, Florence, Italy, May 19, 2015; 7th International Workshop on Modeling Social Media, MSM 2016, Montreal, QC, Canada, April 12, 2016; Revised Selected Papers},
pages = {107–125},
numpages = {19},
keywords = {Crowdsourcing, Recommender systems, User survey},
location = {Porto, Portugal}
}

@inproceedings{10.1109/HICSS.2013.64,
author = {Walter, Thomas P. and Back, Andrea},
title = {A Text Mining Approach to Evaluate Submissions to Crowdsourcing Contests},
year = {2013},
isbn = {9780769548920},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/HICSS.2013.64},
doi = {10.1109/HICSS.2013.64},
abstract = {This survey deals with the problem of evaluating the submissions to crowd sourcing websites on which data is increasing rapidly in both volume and complexity. Usually expert committees are installed to rate submissions, select winners and adjust monetary rewards. Thus, with an increasing number of submissions, this process is getting more complex, time-consuming and hence expensive. In this paper we suggest following text mining methodology, foremost similarity measurements and clustering algorithms, to evaluate the quality of submissions to crowd sourcing contests semi-automatically. We evaluate our approach by comparing text mining based measurement of more than 40'000 submissions with the real-world decisions made by expert committees using Precision and Recall together with F1-score.},
booktitle = {Proceedings of the 2013 46th Hawaii International Conference on System Sciences},
pages = {3109–3118},
numpages = {10},
keywords = {crowdsourcing, ideation contest, information retrieval, text mining},
series = {HICSS '13}
}

@inproceedings{10.1145/3397536.3422335,
author = {Li, Wei and Chen, Haiquan and Ku, Wei-Shinn and Qin, Xiao},
title = {Turbo-GTS: Scaling Mobile Crowdsourcing using Workload-Balancing Bisection Tree},
year = {2020},
isbn = {9781450380195},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3397536.3422335},
doi = {10.1145/3397536.3422335},
abstract = {In mobile crowdsourcing, workers are financially motivated to perform self-selected tasks to maximize their revenue. Unfortunately, the existing task scheduling approaches in mobile crowdsourcing fail to scale for massive tasks and large geographic areas. We present Turbo-GTS, a system that assigns tasks to each worker to maximize the total number of the tasks that can be completed for an entire worker group while taking into account various spatial and temporal constraints, such as task execution duration, task expiration time, and worker/task geographic locations. The core of Turbo-GTS is WBT-NNH and WBT-NUD, our two newly developed scheduling algorithms, which build on the algorithms, QT-NNH and QT-NUD, proposed in our prior work [5]. The key idea is that Turbo-GTS performs dynamic workload balancing among all workers using the proposed Workload-balancing Bisection Tree (WBT) in support of large-scale Geo-Task Scheduling (GTS). Turbo-GTS includes an interactive interface for users to load the current task/worker distributions and compare the task assignment of each worker returned by different algorithms in a real-time fashion. Using the Foursquare mobile user check-in data in New York City and Tokyo, we show the superiority of Turbo-GTS over the state of the art in terms of the total number of the tasks that can be accomplished by the entire worker group and the corresponding running time. We also demonstrate the front-end interface of Turbo-GTS with two exploratory use cases in New York City.},
booktitle = {Proceedings of the 28th International Conference on Advances in Geographic Information Systems},
pages = {155–158},
numpages = {4},
keywords = {Mobile Crowdsourcing, Quadtree, Workload balancing},
location = {Seattle, WA, USA},
series = {SIGSPATIAL '20}
}

@proceedings{10.1145/2506364,
title = {CrowdMM '13: Proceedings of the 2nd ACM international workshop on Crowdsourcing for multimedia},
year = {2013},
isbn = {9781450323963},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {Crowdsourcing for multimedia exploits a combination of human intelligence and a large number of individual human contributions. In the field of multimedia, crowdsourcing is just beginning to realize its full potential to contribute to techniques, systems and data sets that advance the state of the art. The ACM Multimedia 2013 Workshop on Crowdsourcing for Multimedia (CrowdMM 2013) provides a forum for the presentation of crowdsourcing techniques for multimedia and the discussion of innovative ideas that will allow the field of multimedia research to make use of crowdsourcing.The workshop uses presentations, posters and a panel to promote discussion and exchange between researchers concerning the scope and future of crowdsourcing. The workshop defines crowdsourcing in the broad sense: it encompasses both unsolicited human contributions, e.g., tags assigned by users to images, and also solicited contributions, e.g., annotations gathered by making use of crowdsourcing platforms that micro-outsource tasks to a large pool of human workers. As in 2012, the workshop pursues two high-level goals. First, it provides a venue to encourage multimedia research making use of human intelligence and taking advantage of human plurality. Second, the workshop gives an impetus to the multimedia community to define best practices for the use of crowdsourcing in multimedia and to shape emerging paradigms that will allow crowdsourcing to push forward the state of the art in multimedia research.},
location = {Barcelona, Spain}
}

@proceedings{10.5555/2859851,
title = {CSI-SE '15: Proceedings of the 2015 IEEE/ACM 2nd International Workshop on CrowdSourcing in Software Engineering},
year = {2015},
isbn = {9781467370400},
publisher = {IEEE Computer Society},
address = {USA}
}

@inproceedings{10.1109/ICEE.2010.1230,
author = {Peng, Guo},
title = {The Preliminary Settlement of Crowdsourcing Legal Issues},
year = {2010},
isbn = {9780769539973},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/ICEE.2010.1230},
doi = {10.1109/ICEE.2010.1230},
abstract = {Crowdsourcing was defined as the act of taking tasks traditionally performed by an employee or contractor, and outsourcing them to a group (crowd) of people or community in the form of an open call. The legal issues of crowdsourcing is mainly manifested in three aspects: the legal relationship between contract-issuing party and WitKey, the risk of misuse of personal information and the protection of intellectual property rights. Initial solutions to the problems include: to sign an underlying contract, disclosure of basic information on contract-issuing party and the provisions of fixed and processing personal information about Witkey.},
booktitle = {Proceedings of the 2010 International Conference on E-Business and E-Government},
pages = {4898–4900},
numpages = {3},
keywords = {crowdsourcing, legal issues, settlement},
series = {ICEE '10}
}

@inproceedings{10.5555/2484920.2485063,
author = {Tran-Thanh, Long and Venanzi, Matteo and Rogers, Alex and Jennings, Nicholas R.},
title = {Efficient budget allocation with accuracy guarantees for crowdsourcing classification tasks},
year = {2013},
isbn = {9781450319935},
publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
address = {Richland, SC},
abstract = {In this paper we address the problem of budget allocation for redundantly crowdsourcing a set of classification tasks where a key challenge is to find a trade-off between the total cost and the accuracy of estimation. We propose CrowdBudget, an agent-based budget allocation algorithm, that efficiently divides a given budget among different tasks in order to achieve low estimation error. In particular, we prove that CrowdBudget can achieve at most max{0, K/2- O,(√B)} estimation error with high probability, where K is the number of tasks and B is the budget size. This result significantly outperforms the current best theoretical guarantee from Karger et al,. In addition, we demonstrate that our algorithm outperforms existing methods by up to 40\% in experiments based on real-world data from a prominent database of crowdsourced classification responses.},
booktitle = {Proceedings of the 2013 International Conference on Autonomous Agents and Multi-Agent Systems},
pages = {901–908},
numpages = {8},
keywords = {budget limit, crowdsourcing, regret bounds, task allocation},
location = {St. Paul, MN, USA},
series = {AAMAS '13}
}

@inproceedings{10.5555/1866696.1866723,
author = {Grady, Catherine and Lease, Matthew},
title = {Crowdsourcing document relevance assessment with Mechanical Turk},
year = {2010},
publisher = {Association for Computational Linguistics},
address = {USA},
abstract = {We investigate human factors involved in designing effective Human Intelligence Tasks (HITs) for Amazon's Mechanical Turk. In particular, we assess document relevance to search queries via MTurk in order to evaluate search engine accuracy. Our study varies four human factors and measures resulting experimental outcomes of cost, time, and accuracy of the assessments. While results are largely inconclusive, we identify important obstacles encountered, lessons learned, related work, and interesting ideas for future investigation. Experimental data is also made publicly available for further study by the community.},
booktitle = {Proceedings of the NAACL HLT 2010 Workshop on Creating Speech and Language Data with Amazon's Mechanical Turk},
pages = {172–179},
numpages = {8},
location = {Los Angeles, California},
series = {CSLDAMT '10}
}

@inproceedings{10.1007/978-3-319-03889-6_13,
author = {Zhang, Gang and Chen, Haopeng},
title = {Quality Control of Massive Data for Crowdsourcing in Location-Based Services},
year = {2013},
isbn = {978-3-319-03888-9},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-319-03889-6_13},
doi = {10.1007/978-3-319-03889-6_13},
abstract = {Crowdsourcing has become a prospective paradigm for commercial purposes in the past decade, since it is based on a simple but powerful concept that virtually anyone has the potential to plug in valuable information, which brings a lot of benefits such as low cost and high immediacy, particularly in some location-based services (LBS). On the other side, there also exist many problems need to be solved in crowdsourcing. For example, the quality control for crowdsourcing systems has been identified as a significant challenge, which includes how to handle massive data more efficiently, how to discriminate poor quality content in workers’ submission and so on. In this paper, we put forward an approach to control the crowdsourcing quality by evaluating workers’ performance according to their submitted contents. Our experiments have demonstrated the effectiveness and efficiency of the approach.},
booktitle = {Algorithms and Architectures for Parallel Processing: 13th International Conference, ICA3PP 2013, Vietri Sul Mare, Italy, December 18-20, 2013, Proceedings, Part II},
pages = {112–121},
numpages = {10},
keywords = {crowdsourcing, massive data, quality control, location-based services (LBS)},
location = {Vietri sul Mare, Italy}
}

@inproceedings{10.1145/2568225.2568249,
author = {Stol, Klaas-Jan and Fitzgerald, Brian},
title = {Two's company, three's a crowd: a case study of crowdsourcing software development},
year = {2014},
isbn = {9781450327565},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2568225.2568249},
doi = {10.1145/2568225.2568249},
abstract = {Crowdsourcing is an emerging and promising approach which involves delegating a variety of tasks to an unknown workforce - the crowd. Crowdsourcing has been applied quite successfully in various contexts from basic tasks on Amazon Mechanical Turk to solving complex industry problems, e.g. InnoCentive. Companies are increasingly using crowdsourcing to accomplish specific software development tasks. However, very little research exists on this specific topic. This paper presents an in-depth industry case study of crowdsourcing software development at a multinational corporation. Our case study highlights a number of challenges that arise when crowdsourcing software development. For example, the crowdsourcing development process is essentially a waterfall model and this must eventually be integrated with the agile approach used by the company. Crowdsourcing works better for specific software development tasks that are less complex and stand-alone without interdependencies. The development cost was much greater than originally expected, overhead in terms of company effort to prepare specifications and answer crowdsourcing community queries was much greater, and the time-scale to complete contests, review submissions and resolve quality issues was significant. Finally, quality issues were pushed later in the lifecycle given the lengthy process necessary to identify and resolve quality issues. Given the emphasis in software engineering on identifying bugs as early as possible, this is quite problematic.},
booktitle = {Proceedings of the 36th International Conference on Software Engineering},
pages = {187–198},
numpages = {12},
keywords = {Crowdsourcing, case study, challenges, software development},
location = {Hyderabad, India},
series = {ICSE 2014}
}

@inproceedings{10.5555/2029127.2029152,
author = {Koch, Giordano and F\"{u}ller, Johann and Brunswicker, Sabine},
title = {Online crowdsourcing in the public sector: how to design open government platforms},
year = {2011},
isbn = {9783642217951},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {The trend towards "open innovation" has revitalized firm's interest in tapping into external innovation sources. Firms purposively open their business models to connect internal and external ideas, and to co-create value with partners and users. Internet-based crowdsourcing and co-creation platforms have changed the way how firms implement open innovation. They allow new participatory problem solving and value-creation processes. However, the current discussion on open innovation has hardly touched upon the public sector. This paper investigates if crowdsourcing platforms can be applied in the governmental context, and under which conditions. Results show that crowdsourcing may generate strong interest among citizens and may serve as source of new high quality input. However, our findings also indicate that design principles derived from open innovation projects in the corporate world may not be directly applied in the governmental context; they need to be adjusted and complemented.},
booktitle = {Proceedings of the 4th International Conference on Online Communities and Social Computing},
pages = {203–212},
numpages = {10},
keywords = {crowdsourcing, design principles, open government, open innovation, public management, virtual co-creation platform},
location = {Orlando, FL},
series = {OCSC'11}
}

@inproceedings{10.1145/2232817.2232840,
author = {Chen, Yinlin and Bogen, Paul Logasa and Hsieh, Haowei and Fox, Edward A. and Cassel, Lillian N.},
title = {Categorization of computing education resources with utilization of crowdsourcing},
year = {2012},
isbn = {9781450311540},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2232817.2232840},
doi = {10.1145/2232817.2232840},
abstract = {The Ensemble Portal harvests resources from multiple heterogeneous federated collections. Managing these dynamically increasing collections requires an automatic mechanism to categorize records in to corresponding topics. We propose an approach to use existing ACM DL metadata to build classifiers for harvested resources in the Ensemble project. We also present our experience with utilizing the Amazon Mechanical Turk platform to build ground truth training data sets from Ensemble collections.},
booktitle = {Proceedings of the 12th ACM/IEEE-CS Joint Conference on Digital Libraries},
pages = {121–124},
numpages = {4},
keywords = {amazon mechanical turk, classification, digital libraries, machine learning},
location = {Washington, DC, USA},
series = {JCDL '12}
}

@inproceedings{10.5555/2393015.2393073,
author = {Post, Matt and Callison-Burch, Chris and Osborne, Miles},
title = {Constructing parallel corpora for six Indian languages via crowdsourcing},
year = {2012},
publisher = {Association for Computational Linguistics},
address = {USA},
abstract = {Recent work has established the efficacy of Amazon's Mechanical Turk for constructing parallel corpora for machine translation research. We apply this to building a collection of parallel corpora between English and six languages from the Indian subcontinent: Bengali, Hindi, Malayalam, Tamil, Telugu, and Urdu. These languages are low-resource, under-studied, and exhibit linguistic phenomena that are difficult for machine translation. We conduct a variety of baseline experiments and analysis, and release the data to the community.},
booktitle = {Proceedings of the Seventh Workshop on Statistical Machine Translation},
pages = {401–409},
numpages = {9},
location = {Montreal, Canada},
series = {WMT '12}
}

@inproceedings{10.1109/HICSS.2015.549,
author = {Buettner, Ricardo},
title = {A Systematic Literature Review of Crowdsourcing Research from a Human Resource Management Perspective},
year = {2015},
isbn = {9781479973675},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/HICSS.2015.549},
doi = {10.1109/HICSS.2015.549},
abstract = {From a human resource management perspective I review the crowd sourcing literature included in top peer-reviewed journals and conferences, and build up a comprehensive picture. Based on this I identify empirical and design-oriented research needs.},
booktitle = {Proceedings of the 2015 48th Hawaii International Conference on System Sciences},
pages = {4609–4618},
numpages = {10},
keywords = {coordination problem, crowdsourcing, human resource management perspective, literature review, research framework},
series = {HICSS '15}
}

@inproceedings{10.5555/1964698.1964721,
author = {Fu, Wai-Tat and Liao, Vera},
title = {Crowdsourcing quality control of online information: a quality-based cascade model},
year = {2011},
isbn = {9783642196553},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {We extend previous cascade models of social influence to investigate how the exchange of quality information among users may moderate cascade behavior, and the extent to which it may influence the effectiveness of collective user recommendations on quality control of information. We found that while cascades do sometimes occur, their effects depend critically on the accuracies of individual quality assessments of information contents. Contrary to predictions of cascade models of information flow, quality-based cascades tend to reinforce the propagation of individual quality assessments rather than being the primary sources that drive the assessments. We found even small increase in individual accuracies will significantly improve the overall effectiveness of crowdsourcing quality control. Implication to domains such as online health information Web sites or product reviews are discussed.},
booktitle = {Proceedings of the 4th International Conference on Social Computing, Behavioral-Cultural Modeling and Prediction},
pages = {147–154},
numpages = {8},
keywords = {information cascades, social dynamics, user reviews, web quality},
location = {College Park, MD},
series = {SBP'11}
}

@inproceedings{10.1145/2491055.2491075,
author = {Estermann, Beat},
title = {Are memory institutions ready for open data and crowdsourcing? results of a pilot survey from Switzerland},
year = {2013},
isbn = {9781450318525},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2491055.2491075},
doi = {10.1145/2491055.2491075},
abstract = {Since the advent of the World Wide Web, the cultural heritage sector has undergone a series of changes. In a pilot survey among memory institutions (galleries, libraries, archives, museums) in Switzerland we have focused on two recent trends -- open data and crowdsourcing -- asking to what extent heritage institutions are ready to adopt open data policies and to embrace crowdsourcing strategies. The results suggest that so far, only very few institutions have adopted an open data policy. There are however signs that this may soon change: A majority of the surveyed institutions considers open data as important and believes that the opportunities prevail over the risks. Some obstacles however still need to be overcome, in particular the institutions' reservations with regard to "free" licensing and their fear of losing control. With regard to crowdsourcing the data suggest that the adoption process will be slower than for open data. Although approximately 10\% of the responding institutions seem already to experiment with crowdsourcing, there is no general breakthrough in sight, as a majority of respondents remain skeptical with regard to the benefits.},
booktitle = {Proceedings of the 9th International Symposium on Open Collaboration},
articleno = {29},
numpages = {3},
keywords = {crowdsourcing, cultural heritage, digitization, memory institutions, open data, participation, semantic web, user engagement},
location = {Hong Kong, China},
series = {WikiSym '13}
}

@inproceedings{10.1145/3410670.3410862,
author = {Ceccarini, Chiara and Delnevo, Giovanni and Prandi, Catia},
title = {FruGar: Exploiting Deep Learning and Crowdsourcing for Frugal Gardening},
year = {2020},
isbn = {9781450380782},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3410670.3410862},
doi = {10.1145/3410670.3410862},
abstract = {To make smart cities more sustainable, and technologies and smart services accessible to a broader range of people, the frugal innovation paradigm comes in handy. In such a scenario, an interesting issue to investigate is gardening (both home and community gardening), considered a possibility toward sustainable development and environmental resilience. Following this line of thought, in this paper, we present our system, called FruGar (frugal gardening), designed and developed to facilitate casual citizens while gardening. In particular, our approach takes advantage of machine learning and crowdsourcing to provide casual citizens with a frugal tool for plant disease detection.},
booktitle = {Proceedings of the 1st Workshop on Experiences with the Design and Implementation of Frugal Smart Objects},
pages = {7–11},
numpages = {5},
keywords = {convolutional neural network, plant disease identification, web application},
location = {London, United Kingdom},
series = {FRUGALTHINGS'20}
}

@inproceedings{10.5555/2343576.2343643,
author = {Kamar, Ece and Hacker, Severin and Horvitz, Eric},
title = {Combining human and machine intelligence in large-scale crowdsourcing},
year = {2012},
isbn = {0981738117},
publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
address = {Richland, SC},
abstract = {We show how machine learning and inference can be harnessed to leverage the complementary strengths of humans and computational agents to solve crowdsourcing tasks. We construct a set of Bayesian predictive models from data and describe how the models operate within an overall crowd-sourcing architecture that combines the efforts of people and machine vision on the task of classifying celestial bodies defined within a citizens' science project named Galaxy Zoo. We show how learned probabilistic models can be used to fuse human and machine contributions and to predict the behaviors of workers. We employ multiple inferences in concert to guide decisions on hiring and routing workers to tasks so as to maximize the efficiency of large-scale crowdsourcing processes based on expected utility.},
booktitle = {Proceedings of the 11th International Conference on Autonomous Agents and Multiagent Systems - Volume 1},
pages = {467–474},
numpages = {8},
keywords = {complementary computing, consensus tasks, crowdsourcing, decision-theoretic reasoning},
location = {Valencia, Spain},
series = {AAMAS '12}
}

@inproceedings{10.5555/3045118.3045231,
author = {Abbasi-Yadkori, Yasin and Bartlett, Peter L. and Chen, Xi and Malek, Alan},
title = {Large-scale Markov decision problems with KL control cost and its application to crowdsourcing},
year = {2015},
publisher = {JMLR.org},
abstract = {We study average and total cost Markov decision problems with large state spaces. Since the computational and statistical cost of finding the optimal policy scales with the size of the state space, we focus on searching for near-optimality in a low-dimensional family of policies. In particular, we show that for problems with a Kullback-Leibler divergence cost function, we can recast policy optimization as a convex optimization and solve it approximately using a stochastic subgradient algorithm. This method scales in complexity with the family of policies but not the state space. We show that the performance of the resulting policy is close to the best in the low-dimensional family. We demonstrate the efficacy of our approach by optimizing a policy for budget allocation in crowd labeling, an important crowdsourcing application.},
booktitle = {Proceedings of the 32nd International Conference on International Conference on Machine Learning - Volume 37},
pages = {1053–1062},
numpages = {10},
location = {Lille, France},
series = {ICML'15}
}

@inproceedings{10.1145/2396761.2398697,
author = {Kazai, Gabriella and Kamps, Jaap and Milic-Frayling, Natasa},
title = {The face of quality in crowdsourcing relevance labels: demographics, personality and labeling accuracy},
year = {2012},
isbn = {9781450311564},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2396761.2398697},
doi = {10.1145/2396761.2398697},
abstract = {Information retrieval systems require human contributed relevance labels for their training and evaluation. Increasingly such labels are collected under the anonymous, uncontrolled conditions of crowdsourcing, leading to varied output quality. While a range of quality assurance and control techniques have now been developed to reduce noise during or after task completion, little is known about the workers themselves and possible relationships between workers' characteristics and the quality of their work. In this paper, we ask how do the relatively well or poorly-performing crowds, working under specific task conditions, actually look like in terms of worker characteristics, such as demographics or personality traits. Our findings show that the face of a crowd is in fact indicative of the quality of their work.},
booktitle = {Proceedings of the 21st ACM International Conference on Information and Knowledge Management},
pages = {2583–2586},
numpages = {4},
keywords = {crowdsourcing, demographics, personality traits, worker accuracy},
location = {Maui, Hawaii, USA},
series = {CIKM '12}
}

@inproceedings{10.1145/2390803.2390817,
author = {Korshunov, Pavel and Cai, Shuting and Ebrahimi, Touradj},
title = {Crowdsourcing approach for evaluation of privacy filters in video surveillance},
year = {2012},
isbn = {9781450315890},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2390803.2390817},
doi = {10.1145/2390803.2390817},
abstract = {Extensive adoption of video surveillance, affecting many aspects of the daily life, alarms the concerned public about the increasing invasion into personal privacy. To address these concerns, many tools have been proposed for protection of personal privacy in image and video. However, little is understood regarding the effectiveness of such tools and especially their impact on the underlying surveillance tasks. In this paper, we propose conducting a subjective evaluation using crowdsourcing to analyze the tradeoff between the preservation of privacy offered by these tools and the intelligibility of activities under video surveillance. As an example, the proposed method is used to compare several commonly employed privacy protection techniques, such as blurring, pixelization, and masking applied to indoor surveillance video. Facebook based crowdsourcing application was specifically developed to gather the subjective evaluation data. Based on more than one hundred participants, the evaluation results demonstrate that the pixelization filter provides the best performance in terms of balance between privacy protection and intelligibility. The results obtained with crowdsourcing application were compared with results of previous work using more conventional subjective tests showing that they are highly correlated.},
booktitle = {Proceedings of the ACM Multimedia 2012 Workshop on Crowdsourcing for Multimedia},
pages = {35–40},
numpages = {6},
keywords = {crowdsourcing, evaluation methodology, intelligibility, privacy protection tools, video surveillance},
location = {Nara, Japan},
series = {CrowdMM '12}
}

@inproceedings{10.1145/2660114.2660121,
author = {Melenhorst, Mark and Men\'{e}ndez Blanco, Mar\'{\i}a and Larson, Martha},
title = {A Crowdsourcing Procedure for the Discovery of Non-Obvious Attributes of Social Images},
year = {2014},
isbn = {9781450331289},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2660114.2660121},
doi = {10.1145/2660114.2660121},
abstract = {Research on mid-level image representations has conventionally concentrated relatively obvious attributes and overlooked non-obvious attributes, i.e., characteristics that are not readily observable when images are viewed independently of their context or function. Non-obvious attributes are not necessarily easily nameable, but nonetheless they play a systematic role in people's interpretation of images. Clusters of related non-obvious attributes, called interpretation dimensions, emerge when people are asked to compare images, and provide important insight on aspects of social images that are considered relevant. In contrast to aesthetic or affective approaches to image analysis, non-obvious attributes are not related to the personal perspective of the viewer. Instead, they encode a conventional understanding of the world, which is tacit, rather than explicitly expressed. This paper provides an introduction to the notion of non-obvious image attributes of social images and introduces a procedure for discovering non-obvious attributes using crowdsourcing.},
booktitle = {Proceedings of the 2014 International ACM Workshop on Crowdsourcing for Multimedia},
pages = {45–48},
numpages = {4},
keywords = {content-based image indexing, crowdsourcing, human image understanding, social images, triadic elicitation},
location = {Orlando, Florida, USA},
series = {CrowdMM '14}
}

@inproceedings{10.1109/EIDWT.2012.17,
author = {Carter, Sarah and Smith, Martin and Bali, Suveena and Sotiriadis, Stelios and Bessis, Nik and Hill, Richard},
title = {The Use of Crowdsourcing to Aid Quest Design in Games},
year = {2012},
isbn = {9780769547343},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/EIDWT.2012.17},
doi = {10.1109/EIDWT.2012.17},
abstract = {In recent years, the usage of emerging technologies such as Grids, Clouds, data-smashups and crowd sourcing has become more widespread. This owes a lot to the continued growth of the Internet technology which has contributed to people becoming more comfortable about sharing information online, and using the Internet to communicate with others for both personal and professional purposes. The recent surge in smart phone usage has also helped to increase people's use of technology wherever they are - leading to innovative use of mobile phones to source and generate data for a variety of purposes. In this paper, we discuss the recent emerging technologies with a special focus on the possible usage of the crowd sourcing concept. Specifically, we discuss methods of quality verification to assist computer game design scenario case. Within this context, we present a technical architecture to enable quest design in games and conclude with prompting future steps of our work.},
booktitle = {Proceedings of the 2012 Third International Conference on Emerging Intelligent Data and Web Technologies},
pages = {302–305},
numpages = {4},
keywords = {Accessibility, Contributor verification, Crowdsourcing, Emerging technologies, Game Design, Quests},
series = {EIDWT '12}
}

@inproceedings{10.1109/DSDIS.2015.50,
author = {Dubey, Rameshwar and Luo, Zongwei and Xu, Meiling and Wamba, Samuel Fosso},
title = {Developing an Integration Framework for Crowdsourcing and Internet of Things with Applications for Disaster Response},
year = {2015},
isbn = {9781509002146},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/DSDIS.2015.50},
doi = {10.1109/DSDIS.2015.50},
abstract = {The crowdsourcing and internet of things (IoT) have played a significant role in revolutionizing the information age. There is significant literature which has attempted to investigate the role of crowdsourcing and IoT in improving disaster response. However there is hardly any literature which attempted to reflect upon integration of crowdsourcing and IoT to explain better coordination among disaster relief workers to improve disaster response. In response to pressing need, we have attempted to develop a theoretical framework which can help disaster relief workers to improve their coordination using valuable information derived using comprehensive crowdsourcing framework. In this study we have used two-prong research strategies. First we have conducted extensive review of articles published in reputable journals, magazines and blogs by eminent practitioners and policy makers followed by case studies: stampede in Godavari River at Rajahmundry (2015), earthquake in Nepal (2015), flood in Uttarakhand (2013). Finally we have concluded our research findings with further research directions.},
booktitle = {Proceedings of the 2015 IEEE International Conference on Data Science and Data Intensive Systems (DSDIS)},
pages = {520–524},
numpages = {5},
series = {DSDIS '15}
}

@inproceedings{10.1145/2494091.2499574,
author = {Hara, Tenshi and Springer, Thomas and Bombach, Gerd and Schill, Alexander},
title = {Decentralised approach for a reusable crowdsourcing platform utilising standard web servers},
year = {2013},
isbn = {9781450322157},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2494091.2499574},
doi = {10.1145/2494091.2499574},
abstract = {Crowdsourcing has gained increasing interest during the last years as means for solving complex tasks with the help of a flexible group of contributors. The crowd can contribute with collecting data in the field, completing map information or votes for ideas or products. Even though the participation of large numbers of users with heterogeneous devices in crowdsourcing is a highly recurrent task, generic infrastructures for crowdsourcing can be hardly found. Especially the management of users, mobile devices and contributed data has to be repetitively implemented in new projects. To ease the development of crowdsourcing applications, in this paper we propose a generic platform for crowdsourcing supporting diverse crowdsourcing scenarios, the ability to handle large numbers of users and the involvement of heterogeneous mobile devices. The evaluation is based on scalability and performance experiments in order to demonstrate the feasibility of our approach.},
booktitle = {Proceedings of the 2013 ACM Conference on Pervasive and Ubiquitous Computing Adjunct Publication},
pages = {1063–1074},
numpages = {12},
keywords = {crowdsourcing, location-based services},
location = {Zurich, Switzerland},
series = {UbiComp '13 Adjunct}
}

@inproceedings{10.1145/2494603.2480319,
author = {Akiki, Pierre and Bandara, Arosha and Yu, Yijun},
title = {Crowdsourcing user interface adaptations for minimizing the bloat in enterprise applications},
year = {2013},
isbn = {9781450321389},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2494603.2480319},
doi = {10.1145/2494603.2480319},
abstract = {Bloated software systems encompass a large number of features resulting in an increase in visual complexity. Enterprise applications are a common example of such types of systems. Since many users only use a distinct subset of the available features, providing a mechanism to tailor user interfaces according to each user's needs helps in decreasing the bloat thereby reducing the visual complexity. Crowdsourcing can be a means for speeding up the adaptation process by engaging and leveraging the enterprise application communities. This paper presents a tool supported model-driven mechanism for crowdsourcing user interface adaptations. We evaluate our proposed mechanism and tool through a basic preliminary user study.},
booktitle = {Proceedings of the 5th ACM SIGCHI Symposium on Engineering Interactive Computing Systems},
pages = {121–126},
numpages = {6},
keywords = {adaptable user interfaces, bloated ui, crowdsourcing, enterprise applications, model-driven engineering},
location = {London, United Kingdom},
series = {EICS '13}
}

@inproceedings{10.5555/2615731.2616086,
author = {Jain, Shweta and Gujar, Sujit and Zoeter, Onno and Narahari, Y.},
title = {A quality assuring multi-armed bandit crowdsourcing mechanism with incentive compatible learning},
year = {2014},
isbn = {9781450327381},
publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
address = {Richland, SC},
abstract = {We develop a novel multi-armed bandit (MAB) mechanism for the problem of selecting a subset of crowd workers to achieve an assured accuracy for each binary labelling task in a cost optimal way. This problem is challenging because workers have unknown qualities and strategic costs.},
booktitle = {Proceedings of the 2014 International Conference on Autonomous Agents and Multi-Agent Systems},
pages = {1609–1610},
numpages = {2},
keywords = {crowdsourcing, mechanism design, multi-armed bandit},
location = {Paris, France},
series = {AAMAS '14}
}

@inproceedings{10.1109/PIMRC.2015.7343628,
author = {Zhang, Liye and Valaee, Shahrokh and Zhang, Le and Xu, Yubin and Ma, Lin},
title = {Signal propagation-based outlier reduction technique (SPORT) for crowdsourcing in indoor localization using fingerprints},
year = {2015},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/PIMRC.2015.7343628},
doi = {10.1109/PIMRC.2015.7343628},
abstract = {Crowdsourcing allows for rapid deployment of indoor localization systems. However, compared to the conventional methods, crowdsourcing might collect fewer received signal strength (RSS) values, hence result in greater influence to outliers in RSS values. In this paper, we propose an algorithm to detect such outliers and to substitute them with more suitable RSS values. In particular, we investigate the relationship of RSS values between adjacent locations using a signal propagation model and show that the outliers can be corrected using a signal propagation model. We propose the Signal Propagation-based Outlier Reduction Technic (SPORT) for identifying and adjusting outlier values in both the offline training phase and the online localization phase. Experimental results show that SPORT greatly smoothens the radio map and improves the location accuracy.},
booktitle = {2015 IEEE 26th Annual International Symposium on Personal, Indoor, and Mobile Radio Communications (PIMRC)},
pages = {2008–2013},
numpages = {6},
location = {Hong Kong, China}
}

@inproceedings{10.1145/2063576.2063860,
author = {Kazai, Gabriella and Kamps, Jaap and Milic-Frayling, Natasa},
title = {Worker types and personality traits in crowdsourcing relevance labels},
year = {2011},
isbn = {9781450307178},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2063576.2063860},
doi = {10.1145/2063576.2063860},
abstract = {Crowdsourcing platforms offer unprecedented opportunities for creating evaluation benchmarks, but suffer from varied output quality from crowd workers who possess different levels of competence and aspiration. This raises new challenges for quality control and requires an in-depth understanding of how workers' characteristics relate to the quality of their work.In this paper, we use behavioral observations (HIT completion time, fraction of useful labels, label accuracy) to define five worker types: Spammer, Sloppy, Incompetent, Competent, Diligent. Using data collected from workers engaged in the crowdsourced evaluation of the INEX 2010 Book Track Prove It task, we relate the worker types to label accuracy and personality trait information along the `Big Five' personality dimensions.We expect that these new insights about the types of crowd workers and the quality of their work will inform how to design HITs to attract the best workers to a task and explain why certain HIT designs are more effective than others.},
booktitle = {Proceedings of the 20th ACM International Conference on Information and Knowledge Management},
pages = {1941–1944},
numpages = {4},
keywords = {bfi test, crowdsourcing relevance labels, worker typology},
location = {Glasgow, Scotland, UK},
series = {CIKM '11}
}

@inproceedings{10.1145/3299869.3320238,
author = {Han, Siyuan and Xu, Zihuan and Zeng, Yuxiang and Chen, Lei},
title = {Fluid: A Blockchain based Framework for Crowdsourcing},
year = {2019},
isbn = {9781450356435},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3299869.3320238},
doi = {10.1145/3299869.3320238},
abstract = {Recently, crowdsourcing has emerged as a new computing paradigm to solve problems that need human intrinsic, such as image annotation. However, there are two limitations in existing crowdsourcing platforms, i.e. non-transparent incentive mechanism and isolated profiles of workers, which harms the interests of both requesters and workers. Meanwhile, Blockchain technology introduces a solution to build a transparent, immutable data model in the Byzantine environment. Moreover, Blockchain systems (e.g. Ethereum) can also support the Tuning-complete script called smart contracts. Thus, we are motivated to use the feature of the transparent data model and smart contract in Blockchain to address the two limitations. Based on the proposed solutions, we have designed a Blockchain based framework which supports foundations of general crowdsourcing platforms. In addition, our framework also has following novel features: (1) it provides the transparent incentive mechanisms; (2) it supports a trusted worker's profile sharing in a cross-platform mode.},
booktitle = {Proceedings of the 2019 International Conference on Management of Data},
pages = {1921–1924},
numpages = {4},
keywords = {blockchain, crowdsourcing, incentive mechanism},
location = {Amsterdam, Netherlands},
series = {SIGMOD '19}
}

@inproceedings{10.1145/2396761.2398689,
author = {McCreadie, Richard and Macdonald, Craig and Ounis, Iadh and Giles, Jim and Jabr, Ferris},
title = {An examination of content farms in web search using crowdsourcing},
year = {2012},
isbn = {9781450311564},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2396761.2398689},
doi = {10.1145/2396761.2398689},
abstract = {On the Web, content farms produce articles engineered such that search engines rank them highly, in order to turn a profit from online advertising. Recently, content farms have increasingly been the target of demotion strategies by Web search engines, since content farm articles are often considered to be of suspect quality. In this paper, we study the prevalence of content farms in the results returned by three major Web search engines over time. In particular, we develop a crowdsourced approach to identify content farm articles from the results returned by these search engines. Our results show that between the period of March and August 2011, the number of content farm articles observed on a number of indicative queries was reduced by up to 55\% in the top ranks.},
booktitle = {Proceedings of the 21st ACM International Conference on Information and Knowledge Management},
pages = {2551–2554},
numpages = {4},
keywords = {content farms, crowdsourcing, web search},
location = {Maui, Hawaii, USA},
series = {CIKM '12}
}

@inproceedings{10.1145/1963192.1963206,
author = {Chen, Yan-Ying and Hsu, Winston H. and Liao, Hong-Yuan Mark},
title = {Learning facial attributes by crowdsourcing in social media},
year = {2011},
isbn = {9781450306379},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1963192.1963206},
doi = {10.1145/1963192.1963206},
abstract = {Facial attributes such as gender, race, age, hair style, etc., carry rich information for locating designated persons and profiling the communities from image/video collections (e.g., surveillance videos or photo albums). For plentiful facial attributes in photos and videos, collecting costly manual annotations for training detectors is time-consuming. We propose an automatic facial attribute detection method by exploiting the great amount of weakly labelled photos in social media. Our work can (1) automatically extract training images from the semantic-consistent user groups and (2) filter out noisy training photos by multiple mid-level features (by voting). Moreover, we introduce a method to harvest less-biased negative data for preventing uneven distribution of certain attributes. The experiments show that our approach can automatically acquire training photos for facial attributes and is on par with that by manual annotations.},
booktitle = {Proceedings of the 20th International Conference Companion on World Wide Web},
pages = {25–26},
numpages = {2},
keywords = {crowdsourcing, facial attribute, social media},
location = {Hyderabad, India},
series = {WWW '11}
}

@inproceedings{10.1145/2637064.2637102,
author = {Naiem, Ghada Farouk and Inoue, Sozo},
title = {A Method for Assessing User-generated Tests for Online Courses Exploiting Crowdsourcing Concept},
year = {2014},
isbn = {9781450327473},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2637064.2637102},
doi = {10.1145/2637064.2637102},
abstract = {In this research, we focus on the challenge of user-generated tests, where crowdsourcing of users have the chance of making new and innovative questions that increases the allowance for a teacher to create a new test. This approach replaces the dependence of stored questions/ answers pair that encourage memory skills not learning skills. The limitation of how to accept these questions contents is raised up. In this paper we propose a question acceptance method based on two measures that weight the question's score, which are difficulty and goodness factors. Since the formalization of assessment and goodness which we made in this paper are recursive, we also propose a heuristic algorithm for iteratively calculating both values. Moreover, we show simulation result to confirm that the algorithm converges, and that it reflects the difficulty and the goodness factors.},
booktitle = {Proceedings of the 2014 International Workshop on Web Intelligence and Smart Sensing},
pages = {1–6},
numpages = {6},
location = {Saint Etienne, France},
series = {IWWISS '14}
}

@inproceedings{10.1145/2556288.2556986,
author = {Kim, Juho and Nguyen, Phu Tran and Weir, Sarah and Guo, Philip J. and Miller, Robert C. and Gajos, Krzysztof Z.},
title = {Crowdsourcing step-by-step information extraction to enhance existing how-to videos},
year = {2014},
isbn = {9781450324731},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2556288.2556986},
doi = {10.1145/2556288.2556986},
abstract = {Millions of learners today use how-to videos to master new skills in a variety of domains. But browsing such videos is often tedious and inefficient because video player interfaces are not optimized for the unique step-by-step structure of such videos. This research aims to improve the learning experience of existing how-to videos with step-by-step annotations.We first performed a formative study to verify that annotations are actually useful to learners. We created ToolScape, an interactive video player that displays step descriptions and intermediate result thumbnails in the video timeline. Learners in our study performed better and gained more self-efficacy using ToolScape versus a traditional video player.To add the needed step annotations to existing how-to videos at scale, we introduce a novel crowdsourcing workflow. It extracts step-by-step structure from an existing video, including step times, descriptions, and before and after images. We introduce the Find-Verify-Expand design pattern for temporal and visual annotation, which applies clustering, text processing, and visual analysis algorithms to merge crowd output. The workflow does not rely on domain-specific customization, works on top of existing videos, and recruits untrained crowd workers. We evaluated the workflow with Mechanical Turk, using 75 cooking, makeup, and Photoshop videos on YouTube. Results show that our workflow can extract steps with a quality comparable to that of trained annotators across all three domains with 77\% precision and 81\% recall.},
booktitle = {Proceedings of the SIGCHI Conference on Human Factors in Computing Systems},
pages = {4017–4026},
numpages = {10},
keywords = {crowdsourcing, how-to videos, video annotation.},
location = {Toronto, Ontario, Canada},
series = {CHI '14}
}

@inproceedings{10.1109/HICSS.2014.67,
author = {Boughzala, Imed and Vreede, Triparna de and Nguyen, Cuong and Vreede, Gert-Jan de},
title = {Towards a Maturity Model for the Assessment of Ideation in Crowdsourcing Projects},
year = {2014},
isbn = {9781479925049},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/HICSS.2014.67},
doi = {10.1109/HICSS.2014.67},
abstract = {Social media technology has enabled virtual collaborative environments such that people can actively interact, share knowledge, coordinate activities, solve problems and co-create value. Organizations have begun to leverage approaches and technologies to involve numerous people from outside their boundaries to perform organizational tasks. Crowd sourcing is a collaboration model enabled by people-centric web technologies to solve individual, organizational, and societal problems using a dynamically formed crowd of people who respond to an open call for participation. Despite the success and popularity of this phenomenon, there appears to be a lack of guidance on how to organize the ideation processes in crowd sourcing. To address this need, we propose a Crowd sourcing Ideation Maturity Assessment Model (CIMAM). The CIMAM is intended to be sufficiently generic to be applied to different types of crowd sourcing initiatives/projects. It can be used by external assessors or by crowd sourcing organizers themselves for self-assessments. CIMAM was developed through a literature review and built on the constructs of Pedersen et al. model. This paper contributes to research by examining the various factors influencing crowd engagement and productivity.},
booktitle = {Proceedings of the 2014 47th Hawaii International Conference on System Sciences},
pages = {483–490},
numpages = {8},
keywords = {Crowdsourcing, Ideation, Maturity models, design science, mass collaboration},
series = {HICSS '14}
}

@inproceedings{10.1145/2691195.2691208,
author = {Halder, Buddhadeb},
title = {Crowdsourcing collection of data for crisis governance in the post-2015 world: potential offers and crucial challenges},
year = {2014},
isbn = {9781605586113},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2691195.2691208},
doi = {10.1145/2691195.2691208},
abstract = {The practice of 'crowdsourcing' under the new technological regime has opened doors of huge data repositories. In recent years, crowdsourcing have expanded rapidly allowing citizens to connect with each other, governments to connect with common mass, to coordinate disaster response work, to map political conflicts, acquiring information quickly and participating in issues that affect day-to-day life of citizens. Crowdsourcing has the potentiality to offer smart governance by gathering and analyzing massive data from citizens. As data is a key enabler to proper public governance, this paper aims to provide a picture of potential offers that 'crowdsourcing' could make in support of crisis governance in the Post-2015 World, while it illustrates some critical challenges of data protection and privacy in different service sectors. Lastly, with a brief analysis on privacy, online data protection; and safety level of some crowdsourcing tools, this paper proposes brief guidelines for different stakeholders and some future works to avoid some mismanagement of crowdsourced data to protect data, privacy and security of end users.},
booktitle = {Proceedings of the 8th International Conference on Theory and Practice of Electronic Governance},
pages = {1–10},
numpages = {10},
keywords = {PET, big data, crisis governance, data, online data protection, policy, privacy, public governance},
location = {Guimaraes, Portugal},
series = {ICEGOV '14}
}

@inproceedings{10.5555/1866696.1866709,
author = {Finin, Tim and Murnane, Will and Karandikar, Anand and Keller, Nicholas and Martineau, Justin and Dredze, Mark},
title = {Annotating named entities in Twitter data with crowdsourcing},
year = {2010},
publisher = {Association for Computational Linguistics},
address = {USA},
abstract = {We describe our experience using both Amazon Mechanical Turk (MTurk) and Crowd-Flower to collect simple named entity annotations for Twitter status updates. Unlike most genres that have traditionally been the focus of named entity experiments, Twitter is far more informal and abbreviated. The collected annotations and annotation techniques will provide a first step towards the full study of named entity recognition in domains like Facebook and Twitter. We also briefly describe how to use MTurk to collect judgements on the quality of "word clouds."},
booktitle = {Proceedings of the NAACL HLT 2010 Workshop on Creating Speech and Language Data with Amazon's Mechanical Turk},
pages = {80–88},
numpages = {9},
location = {Los Angeles, California},
series = {CSLDAMT '10}
}

@inproceedings{10.5555/2908698.2908717,
author = {Lease, Matthew},
title = {On quality control and machine learning in crowdsourcing},
year = {2011},
publisher = {AAAI Press},
abstract = {The advent of crowdsourcing has created a variety of new opportunities for improving upon traditional methods of data collection and annotation. This in turn has created intriguing new opportunities for data-driven machine learning (ML). Convenient access to crowd workers for simple data collection has further generalized to leveraging more arbitrary crowd-based human computation (von Ahn 2005) to supplement automated ML. While new potential applications of crowdsourcing continue to emerge, a variety of practical and sometimes unexpected obstacles have already limited the degree to which its promised potential can be actually realized in practice. This paper considers two particular aspects of crowdsourcing and their interplay, data quality control (QC) and ML, reflecting on where we have been, where we are, and where we might go from here.},
booktitle = {Proceedings of the 11th AAAI Conference on Human Computation},
pages = {97–102},
numpages = {6},
series = {AAAIWS'11-11}
}

@inproceedings{10.1145/2009916.2010089,
author = {Huang, Yen-Ta and Cheng, An-Jung and Hsieh, Liang-Chi and Hsu, Winston and Chang, Kuo-Wei},
title = {Region-based landmark discovery by crowdsourcing geo-referenced photos},
year = {2011},
isbn = {9781450307574},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2009916.2010089},
doi = {10.1145/2009916.2010089},
abstract = {We propose a novel model for landmark discovery that locates region-based landmarks on map in contrast to the traditional point-based landmarks. The proposed method preserves more information and automatically identifies candidate regions on map by crowdsourcing geo-referenced photos. Gaussian kernel convolution is applied to remove noises and generate detected region. We adopt F1 measure to evaluate discovered landmarks and manually check the association between tags and regions. The experiment results show that more than 90\% of attractions in the selected city can be correctly located by this method.},
booktitle = {Proceedings of the 34th International ACM SIGIR Conference on Research and Development in Information Retrieval},
pages = {1141–1142},
numpages = {2},
keywords = {crowdsourcing, geo-referenced photo., region-based},
location = {Beijing, China},
series = {SIGIR '11}
}

@proceedings{10.1145/2787394,
title = {C2B(1)D '15: Proceedings of the 2015 ACM SIGCOMM Workshop on Crowdsourcing and Crowdsharing of Big (Internet) Data},
year = {2015},
isbn = {9781450335393},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {On behalf of the organizers of C2B(I)D we welcome you to London. The workshop came about due to our realization of the steady growth in Internet-wide measurement infrastructures that have been applied towards a wide range of experiments. These experiments have created "big (Internet) data" with rich semantic content of the structure, dynamics, and usage of today's Internet at all levels ranging from physical, to application and service layer. Internet research remains hampered by wellknown issues, such as limited geographic and network diversity captured and the ever-present tension between privacy, measurement visibility and experimental control. Researchers have often re-purposed less-than-ideal, but readily available data collected by third parties, despite the many associated risks.To link the various strands of work of researchers who have resorted to crowdsourcing by enlisting a large and diverse set of end hosts and turning them into vantage points, we sought to address the key underlying issues: How to enlist these vantage points in the right locations? What sorts of experiments are technically and ethically viable? What is the right experimental model? How could we build a federation of platforms and curate and store crowdsourced big Internet data? We sought papers from the community and received a diverse set of papers from different slices of the research community.Our work was greatly reduced by the program committee drawn from a broad swath of industry, academia, and governmental organizations. They reviewed the papers in a short time and helped generate the technical program that includes a keynote, 3 technical sessions, and a panel. The sessions illustrate the breadth and depth of this emerging field and cover aspects that are concerned with the design and use of crowdsourcing systems and their many applications to a diverse set of networking problems (e.g., coverage prediction in mobile networks, characterization of broadband access, protocol design).},
location = {London, United Kingdom}
}

@inproceedings{10.1145/2638728.2638762,
author = {Huang, Yun and Wang, Yang and White, Corey},
title = {Designing a mobile system for public safety using open crime data and crowdsourcing},
year = {2014},
isbn = {9781450330473},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2638728.2638762},
doi = {10.1145/2638728.2638762},
abstract = {With more cities opening up crime data and the proliferation of participatory sensing, we explore ways to improve public safety of a local community by using open crime data and crowdsourcing. We first conducted an online survey to better understand the public safety needs of the Syracuse University (SU) community. Inspired by the survey results, we developed and deployed an Android mobile app in collaboration with the Department of Public Safety (DPS) at SU; the app integrates published safety incidents on a Google Map and SU campus alerts. We present our experience of co-designing this system with the DPS, challenges and experience of our initial app release. To design effective crowdsourcing of public safety information, we conducted a lab experiment to investigate what factors affect people's sharing decisions. The results suggest that both time of day and type of location significantly affect people's sharing decisions. These insights inform a re-design of our system to "nudge" people to report safety related information timely.},
booktitle = {Proceedings of the 2014 ACM International Joint Conference on Pervasive and Ubiquitous Computing: Adjunct Publication},
pages = {67–70},
numpages = {4},
keywords = {crowdsourcing, mobile app, public safety},
location = {Seattle, Washington},
series = {UbiComp '14 Adjunct}
}

@inproceedings{10.1145/2461381.2461388,
author = {Chon, Yohan and Kim, Yunjong and Cha, Hojung},
title = {Autonomous place naming system using opportunistic crowdsensing and knowledge from crowdsourcing},
year = {2013},
isbn = {9781450319591},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2461381.2461388},
doi = {10.1145/2461381.2461388},
abstract = {A user's location information is commonly used in diverse mobile services, yet providing the actual name or semantic meaning of a place is challenging. Previous works required manual user interventions for place naming, such as searching by additional keywords and/or selecting place in a list. We believe that applying mobile sensing techniques to this problem can greatly reduce user intervention. In this paper, we present an autonomous place naming system using opportunistic crowdsensing and knowledge from crowdsourcing. Our goal is to provide a place name from a person's perspective: that is, functional name (e.g., food place, shopping place), business name (e.g., Starbucks, Apple Store), or personal name (e.g., my home, my workplace). The main idea is to bridge the gap between crowdsensing data from smartphone users and location information in social network services. The proposed system automatically extracts a wide range of semantic features about the places from both crowdsensing data and social networks to model a place name. We then infer the place name by linking the crowdsensing data with knowledge in social networks. Extensive evaluations with real deployments show that the proposed system outperforms the related approaches and greatly reduces user intervention for place naming.},
booktitle = {Proceedings of the 12th International Conference on Information Processing in Sensor Networks},
pages = {19–30},
numpages = {12},
keywords = {location naming, location-based services, smartphone sensing},
location = {Philadelphia, Pennsylvania, USA},
series = {IPSN '13}
}

@inproceedings{10.1007/978-3-642-31040-9_8,
author = {Burger, John D. and Doughty, Emily and Bayer, Sam and Tresner-Kirsch, David and Wellner, Ben and Aberdeen, John and Lee, Kyungjoon and Kann, Maricel G. and Hirschman, Lynette},
title = {Validating candidate gene-mutation relations in MEDLINE abstracts via crowdsourcing},
year = {2012},
isbn = {9783642310393},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-642-31040-9_8},
doi = {10.1007/978-3-642-31040-9_8},
abstract = {We describe an experiment to elicit judgments on the validity of gene-mutation relations in MEDLINE abstracts via crowdsourcing. The biomedical literature contains rich information on such relations, but the correct pairings are difficult to extract automatically because a single abstract may mention multiple genes and mutations. We ran an experiment presenting candidate gene-mutation relations as Amazon Mechanical Turk HITs (human intelligence tasks). We extracted candidate mutations from a corpus of 250 MEDLINE abstracts using EMU combined with curated gene lists from NCBI. The resulting document-level annotations were projected into the abstract text to highlight mentions of genes and mutations for review. Reviewers returned results within 36 hours. Initial weighted results evaluated against a gold standard of expert curated gene-mutation relations achieved 85\% accuracy, with the best reviewer achieving 91\% accuracy. We expect performance to increase with further experimentation, providing a scalable approach for rapid manual curation of important biological relations.},
booktitle = {Proceedings of the 8th International Conference on Data Integration in the Life Sciences},
pages = {83–91},
numpages = {9},
keywords = {annotation, crowdsourcing, mutations, text mining},
location = {College Park, MD},
series = {DILS'12}
}

@inproceedings{10.5555/1884110.1884148,
author = {Vukovic, Maja and Laredo, Jim and Rajagopal, Sriram},
title = {Challenges and experiences in deploying enterprise crowdsourcing service},
year = {2010},
isbn = {3642139108},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {The value of crowdsourcing, arising from an instant access to a scalable expert network on-line, has been demonstrated by many success stories, such as GoldCorp, Netflix, and TopCoder. For enterprises, crowdsourcing promises significant cost-savings, quicker task completion times, and formation of expert communities (both within and outside the enterprise). Many aspects of the vision of enterprise crowdsourcing are under vigorous refinement. The reasons for this lack of progress, beyond the isolated and purpose-specific crowdsourcing efforts, are manifold. In this paper, we present our experience in deploying an enterprise crowdsourcing service in the IT Inventory Management domain. We focus on the technical and sociological challenges of creating enterprise crowdsourcing service that are generalpurpose, and that extend beyond mere specific-purpose, run-once prototypes. Such systems are deployed to the extent that they become an integrated part of business processes. Only when such degree of integration is achieved, the enteprises can fully adopt crowdsourcing and reap its benefits. We discuss the challenges in creating and deploying the enterprise crowdsourcing platform, and articulate current technical, governance and sociological issues towards defining a research agenda.},
booktitle = {Proceedings of the 10th International Conference on Web Engineering},
pages = {460–467},
numpages = {8},
keywords = {crowdsourcing process, enterprise crowdsourcing, governance},
location = {Vienna, Austria},
series = {ICWE'10}
}

@inproceedings{10.1109/COMPSAC.2015.117,
author = {Kubota, Takuya and Aritsugi, Masayoshi},
title = {How Many Ground Truths Should We Insert? Having Good Quality of Labeling Tasks in Crowdsourcing},
year = {2015},
isbn = {9781467365642},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/COMPSAC.2015.117},
doi = {10.1109/COMPSAC.2015.117},
abstract = {Having a lot of labels of good quality by crowd sourcing has attracted considerable interest recently. Ground truths can be helpful to this end, but prior work does not adequately address how many ground truths should be used. This paper presents a method for determining the number of ground truths. The number is determined by iteratively calculating the expected quality of labels if a ground truth is inserted into labeling tasks and comparing it with the limit of estimation quality of labels expectedly obtained by crowd sourcing. Our method can be applied to general EM algorithm-based approaches to estimating consensus labels of good quality. We compare our method with an EM algorithm-based approach, which is adopted to our method in the discussions of this paper, in terms of both efficiency of collecting labels from crowd and quality of labels obtained from the collected ones.},
booktitle = {Proceedings of the 2015 IEEE 39th Annual Computer Software and Applications Conference - Volume 02},
pages = {796–805},
numpages = {10},
keywords = {Condorcet Jury Theorem, EM algorithm, human computation},
series = {COMPSAC '15}
}

@inproceedings{10.5555/1920331.1920448,
author = {Geisler, Gary and Willard, Geoff and Whitworth, Eryn},
title = {Crowdsourcing the indexing of film and television media},
year = {2010},
publisher = {American Society for Information Science},
address = {USA},
abstract = {In this paper we describe a project that explores how advances in information technology could be used to make film and television media more accessible to both scholarly and non-scholarly audiences. By indexing, at a detailed level, a range of time-synchronized and non-time-synchronized elements in a test collection of 12 films and 8 television programs, we demonstrate how structured data representing many aspects of media content can be produced in a streamlined manner, and discuss how this work could potentially be augmented with automated indexing to be more efficient. We present examples of how this data can be utilized to produce a variety of tools and artifacts that make film and television media more accessible, and suggest that crowdsourcing could be an effective strategy for accomplishing this work on a larger scale. This research contributes to the growing body of literature exploring how multimedia collections can be made more accessible and useful for a variety of purposes.},
booktitle = {Proceedings of the 73rd ASIS&amp;T Annual Meeting on Navigating Streams in an Information Ecosystem - Volume 47},
articleno = {82},
numpages = {10},
keywords = {crowdsourcing, film and television, indexing, media, moving images, video, visualization},
location = {Pittsburgh, Pennsylvania},
series = {ASIS&amp;T '10}
}

@inproceedings{10.1145/2457413.2457426,
author = {Liao, Chen-Chih and Hsu, Cheng-Hsin},
title = {A detour planning algorithm in crowdsourcing systems for multimedia content gathering},
year = {2013},
isbn = {9781450318938},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2457413.2457426},
doi = {10.1145/2457413.2457426},
abstract = {Crowdsourcing is a popular paradigm that outsources tasks to general publics. In crowdsourcing systems, requesters submit requests and workers perform selective requests for rewards. We propose a crowdsourcing system for multimedia content gathering, which is a new class of crowdsourcing systems with requests that must be performed at specific locations and time. The workers in our systems gather multimedia content using their smartphones and share the collected content over the Internet. Our system computes a detour path for each worker, who supplies his/her destination with a deadline to the system, and do not mind to take detour paths and perform some requests for profits. More precisely, we develop a detour planning algorithm to produce the optimal detour paths for individual workers. Using extensive trace-driven simulations, we demonstrate the effectiveness and efficiency of our algorithm: for example, it helps workers to achieve optimal profits (up to ~ 100\% improvement compared to baseline solutions) and runs in real-time (&lt; 82 ms). Developing a working prototype on Android OS and addressing other challenging aspects of the considered systems are among our future tasks.},
booktitle = {Proceedings of the 5th Workshop on Mobile Video},
pages = {55–60},
numpages = {6},
location = {Oslo, Norway},
series = {MoVid '13}
}

@inproceedings{10.5555/2209408.2209425,
author = {Theodosiou, Zenonas and Tsapatsoulis, Nicolas},
title = {On the creation of visual models for keywords through crowdsourcing},
year = {2012},
isbn = {9781618040749},
publisher = {World Scientific and Engineering Academy and Society (WSEAS)},
address = {Stevens Point, Wisconsin, USA},
abstract = {Crowdsourcing annotation is a recent development since a complete and elaborate annotation of the content of an image is an extremely labour-intensive and time consuming task. In this paper we examine the possibility to build accurate visual models for keywords created through crowdsourcing. Specifically, 8 different keywords related to athletics domain have been modelled using MPEG-7 and Histogram of Oriented Gradients (HOG) low level features and the Sequential Minimal Optimization (SMO) classifier. The experimental results have been examined using accuracy metrics and are very promising showing the ability of the visual models to classify the images into the 8 classes with the highest average accuracy rate of 73.13\% in the purpose of the HOG features.},
booktitle = {Proceedings of the 11th International Conference on Applications of Electrical and Computer Engineering},
pages = {96–100},
numpages = {5},
keywords = {crowdsourced annotation, low level features, visual models},
location = {Athens, Greece},
series = {ACA'12}
}

@inproceedings{10.1007/978-3-319-05359-2_11,
author = {Burger, Valentin and Hirth, Matthias and Schwartz, Christian and Hoβfeld, Tobias and Tran-Gia, Phuoc},
title = {Increasing the Coverage of Vantage Points in Distributed Active Network Measurements by Crowdsourcing},
year = {2014},
isbn = {9783319053585},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-319-05359-2_11},
doi = {10.1007/978-3-319-05359-2_11},
abstract = {Internet video constitutes more than half of all consumer traffic. Most of the video traffic is delivered by content delivery networks (CDNs). The huge amount of traffic from video CDNs poses problems to access providers. To understand and monitor the impact of video traffic on access networks and the topology of CDNs, distributed active measurements are needed. Recently used measurement platforms are mainly hosted in National Research and Education Networks (NRENs). However, the view of these platforms on the CDN is very limited, since the coverage of NRENs is low in developing countries. Furthermore, campus networks do not reflect the characteristics of end user access networks. We propose to use crowdsourcing to increase the coverage of vantage points in distributed active network measurements. In this study, we compare measurements of a global CDN conducted in PlanetLab with measurements assigned to workers of a crowdsourcing platform. Thus, the coverage of vantage points and the sampled part of the global video CDN are analyzed. Our results show that the capability of PlanetLab to measure global CDNs is rather low, since the vast majority of requests is directed to the US. By using a crowdsourcing platform we obtain a diverse set of vantage points that reveals more than twice as many autonomous systems deploying video servers.},
booktitle = {Proceedings of the 17th International GI/ITG Conference on Measurement, Modelling, and Evaluation of Computing Systems and Dependability and Fault Tolerance - Volume 8376},
pages = {151–161},
numpages = {11},
location = {Bamberg, Germany},
series = {MMB \&amp; DFT 2014}
}

@inproceedings{10.1145/2212776.2223826,
author = {Zhu, Shaojian and Kane, Shaun and Feng, Jinjuan and Sears, Andrew},
title = {A crowdsourcing quality control model for tasks distributed in parallel},
year = {2012},
isbn = {9781450310161},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2212776.2223826},
doi = {10.1145/2212776.2223826},
abstract = {Quality control for crowdsourcing systems has been identified as a significant challenge [2]. We propose a data-driven model for quality control in the context of crowdsourcing systems with the goal of assessing the quality of each individual contribution for parallel distributed tasks (allowing multiple people working on a same task). The model is initiated with a data training process providing a rough estimate for several quality-related performance measures (e.g. time spent on a task). The initial estimates are combined with observations of results produced by workers to estimate the quality for each individual contribution. We conduct a study to evaluate the model in the context of improving speech recognition-based text correction using MTurk services. Results indicate that the model accurately predicts quality for more than 92\% of the non-negative (useful) contributions and 96\% of the negative (useless) ones.},
booktitle = {CHI '12 Extended Abstracts on Human Factors in Computing Systems},
pages = {2501–2506},
numpages = {6},
keywords = {crowdsourcing, parallel distribution, quality control},
location = {Austin, Texas, USA},
series = {CHI EA '12}
}

@inproceedings{10.5555/2900929.2901104,
author = {Nath, Swaprava and Dayama, Pankaj and Garg, Dinesh and Narahari, Y. and Zou, James},
title = {Threats and trade-offs in resource critical crowdsourcing tasks over networks},
year = {2012},
publisher = {AAAI Press},
abstract = {In recent times, crowdsourcing over social networks has emerged as an active tool for complex task execution. In this paper, we address the problem faced by a planner to incentivize agents in the network to execute a task and also help in recruiting other agents for this purpose.We study this mechanism design problem under two natural resource optimization settings: (1) cost critical tasks, where the planner's goal is to minimize the total cost, and (2) time critical tasks, where the goal is to minimize the total time elapsed before the task is executed. We define a set of fairness properties that should be ideally satisfied by a crowdsourcing mechanism. We prove that no mechanism can satisfy all these properties simultaneously. We relax some of these properties and define their approximate counterparts. Under appropriate approximate fairness criteria, we obtain a non-trivial family of payment mechanisms. Moreover, we provide precise characterizations of cost critical and time critical mechanisms.},
booktitle = {Proceedings of the Twenty-Sixth AAAI Conference on Artificial Intelligence},
pages = {2447–2448},
numpages = {2},
location = {Toronto, Ontario, Canada},
series = {AAAI'12}
}

@inproceedings{10.1145/2591888.2591899,
author = {Seidel, Claudius E. and Thapa, Basanta E. P. and Plattfaut, Ralf and Niehaves, Bj\"{o}rn},
title = {Selective crowdsourcing for open process innovation in the public sector: are expert citizens really willing to participate?},
year = {2013},
isbn = {9781450324564},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2591888.2591899},
doi = {10.1145/2591888.2591899},
abstract = {In light of current challenges that modern societies are facing (e.g. demographic change, financial budgetary constraints, and demand for individualized public services), public administrations need to find innovative ways to deliver public services more efficiently. One possible solution to the dilemma of shrinking resources and increasing demands is open innovation. Various papers have already established the idea of crowdsourcing as a means of open innovation in the public sector. In order to enrich theory and practice in the field of collaborative innovation processes, this research focuses on the willingness of citizens to participate in crowdsourcing for innovation. More specifically, we highlight the role of expert citizens in selective crowdsourcing for complex tasks in the public sector with the concrete example of process innovations. We examine different levels of willingness to participate in crowdsourcing by means of a quantitative analysis of a questionnaire survey with n=128 German citizens. Our analysis shows that citizens are indeed motivated to participate in selective crowdsourcing to generate solutions to complex problems in the public sector. Although mobilizing adequate experts for complex tasks may seem challenging, we find that expert citizens actually have a higher willingness to collaborate on complex as well as simple tasks than non-experts. Additionally, financial incentives remain a relevant instrument in the design of citizensourcing projects. Ultimately, the role of age as an influence to participate in crowdsourcing will be discussed.},
booktitle = {Proceedings of the 7th International Conference on Theory and Practice of Electronic Governance},
pages = {64–72},
numpages = {9},
keywords = {citizensourcing, incentivization, motivation, process innovation, public administration, selective crowdsourcing},
location = {Seoul, Republic of Korea},
series = {ICEGOV '13}
}

@inproceedings{10.1145/2675316.2675319,
author = {Fonteles, Andr\'{e} Sales and Bouveret, Sylvain and Gensel, J\'{e}r\^{o}me},
title = {Towards matching improvement between spatio-temporal tasks and workers in mobile crowdsourcing market systems},
year = {2014},
isbn = {9781450331425},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2675316.2675319},
doi = {10.1145/2675316.2675319},
abstract = {Crowdsourcing market systems (CMS) are platforms that enable one to publish tasks that others are intended to accomplished. Usually, these are systems where users, called workers, perform tasks using desktop computers. Recently, mobile CMSs have appeared with tasks that exploit the mobility and the location of workers. For example, if a third party system requires a picture of a given place, it may publish a task asking for some worker to go there, take this picture and upload it. One problem of CMSs is that the more tasks they have, the harder it is for workers to find and choose one they are interested in. Besides, workers who accomplish tasks may have no particular experience and consequently provide bad results for tasks. In order to improve the matching between workers and spatio-temporal tasks in mobile CMSs, we propose a conceptual framework that consists of two mechanisms. One considers the requirements of a task for selecting suitable workers, while the other recommends tasks for a worker according to his preferences and skills. As a result, workers spend less time searching tasks, more working on it, providing results with higher quality.},
booktitle = {Proceedings of the Third ACM SIGSPATIAL International Workshop on Mobile Geographic Information Systems},
pages = {43–50},
numpages = {8},
keywords = {crowdsourcing market systems, mobile crowdsourcing, recommender systems, spatial crowdsourcing, task matching},
location = {Dallas, Texas},
series = {MobiGIS '14}
}

@proceedings{10.1145/2390803,
title = {CrowdMM '12: Proceedings of the ACM multimedia 2012 workshop on Crowdsourcing for multimedia},
year = {2012},
isbn = {9781450315890},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {Crowdsourcing for multimedia involves exploiting both human intelligence and the combination of a large number of individual human contributions (i.e., the 'wisdom of the crowd') to develop techniques, systems and data sets that advance the state of the art. The ACM Multimedia 2012 Workshop on Crowdsourcing for Multimedia (CrowdMM 2012) provides a forum presenting crowdsourcing techniques for multimedia, as well as innovative ideas exemplifying how multimedia research can benefit from crowdsourcing.Through presented papers, invited talks and a panel, the workshop aims to promote interactive discussion on the scope and research potentials of crowdsourcing. The workshop views crowdsourcing in the broad sense: it encompasses both unsolicited human contributions, e.g., tags assigned by users to images, and also solicited contributions, e.g., annotations gathered by making use of crowdsourcing platforms that micro-outsource tasks to a large pool of human workers. The high-level goals of the workshop are twofold. First, it provides a venue to encourage multimedia research making use of human intelligence and taking advantage of human plurality. Second, the workshop gives an impetus to the multimedia community to define best practices for the use of crowdsourcing in multimedia and to shape emerging paradigms that will allow crowdsourcing to push forward the state of the art in multimedia research.We are excited about the variety of papers that are included in the workshop program, which reflect the global momentum in research in this field. The selected works cover several key topics in the crowdsourcing research spectrum, such as how to control the quality of crowd input, how to present complex tasks for a crowd to solve, and how to apply crowdsourcing in a variety of novel applications. We also like to extend a special word of thanks to Prof. Masataka Goto of the National Institute of Advanced Industrial Science and Technology (AIST), Japan, for his keynote speech entitled "PodCastle and Songle: Crowdsourcing-Based Web Services for Spoken Content Retrieval and Active Music Listening." His keynote discusses how crowdsourcing can be utilized to advance speech-recognition and music-understanding technologies and systems.},
location = {Nara, Japan}
}

@inproceedings{10.1007/978-3-642-36415-0_9,
author = {Jones, Gareth J. F.},
title = {An introduction to crowdsourcing for language and multimedia technology research},
year = {2012},
isbn = {9783642364143},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-642-36415-0_9},
doi = {10.1007/978-3-642-36415-0_9},
abstract = {Language and multimedia technology research often relies on large manually constructed datasets for training or evaluation of algorithms and systems. Constructing these datasets is often expensive with significant challenges in terms of recruitment of personnel to carry out the work. Crowdsourcing methods using scalable pools of workers available on-demand offers a flexible means of rapid low-cost construction of many of these datasets to support existing research requirements and potentially promote new research initiatives that would otherwise not be possible.},
booktitle = {Proceedings of the 2012 International Conference on Information Retrieval Meets Information Visualization},
pages = {132–154},
numpages = {23},
keywords = {crowdsourcing, human computation, human language technologies, multimedia technologies},
location = {Zinal, Switzerland},
series = {PROMISE'12}
}

@inproceedings{10.1007/978-3-319-26190-4_20,
author = {El Maarry, Kinda and G\"{u}ntzer, Ulrich and Balke, Wolf-Tilo},
title = {A Majority of Wrongs Doesn't Make It Right - On Crowdsourcing Quality for Skewed Domain Tasks},
year = {2015},
isbn = {9783319261898},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-319-26190-4_20},
doi = {10.1007/978-3-319-26190-4_20},
abstract = {Today, crowdsourcing has emerged as a promising paradigm for annotating, structuring, and managing Web data. Still, as long as the problem of the crowd workers' trustworthiness in terms of result quality is not essentially solved, all these efforts remain doubtful. Therefore, in this paper we look at today's dominant quality assurance techniques and investigate how they cope with Web data, i.e. typical long-tail distributions, making it easy for strategic spammers to guess the prevalent answers and thus to go undetected. We provide a thorough theoretical analysis, quantifying the success of different methods on such skewed domains by means of test theory and show their individual weaknesses. Exploiting our case study analysis, we propose a simple privacy-preserving, task-agnostic model to improve test reliability, while actually decreasing overhead costs for quality assurance. Finally, we show the stability of our method for even higher numbers of spammers in controlled crowdsourcing experiments.},
booktitle = {Proceedings, Part I, of the 16th International Conference on Web Information Systems Engineering --- WISE 2015 - Volume 9418},
pages = {293–308},
numpages = {16},
keywords = {Crowdsourcing, Fraud detection, Quality control, Result quality}
}

@inproceedings{10.1145/3342280.3342307,
author = {Mok, Ricky K. P. and Kawaguti, Ginga and claffy, kc},
title = {QUINCE: A unified crowdsourcing-based QoE measurement platform},
year = {2019},
isbn = {9781450368865},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3342280.3342307},
doi = {10.1145/3342280.3342307},
abstract = {Assessing QoE in situ is a challenging task. Researchers have employed crowdsourcing-based approaches to achieve scale and diversity of subjects, but not without confounding factors. Apart from subjective bias of users, environmental factors introduce variance to QoE measurements. We propose QUINCE, a QoE measurement platform, which uses a gamified approach to enable longitudinal study with repeated and varying measurements in a single platform. We leveraged existing Internet measurement data and infrastructures to integrate three different types of network and QoE measurements to yield a more comprehensive view of subjects. Our preliminarily results show that QUINCE achieves a high level of engagement from subjects and collects data that is useful for correlating network performance and YouTube video streaming QoE.},
booktitle = {Proceedings of the ACM SIGCOMM 2019 Conference Posters and Demos},
pages = {60–62},
numpages = {3},
keywords = {QoE, crowdsourcing, network measurement},
location = {Beijing, China},
series = {SIGCOMM Posters and Demos '19}
}

@inproceedings{10.1109/IMIS.2011.89,
author = {Hirth, Matthias and Hoβfeld, Tobias and Tran-Gia, Phuoc},
title = {Anatomy of a Crowdsourcing Platform - Using the Example of Microworkers.com},
year = {2011},
isbn = {9780769543727},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/IMIS.2011.89},
doi = {10.1109/IMIS.2011.89},
abstract = {Since Jeff Howe introduced the term "crowdsourcing" in 2006 for the first time, crowd sourcing has be come a growing market in the current Internet. Thousands of workers categorize images, write articles or perform other small tasks on platforms like Amazon Mechanical Turk (MTurk), Micro workers or Short Task. In this work, we want to give an inside view of the usage data from Micro workers and show that there are significant differences to the well studied MTurk. Further, we have a look at Micro workers from the perspective for a worker, an employer and the platform owner, in order to answer their most important questions: What jobs are most paid? How do I get my work done most quickly? When are the users of my platform active?},
booktitle = {Proceedings of the 2011 Fifth International Conference on Innovative Mobile and Internet Services in Ubiquitous Computing},
pages = {322–329},
numpages = {8},
keywords = {Mechanical Turk, Microworkers, crowdsourcing, user statistics},
series = {IMIS '11}
}

@inproceedings{10.1145/2506182.2506194,
author = {Uzun, Abdulbaki and Lehmann, Lorenz and Geismar, Thilo and K\"{u}pper, Axel},
title = {Turning the OpenMobileNetwork into a live crowdsourcing platform for semantic context-aware services},
year = {2013},
isbn = {9781450319720},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2506182.2506194},
doi = {10.1145/2506182.2506194},
abstract = {The OpenMobileNetwork is a semantic model for mobile network topologies created by following the principles of Linked Data. By correlating data in the Linking Open Data Cloud with the OpenMobileNetwork, innovative Semantic Context-aware Services (CAS) can be realized that are not solely driven by classic context data (e.g., geographic location), but also include further information from the semantics of the context in use. So far, this open-source initiative provided rather static network topology data triplified from open-source cell databases, such as OpenCellID or OpenBMap. Integrating dynamic and live network context data (e.g., current traffic and number of users in a mobile network cell) by exploiting crowdsourcing methods will further improve the Semantic CAS experience since the historic and live state of a mobile network cell is a valuable data source to be taken into consideration when providing personalized services. The challenge in realizing such a crowdsourcing approach lies in motivating a significant number of users to contribute with their data. For this purpose, we have turned the OpenMobileNetwork from a static dataset into a Live Crowdsourcing Platform for Semantic CAS including a semantic cellular database based on extended network context ontologies, two smartphone clients, and a Measurement Framework for gamifying the crowdsourcing process of collecting network measurements. The measurement statistics highlight the effectiveness of this approach.},
booktitle = {Proceedings of the 9th International Conference on Semantic Systems},
pages = {89–96},
numpages = {8},
keywords = {context-aware services, linked data, mobile networks},
location = {Graz, Austria},
series = {I-SEMANTICS '13}
}

@inproceedings{10.1145/3415958.3433075,
author = {Kouvela, Maria and Dimitriadis, Ilias and Vakali, Athena},
title = {Bot-Detective: An explainable Twitter bot detection service with crowdsourcing functionalities},
year = {2020},
isbn = {9781450381154},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3415958.3433075},
doi = {10.1145/3415958.3433075},
abstract = {Popular microblogging platforms (such as Twitter) offer a fertile ground for open communication among humans, however, they also attract many bots and automated accounts "disguised" as human users. Typically, such accounts favor malicious activities such as phishing, public opinion manipulation and hate speech spreading, to name a few. Although several AI driven bot detection methods have been implemented, the justification of bot classification and characterization remains quite opaque and AI decisions lack in ethical responsibility. Most of these approaches operate with AI black-boxed algorithms and their efficiency is often questionable. In this work we propose Bot-Detective, a web service that takes into account both the efficient detection of bot users and the interpretability of the results as well. Our main contributions are summarized as follows: i) we propose a novel explainable bot-detection approach, which, to the best of authors' knowledge, is the first one to offer interpretable, responsible, and AI driven bot identification in Twitter, ii) we deploy a publicly available bot detection Web service which integrates an explainable ML framework along with users feedback functionality under an effective crowdsourcing mechanism; iii) we build the proposed service under a newly created annotated dataset by exploiting Twitter's rules and existing tools. This dataset is publicly shared for further use. In situ experimentation has showcased that Bot-Detective produces comprehensive and accurate results, with a promising service take up at scale.},
booktitle = {Proceedings of the 12th International Conference on Management of Digital EcoSystems},
pages = {55–63},
numpages = {9},
keywords = {bot detection, explainable AI, social bots, social influence, social networks},
location = {Virtual Event, United Arab Emirates},
series = {MEDES '20}
}

@inproceedings{10.1145/2442576.2442591,
author = {Kim, Sung-Hee and Yun, Hyokun and Yi, Ji Soo},
title = {How to filter out random clickers in a crowdsourcing-based study?},
year = {2012},
isbn = {9781450317917},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2442576.2442591},
doi = {10.1145/2442576.2442591},
abstract = {Crowdsourcing-based user studies have become increasingly popular in information visualization (InfoVis) and visual analytics (VA). However, it is still unclear how to deal with some undesired crowdsourcing workers, especially those who submit random responses simply to gain wages (random clickers, henceforth). In order to mitigate the impacts of random clickers, several studies simply exclude outliers, but this approach has a potential risk of losing data from participants whose performances are extreme even though they participated faithfully. In this paper, we evaluated the degree of randomness in responses from a crowdsourcing worker to infer whether the worker is a random clicker. Thus, we could reliably filter out random clickers and found that resulting data from crowdsourcing-based user studies were comparable with those of a controlled lab study. We also tested three representative reward schemes (piece-rate, quota, and punishment schemes) with four different levels of compensations ($0.00, $0.20, $1.00, and $4.00) on a crowdsourcing platform with a total of 1,500 crowdsourcing workers to investigate the influences that different payment conditions have on the number of random clickers. The results show that higher compensations decrease the proportion of random clickers, but such increase in participation quality cannot justify the associated additional costs. A detailed discussion on how to optimize the payment scheme and amount to obtain high-quality data economically is provided.},
booktitle = {Proceedings of the 2012 BELIV Workshop: Beyond Time and Errors - Novel Evaluation Methods for Visualization},
articleno = {15},
numpages = {7},
keywords = {SimulSort, crowdsouring-based study, information visualization, payment amount, payment schemes, random clickers},
location = {Seattle, Washington, USA},
series = {BELIV '12}
}

@inproceedings{10.1145/1357054.1357127,
author = {Kittur, Aniket and Chi, Ed H. and Suh, Bongwon},
title = {Crowdsourcing user studies with Mechanical Turk},
year = {2008},
isbn = {9781605580111},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1357054.1357127},
doi = {10.1145/1357054.1357127},
abstract = {User studies are important for many aspects of the design process and involve techniques ranging from informal surveys to rigorous laboratory studies. However, the costs involved in engaging users often requires practitioners to trade off between sample size, time requirements, and monetary costs. Micro-task markets, such as Amazon's Mechanical Turk, offer a potential paradigm for engaging a large number of users for low time and monetary costs. Here we investigate the utility of a micro-task market for collecting user measurements, and discuss design considerations for developing remote micro user evaluation tasks. Although micro-task markets have great potential for rapidly collecting user measurements at low costs, we found that special care is needed in formulating tasks in order to harness the capabilities of the approach.},
booktitle = {Proceedings of the SIGCHI Conference on Human Factors in Computing Systems},
pages = {453–456},
numpages = {4},
keywords = {Mechanical Turk, Wikipedia, micro task, remote user study},
location = {Florence, Italy},
series = {CHI '08}
}

@inproceedings{10.1109/eScience.2012.6404453,
author = {Hu, Zhenghui and Wu, Wenjun},
title = {A satellite data portal developed for crowdsourcing data analysis and interpretation},
year = {2012},
isbn = {9781467344678},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/eScience.2012.6404453},
doi = {10.1109/eScience.2012.6404453},
abstract = {Satellite data products derived from the remote sensing observations describe features of the land, ocean and atmosphere. And by data processing, they can be used to study processes and trends on local/global scale for real-time environmental research and applications. However, the advances of cutting-edge remote sensing technology bring the challenge of data deluge for satellite data analysis and interpretation. With combinations of human intelligence and machine intelligence, we develop a satellite data portal for crowdsourcing data analysis and interpretation through teaching and learning to cope with the overwhelming data deluge. Compared with all the existing data portals and crowdsourcing systems, it is the first attempt to embed crowdsourcing into a data portal to provide integrated services of satellite data access and analysis.},
booktitle = {Proceedings of the 2012 IEEE 8th International Conference on E-Science (e-Science)},
pages = {1–8},
numpages = {8},
keywords = {Communities, Data analysis, Education, Portals, Publishing, Remote sensing, Satellites, crowdsourcing, data anaysis and interpretation, data deluge, data portal, remote sensing},
series = {E-SCIENCE '12}
}

@inproceedings{10.1145/2785830.2785889,
author = {Huang, Yun and White, Corey and Xia, Huichuan and Wang, Yang},
title = {Modeling Sharing Decision of Campus Safety Reports and Its Design Implications to Mobile Crowdsourcing for Safety},
year = {2015},
isbn = {9781450336529},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2785830.2785889},
doi = {10.1145/2785830.2785889},
abstract = {Current campus communication regarding safety-related issues can be improved for both efficiency and accessibility. We observed a unique opportunity to develop a mobile crowdsourcing system, which allows university community members to report safety related incidents to the campus police department and to share their reports with other users of the system. To better inform the design of such a system, we applied drift-diffusion models in cognitive psychology to model the effect of various factors on users' sharing tendency. We conducted a laboratory experiment with 30 participants. We also ran an MTurk study with 230 participants to explore the feature of anonymous sharing in the application design. In this paper we report various results, including the findings that the time of day, location, and type of crime each affects the likelihood and timeliness of sharing safety reports in several different ways. We also discuss the implications for design of mobile crowdsourcing systems for public safety in general.},
booktitle = {Proceedings of the 17th International Conference on Human-Computer Interaction with Mobile Devices and Services},
pages = {400–409},
numpages = {10},
keywords = {Decision Model, Mobile Crowdsourcing, Public Safety},
location = {Copenhagen, Denmark},
series = {MobileHCI '15}
}

@inproceedings{10.1145/3488560.3502182,
author = {Soprano, Michael and Roitero, Kevin and Bombassei De Bona, Francesco and Mizzaro, Stefano},
title = {Crowd_Frame: A Simple and Complete Framework to Deploy Complex Crowdsourcing Tasks Off-the-shelf},
year = {2022},
isbn = {9781450391320},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3488560.3502182},
doi = {10.1145/3488560.3502182},
abstract = {Due to their relatively low cost and ability to scale, crowdsourcing based approaches are widely used to collect a large amount of human annotated data. To this aim, multiple crowdsourcing platforms exist, where requesters can upload tasks and workers can carry them out and obtain payment in return. Such platforms share a task design and deploy workflow that is often counter-intuitive and cumbersome. To address this issue, we propose Crowd_Frame, a simple and complete framework which allows to develop and deploy diverse types of complex crowdsourcing tasks in an easy and customizable way. We show the abilities of the proposed framework and we make it available to researchers and practitioners.},
booktitle = {Proceedings of the Fifteenth ACM International Conference on Web Search and Data Mining},
pages = {1605–1608},
numpages = {4},
keywords = {crowdsourcing, framework, user behavior},
location = {Virtual Event, AZ, USA},
series = {WSDM '22}
}

@inproceedings{10.1145/2470654.2470744,
author = {Hara, Kotaro and Le, Vicki and Froehlich, Jon},
title = {Combining crowdsourcing and google street view to identify street-level accessibility problems},
year = {2013},
isbn = {9781450318990},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2470654.2470744},
doi = {10.1145/2470654.2470744},
abstract = {Poorly maintained sidewalks, missing curb ramps, and other obstacles pose considerable accessibility challenges; however, there are currently few, if any, mechanisms to determine accessible areas of a city a priori. In this paper, we investigate the feasibility of using untrained crowd workers from Amazon Mechanical Turk (turkers) to find, label, and assess sidewalk accessibility problems in Google Street View imagery. We report on two studies: Study 1 examines the feasibility of this labeling task with six dedicated labelers including three wheelchair users; Study 2 investigates the comparative performance of turkers. In all, we collected 13,379 labels and 19,189 verification labels from a total of 402 turkers. We show that turkers are capable of determining the presence of an accessibility problem with 81\% accuracy. With simple quality control methods, this number increases to 93\%. Our work demonstrates a promising new, highly scalable method for acquiring knowledge about sidewalk accessibility.},
booktitle = {Proceedings of the SIGCHI Conference on Human Factors in Computing Systems},
pages = {631–640},
numpages = {10},
keywords = {accessible urban navigation, crowdsourcing accessibility, google street view, image labeling, mechanical turk},
location = {Paris, France},
series = {CHI '13}
}

@inproceedings{10.1145/1641309.1641348,
author = {Ganjisaffar, Yasser and Javanmardi, Sara and Lopes, Cristina},
title = {Leveraging crowdsourcing heuristics to improve search in Wikipedia},
year = {2009},
isbn = {9781605587301},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1641309.1641348},
doi = {10.1145/1641309.1641348},
abstract = {Wikipedia articles are usually accompanied with history pages, categories and talk pages. The meta--data available in these pages can be analyzed to gain a better understanding of the content and quality of the articles. We analyze the quality of search results of the current major Web search engines (Google, Yahoo! and Live) in Wikipedia. We discuss how the rich meta--data available in wiki pages can be used to provide better search results in Wikipedia. We investigate the effect of incorporating the extent of review of an article into ranking of search results. The extent of review is measured by the number of distinct editors who have contributed to the articles and is extracted by processing Wikipedia's history pages. Our experimental results show that re--ranking search results of the three major Web search engines, using the review feature, improves quality of their rankings for Wikipedia--specific searches.},
booktitle = {Proceedings of the 5th International Symposium on Wikis and Open Collaboration},
articleno = {27},
numpages = {2},
location = {Orlando, Florida},
series = {WikiSym '09}
}

@inproceedings{10.1109/COMPSAC.2013.133,
author = {Li, Ke and Xiao, Junchao and Wang, Yongji and Wang, Qing},
title = {Analysis of the Key Factors for Software Quality in Crowdsourcing Development: An Empirical Study on TopCoder.com},
year = {2013},
isbn = {9780769549866},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/COMPSAC.2013.133},
doi = {10.1109/COMPSAC.2013.133},
abstract = {Crowdsourcing is a distributed problem-solving and production model. It takes advantage of the internet technology, helps enterprises save cost and improve efficiency. However, uncertain quality is a significant challenge for crowdsourcing. On the basis of the existing literatures, this paper proposes 23 software quality factors from two aspects: platform and project. By using multiple regression analysis on the data of one of the most successful software crowdsourcing platforms TopCoder.com, this paper analyzes the impact of the factors on software quality and identifies six key factors, including the average quality score of the platform, the number of contemporary projects, the length of component document, the number of registered developers, the maximum rating of submitted developers, and the design score. According to the result, this paper suggests four aspects for enterprises to improve software quality: choosing the prosperous period of platform to post a project, reducing the scale of projects, attracting more and higher skillful developers to participate, and improving software design score.},
booktitle = {Proceedings of the 2013 IEEE 37th Annual Computer Software and Applications Conference},
pages = {812–817},
numpages = {6},
keywords = {crowdsourcing, factors, object-oriented design metrics, software development, software quality},
series = {COMPSAC '13}
}

@inproceedings{10.1109/TAAI.2013.53,
author = {Lee, Hu-Cheng and Wu, Chao-Lin and Chen, Ling-Jyh},
title = {A Crowdsourcing-Based Approach to Assess Concentration Levels of Students in Class Videos},
year = {2013},
isbn = {9781479925292},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/TAAI.2013.53},
doi = {10.1109/TAAI.2013.53},
abstract = {Concentration is important for students to conduct efficient learning in a class, and an effective assessment of students' concentration level in a class is useful for students to review class materials after lessons, as well as for lecturers to adjust their teaching strategies for self-improvement. Although a number of concentration assessment approaches have been proposed, conventional approaches are generally time/money expensive (e.g., expert opinions), inaccurate (e.g., computer vision-based approaches), and intrusive (e.g., wearable sensor-based approaches). In this study, we propose a novel approach, called Concentration Level Assessment System (CLAS), which combines a markovian Doze-and-Wake Model (DAWM) and the emerging crowdsourcing technique to enable effective concentration assessment of class videos. Using realistic datasets of class videos, we conduct a comprehensive set of synthetic analysis and Internet experiments, the results demonstrate that CLAS is capable of yielding an accuracy up to 98\% with 86\% cost savings. Moreover, CLAS is simple, effective, and scalable, and it shows promises in facilitating advanced applications for efficiency, productivity, and safety in the future.},
booktitle = {Proceedings of the 2013 Conference on Technologies and Applications of Artificial Intelligence},
pages = {228–233},
numpages = {6},
keywords = {concentration assessment, crowdsourcing, experiment},
series = {TAAI '13}
}

@inproceedings{10.5555/2392800.2392841,
author = {Bessho, Fumihiro and Harada, Tatsuya and Kuniyoshi, Yasuo},
title = {Dialog system using real-time crowdsourcing and Twitter large-scale corpus},
year = {2012},
publisher = {Association for Computational Linguistics},
address = {USA},
abstract = {We propose a dialog system that creates responses based on a large-scale dialog corpus retrieved from Twitter and real-time crowd-sourcing. Instead of using complex dialog management, our system replies with the utterance from the database that is most similar to the user input. We also propose a real-time crowdsourcing framework for handling the case in which there is no adequate response in the database.},
booktitle = {Proceedings of the 13th Annual Meeting of the Special Interest Group on Discourse and Dialogue},
pages = {227–231},
numpages = {5},
location = {Seoul, South Korea},
series = {SIGDIAL '12}
}

@inproceedings{10.1145/2858036.2858588,
author = {Kaufman, Geoff and Flanagan, Mary and Punjasthitkul, Sukdith},
title = {Investigating the Impact of 'Emphasis Frames' and Social Loafing on Player Motivation and Performance in a Crowdsourcing Game},
year = {2016},
isbn = {9781450333627},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2858036.2858588},
doi = {10.1145/2858036.2858588},
abstract = {With an increasing reliance on crowdsourcing games as data-gathering tools, it is imperative to understand how to motivate and sustain high levels of voluntary contribution. To this end, the present work directly compared the impact of various "emphasis frames," highlighting distinct intrinsic motivational factors, used to describe an online game in which players provide descriptive metadata "tags" for digitized images. An initial study showed that, compared to frames emphasizing personal enjoyment or altruistic motivations, a frame emphasizing a "growing community of players" solicited significantly fewer contributions. A second study tested the hypothesis that this lower level of contribution resulted from social loafing (the tendency to exert less effort in collective tasks in which contributions are anonymous and pooled). Results revealed that, compared to a no-frame control condition, a frame emphasizing the preponderance of other players reduced contribution levels and game replay likelihood, whereas a frame emphasizing the scarcity of fellow players increased contribution and replay levels. Various strategies for counteracting social loafing in crowdsourcing contexts are discussed.},
booktitle = {Proceedings of the 2016 CHI Conference on Human Factors in Computing Systems},
pages = {4122–4128},
numpages = {7},
keywords = {crowdsourcing games, engagement, human computation, metadata, motivation, social loafing},
location = {San Jose, California, USA},
series = {CHI '16}
}

@proceedings{10.1145/2442657,
title = {CrowdKDD '12: Proceedings of the First International Workshop on Crowdsourcing and Data Mining},
year = {2012},
isbn = {9781450315579},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Beijing, China}
}

@inproceedings{10.5555/2586115.2586745,
author = {Lee, Hu-Cheng and Wu, Chao-Lin and Chen, Ling-Jyh},
title = {A Crowdsourcing-Based Approach to Assess Concentration Levels of Students in Class Videos},
year = {2013},
isbn = {9781479925285},
publisher = {IEEE Computer Society},
address = {USA},
abstract = {Concentration is important for students to conduct efficient learning in a class, and an effective assessment of students' concentration level in a class is useful for students to review class materials after lessons, as well as for lecturers to adjust their teaching strategies for self-improvement. Although a number of concentration assessment approaches have been proposed, conventional approaches are generally time/money expensive (e.g., expert opinions), inaccurate (e.g., computer vision-based approaches), and intrusive (e.g., wearable sensor-based approaches). In this study, we propose a novel approach, called Concentration Level Assessment System (CLAS), which combines a markovian Doze-and-Wake Model (DAWM) and the emerging crowdsourcing technique to enable effective concentration assessment of class videos. Using realistic datasets of class videos, we conduct a comprehensive set of synthetic analysis and Internet experiments, the results demonstrate that CLAS is capable of yielding an accuracy up to 98\% with 86\% cost savings. Moreover, CLAS is simple, effective, and scalable, and it shows promises in facilitating advanced applications for efficiency, productivity, and safety in the future.},
booktitle = {Proceedings of the 2013 Conference on Technologies and Applications of Artificial Intelligence},
pages = {228–233},
numpages = {6},
keywords = {concentration assessment, crowdsourcing, experiment},
series = {TAAI '13}
}

@inproceedings{10.1007/978-3-642-35311-6_16,
author = {Nath, Swaprava and Dayama, Pankaj and Garg, Dinesh and Narahari, Yadati and Zou, James},
title = {Mechanism design for time critical and cost critical task execution via crowdsourcing},
year = {2012},
isbn = {9783642353109},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-642-35311-6_16},
doi = {10.1007/978-3-642-35311-6_16},
abstract = {An exciting application of crowdsourcing is to use social networks in complex task execution. In this paper, we address the problem of a planner who needs to incentivize agents within a network in order to seek their help in executing an atomic task as well as in recruiting other agents to execute the task. We study this mechanism design problem under two natural resource optimization settings: (1) cost critical tasks, where the planner's goal is to minimize the total cost, and (2) time critical tasks, where the goal is to minimize the total time elapsed before the task is executed. We identify a set of desirable properties that should ideally be satisfied by a crowdsourcing mechanism. In particular, sybil-proofness and collapse-proofness are two complementary properties in our desiderata. We prove that no mechanism can satisfy all the desirable properties simultaneously. This leads us naturally to explore approximate versions of the critical properties. We focus our attention on approximate sybil-proofness and our exploration leads to a parametrized family of payment mechanisms which satisfy collapse-proofness. We characterize the approximate versions of the desirable properties in cost critical and time critical domain.},
booktitle = {Proceedings of the 8th International Conference on Internet and Network Economics},
pages = {212–226},
numpages = {15},
location = {Liverpool, UK},
series = {WINE'12}
}

@proceedings{10.5555/2874376,
title = {CrowdSem'13: Proceedings of the 1st International Conference on Crowdsourcing the Semantic Web - Volume 1030},
year = {2013},
publisher = {CEUR-WS.org},
address = {Aachen, DEU},
location = {Sydney, Australia}
}

@inproceedings{10.5555/1996889.1996911,
author = {Kazai, Gabriella},
title = {In search of quality in crowdsourcing for search engine evaluation},
year = {2011},
isbn = {9783642201608},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {Crowdsourcing is increasingly looked upon as a feasible alternative to traditional methods of gathering relevance labels for the evaluation of search engines, offering a solution to the scalability problem that hinders traditional approaches. However, crowdsourcing raises a range of questions regarding the quality of the resulting data. What indeed can be said about the quality of the data that is contributed by anonymous workers who are only paid cents for their efforts? Can higher pay guarantee better quality? Do better qualified workers produce higher quality labels? In this paper, we investigate these and similar questions via a series of controlled crowdsourcing experiments where we vary pay, required effort and worker qualifications and observe their effects on the resulting label quality, measured based on agreement with a gold set.},
booktitle = {Proceedings of the 33rd European Conference on Advances in Information Retrieval},
pages = {165–176},
numpages = {12},
keywords = {IR evaluation, crowdsourcing, relevance data gathering},
location = {Dublin, Ireland},
series = {ECIR'11}
}

@inproceedings{10.1109/HICSS.2015.341,
author = {Ghosh, Kaushik and Sen, Kabir},
title = {A Conceptual Model to Understand the Factors that Drive Individual Participation in Crowdsourcing for Medical Diagnosis},
year = {2015},
isbn = {9781479973675},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/HICSS.2015.341},
doi = {10.1109/HICSS.2015.341},
abstract = {In healthcare, assisted by collective knowledge of a large group of individuals, crowd sourcing is enabling prediction of disease outbreaks and diagnosis of rare medical conditions. This article is a 'Research-in Progress' that examines the role of Web-based platforms in driving participation of individuals in crowd sourcing services for medical diagnosis. Based on existing literature, a conceptual research model is developed that outlines factors that drive individual participation. A set of propositions based on the research model are developed. A research methodology is proposed with a plan for the empirical analysis. Contributions and implications are discussed.},
booktitle = {Proceedings of the 2015 48th Hawaii International Conference on System Sciences},
pages = {2815–2823},
numpages = {9},
keywords = {Contributor, Crowdsourcing, Medical Diagnosis, Participation, Seeker},
series = {HICSS '15}
}

@inproceedings{10.1007/978-3-642-20161-5_17,
author = {Kazai, Gabriella},
title = {In Search of Quality in Crowdsourcing for Search Engine Evaluation},
year = {2011},
isbn = {9783642201608},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-642-20161-5_17},
doi = {10.1007/978-3-642-20161-5_17},
abstract = {Crowdsourcing is increasingly looked upon as a feasible alternative to traditional methods of gathering relevance labels for the evaluation of search engines, offering a solution to the scalability problem that hinders traditional approaches. However, crowdsourcing raises a range of questions regarding the quality of the resulting data. What indeed can be said about the quality of the data that is contributed by anonymous workers who are only paid cents for their efforts? Can higher pay guarantee better quality? Do better qualified workers produce higher quality labels? In this paper, we investigate these and similar questions via a series of controlled crowdsourcing experiments where we vary pay, required effort and worker qualifications and observe their effects on the resulting label quality, measured based on agreement with a gold set.},
booktitle = {Proceedings of the 33rd European Conference on Advances in Information Retrieval - Volume 6611},
pages = {165–176},
numpages = {12},
keywords = {IR evaluation, crowdsourcing, relevance data gathering},
location = {Dublin, Ireland},
series = {ECIR 2011}
}

@inproceedings{10.1109/IMIS.2011.91,
author = {Hirth, Matthias and Hoβfeld, Tobias and Tran-Gia, Phuoc},
title = {Cost-Optimal Validation Mechanisms and Cheat-Detection for Crowdsourcing Platforms},
year = {2011},
isbn = {9780769543727},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/IMIS.2011.91},
doi = {10.1109/IMIS.2011.91},
abstract = {Crowd sourcing is becoming more and more important for commercial purposes. With the growth of crowd sourcing platforms like MTurk or Micro workers, a huge work force and a large knowledge base can be easily accessed and utilized. But due to the anonymity of the workers, they are encouraged to cheat the employers in order to maximize their income. Thus, this paper presents two crowd-based approaches to validate the submitted work. Both approaches are evaluated with regard to their detection quality, their costs and their applicability to different types of typical crowd sourcing tasks.},
booktitle = {Proceedings of the 2011 Fifth International Conference on Innovative Mobile and Internet Services in Ubiquitous Computing},
pages = {316–321},
numpages = {6},
keywords = {cheat-detection mechanism, crowdsourcing},
series = {IMIS '11}
}

@inproceedings{10.1145/3242587.3242598,
author = {Bragg, Jonathan and Mausam and Weld, Daniel S.},
title = {Sprout: Crowd-Powered Task Design for Crowdsourcing},
year = {2018},
isbn = {9781450359481},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3242587.3242598},
doi = {10.1145/3242587.3242598},
abstract = {While crowdsourcing enables data collection at scale, ensuring high-quality data remains a challenge. In particular, effective task design underlies nearly every reported crowdsourcing success, yet remains difficult to accomplish. Task design is hard because it involves a costly iterative process: identifying the kind of work output one wants, conveying this information to workers, observing worker performance, understanding what remains ambiguous, revising the instructions, and repeating the process until the resulting output is satisfactory. To facilitate this process, we propose a novel meta-workflow that helps requesters optimize crowdsourcing task designs and Sprout, our open-source tool, which implements this workflow. Sprout improves task designs by (1) eliciting points of confusion from crowd workers, (2) enabling requesters to quickly understand these misconceptions and the overall space of questions, and (3) guiding requesters to improve the task design in response. We report the results of a user study with two labeling tasks demonstrating that requesters strongly prefer Sprout and produce higher-rated instructions compared to current best practices for creating gated instructions (instructions plus a workflow for training and testing workers). We also offer a set of design recommendations for future tools that support crowdsourcing task design.},
booktitle = {Proceedings of the 31st Annual ACM Symposium on User Interface Software and Technology},
pages = {165–176},
numpages = {12},
location = {Berlin, Germany},
series = {UIST '18}
}

@inproceedings{10.1145/2666539.2666567,
author = {Machado, Leticia and Pereira, Graziela and Prikladnicki, Rafael and Carmel, Erran and de Souza, Cleidson R. B.},
title = {Crowdsourcing in the Brazilian IT industry: what we know and what we don't know},
year = {2014},
isbn = {9781450332248},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2666539.2666567},
doi = {10.1145/2666539.2666567},
abstract = {Crowdsourcing means outsourcing to a large network of people – a crowd. It has emerged as a new option for a global labor market; it is another valuable option in the ´make or buy´ software decision and has been gaining attention in countries where global software engineering plays a significant role, such as Brazil. The adoption of this practice in the Brazilian IT industry is not well known yet. For this reason, this paper presents findings from an empirical study about the topic, in the context of a multi-year study that has the goal of investigating how the Brazilian software labor and industry market is being transformed and disrupted by crowdsourcing. We interviewed professionals from several companies and identified how crowdsourcing is being adopted in Brazil, including possible benefits, main concerns and factors that may avoid some companies to adopt it from three different perspectives: the buyers, the platforms and the crowd. We also share our thoughts about the future of crowdsourcing in the country in the coming years.},
booktitle = {Proceedings of the 1st International Workshop on Crowd-Based Software Development Methods and Technologies},
pages = {7–12},
numpages = {6},
keywords = {Crowdsourcing, human labor, software development},
location = {Hong Kong, China},
series = {CrowdSoft 2014}
}

@inproceedings{10.1007/978-3-642-27997-3_19,
author = {Jayakanthan, Ranganathan and Sundararajan, Deepak},
title = {Enterprise crowdsourcing solution for software development in an outsourcing organization},
year = {2011},
isbn = {9783642279966},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-642-27997-3_19},
doi = {10.1007/978-3-642-27997-3_19},
abstract = {Enterprise Crowdsourcing has the potential to be a very powerful and disruptive paradigm for human resource deployment, project development and project management as we know them. This paper details ongoing work at TCS Innovation Labs --- Web 2.0, Tata Consultancy Services, Chennai, India to develop an Enterprise Crowdsourcing Solution to tackle the various processes involved in the development of software by leveraging the untapped human resource in the organization. Large IT organizations have a lot of untapped manpower in the form of trainees, the bench strength and people involved in roles which do not fully employ their strengths in particular technologies they are experts in. This system aims to allow untapped talent to get access to challenging tasks part of other projects and work on them while providing a disruptive way to allocate resources in a conventional software development environment.},
booktitle = {Proceedings of the 11th International Conference on Current Trends in Web Engineering},
pages = {177–180},
numpages = {4},
keywords = {collaborative work, crowdsourcing, enterprise crowdsourcing, social networking, web 2.0},
location = {Paphos, Cyprus},
series = {ICWE'11}
}

@inproceedings{10.1145/3290605.3300761,
author = {Chen, Quanze and Bragg, Jonathan and Chilton, Lydia B. and Weld, Dan S.},
title = {Cicero: Multi-Turn, Contextual Argumentation for Accurate Crowdsourcing},
year = {2019},
isbn = {9781450359702},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3290605.3300761},
doi = {10.1145/3290605.3300761},
abstract = {Traditional approaches for ensuring high quality crowdwork have failed to achieve high-accuracy on difficult problems. Aggregating redundant answers often fails on the hardest problems when the majority is confused. Argumentation has been shown to be effective in mitigating these drawbacks. However, existing argumentation systems only support limited interactions and show workers general justifications, not context-specific arguments targeted to their reasoning. This paper presents Cicero, a new workflow that improves crowd accuracy on difficult tasks by engaging workers in multi-turn, contextual discussions through real-time, synchronous argumentation. Our experiments show that compared to previous argumentation systems which only improve the average individual worker accuracy by 6.8 percentage points on the Relation Extraction domain, our workflow achieves 16.7 percentage point improvement. Furthermore, previous argumentation approaches don't apply to tasks with many possible answers; in contrast, Cicero works well in these cases, raising accuracy from 66.7\% to 98.8\% on the Codenames domain.},
booktitle = {Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems},
pages = {1–14},
numpages = {14},
keywords = {argumentation, crowdsourcing, dialog},
location = {Glasgow, Scotland Uk},
series = {CHI '19}
}

@inproceedings{10.1145/3184558.3191543,
author = {Zaveri, Amrapali and Serrano, Pedro Hernandez and Desai, Manisha and Dumontier, Michel},
title = {CrowdED: Guideline for Optimal Crowdsourcing Experimental Design},
year = {2018},
isbn = {9781450356404},
publisher = {International World Wide Web Conferences Steering Committee},
address = {Republic and Canton of Geneva, CHE},
url = {https://doi.org/10.1145/3184558.3191543},
doi = {10.1145/3184558.3191543},
abstract = {Crowdsourcing involves the creating of HITs (Human Intelligent Tasks), submitting them to a crowdsourcing platform and providing a monetary reward for each HIT. One of the advantages of using crowdsourcing is that the tasks can be highly parallelized, that is, the work is performed by a high number of workers in a decentralized setting. The design also offers a means to cross-check the accuracy of the answers by assigning each task to more than one person and thus relying on majority consensus as well as reward the workers according to their performance and productivity. Since each worker is paid per task, the costs can significantly increase, irrespective of the overall accuracy of the results. Thus, one important question when designing such crowdsourcing tasks that arise is how many workers to employ and how many tasks to assign to each worker when dealing with large amounts of tasks. That is, the main research questions we aim to answer is: 'Can we a-priori estimate optimal workers and tasks' assignment to obtain maximum accuracy on all tasks'. Thus, we introduce a two-staged statistical guideline, CrowdED, for optimal crowdsourcing experimental design in order to a-priori estimate optimal workers and tasks' assignment to obtain maximum accuracy on all tasks. We describe the algorithm and present preliminary results and discussions. We implement the algorithm in Python and make it openly available on Github, provide a Jupyter Notebook and a R Shiny app for users to re-use, interact and apply in their own crowdsourcing experiments.},
booktitle = {Companion Proceedings of the The Web Conference 2018},
pages = {1109–1116},
numpages = {8},
keywords = {biomedical, crowdsourcing, data quality, data science, fair, metadata, reproducibility},
location = {Lyon, France},
series = {WWW '18}
}

@inproceedings{10.5555/2484920.2485201,
author = {Yu, Han and Shen, Zhiqi and Miao, Chunyan and An, Bo},
title = {A reputation-aware decision-making approach for improving the efficiency of crowdsourcing systems},
year = {2013},
isbn = {9781450319935},
publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
address = {Richland, SC},
abstract = {A crowdsourcing system is a useful platform for utilizing the intelligence and skills of the mass. Nevertheless, like any open system that involves the exchange of things of value, selfish and malicious behaviors exist in crowdsourcing systems and need to be mitigated. Trust management has been proven to be a viable solution in many systems. However, a major difference between crowdsourcing systems and existing trust models designed for multi-agent systems is that human trustees have limited task processing capacity per unit time compared to an intelligent agent program. This paper recognizes a problem in current trust-aware decision-making methods for task assignment in crowdsourcing platforms. On the one hand, trust-based methods over-assign tasks to trusted workers, while on the other hand, workload-based solutions do not give sufficient guarantees on the quality of work. The proposed solution, the social welfare optimizing reputation-aware decision-making (SWORD) approach, strikes a balance between the two and is shown through extensive simulations to significantly improve social welfare of crowdsourcing platforms compared to related work.},
booktitle = {Proceedings of the 2013 International Conference on Autonomous Agents and Multi-Agent Systems},
pages = {1315–1316},
numpages = {2},
keywords = {decision-making, reputation, trust},
location = {St. Paul, MN, USA},
series = {AAMAS '13}
}

@inproceedings{10.1145/3265689.3265716,
author = {Yang, Qian and Cui, Lizhen and Zheng, Miao and Liu, Shijun and Guo, Wei and Lu, Xudong and Zheng, Yongqing and Li, Qingzhong},
title = {LBTask: A Benchmark for Spatial Crowdsourcing Platforms},
year = {2018},
isbn = {9781450365871},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3265689.3265716},
doi = {10.1145/3265689.3265716},
abstract = {The popularity of smart phones has made rapid development of crowdsourcing. The emergence of these crowdsourcing software has brought great convenience to our life. Traditional crowdsourcing platforms, such as Amazon Mechanical Turk and Crowdflower, publish some tasks on the site, Workers choose the tasks that are of interest and submit the answers to the tasks by browsing the tasks on the platform. And spatial crowdsourcing platforms (like gMission) are used to assign crowdsourcing tasks related to location. However, most crowdsourcing platforms support a small number of assignment and quality control algorithms. In this paper, a benchmark for spatial crowdsourcing platforms, called LBTask, is designed in order to adapt to the emergence of spatial crowdsourcing tasks, which focuses on solving location aware crowdsourcing tasks. Compared with other crowdsourcing platforms, LBTask can support various assignment and quality control algorithms in the architecture according to different strategies. In the distribution and assignment of tasks, the position factors of tasks and workers are taken into consideration in addition to considering the time and other factors.},
booktitle = {Proceedings of the 3rd International Conference on Crowd Science and Engineering},
articleno = {27},
numpages = {6},
keywords = {spatial crowdsourcing platform, task assignment},
location = {Singapore, Singapore},
series = {ICCSE'18}
}

@inproceedings{10.1145/1979742.1979802,
author = {Mujumdar, Dhawal and Kallenbach, Manuel and Liu, Brandon and Hartmann, Bj\"{o}rn},
title = {Crowdsourcing suggestions to programming problems for dynamic web development languages},
year = {2011},
isbn = {9781450302685},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1979742.1979802},
doi = {10.1145/1979742.1979802},
abstract = {Developers increasingly consult online examples and message boards to find solutions to common programming tasks. On the web, finding solutions to debugging problems is harder than searching for working code. Prior research introduced a social recommender system, HelpMeOut, that crowdsources debugging suggestions by presenting fixes to errors that peers have applied in the past. However, HelpMeOut only worked for statically typed, compiled programming languages like Java. We investigate how suggestions can be provided for dynamic, interpreted web development languages. Our primary insight is to instrument test-driven development to collect examples of bug fixes. We present Crowd::Debug, a tool for Ruby programmers that realizes these benefits.},
booktitle = {CHI '11 Extended Abstracts on Human Factors in Computing Systems},
pages = {1525–1530},
numpages = {6},
keywords = {debugging, recommender systems, test-driven development},
location = {Vancouver, BC, Canada},
series = {CHI EA '11}
}

@inproceedings{10.1145/2536714.2536717,
author = {Faggiani, Adriano and Gregori, Enrico and Lenzini, Luciano and Luconi, Valerio and Vecchio, Alessio},
title = {Lessons learned from the design, implementation, and management of a smartphone-based crowdsourcing system},
year = {2013},
isbn = {9781450324304},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2536714.2536717},
doi = {10.1145/2536714.2536717},
abstract = {Ubiquitousness of smartphones, when combined with the power of crowdsourcing, enables radically novel application scenarios, where a massive amount of mobile users scattered over wide geographical regions cooperate towards a single goal. Nevertheless these new possibilities come at the cost of additional complexity, such as the presence of humans in the control loop, scarce resources of mobile devices, increased management costs due the large number of users. In this paper we report and discuss the lessons learned from the design, implementation and management of Portolan, a smartphone-based crowdsourcing system aimed at monitoring large-scale networks.},
booktitle = {Proceedings of First International Workshop on Sensing and Big Data Mining},
pages = {1–6},
numpages = {6},
keywords = {Crowdsourcing, network sensing, smartphone},
location = {Roma, Italy},
series = {SENSEMINE'13}
}

@inproceedings{10.5555/2452579.2455511,
author = {Chen, Qi and Wang, Gang and Tan, Chew Lim},
title = {Web Image Organization and Object Discovery by Actively Creating Visual Clusters through Crowdsourcing},
year = {2012},
isbn = {9780769549156},
publisher = {IEEE Computer Society},
address = {USA},
abstract = {In this paper, we propose to organize web images by actively creating visual clusters via crowd sourcing. We develop a two-phase framework to efficiently and effectively combine computers and a large number of human workers to build high quality visual clusters. The first phase partitions an image collection into multiple clusters, the second phase refines each generated cluster independently. In both phases, informative images are selected by computers and manually labeled by the crowds to learn improved models. Our method can be naturally extended to discover object categories in a collection of image segments. Experimental results on several data sets demonstrate the promise of our developed approach on both web image organization and object discovery tasks.},
booktitle = {Proceedings of the 2012 IEEE 24th International Conference on Tools with Artificial Intelligence - Volume 01},
pages = {419–427},
numpages = {9},
keywords = {active clustering, crowdsourcing, image organization, object discovery},
series = {ICTAI '12}
}

@inproceedings{10.1145/3289600.3291035,
author = {Han, Lei and Roitero, Kevin and Gadiraju, Ujwal and Sarasua, Cristina and Checco, Alessandro and Maddalena, Eddy and Demartini, Gianluca},
title = {All Those Wasted Hours: On Task Abandonment in Crowdsourcing},
year = {2019},
isbn = {9781450359405},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3289600.3291035},
doi = {10.1145/3289600.3291035},
abstract = {Crowdsourcing has become a standard methodology to collect manually annotated data such as relevance judgments at scale. On crowdsourcing platforms like Amazon MTurk or FigureEight, crowd workers select tasks to work on based on different dimensions such as task reward and requester reputation. Requesters then receive the judgments of workers who self-selected into the tasks and completed them successfully. Several crowd workers, however, preview tasks, begin working on them, reaching varying stages of task completion without finally submitting their work. Such behavior results in unrewarded effort which remains invisible to requesters. In this paper, we conduct the first investigation into the phenomenon of task abandonment, the act of workers previewing or beginning a task and deciding not to complete it. We follow a three-fold methodology which includes 1) investigating the prevalence and causes of task abandonment by means of a survey over different crowdsourcing platforms, 2) data-driven analyses of logs collected during a large-scale relevance judgment experiment, and 3) controlled experiments measuring the effect of different dimensions on abandonment. Our results show that task abandonment is a widely spread phenomenon. Apart from accounting for a considerable amount of wasted human effort, this bears important implications on the hourly wages of workers as they are not rewarded for tasks that they do not complete. We also show how task abandonment may have strong implications on the use of collected data (for example, on the evaluation of IR systems).},
booktitle = {Proceedings of the Twelfth ACM International Conference on Web Search and Data Mining},
pages = {321–329},
numpages = {9},
keywords = {crowdsourcing, relevance judgments, task abandonment},
location = {Melbourne VIC, Australia},
series = {WSDM '19}
}

@inproceedings{10.1007/978-3-319-10470-6_55,
author = {Maier-Hein, Lena and Mersmann, Sven and Kondermann, Daniel and Bodenstedt, Sebastian and Sanchez, Alexandro and Stock, Christian and Kenngott, Hannes Gotz and Eisenmann, Mathias and Speidel, Stefanie},
title = {Can Masses of Non-Experts Train Highly Accurate Image Classifiers? A Crowdsourcing Approach to Instrument Segmentation in Laparoscopic Images},
year = {2022},
isbn = {978-3-319-10469-0},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-319-10470-6_55},
doi = {10.1007/978-3-319-10470-6_55},
abstract = {Machine learning algorithms are gaining increasing interest in the context of computer-assisted interventions. One of the bottlenecks so far, however, has been the availability of training data, typically generated by medical experts with very limited resources. Crowdsourcing is a new trend that is based on outsourcing cognitive tasks to many anonymous untrained individuals from an online community. In this work, we investigate the potential of crowdsourcing for segmenting medical instruments in endoscopic image data. Our study suggests that (1) segmentations computed from annotations of multiple anonymous non-experts are comparable to those made by medical experts and (2) training data generated by the crowd is of the same quality as that annotated by medical experts. Given the speed of annotation, scalability and low costs, this implies that the scientific community might no longer need to rely on experts to generate reference or training data for certain applications. To trigger further research in endoscopic image processing, the data used in this study will be made publicly available.},
booktitle = {Medical Image Computing and Computer-Assisted Intervention – MICCAI 2014},
pages = {438–445},
numpages = {8},
keywords = {Random Forest, Training Image, Majority Vote, True Positive Rate, Compute Tomography Colonography}
}

@inproceedings{10.1145/3372787.3389301,
author = {Saito, Shinobu and Iimura, Yukako},
title = {Hybrid sourcing: novel combination of crowdsourcing and inner-sourcing for software developments},
year = {2020},
isbn = {9781450370936},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3372787.3389301},
doi = {10.1145/3372787.3389301},
abstract = {Sourcing the right IT engineers is critical for project success. In recent years, two sourcing strategies have been grabbing attention: crowdsourcing and inner-sourcing. Each has their own good points and bad points. Crowdsourcing allows organizations to recruit IT engineers from outside on demand. However, organizations working on closed-source code with confidential information might be worried about security concerns (e.g., information leak). The other strategy, inner-sourcing, can make any IT engineer in an organization become a member of all projects by adopting open source software development practices. This improves the mobility of IT engineers between projects inside the organization. However, there is a limit to the types of IT engineers that one organization can have. In this report, we propose a hybrid sourcing approach. It integrates the two sourcing strategies to develop software - crowdsourcing and inner-sourcing. This approach distributes the development tasks to either software crowdsourcing or inner-sourcing according to task type. As a case study, we adopt hybrid sourcing approach for an industrial project. The project developed a web application system for a bus company. We evaluate the effectiveness and future issues of hybrid sourcing.},
booktitle = {Proceedings of the 15th International Conference on Global Software Engineering},
pages = {81–85},
numpages = {5},
keywords = {crowdsourcing, inner-sourcing, microtask distribution, microtasking, sourcing strategy},
location = {Seoul, Republic of Korea},
series = {ICGSE '20}
}

@inproceedings{10.1007/978-3-319-15168-7_53,
author = {Miglietta, Angelo and Parisi, Emanuele},
title = {Means and Roles of Crowdsourcing Vis-\`{A}-Vis CrowdFunding for the Creation of Stakeholders Collective Benefits},
year = {2015},
isbn = {978-3-319-15167-0},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-319-15168-7_53},
doi = {10.1007/978-3-319-15168-7_53},
abstract = {This work aims at assessing characteristics and roles of Crowdsourced activities vis-\`{a}-vis online CrowdFunding platforms, assessing potential collective benefits for stakeholders that arise from social media individual activities and investment decisions of users-investors. CrowdFunding platforms in fact leverage crowds and undefined pools of potential investors to screen, select and spread each CrowdFunding initiative in a detailed and thorough way – hence allowing users to perform several tasks that are traditionally carried out throughout IT models and static criteria.We identify 5 key roles played by Crowdsourcing Systems (CS) and we develop a potential model aimed at screening positive outcomes that benefit the collectivity (stakeholders). The model evaluates Crowdsourced activities as indicators for the creation of sustainable value for the enterprise and therefore for the collectivity of stakeholders. In order to test the model, we are currently deploying an Equity CrowdFunding platform embedding strong Crowdsourced tasks.In conclusion, we classify opportunities, limits and potential for a successful deployment of Crowdsourced tasks in CrowdFunding.},
booktitle = {Social Informatics: SocInfo 2014 International Workshops, Barcelona, Spain, November 11, 2014, Revised Selected Papers},
pages = {438–447},
numpages = {10},
keywords = {CrowdFunding, Human Computation, Crowdsourcing Systems, Collaborative Computing},
location = {Barcelona, Spain}
}

@inproceedings{10.1145/3178876.3186035,
author = {Almeida, Mario and Bilal, Muhammad and Finamore, Alessandro and Leontiadis, Ilias and Grunenberger, Yan and Varvello, Matteo and Blackburn, Jeremy},
title = {CHIMP: Crowdsourcing Human Inputs for Mobile Phones},
year = {2018},
isbn = {9781450356398},
publisher = {International World Wide Web Conferences Steering Committee},
address = {Republic and Canton of Geneva, CHE},
url = {https://doi.org/10.1145/3178876.3186035},
doi = {10.1145/3178876.3186035},
abstract = {While developing mobile apps is becoming easier, testing and characterizing their behavior is still hard. On the one hand, the de facto testing tool, called "Monkey," scales well due to being based on random inputs, but fails to gather inputs useful in understanding things like user engagement and attention. On the other hand, gathering inputs and data from real users requires distributing instrumented apps, or even phones with pre-installed apps, an expensive and inherently unscaleable task. To address these limitations we present CHIMP, a system that integrates automated tools and large-scale crowdsourced inputs. CHIMP is different from previous approaches in that it runs apps in a virtualized mobile environment that thousands of users all over the world can access via a standard Web browser. CHIMP is thus able to gather the full range of real-user inputs, detailed run-time traces of apps, and network traffic. We thus describe CHIMP»s design and demonstrate the efficiency of our approach by testing thousands of apps via thousands of crowdsourced users. We calibrate CHIMP with a large-scale campaign to understand how users approach app testing tasks. Finally, we show how CHIMP can be used to improve both traditional app testing tasks, as well as more novel tasks such as building a traffic classifier on encrypted network flows.},
booktitle = {Proceedings of the 2018 World Wide Web Conference},
pages = {45–54},
numpages = {10},
keywords = {crowdsourcing, mobile, testing, virtualization},
location = {Lyon, France},
series = {WWW '18}
}

@inproceedings{10.1145/2676652.2676656,
author = {Rainer, Benjamin and Timmerer, Christian},
title = {Quality of Experience of Web-based Adaptive HTTP Streaming Clients in Real-World Environments using Crowdsourcing},
year = {2014},
isbn = {9781450332811},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2676652.2676656},
doi = {10.1145/2676652.2676656},
abstract = {Multimedia streaming over HTTP has gained momentum with the approval of the MPEG-DASH standard and many research papers evaluated various aspects thereof but mainly within controlled environments. However, the actual behaviour of a DASH client within real-world environments has not yet been evaluated. The aim of this paper is to compare the QoE performance of existing DASH-based Web clients within real-world environments using crowdsourcing. Therefore, we select Google's YouTube player and two open source implementations of the MPEG-DASH standard, namely the DASH-JS from Alpen-Adria-Universitaet Klagenfurt and the dash.js which is the official reference client of the DASH Industry Forum. Based on a predefined content configuration, which is comparable among the clients, we run a crowdsourcing campaign to determine the QoE of each implementation in order to determine the current state-of-the-art for MPEG-DASH systems within real-world environments. The gathered data and its analysis will be presented in the paper. It provides insights with respect to the QoE performance of current Web-based adaptive HTTP streaming systems.},
booktitle = {Proceedings of the 2014 Workshop on Design, Quality and Deployment of Adaptive Video Streaming},
pages = {19–24},
numpages = {6},
keywords = {crowdsourcing, dash, dynamic adaptive streaming over http, mpeg, qoe, quality of experience, subjective quality assessment},
location = {Sydney, Australia},
series = {VideoNext '14}
}

@inproceedings{10.1145/1743384.1743478,
author = {Nowak, Stefanie and R\"{u}ger, Stefan},
title = {How reliable are annotations via crowdsourcing: a study about inter-annotator agreement for multi-label image annotation},
year = {2010},
isbn = {9781605588155},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1743384.1743478},
doi = {10.1145/1743384.1743478},
abstract = {The creation of golden standard datasets is a costly business. Optimally more than one judgment per document is obtained to ensure a high quality on annotations. In this context, we explore how much annotations from experts differ from each other, how different sets of annotations influence the ranking of systems and if these annotations can be obtained with a crowdsourcing approach. This study is applied to annotations of images with multiple concepts. A subset of the images employed in the latest ImageCLEF Photo Annotation competition was manually annotated by expert annotators and non-experts with Mechanical Turk. The inter-annotator agreement is computed at an image-based and concept-based level using majority vote, accuracy and kappa statistics. Further, the Kendall τ and Kolmogorov-Smirnov correlation test is used to compare the ranking of systems regarding different ground-truths and different evaluation measures in a benchmark scenario. Results show that while the agreement between experts and non-experts varies depending on the measure used, its influence on the ranked lists of the systems is rather small. To sum up, the majority vote applied to generate one annotation set out of several opinions, is able to filter noisy judgments of non-experts to some extent. The resulting annotation set is of comparable quality to the annotations of experts.},
booktitle = {Proceedings of the International Conference on Multimedia Information Retrieval},
pages = {557–566},
numpages = {10},
keywords = {crowdsourcing, inter-annotator agreement},
location = {Philadelphia, Pennsylvania, USA},
series = {MIR '10}
}

@inproceedings{10.5555/1927229.1927276,
author = {La Vecchia, Gioacchino and Cisternino, Antonio},
title = {Collaborative workforce, business process crowdsourcing as an alternative of BPO},
year = {2010},
isbn = {3642169848},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {Crowdsourcing is the act of outsourcing activities to networked people. This paper presents Business Process Crowdsourcing, an alternative to Business Process Outsourcing where crowd activities are coordinated, work force contributions not wasted and final result guaranteed. The positioning paper shows how to transform canonical business processes in crowdsourced business processes where Web 2.0, social networks, and business process management are combined to deploy business critical process to the Internet, getting the same level of quality and control of traditional outsourcing approaches with conventional workforce.},
booktitle = {Proceedings of the 10th International Conference on Current Trends in Web Engineering},
pages = {425–430},
numpages = {6},
keywords = {business process management, collaborative intelligence, crowdsourcing, social production},
location = {Vienna, Austria},
series = {ICWE'10}
}

@inproceedings{10.1109/UCC.2013.98,
author = {Schultheiss, Daniel and Blieske, Anja and Solf, Anja and Staeudtner, Saskia},
title = {How to Encourage the Crowd? A Study about User Typologies and Motivations on Crowdsourcing Platforms},
year = {2013},
isbn = {9780769551524},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/UCC.2013.98},
doi = {10.1109/UCC.2013.98},
abstract = {Studies about user participation on crowd sourcing platforms have revealed lists of different motivational factors. Reward and payment seems to be crucial at least on market places for creative ideas or workforce. However, past surveys or experiments mostly concentrated on one platform each with little theoretic background. The present exploratory study within the theoretic frame of motivation and creativity science appealed to more heterogeneity of platforms to be analyzed. Thus a broader view on user motivation was possible and a typology of crowd sourcers revealed four clusters of different platform users: female creatives, male technicians, academics and alternative all-rounders.},
booktitle = {Proceedings of the 2013 IEEE/ACM 6th International Conference on Utility and Cloud Computing},
pages = {506–509},
numpages = {4},
keywords = {crowdsourcing, extrinsic, intrinsic, motivation, socio-demographics, typology},
series = {UCC '13}
}

@inproceedings{10.1145/3364335.3364385,
author = {Rey, William P. and Balderama, Neil Anne Mae S. and Hipulan, Sebastianne L. and Salayon, Arney Azzih C.},
title = {MELDEN: An Android Based Mobile Crime Reporting Application Using Crowdsourcing},
year = {2019},
isbn = {9781450376532},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3364335.3364385},
doi = {10.1145/3364335.3364385},
abstract = {Among the most common crimes in the Philippines are theft, physical assault and robbery. Melden is an Android Based mobile crime reporting application that uses crowdsourcing to allow individuals to report various street crime with the reports directing to barangay officials. With the use of cross-referencing for validation of reports, the application aims for an easier, faster and reliable way of reporting of incidents to the barangay officials. The applications help with both the reporting of incidents and in gathering information about the incidents.},
booktitle = {Proceedings of the 5th International Conference on Industrial and Business Engineering},
pages = {198–201},
numpages = {4},
keywords = {Crime, Crime Report, Crowdsourcing, Report, Street Crimes},
location = {Hong Kong, Hong Kong},
series = {ICIBE '19}
}

@inproceedings{10.5555/1927229.1927273,
author = {Oliveira, F\'{a}bio and Ramos, Isabel and Santos, Leonel},
title = {Definition of a crowdsourcing innovation service for the European SMEs},
year = {2010},
isbn = {3642169848},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {Based on literature review and on the study of the most known and referred Crowdsourcing brokers, there's a clear trend to implement this model by large companies and mainly within the North American context. Our research team is focused in bringing this approach closer to the European culture, more specifically the cultural factors underlying the dynamics and motivation of communities available to solve the innovation challenges of Small and Medium Enterprises (SMEs), that we call Crowdsourcing Innovation. We believe that, due to the common lack of resources for innovation in these companies, a service capable of involving them in large networks filled with useful and reachable knowledge, and capable of supporting these companies through all the innovation process, is crucial to the future competitiveness of the European SMEs. Although our team is focusing on several aspects related to Crowdsourcing, my main research focuses the information services and supporting applications to create a web platform adapted to the key economical, organizational, legal and cultural differences that make current Crowdsourcing Innovation businesses less popular among European SMEs than in North America.},
booktitle = {Proceedings of the 10th International Conference on Current Trends in Web Engineering},
pages = {412–416},
numpages = {5},
keywords = {crowdsourcing innovation, european SMEs, intermediaries},
location = {Vienna, Austria},
series = {ICWE'10}
}

@inproceedings{10.1145/3126858.3126897,
author = {de Amorim, Marcello N. and Segundo, Ricardo M.C. and Santos, Celso A.S. and Tavares, Orivaldo de L.},
title = {Video Annotation by Cascading Microtasks: a Crowdsourcing Approach},
year = {2017},
isbn = {9781450350969},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3126858.3126897},
doi = {10.1145/3126858.3126897},
abstract = {This paper presents a general approach to perform crowdsourcing video annotation without requiring trained workers nor experts. It consists of dividing complex annotation tasks into simple and small microtasks and cascading them to generate a final result. Moreover, this approach allows using simple annotation tools rather than complex and expensive annotation systems. Also, it tends to avoid activities that may be tedious and time-consuming for workers. The cascade microtasks strategy is included in a workflow of three steps: Preparation, Annotation, and Presentation. A crowdsourcing video annotation process in which four different microtasks were cascaded was developed to evaluate the proposed approach. In the process, extra content such as images, text, hyperlinks and other elements are applied in the video enrichment. To support the experiment was developed a toolkit that includes Web-based annotation tools and aggregation methods, besides a presentation system for the annotated videos. This toolkit is open source and can be downloaded and used to replicate this experiment, as so to construct different crowdsourcing video annotation systems.},
booktitle = {Proceedings of the 23rd Brazillian Symposium on Multimedia and the Web},
pages = {49–56},
numpages = {8},
keywords = {crowdsourcing, human computation, microtasks, multimedia systems, video annotation, video enrichment},
location = {Gramado, RS, Brazil},
series = {WebMedia '17}
}

@inproceedings{10.1007/978-3-642-13489-0_46,
author = {Lin, Huairen and Davis, Joseph},
title = {Computational and crowdsourcing methods for extracting ontological structure from folksonomy},
year = {2010},
isbn = {3642134882},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-642-13489-0_46},
doi = {10.1007/978-3-642-13489-0_46},
abstract = {This paper investigates the unification of folksonomies and ontologies in such a way that the resulting structures can better support exploration and search on the World Wide Web. First, an integrated computational method is employed to extract the ontological structures from folksonomies. It exploits the power of low support association rule mining supplemented by an upper ontology such as WordNet. Promising results have been obtained from experiments using tag datasets from Flickr and Citeulike. Next, a crowdsourcing method is introduced to channel online users' search efforts to help evolve the extracted ontology.},
booktitle = {Proceedings of the 7th International Conference on The Semantic Web: Research and Applications - Volume Part II},
pages = {472–477},
numpages = {6},
location = {Heraklion, Greece},
series = {ESWC'10}
}

