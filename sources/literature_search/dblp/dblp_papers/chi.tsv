Tasks of a Different Color: How Crowdsourcing Practices Differ per Complex Task Type and Why This Matters.	Yihong Wang, Konstantinos Papangelis, Ioanna Lykourentzou, Michael Saker, Alan Chamberlain, Vassilis-Javed Khan, Hai-Ning Liang, Yong Yue	chi2023	Crowdsourcing in China is a thriving industry. Among its most interesting structures, we find crowdfarms, in which crowdworkers self-organize as small organizations to tackle macrotasks. Little, however, is known as to which practices these crowdfarms use to tackle the macrotasks, and this goes hand in hand with the current practice of the HCI research community to treat all forms of complex crowdsourcing work as practically the same. However, macrotasks differ substantially regarding structure and decomposability. Treating them under one umbrella term - macrotasking - can lead to an imprecise understanding of the workforce involved. We address this gap by examining the work practices of 31 Chinese crowdfarms on the four main macrotask types, namely: modular, interlaced, wicked, and container macrotasks. Our results confirm essential differences in how these nascent crowd organizations address different macrotasks and shed light on what platforms can do to improve the uptake of such work.
Can we crowdsource Tacton similarity perception and metaphor ratings?	Dong-Jae Kwon, Ramzi Abou Chahine, Chungman Lim, Hasti Seifi, Gunhyuk Park	chi2023	High-fidelity vibration actuators in recent mobile phones allow designers to crowdsource user evaluation of vibrotactile (VT) Tactons. Yet, little work has examined whether online crowdsourcing platforms can provide comparable results to lab studies. To address this question, we conducted two studies with iOS devices in the lab and crowdsourced settings. In Study I, 40 users provided pairwise similarity ratings for 12 VT Tactons that varied in their parameters (e.g., duration). In Study II, 40 new users rated pairwise similarities for 14 Tactons representing different metaphors (e.g., heartbeat). They also rated the Tactons’ match to the metaphors. In both studies, the resulting similarities and perceptual spaces strongly correlated in the lab and crowdsourced settings. Furthermore, 60% of the metaphor ratings were statistically equivalent in the two settings. We discuss the results and outline directions for future work on haptic crowdsourcing.
Interface Design for Crowdsourcing Hierarchical Multi-Label Text Annotations.	Rickard Stureborg, Bhuwan Dhingra, Jun Yang	chi2023	Human data labeling is an important and expensive task at the heart of supervised learning systems. Hierarchies help humans understand and organize concepts. We ask whether and how concept hierarchies can inform the design of annotation interfaces to improve labeling quality and efficiency. We study this question through annotation of vaccine misinformation, where the labeling task is difficult and highly subjective. We investigate 6 user interface designs for crowdsourcing hierarchical labels by collecting over 18,000 individual annotations. Under a fixed budget, integrating hierarchies into the design improves crowdsource workers’ F1 scores. We attribute this to (1) Grouping similar concepts, improving F1 scores by +0.16 over random groupings, (2) Strong relative performance on high-difficulty examples (relative F1 score difference of +0.40), and (3) Filtering out obvious negatives, increasing precision by +0.07. Ultimately, labeling schemes integrating the hierarchy outperform those that do not — achieving mean F1 of 0.70.
Evaluating Large Language Models in Generating Synthetic HCI Research Data: a Case Study.	Perttu Hämäläinen, Mikke Tavast, Anton Kunnari	chi2023	Collecting data is one of the bottlenecks of Human-Computer Interaction (HCI) research. Motivated by this, we explore the potential of large language models (LLMs) in generating synthetic user research data. We use OpenAI’s GPT-3 model to generate open-ended questionnaire responses about experiencing video games as art, a topic not tractable with traditional computational user models. We test whether synthetic responses can be distinguished from real responses, analyze errors of synthetic data, and investigate content similarities between synthetic and real data. We conclude that GPT-3 can, in this context, yield believable accounts of HCI experiences. Given the low cost and high speed of LLM data generation, synthetic data should be useful in ideating and piloting new experiments, although any findings must obviously always be validated with real data. The results also raise concerns: if employed by malicious users of crowdsourcing services, LLMs may make crowdsourcing of self-report data fundamentally unreliable.
NewsComp: Facilitating Diverse News Reading through Comparative Annotation.	Md Momen Bhuiyan, Sang Won Lee, Nitesh Goyal, Tanushree Mitra	chi2023	To support efficient, balanced news consumption, merging articles from diverse sources into one, potentially through crowdsourcing, could alleviate some hurdles. However, the merging process could also impact annotators’ attitudes towards the content. To test this theory, we propose comparative news annotation; that is, annotating similarities and differences between a pair of articles. By developing and deploying NewsComp—a prototype system—we conducted a between-subjects experiment (N = 109) to examine how users’ annotations compare to experts’, and how comparative annotation affects users’ perceptions of article credibility and quality. We found that comparative annotation can marginally impact users’ credibility perceptions in certain cases; it did not impact perceptions of quality. While users’ annotations were not on par with experts’, they showed greater precision in finding similarities than in identifying disparate important statements. The comparison process also led users to notice differences in information placement and depth, degree of factuality/opinion, and empathetic/inflammatory language use. We discuss implications for the design of future comparative annotation tasks.
CrowdSurfer: Seamlessly Integrating Crowd-Feedback Tasks into Everyday Internet Surfing.	Saskia Haug, Ivo Benke, Daniel Fischer, Alexander Maedche	chi2023	Crowd feedback overcomes scalability issues of feedback collection on interactive website designs. However, collecting feedback on crowdsourcing platforms decouples the feedback provider from the context of use. This creates more effort for crowdworkers to immerse into such context in crowdsourcing tasks. In this paper, we present CrowdSurfer, a browser extension that seamlessly integrates design feedback collection in crowdworkers’ everyday internet surfing. This enables the scalable collection of in situ feedback and, in parallel, allows crowdworkers to flexibly integrate their work into their daily activities. In a field study, we compare the CrowdSurfer against traditional feedback collection. Our qualitative and quantitative results reveal that, while in situ feedback with the CrowdSurfer is not necessarily better, crowdworkers appreciate the effortless, enjoyable, and innovative method to conduct feedback tasks. We contribute with our findings on in situ feedback collection and provide recommendations for the integration of crowdworking tasks in everyday internet surfing.
Exploring the design space for Crowdsourcing Journey eXperience.	Selin Zileli, Richard C. Gomer, m. c. schraefel	chi2023a	This paper aims to establish a design space for tools to crowdsource data about Journey eXperience (JX). Understanding JX could provide a range of benefits in various domains, including transport planning, urban design, and individual route planning, and support efforts to achieve more sustainable transport systems. A key challenge in studying JX is finding effective ways to collect data. We present initial results from a diary study that explores what JX means and how JX data might be collected. We show how the experience of journeying through space differs from experiences of being in space, provide an initial description of factors that influence JX, and offer high-level insights of relevance to the design space of JX data collection.
"""I'm"" Lost in Translation: Pronoun Missteps in Crowdsourced Data Sets."	Katie Seaborn, Yeongdae Kim	chi2023a	As virtual assistants continue to be taken up globally, there is an ever-greater need for these speech-based systems to communicate naturally in a variety of languages. Crowdsourcing initiatives have focused on multilingual translation of big, open data sets for use in natural language processing (NLP). Yet, language translation is often not one-to-one, and biases can trickle in. In this late-breaking work, we focus on the case of pronouns translated between English and Japanese in the crowdsourced Tatoeba database. We found that masculine pronoun biases were present overall, even though plurality in language was accounted for in other ways. Importantly, we detected biases in the translation process that reflect nuanced reactions to the presence of feminine, neutral, and/or non-binary pronouns. We raise the issue of translation bias for pronouns and offer a practical solution to embed plurality in NLP data sets.
Is cartoonized life-vlogger the key to increasing adoption of activity-oriented wearable camera systems?	Glenn Fernandes, Helen Zhu, Mahdi Pedram, Jacob Schauer, Soroush Shahi, Christopher Romano, Darren Gergle, Nabil Alshurafa	chi2023a	Health science researchers studying human behavior rely on wearable cameras to visually confirm behaviors in real-world settings. However, privacy concerns significantly impede their adoption. Lens orientation and activity-oriented cameras have potential in balancing the need to visually validate the wearers’ activities while reducing privacy concerns. To increase adoption and further alleviate privacy concerns while maintaining utility, generative stylizing approaches, like cartooning using generative adversarial networks (GANs), have recently shown promise. We investigate different cartoon-based obfuscation of activity-oriented footage through two studies. The first deploys crowdsourcing methods (n=60), while the second is experiential, where participants (n=49) don the device for an entire day and report concerns on their footage. Our findings support that cartoonization of activity-oriented data significantly reduces privacy concerns, particularly among bystanders in high privacy-concerning scenarios, while maintaining context verification (90% of participants). Through thematic analysis, we provide further insight for the community on best practices for cartoonization of activity-oriented videos.
Tacit Knowledge Elicitation for Shop-floor Workers with an Intelligent Assistant.	Samuel Kernan Freire, Chaofan Wang, Santiago Ruiz-Arenas, Evangelos Niforatos	chi2023a	Many industries face the challenge of capturing workers’ knowledge to share it, particularly tacit knowledge. The operation of complex systems such as a manufacturing line is knowledge-intensive. Considering this knowledge’s breadth and dynamic nature, existing knowledge-sharing solutions are inefficient and resource intensive. Conversational user interfaces are an efficient way to convey information that mimics how humans share knowledge; however, we know little about how to design them specifically for knowledge sharing, especially regarding tacit knowledge. In this work, we present an intelligent assistant that we have developed to support the elicitation of tacit knowledge from workers through systematic reflection. The system can interact with workers by voice or text and generate visualizations of shop floor data to support reflective prompts.
The Human Factors of AI-Empowered Knowledge Sharing.	Samuel Kernan Freire	chi2023a	Many industries are facing the challenge of how to capture workers’ knowledge such that it can be shared, in particular tacit knowledge. The operation of complex systems such as a manufacturing line is knowledge-intensive, especially if the operator must frequently reconfigure it for different products. Considering the breadth and dynamic nature of this knowledge, existing solutions for sharing knowledge (e.g., word-of-mouth, issue reports, document creation, and decision support systems) are inefficient and/or resource-intensive. Conversational user interfaces are an efficient way to convey information that mimics the way humans share knowledge; however, we know little about how to design them specifically for this purpose, especially regarding tacit knowledge. In this work, my main goal is to investigate how a cognitive assistant can be designed to facilitate (tacit) knowledge transfer between users of dynamic complex systems. I aim to achieve this by outlining the design requirements, challenges, and opportunities in factories; by collaboratively designing, implementing, and evaluating a cognitive assistant for sharing knowledge; studying the effects of design characteristics on aspects such as user experience; and finally, creating a set of design guidelines.
Crowdsourcing Data for Safer Travel Experiences for Women in India.	Xiao Yuan Huang, Emma West, Sai Samba Karthikeya Pinnelli	chi2023a	In 2018, the Thomas Reuters perception survey ranked India as the most dangerous country for women due to the high risk of sexual, and non-sexual violence, and harassment [2]. In a study conducted by Jagori and UN Women in Delhi, it was observed that the highest number of incidents reported by women occurred on buses, other public transportation, and on the streets [3]. With the rise of women traveling and entering the workforce, it's critical to observe how Information and Communications for Technology and Development (ICTD) may be designed to increase female safety while traveling. Through a diary study, competitor analysis, and user interview methodology, our team uncovered unique challenges in public transportation, social and gender role expectations, and familial interventions that shape the safety of women in Southern India. Informed by our research, we created Navi.io, a mobile safety application that enables users to feel safer and make informed decisions when traveling by utilizing crowdsourced data and connecting with contacts in transit.
Great Chain of Agents: The Role of Metaphorical Representation of Agents in Conversational Crowdsourcing.	Ji-Youn Jung, Sihang Qiu, Alessandro Bozzon, Ujwal Gadiraju	chi2022	Conversational agents are being widely adopted across several domains to serve a variety of purposes ranging from providing intelligent assistance to companionship. Recent literature has shown that users develop intuitive folk theories and a metaphorical understanding of conversational agents (CAs) due to the lack of a mental model of the agents. However, investigation of metaphorical agent representation in the HCI community has mainly focused on the human level, despite non-human metaphors for agents being prevalent in the real world. We adopted Lakoff and Turner’s ‘Great Chain of Being’ framework to systematically investigate the impact of using non-human metaphors to represent conversational agents on worker engagement in crowdsourcing marketplaces. We designed a text-based conversational agent that assists crowd workers in task execution. Through a between-subjects experimental study (N = 341), we explored how different human and non-human metaphors affect worker engagement, the perceived cognitive load of workers, intrinsic motivation, and their trust in the agents. Our findings bridge the gap of how users experience CAs with non-human metaphors in the context of conversational crowdsourcing.
An Exploratory Study of Sharing Strategic Programming Knowledge.	Maryam Arab, Thomas D. LaToza, Jenny T. Liang, Amy J. Ko	chi2022	In many domains, strategic knowledge is documented and shared through checklists and handbooks. In software engineering, however, developers rarely share strategic knowledge for approaching programming problems, in contrast to other artifacts and despite its importance to productivity and success. To understand barriers to sharing, we simulated a programming strategy knowledge-sharing platform, asking experienced developers to articulate a programming strategy and others to use these strategies while providing feedback. Throughout, we asked strategy authors and users to reflect on the challenges they faced. Our analysis revealed that developers could share strategic knowledge. However, they struggled in choosing a level of detail and understanding the diversity of the potential audience. While authors required substantial feedback, users struggled to give it and authors to interpret it. Our results suggest that sharing strategic knowledge differs from sharing code and raises challenging questions about how knowledge-sharing platforms should support search and feedback.
Ga11y: An Automated GIF Annotation System for Visually Impaired Users.	Mingrui Ray Zhang, Mingyuan Zhong, Jacob O. Wobbrock	chi2022	Animated GIF images have become prevalent in internet culture, often used to express richer and more nuanced meanings than static images. But animated GIFs often lack adequate alternative text descriptions, and it is challenging to generate such descriptions automatically, resulting in inaccessible GIFs for blind or low-vision (BLV) users. To improve the accessibility of animated GIFs for BLV users, we provide a system called Ga11y (pronounced “galley”), for creating GIF annotations. Ga11y combines the power of machine intelligence and crowdsourcing and has three components: an Android client for submitting annotation requests, a backend server and database, and a web interface where volunteers can respond to annotation requests. We evaluated three human annotation interfaces and employ the one that yielded the best annotation quality. We also conducted a multi-stage evaluation with 12 BLV participants from the United States and China, receiving positive feedback.
Method for Appropriating the Brief Implicit Association Test to Elicit Biases in Users.	Tilman Dingler, Benjamin Tag, David A. Eccles, Niels van Berkel, Vassilis Kostakos	chi2022	Implicit tendencies and cognitive biases play an important role in how information is perceived and processed, a fact that can be both utilised and exploited by computing systems. The Implicit Association Test (IAT) has been widely used to assess people’s associations of target concepts with qualitative attributes, such as the likelihood of being hired or convicted depending on race, gender, or age. The condensed version–the Brief IAT–aims to implicit biases by measuring the reaction time to concept classifications. To use this measure in HCI research, however, we need a way to construct and validate target concepts, which tend to quickly evolve and depend on geographical and cultural interpretations. In this paper, we introduce and evaluate a new method to appropriate the BIAT using crowdsourcing to measure people’s leanings on polarising topics. We present a web-based tool to test participants’ bias on custom themes, where self-assessments often fail. We validated our approach with 14 domain experts and assessed the fit of crowdsourced test construction. Our method allows researchers of different domains to create and validate bias tests that can be geographically tailored and updated over time. We discuss how our method can be applied to surface implicit user biases and run studies where cognitive biases may impede reliable results.
Birds of a feather don't fact-check each other: Partisanship and the evaluation of news in Twitter's Birdwatch crowdsourced fact-checking program.	Jennifer Allen, Cameron Martel, David G. Rand	chi2022	There is a great deal of interest in the role that partisanship, and cross-party animosity in particular, plays in interactions on social media. Most prior research, however, must infer users’ judgments of others’ posts from engagement data. Here, we leverage data from Birdwatch, Twitter’s crowdsourced fact-checking pilot program, to directly measure judgments of whether other users’ tweets are misleading, and whether other users’ free-text evaluations of third-party tweets are helpful. For both sets of judgments, we find that contextual features – in particular, the partisanship of the users – are far more predictive of judgments than the content of the tweets and evaluations themselves. Specifically, users are more likely to write negative evaluations of tweets from counter-partisans; and are more likely to rate evaluations from counter-partisans as unhelpful. Our findings provide clear evidence that Birdwatch users preferentially challenge content from those with whom they disagree politically. While not necessarily indicating that Birdwatch is ineffective for identifying misleading content, these results demonstrate the important role that partisanship can play in content evaluation. Platform designers must consider the ramifications of partisanship when implementing crowdsourcing programs.
Barriers to Expertise in Citizen Science Games.	Josh Aaron Miller, Seth Cooper	chi2022	Expertise-centric citizen science games (ECCSGs) can be powerful tools for crowdsourcing scientific knowledge production. However, to be effective these games must train their players on how to become experts, which is difficult in practice. In this study, we investigated the path to expertise and the barriers involved by interviewing players of three ECCSGs: Foldit, Eterna, and Eyewire. We then applied reflexive thematic analysis to generate themes of their experiences and produce a model of expertise and its barriers. We found expertise is constructed through a cycle of exploratory and social learning but prevented by instructional design issues. Moreover, exploration is slowed by a lack of polish to the game artifact, and social learning is disrupted by a lack of clear communication. Based on our analysis we make several recommendations for CSG developers, including: collaborating with professionals of required skill sets; providing social features and feedback systems; and improving scientific communication.
Recruiting Participants With Programming Skills: A Comparison of Four Crowdsourcing Platforms and a CS Student Mailing List.	Mohammad Tahaei, Kami Vaniea	chi2022	Reliably recruiting participants with programming skills is an ongoing challenge for empirical studies involving software development technologies, often leading to the use of crowdsourcing platforms and computer science (CS) students. In this work, we use five existing survey instruments to explore the programming skills, privacy and security attitudes, and secure development self-efficacy of participants from a CS student mailing list and four crowdsourcing platforms (Appen, Clickworker, MTurk, and Prolific). We recruited 613 participants who claimed to have programming skills and assessed recruitment channels regarding costs, quality, programming skills, as well as privacy and security attitudes. We find that 27% of crowdsourcing participants, 40% of crowdsourcing participants who self-report to be developers, and 89% of CS students answered all programming skill questions correctly. CS students were the most cost-effective recruitment channel and rated themselves lower than crowdsourcing participants about secure development self-efficacy.
REGROW: Reimagining Global Crowdsourcing for Better Human-AI Collaboration.	Andy Alorwu, Saiph Savage, Niels van Berkel, Dmitry Ustalov, Alexey Drutsa, Jonas Oppenlaender, Oliver Bates, Danula Hettiachchi, Ujwal Gadiraju, Jorge Gonçalves, Simo Hosio	chi2022a	Crowdworkers silently enable much of today’s AI-based products, with several online platforms offering a myriad of data labelling and content moderation tasks through convenient labour marketplaces. The HCI community has been increasingly interested in investigating the worker-centric issues inherent in the current model and seeking for potential improvements that could be implemented in the future. This workshop explores how a reimagined perspective on crowdsourcing platforms could provide a more equitable, fair, and rewarding experience. This includes not only the workers but also the platforms, who could benefit e.g. from better processes for worker onboarding, skills-development, and growth. We invite visionary takes in various formats on this topic to spread awareness of worker-centric research and developments to the CHI community. As a result of interactive ideation work in the workshop, we articulate a future direction roadmap for research centred around crowdsourcing platforms. Finally, as a specific interest area, the workshop seeks to study crowdwork from the context of the Global South, which has been arising as an important but critically understudied crowdsourcing market in recent years.
What Kinds of Experiences Do You Desire? A Preliminary Study of the Desired Experiences of Contributors to Location-Based Mobile Crowdsourcing.	Fang-Yu Lin, Chia-Yi Lee, Yi-Ting Ho, Yao-Kuang Chen, Yu-Chun (Grace) Yen, Yung-Ju Chang	chi2022a	Mobile crowdsourcing enables people to learn location-related information from others with diverse experiences and opinions. However, little research has investigated the expected quality of the location-related information users of mobile-crowdsourcing platforms, and the levels and types of relevant experience such users expect crowd members to possess, respectively. To fill this gap, we first conducted an interview study with 22 participants, which yielded five key information properties of the answers to location-based questions: objectivity, relativity, specificity, temporal regularity, and variability. Based on his//her stated perceptions of these properties of the requested information, we deemed each participant to desire at least one, and up to 10 main qualities of the information, and seven main aspects of contributors’ experience. A follow-up survey study was then used to quantify the characteristics of a list of location-related information according to the information properties that the 139 respondents perceived that information to have.
Debiased Label Aggregation for Subjective Crowdsourcing Tasks.	Shaun Wallace, Tianyuan Cai, Brendan Le, Luis A. Leiva	chi2022a	Human Intelligence Tasks (HITs) allow people to collect and curate labeled data from multiple annotators. Then labels are often aggregated to create an annotated dataset suitable for supervised machine learning tasks. The most popular label aggregation method is majority voting, where each item in the dataset is assigned the most common label from the annotators. This approach is optimal when annotators are unbiased domain experts. In this paper, we propose Debiased Label Aggregation (DLA) an alternative method for label aggregation in subjective HITs, where cross-annotator agreement varies. DLA leverages user voting behavior patterns to weight labels. Our experiments show that DLA outperforms majority voting in several performance metrics; e.g. a percentage increase of 20 points in the F1 measure before data augmentation, and a percentage increase of 35 points in the same measure after data augmentation. Since DLA is deceptively simple, we hope it will help researchers to tackle subjective labeling tasks.
Gamification strategies to improve the motivation and performance in accessibility information collection.	Akihiro Miyata, Yusaku Murayama, Akihiro Furuta, Kazuki Okugawa, Keihiro Ochiai, Yuko Murayama	chi2022a	The high cost of dispatching professionals to physically audit locations limits the coverage of accessibility maps. Vision-based techniques and crowdsourcing using streetscape imagery are beneficial as they do not entail physical visits by people to actual locations. However, they exhibit limitations such as outdated photos and occlusions. In-the-field crowdsourcing enables physical collection of accessibility information at a low cost. The effectiveness of this technique is contingent on people with free time and high motivation to visit actual locations. In this paper, we introduce a crowdsourcing platform for constructing accessibility maps that support people with diverse free times and motivations. A combination of field auditing and field sensing, coupled with including and excluding gamification is explored. Game types are determined based on the characteristics of field auditing and field sensing. The experimental results confirm that gamification improves the motivation levels and performance of participants in certain aspects of crowdsourced field auditing and crowdsourced field sensing. Furthermore, the results indicate that item collection games have a greater impact on the motivation for taking unusual roads to collect accessibility information than continuous walking games.
Understanding Crowdsourcing Requesters' Wage Setting Behaviors.	Kotaro Hara, Yudai Tanaka	chi2022a	Requesters on crowdsourcing platforms like Amazon Mechanical Turk (AMT) compensate workers inadequately. One potential reason for the underpayment is that the AMT’s requester interface provides limited information about estimated wages, preventing requesters from knowing if they are offering a fair piece-rate reward. To assess if presenting wage information affects requesters’ reward setting behaviors, we conducted a controlled study with 63 participants. We had three levels for a between-subjects factor in a mixed design study, where we provided participants with: no wage information, wage point estimate, and wage distribution. Each participant had three stages of adjusting the reward and controlling the estimated wage. Our analysis with Bayesian growth curve modeling suggests that the estimated wage derived from the participant-set reward increased from $2.56/h to $2.69/h and $2.33/h to $2.74/h when we provided point estimate and distribution information respectively. The wage decreased from $2.06/h to $1.99/h in the control condition.
Learning to Automate Chart Layout Configurations Using Crowdsourced Paired Comparison.	Aoyu Wu, Liwenhan Xie, Bongshin Lee, Yun Wang, Weiwei Cui, Huamin Qu	chi2021	We contribute a method to automate parameter configurations for chart layouts by learning from human preferences. Existing charting tools usually determine the layout parameters using predefined heuristics, producing sub-optimal layouts. People can repeatedly adjust multiple parameters (e.g., chart size, gap) to achieve visually appealing layouts. However, this trial-and-error process is unsystematic and time-consuming, without a guarantee of improvement. To address this issue, we develop Layout Quality Quantifier (LQ2), a machine learning model that learns to score chart layouts from paired crowdsourcing data. Combined with optimization techniques, LQ2 recommends layout parameters that improve the charts’ layout quality. We apply LQ2 on bar charts and conduct user studies to evaluate its effectiveness by examining the quality of layouts it produces. Results show that LQ2 can generate more visually appealing layouts than both laypeople and baselines. This work demonstrates the feasibility and usages of quantifying human preferences and aesthetics for chart layouts.
An Examination of the Work Practices of Crowdfarms.	Yihong Wang, Konstantinos Papangelis, Michael Saker, Ioanna Lykourentzou, Vassilis-Javed Khan, Alan Chamberlain, Jonathan Grudin	chi2021	Crowdsourcing is a new value creation business model. Annual revenue of the Chinese market alone is hundreds of millions of dollars, yet few studies have focused on the practices of the Chinese crowdsourcing workforce, and those that do mainly focus on solo crowdworkers. We have extended our study of solo crowdworker practices to include crowdfarms, a relatively new entry to the gig economy: small companies that carry out crowdwork as a key part of their business. We report here on interviews of people who work in 53 crowdfarms. We describe how crowdfarms procure jobs, carry out macrotasks and microtasks, manage their reputation, and employ different management practices to motivate crowdworkers and customers.
Assessing MyData Scenarios: Ethics, Concerns, and the Promise.	Andy Alorwu, Saba Kheirinejad, Niels van Berkel, Marianne Kinnula, Denzil Ferreira, Aku Visuri, Simo Hosio	chi2021	Public controversies around the unethical use of personal data are increasing, spotlighting data ethics as an increasingly important field of study. MyData is a related emerging vision that emphasizes individuals’ control of their personal data. In this paper, we investigate people’s perceptions of various data management scenarios by measuring the perceived ethicality and level of felt concern concerning the scenarios. We deployed a set of 96 unique scenarios to an online crowdsourcing platform for assessment and invited a representative sample of the participants to a second-stage questionnaire about the MyData vision and its potential in the field of healthcare. Our results provide a timely investigation into how topical data-related practices affect the perceived ethicality and the felt concern. The questionnaire analysis reveals great potential in the MyData vision. Through the combined quantitative and qualitative results, we contribute to the field of data ethics.
Effect of Information Presentation on Fairness Perceptions of Machine Learning Predictors.	Niels van Berkel, Jorge Gonçalves, Daniel Russo, Simo Hosio, Mikael B. Skov	chi2021	The uptake of artificial intelligence-based applications raises concerns about the fairness and transparency of AI behaviour. Consequently, the Computer Science community calls for the involvement of the general public in the design and evaluation of AI systems. Assessing the fairness of individual predictors is an essential step in the development of equitable algorithms. In this study, we evaluate the effect of two common visualisation techniques (text-based and scatterplot) and the display of the outcome information (i.e., ground-truth) on the perceived fairness of predictors. Our results from an online crowdsourcing study (N = 80) show that the chosen visualisation technique significantly alters people’s fairness perception and that the presented scenario, as well as the participant’s gender and past education, influence perceived fairness. Based on these results we draw recommendations for future work that seeks to involve non-experts in AI fairness evaluations.
#StayHome #WithMe: How Do YouTubers Help with COVID-19 Loneliness?	Shuo Niu, Ava Bartolome, Cat Mai, Nguyen Binh Ha	chi2021	Loneliness threatens public mental wellbeing during COVID-19. In response, YouTube creators participated in the #StayHome #WithMe movement (SHWM) and made myriad videos for people experiencing loneliness or boredom at home. User-shared videos generate parasocial attachment and virtual connectedness. However, there is limited knowledge of how creators contributed videos during disasters to provide social provisions as disaster-relief. Grounded on Weiss’s loneliness theory, this work analyzed 1488 SHWM videos to examine video sharing as a pathway to social provisions. Findings suggested that skill and knowledge sharing, entertaining arts, homelife activities, live chatting, and gameplay were the most popular video styles. YouTubers utilized parasocial relationships to form a space for staying away from the disaster. SHWM YouTubers provided friend-like, mentor-like, and family-like provisions through videos in different styles. Family-like provisions led to the highest overall viewer engagement. Based on the findings, design implications for supporting viewers’ mental wellbeing in disasters are discussed.
Crowdsourcing More Effective Initializations for Single-Target Trackers Through Automatic Re-querying.	Stephan J. Lemmer, Jean Y. Song, Jason J. Corso	chi2021	In single-target video object tracking, an initial bounding box is drawn around a target object and propagated through a video. When this bounding box is provided by a careful human expert, it is expected to yield strong overall tracking performance that can be mimicked at scale by novice crowd workers with the help of advanced quality control methods. However, we show through an investigation of 900 crowdsourced initializations that such quality control strategies are inadequate for this task in two major ways: first, the high level of redundancy in these methods (e.g., averaging multiple responses to reduce error) is unnecessary, as 23% of crowdsourced initializations perform just as well as the gold-standard initialization. Second, even nearly perfect initializations can lead to degraded long-term performance due to the complexity of object tracking. Considering these findings, we evaluate novel approaches for automatically selecting bounding boxes to re-query, and introduce Smart Replacement, an efficient method that decides whether to use the crowdsourced replacement initialization.
Directed Diversity: Leveraging Language Embedding Distances for Collective Creativity in Crowd Ideation.	Samuel Rhys Cox, Yunlong Wang, Ashraf M. Abdul, Christian von der Weth, Brian Y. Lim	chi2021	Crowdsourcing can collect many diverse ideas by prompting ideators individually, but this can generate redundant ideas. Prior methods reduce redundancy by presenting peers’ ideas or peer-proposed prompts, but these require much human coordination. We introduce Directed Diversity, an automatic prompt selection approach that leverages language model embedding distances to maximize diversity. Ideators can be directed towards diverse prompts and away from prior ideas, thus improving their collective creativity. Since there are diverse metrics of diversity, we present a Diversity Prompting Evaluation Framework consolidating metrics from several research disciplines to analyze along the ideation chain — prompt selection, prompt creativity, prompt-ideation mediation, and ideation creativity. Using this framework, we evaluated Directed Diversity in a series of a simulation study and four user studies for the use case of crowdsourcing motivational messages to encourage physical activity. We show that automated diverse prompting can variously improve collective creativity across many nuanced metrics of diversity.
Comparing Generic and Community-Situated Crowdsourcing for Data Validation in the Context of Recovery from Substance Use Disorders.	Sabirat Rubya, Joseph Numainville, Svetlana Yarosh	chi2021	Targeting the right group of workers for crowdsourcing often achieves better quality results. One unique example of targeted crowdsourcing is seeking community-situated workers whose familiarity with the background and the norms of a particular group can help produce better outcome or accuracy. These community-situated crowd workers can be recruited in different ways from generic online crowdsourcing platforms or from online recovery communities. We evaluate three different approaches to recruit generic and community-situated crowd in terms of the time and the cost of recruitment, and the accuracy of task completion. We consider the context of Alcoholics Anonymous (AA), the largest peer support group for recovering alcoholics, and the task of identifying and validating AA meeting information. We discuss the benefits and trade-offs of recruiting paid vs. unpaid community-situated workers and provide implications for future research in the recovery context and relevant domains of HCI, and for the design of crowdsourcing ICT systems.
Remote and Collaborative Virtual Reality Experiments via Social VR Platforms.	David Saffo, Sara Di Bartolomeo, Caglar Yildirim, Cody Dunne	chi2021	Virtual reality (VR) researchers struggle to conduct remote studies. Previous work has focused on working around limitations imposed by traditional crowdsourcing methods. However, the potential for leveraging social VR platforms for HCI evaluations is largely unexplored. These platforms have large VR-ready user populations, distributed synchronous virtual environments, and support for user-generated content. We demonstrate how social VR platforms can be used to practically and ethically produce valid research results by replicating two studies using one such platform (VRChat): a quantitative study on Fitts’ Law and a qualitative study on tabletop collaboration. Our replication studies exhibited analogous results to the originals, indicating the research validity of this approach. Moreover, we easily recruited experienced VR users with their own hardware for synchronous, remote, and collaborative participation. We further provide lessons learned for future researchers experimenting using social VR platforms. This paper and all supplemental materials are available at osf.io/c2amz.
Karamad: A Voice-based Crowdsourcing Platform for Underserved Populations.	Shan M. Randhawa, Tallal Ahmad, Jay Chen, Agha Ali Raza	chi2021	Crowdsourcing enables the completion of large-scale and hard-to-automate tasks, while allowing people to earn money. However, 3.6 billion people – a workforce comprising 46.4% of the world population – who could benefit most from this source of income lack the access and literacy to use computers, smartphones, and the internet. In this paper we present Karamad, a voice-based crowdsourcing platform that allows workers in low-resource regions to complete crowd work using low-end phones and receive payments as mobile airtime balance. We explore the usefulness, scalability, and sustainability of Karamad in Pakistan through a 6-month deployment. Without any advertising, training, or airtime subsidies, Karamad organically engaged 725 workers who completed 3,939 tasks (involving 43,006 components) including translations, dataset generation, and surveys on demographics, accessibility, disability, health, employment, and literacy. Collectively, the workers produced a valuable service market for potential customers and included female, unemployed, non-literate, and blind users.
Hardhats and Bungaloos: Comparing Crowdsourced Design Feedback with Peer Design Feedback in the Classroom.	Jonas Oppenlaender, Elina Kuosmanen, Andrés Lucero, Simo Hosio	chi2021	Feedback is an important aspect of design education, and crowdsourcing has emerged as a convenient way to obtain feedback at scale. In this paper, we investigate how crowdsourced design feedback compares to peer design feedback within a design-oriented HCI class and across two metrics: perceived quality and perceived fairness. We also examine the perceived monetary value of crowdsourced feedback, which provides an interesting contrast to the typical requester-centric view of the value of labor on crowdsourcing platforms. Our results reveal that the students (N = 106) perceived the crowdsourced design feedback as inferior to peer design feedback in multiple ways. However, they also identified various positive aspects of the online crowds that peers cannot provide. We discuss the meaning of the findings and provide suggestions for teachers in HCI and other researchers interested in crowd feedback systems on using crowds as a potential complement to peers.
Understanding Narrative Linearity for Telling Expressive Time-Oriented Stories.	Xingyu Lan, Xinyue Xu, Nan Cao	chi2021	Creating expressive narrative visualization often requires choosing a well-planned narrative order that invites the audience in. The narrative can either follow the linear order of story events (chronology), or deviate from linearity (anachronies). While evidence exists that anachronies in novels and films can enhance story expressiveness, little is known about how they can be incorporated into narrative visualization. To bridge this gap, this work introduces the idea of narrative linearity to visualization and investigates how different narrative orders affect the expressiveness of time-oriented stories. First, we conducted preliminary interviews with seven experts to clarify the motivations and challenges of manipulating narrative linearity in time-oriented stories. Then, we analyzed a corpus of 80 time-oriented stories and identified six most salient patterns of narrative orders. Next, we conducted a crowdsourcing study with 221 participants. Results indicated that anachronies have the potential to make time-oriented stories more expressive without hindering comprehensibility.
Can Anthropographics Promote Prosociality?A Review and Large-Sample Study.	Luiz Augusto de Macêdo Morais, Yvonne Jansen, Nazareno Andrade, Pierre Dragicevic	chi2021	Visualizations designed to make readers compassionate with the persons whose data is represented have been called anthropographics and are commonly employed by practitioners. Empirical studies have recently examined whether anthropographics indeed promote empathy, compassion, or the likelihood of prosocial behavior, but findings have been inconclusive so far. This work contributes a detailed overview of past experiments, and two new experiments that use large samples and a combination of design strategies to maximize the possibility of finding an effect. We tested an information-rich anthropographic against a simple bar chart, asking participants to allocate hypothetical money in a crowdsourcing study. We found that the anthropographic had, at best, a small effect on money allocation. Such a small effect may be relevant for large-scale donation campaigns, but the large sample sizes required to observe an effect and the noise involved in measuring it make it very difficult to study in more depth. Data and code are available at https://osf.io/xqae2/.
"""I Got Some Free Time"": Investigating Task-execution and Task-effort Metrics in Mobile Crowdsourcing Tasks."	Chia-En Chiang, Yu-Chun Chen, Fang-Yu Lin, Felicia Feng, Hao-An Wu, Hao-Ping Lee, Chang-Hsuan Yang, Yung-Ju Chang	chi2021	Using a mixed-methods approach over six weeks, we studied 30 smartphone users’ task choices, task execution and effort devoted to two commercial mobile crowdsourcing platforms in the wild. We focused on the influence of activity contexts, characterized by breakpoint situations and activity attributes. In line with their stated preferences, the participants were more likely to proactively perform mobile crowdsourcing tasks during transitions between activities than during an ongoing activity and during long breaks, respectively. Their task choices were influenced by various activity attributes, and more impacted by their current and preceding activities than their upcoming ones. Two of our three target outcomes, task execution and task choice, were also influenced by individuals’ stress and energy levels. Our qualitative data provide further insights into participants’ decisions about which crowdsourcing tasks to perform and when; and our results’ implications for the design of future mobile crowdsourcing task-prompting mechanisms are also discussed.
It's About Time: A View of Crowdsourced Data Before and During the Pandemic.	Evgenia Christoforou, Pinar Barlas, Jahna Otterbacher	chi2021	Data attained through crowdsourcing have an essential role in the development of computer vision algorithms. Crowdsourced data might include reporting biases, since crowdworkers usually describe what is “worth saying” in addition to images’ content. We explore how the unprecedented events of 2020, including the unrest surrounding racial discrimination, and the COVID-19 pandemic, might be reflected in responses to an open-ended annotation task on people images, originally executed in 2018 and replicated in 2020. Analyzing themes of Identity and Health conveyed in workers’ tags, we find evidence that supports the potential for temporal sensitivity in crowdsourced data. The 2020 data exhibit more race-marking of images depicting non-Whites, as well as an increase in tags describing Weight. We relate our findings to the emerging research on crowdworkers’ moods. Furthermore, we discuss the implications of (and suggestions for) designing tasks on proprietary platforms, having demonstrated the possibility for additional, unexpected variation in crowdsourced data due to significant events.
Crowdsourcing Design Guidance for Contextual Adaptation of Text Content in Augmented Reality.	John J. Dudley, Jason T. Jacques, Per Ola Kristensson	chi2021	Augmented Reality (AR) can deliver engaging user experiences that seamlessly meld virtual content with the physical environment. However, building such experiences is challenging due to the developer’s inability to assess how uncontrolled deployment contexts may influence the user experience. To address this issue, we demonstrate a method for rapidly conducting AR experiments and real-world data collection in the user’s own physical environment using a privacy-conscious mobile web application. The approach leverages the large number of distinct user contexts accessible through crowdsourcing to efficiently source diverse context and perceptual preference data. The insights gathered through this method complement emerging design guidance and sample-limited lab-based studies. The utility of the method is illustrated by re-examining the design challenge of adapting AR text content to the user’s environment. Finally, we demonstrate how gathered design insight can be operationalized to provide adaptive text content functionality in an AR headset.
Task Design for Crowdsourcing Complex Cognitive Skills.	Gaoping Huang, Meng-Han Wu, Alexander J. Quinn	chi2021a	Task design for crowdsourcing is a key factor limiting the quality of crowd-sourced results. This case study presents our design process for a complex cognitive task: generating Dimension/Values for categorizing ideas. Conveying the task to workers was a formidable design challenge. We present five strategies and lessons learned from testing with workers. Decomposing the cognitive process and testing mastery of each cognitive subprocess enabled us to finally convey the task requirements and obtain useful results.
From the Margins to the Centre: Defining New Mission and Vision for HCI Research in South Asia.	Pranjal Jain, Samia Ibtasam, Sumita Sharma, Nilavra Bhattacharya, Anupriya Tuli, Dilrukshi Gamage, Dhruv Jain, Rucha Tulaskar, Priyank Chandra, Lubna Razaq, Rahat Jahangir Rony, Deepak Ranjan Padhi, Mohit Jain, Suleman Shahid, Nova Ahmed, Devanuj Kanta Balkrishan, Pushpendra Singh	chi2021a	The past two decades have seen an increase in the amount of research in the CHI community from South Asia with a focus on designing for the unique and diverse socio-cultural, political, infrastructural, and geographical background of the region. However, the studies presented to the CHI community primarily focus on working with and unpacking the regional contextual constraints (of the users and the infrastructures), thus taking a developmental stance. In this online workshop, we aim to broaden the perspective of the CHI research and community towards the contributions from the region including and beyond development, by bringing together researchers, designers, and practitioners working or are interested in working within these regions on diverse topics such as universal education, global healthcare, accessibility, sustainability, and more. Through the workshop discussion, group design activity, and brainstorming, we aim to provide a space for symbiotic knowledge sharing, and defining shared visions and missions for HCI activities in South Asia for including and moving beyond the development agenda.
You Are (Not) The Robot: Variable Perspective Motion Control of a Social Telepresence Robot.	Michael Suguitan, Guy Hoffman	chi2021a	COVID-19 has dramatically limited opportunities for in-person human-robot interaction research and shifted focus towards remote technologies such as telepresence robots. Telepresence robots enable rich communication and agency through their physical presence and controllability, but their screen-oriented designs and button-centric controls abstract users away from their own physicality. In this demonstration, we present a telepresence system for remotely controlling a social robot using a smartphone’s motion sensors. Users can select between a first-person perspective from the robot’s internal camera or a third-person perspective showing the robot’s whole body. Users can also record their movements for later playback. This system has applications as an embodied remote communication platform and for crowdsourcing demonstrations of user-crafted robot movements.
VisLab: Crowdsourcing Visualization Experiments in the Wild.	Jinhan Choi, Changhoon Oh, Bongwon Suh, Nam Wook Kim	chi2021a	When creating a visualization to understand and communicate data, we face different design choices. Even though past empirical research provides foundational knowledge for visualization design, practitioners still rely on their hunches to deal with intricate trade-offs in the wild. On the other hand, researchers lack the time and resources to rigorously explore the growing design space through controlled experiments. In this work, we aim to address this two-fold problem by crowdsourcing visualization experiments. We developed VisLab, an online platform in which anyone can design and deploy experiments to evaluate their visualizations. To alleviate the complexity of experiment design and analysis, our platform provides scaffold templates and analytic dashboards. To motivate broad participation in the experiments, the platform enables anonymous participation and provides personalized performance feedback. We present use case scenarios that demonstrate the usability and usefulness of the platform in addressing the different needs of practitioners, researchers, and educators.
SAD: A Stress Annotated Dataset for Recognizing Everyday Stressors in SMS-like Conversational Systems.	Matthew Louis Mauriello, Thierry Lincoln, Grace Hon, Dorien Simon, Dan Jurafsky, Pablo Paredes	chi2021a	There is limited infrastructure for providing stress management services to those in need. To address this problem, chatbots are viewed as a scalable solution. However, one limiting factor is having clear definitions and examples of daily stress on which to build models and methods for routing appropriate advice during conversations. We developed a dataset of 6850 SMS-like sentences that can be used to classify input using a scheme of 9 stressor categories derived from: stress management literature, live conversations from a prototype chatbot system, crowdsourcing, and targeted web scraping from an online repository. In addition to releasing this dataset, we show results that are promising for classification purposes. Our contributions include: (i) a categorization of daily stressors, (ii) a dataset of SMS-like sentences, (iii) an analysis of this dataset that demonstrates its potential efficacy, and (iv) a demonstration of its utility for implementation via a simulation of model response times.
A Crowdsourcing Platform for Constructing Accessibility Maps Supporting Multiple Participation Modes.	Akihiro Miyata, Kazuki Okugawa, Yuki Yamato, Tadashi Maeda, Yusaku Murayama, Megumi Aibara, Masakazu Furuichi, Yuko Murayama	chi2021a	Accessibility maps enable impaired/elderly people to move around more smoothly and with less risk. However, very few examples satisfy both the accuracy and coverage requirements, owing to the high cost of physically auditing everchanging roads and pathways. Although crowdsourcing approaches can ostensibly solve this problem, existing studies have relied on volunteers with free time and high motivation. In this paper, we propose a crowdsourcing platform for constructing accessibility maps to support four participation modes: Reporter for people having plenty of free time and high motivation; Gaming reporter for people having plenty of free time but with low motivation; Walker for people lacking enough free time but with high motivation; and Gaming walker for people lacking enough free time and with low motivation. This design allows people to select a suitable participation method, depending on their time and motivation. In this study, we developed a prototype by integrating deep learning techniques, a game design theory, and heatmap visualization.
Understanding and Designing for Disaster Preparation on Social Media.	Irene Chen, Jennifer L. Gibbs, Junchao Lin	chi2021a	The widespread adoption of social media brings both challenges and opportunities for the field of disaster management. However, there has been limited scholarly work on social media usage for disaster preparation. Our work explores how social media sites can be designed to help individuals and communities prepare for disasters, in particular with regard to crowdsourcing information, digital mobilization, and community resilience. We present preliminary findings from 4 online focus groups (N=31), which reveal two emergent themes that may prove useful for future research and design in HCI. We also propose four design recommendations for social media sites as tools for disaster preparation. These findings support the design and evaluation of social media sites to aid in community-based disaster preparedness.
Will the Crowd Game the Algorithm?: Using Layperson Judgments to Combat Misinformation on Social Media by Downranking Distrusted Sources.	Ziv Epstein, Gordon Pennycook, David G. Rand	chi2020	"How can social media platforms fight the spread of misinformation? One possibility is to use newsfeed algorithms to downrank content from sources that users rate as untrustworthy. But will laypeople be handicapped by motivated reasoning or lack of expertise, and thus unable to identify misinformation sites? And will they ""game"" this crowdsourcing mechanism in order to promote content that aligns with their partisan agendas? We conducted a survey experiment in which =984 Americans indicated their trust in numerous news sites. To study the tendency of people to game the system, half of the participants were told their responses would inform social media ranking algorithms. Participants trusted mainstream sources much more than hyper-partisan or fake news sources, and their ratings were highly correlated with professional fact-checker judgments. Critically, informing participants that their responses would influence ranking algorithms did not diminish these results, despite the manipulation increasing the political polarization of trust ratings."
Brainsourcing: Crowdsourcing Recognition Tasks via Collaborative Brain-Computer Interfacing.	Keith M. Davis, Lauri Kangassalo, Michiel M. A. Spapé, Tuukka Ruotsalo	chi2020	This paper introduces brainsourcing: utilizing brain responses of a group of human contributors each performing a recognition task to determine classes of stimuli. We investigate to what extent it is possible to infer reliable class labels using data collected utilizing electroencephalography (EEG) from participants given a set of common stimuli. An experiment (N=30) measuring EEG responses to visual features of faces (gender, hair color, age, smile) revealed an improved F1 score of 0.94 for a crowd of twelve participants compared to an F1 score of 0.67 derived from individual participants and a random chance of 0.50. Our results demonstrate the methodological and pragmatic feasibility of brainsourcing in labeling tasks and opens avenues for more general applications using brain-computer interfacing in a crowdsourced setting.
"""Hi! I am the Crowd Tasker"" Crowdsourcing through Digital Voice Assistants."	Danula Hettiachchi, Zhanna Sarsenbayeva, Fraser Allison, Niels van Berkel, Tilman Dingler, Gabriele Marini, Vassilis Kostakos, Jorge Gonçalves	chi2020	Inspired by the increasing prevalence of digital voice assistants, we demonstrate the feasibility of using voice interfaces to deploy and complete crowd tasks. We have developed Crowd Tasker, a novel system that delivers crowd tasks through a digital voice assistant. In a lab study, we validate our proof-of-concept and show that crowd task performance through a voice assistant is comparable to that of a web interface for voice-compatible and voice-based crowd tasks for native English speakers. We also report on a field study where participants used our system in their homes. We find that crowdsourcing through voice can provide greater flexibility to crowd workers by allowing them to work in brief sessions, enabling multi-tasking, and reducing the time and effort required to initiate tasks. We conclude by proposing a set of design guidelines for the creation of crowd tasks for voice and the development of future voice-based crowdsourcing systems.
Du Bois Wrapped Bar Chart: Visualizing Categorical Data with Disproportionate Values.	Alireza Karduni, Ryan Wesslen, Isaac Cho, Wenwen Dou	chi2020	We propose a visualization technique, Du Bois wrapped bar chart, inspired by work of W.E.B Du Bois. Du Bois wrapped bar charts enable better large-to-small bar comparison by wrapping large bars over a certain threshold. We first present two crowdsourcing experiments comparing wrapped and standard bar charts to evaluate (1) the benefit of wrapped bars in helping participants identify and compare values; (2) the characteristics of data most suitable for wrapped bars. In the first study (n=98) using real-world datasets, we find that wrapped bar charts lead to higher accuracy in identifying and estimating ratios between bars. In a follow-up study (n=190) with 13 simulated datasets, we find participants were consistently more accurate with wrapped bar charts when certain category values are disproportionate as measured by entropy and H-spread. Finally, in an in-lab study, we investigate participants' experience and strategies, leading to guidelines for when and how to use wrapped bar charts.
Crowdsourced Detection of Emotionally Manipulative Language.	Jordan S. Huffaker, Jonathan K. Kummerfeld, Walter S. Lasecki, Mark S. Ackerman	chi2020	"Detecting rhetoric that manipulates readers' emotions requires distinguishing intrinsically emotional content (IEC; e.g., a parent losing a child) from emotionally manipulative language (EML; e.g., using fear-inducing language to spread anti-vaccine propaganda). However, this remains an open classification challenge for both automatic and crowdsourcing approaches. Machine Learning approaches only work in narrow domains where labeled training data is available, and non-expert annotators tend to conflate IEC with EML. We introduce an approach, anchor comparison, that leverages workers' ability to identify and remove instances of EML in text to create a paraphrased ""anchor text"", which is then used as a comparison point to classify EML in the original content. We evaluate our approach with a dataset of news-style text snippets and show that precision and recall can be tuned for system builders' needs. Our contribution is a crowdsourcing approach that enables non-expert disentanglement of social references from content."
Evaluating Multivariate Network Visualization Techniques Using a Validated Design and Crowdsourcing Approach.	Carolina Nobre, Dylan Wootton, Lane Harrison, Alexander Lex	chi2020	Visualizing multivariate networks is challenging because of the trade-offs necessary for effectively encoding network topology and encoding the attributes associated with nodes and edges. A large number of multivariate network visualization techniques exist, yet there is little empirical guidance on their respective strengths and weaknesses. In this paper, we describe a crowdsourced experiment, comparing node-link diagrams with on-node encoding and adjacency matrices with juxtaposed tables. We find that node-link diagrams are best suited for tasks that require close integration between the network topology and a few attributes. Adjacency matrices perform well for tasks related to clusters and when many attributes need to be considered. We also reflect on our method of using validated designs for empirically evaluating complex, interactive visualizations in a crowdsourced setting. We highlight the importance of training, compensation, and provenance tracking.
Improving Worker Engagement Through Conversational Microtask Crowdsourcing.	Sihang Qiu, Ujwal Gadiraju, Alessandro Bozzon	chi2020	The rise in popularity of conversational agents has enabled humans to interact with machines more naturally. Recent work has shown that crowd workers in microtask marketplaces can complete a variety of human intelligence tasks (HITs) using conversational interfaces with similar output quality compared to the traditional Web interfaces. In this paper, we investigate the effectiveness of using conversational interfaces to improve worker engagement in microtask crowdsourcing. We designed a text-based conversational agent that assists workers in task execution, and tested the performance of workers when interacting with agents having different conversational styles. We conducted a rigorous experimental study on Amazon Mechanical Turk with 800 unique workers, to explore whether the output quality, worker engagement and the perceived cognitive load of workers can be affected by the conversational agent and its conversational styles. Our results show that conversational interfaces can be effective in engaging workers, and a suitable conversational style has potential to improve worker engagement.
Crowdsourcing the Perception of Machine Teaching.	Jonggi Hong, Kyungjun Lee, June Xu, Hernisa Kacorri	chi2020	Teachable interfaces can empower end-users to attune machine learning systems to their idiosyncratic characteristics and environment by explicitly providing pertinent training examples. While facilitating control, their effectiveness can be hindered by the lack of expertise or misconceptions. We investigate how users may conceptualize, experience, and reflect on their engagement in machine teaching by deploying a mobile teachable testbed in Amazon Mechanical Turk. Using a performance-based payment scheme, Mechanical Turkers (N=100) are called to train, test, and re-train a robust recognition model in real-time with a few snapshots taken in their environment. We find that participants incorporate diversity in their examples drawing from parallels to how humans recognize objects independent of size, viewpoint, location, and illumination. Many of their misconceptions relate to consistency and model capabilities for reasoning. With limited variation and edge cases in testing, the majority of them do not change strategies on a second training attempt.
Crowdsourcing in China: Exploring the Work Experiences of Solo Crowdworkers and Crowdfarm Workers.	Yihong Wang, Konstantinos Papangelis, Michael Saker, Ioanna Lykourentzou, Alan Chamberlain, Vassilis-Javed Khan	chi2020	Recent research highlights the potential of crowdsourcing in China. Yet very few studies explore the workplace context and experiences of Chinese crowdworkers. Those that do, focus mainly on the work experiences of solo crowdworkers but do not deal with issues pertaining to the substantial amount of people working in 'crowdfarms'. This article addresses this gap as one of its primary concerns. Drawing on a study that involves 48 participants, our research explores, compares and contrasts the work experiences of solo crowdworkers to those of crowdfarm workers. Our findings illustrate that the work experiences and context of the solo workers and crowdfarm workers are substantially different, with regards to their motivations, the ways they engage with crowdsourcing, the tasks they work on, and the crowdsourcing platforms they utilize. Overall, our study contributes to furthering the understandings on the work experiences of crowdworkers in China.
Creativity on Paid Crowdsourcing Platforms.	Jonas Oppenlaender, Kristy Milland, Aku Visuri, Panos Ipeirotis, Simo Hosio	chi2020	Crowdsourcing platforms are increasingly being harnessed for creative work. The platforms' potential for creative work is clearly identified, but the workers' perspectives on such work have not been extensively documented. In this paper, we uncover what the workers have to say about creative work on paid crowdsourcing platforms. Through a quantitative and qualitative analysis of a questionnaire launched on two different crowdsourcing platforms, our results revealed clear differences between the workers on the platforms in both preferences and prior experience with creative work. We identify common pitfalls with creative work on crowdsourcing platforms, provide recommendations for requesters of creative work, and discuss the meaning of our findings within the broader scope of creativity-oriented research. To the best of our knowledge, we contribute the first extensive worker-oriented study of creative work on paid crowdsourcing platforms.
Twitter A11y: A Browser Extension to Make Twitter Images Accessible.	Cole Gleason, Amy Pavel, Emma McCamey, Christina Low, Patrick Carrington, Kris M. Kitani, Jeffrey P. Bigham	chi2020	Social media platforms are integral to public and private discourse, but are becoming less accessible to people with vision impairments due to an increase in user-posted images. Some platforms (i.e. Twitter) let users add image descriptions (alternative text), but only 0.1% of images include these. To address this accessibility barrier, we created Twitter A11y, a browser extension to add alternative text on Twitter using six methods. For example, screenshots of text are common, so we detect textual images, and create alternative text using optical character recognition. Twitter A11y also leverages services to automatically generate alternative text or reuse them from across the web. We compare the coverage and quality of Twitter A11y's six alt-text strategies by evaluating the timelines of 50 self-identified blind Twitter users. We find that Twitter A11y increases alt-text coverage from 7.6% to 78.5%, before crowdsourcing descriptions for the remaining images. We estimate that 57.5% of returned descriptions are high-quality. We then report on the experiences of 10 participants with visual impairments using the tool during a week-long deployment. Twitter A11y increases access to social media platforms for people with visual impairments by providing high-quality automatic descriptions for user-posted images.
Understanding Privacy-Related Questions on Stack Overflow.	Mohammad Tahaei, Kami Vaniea, Naomi Saphra	chi2020	"We analyse Stack Overflow (SO) to understand challenges and confusions developers face while dealing with privacy-related topics. We apply topic modelling techniques to 1,733 privacy-related questions to identify topics and then qualitatively analyse a random sample of 315 privacy-related questions. Identified topics include privacy policies, privacy concerns, access control, and version changes. Results show that developers do ask SO for support on privacy-related issues. We also find that platforms such as Apple and Google are defining privacy requirements for developers by specifying what ""sensitive"" information is and what types of information developers need to communicate to users (e.g. privacy policies). We also examine the accepted answers in our sample and find that 28% of them link to official documentation and more than half are answered by SO users without references to any external resources."
Choice of Voices: A Large-Scale Evaluation of Text-to-Speech Voice Quality for Long-Form Content.	Julia Cambre, Jessica Colnago, Jim Maddock, Janice Y. Tsai, Jofish Kaye	chi2020	The advancement of text-to-speech (TTS) voices and a rise of commercial TTS platforms allow people to easily experience TTS voices across a variety of technologies, applications, and form factors. As such, we evaluated TTS voices for long-form content: not individual words or sentences, but voices that are pleasant to listen to for several minutes at a time. We introduce a method using a crowdsourcing platform and an online survey to evaluate voices based on listening experience, perception of clarity and quality, and comprehension. We evaluated 18 TTS voices, three human voices, and a text-only control condition. We found that TTS voices are close to rivaling human voices, yet no single voice outperforms the others across all evaluation dimensions. We conclude with considerations for selecting text-to-speech voices for long-form content.
TurkEyes: A Web-Based Toolbox for Crowdsourcing Attention Data.	Anelise Newman, Barry A. McNamara, Camilo Fosco, Yun Bin Zhang, Pat Sukhum, Matthew Tancik, Nam Wook Kim, Zoya Bylinskii	chi2020	"Eye movements provide insight into what parts of an image a viewer finds most salient, interesting, or relevant to the task at hand. Unfortunately, eye tracking data, a commonly-used proxy for attention, is cumbersome to collect. Here we explore an alternative: a comprehensive web-based toolbox for crowdsourcing visual attention. We draw from four main classes of attention-capturing methodologies in the literature. ZoomMaps is a novel zoom-based interface that captures viewing on a mobile phone. CodeCharts is a self-reporting methodology that records points of interest at precise viewing durations. ImportAnnots is an ""annotation"" tool for selecting important image regions, and cursor-based BubbleView lets viewers click to deblur a small area. We compare these methodologies using a common analysis framework in order to develop appropriate use cases for each interface. This toolbox and our analyses provide a blueprint for how to gather attention data at scale without an eye tracker."
Investigating Opportunities for Crowdsourcing in Church-Based Health Interventions: A Participatory Design Study.	Elizabeth Stowell, Teresa K. O'Leary, Everlyne Kimani, Michael K. Paasche-Orlow, Timothy W. Bickmore, Andrea G. Parker	chi2020	Churches play a major role in providing social support to address health inequities within Black communities, in part by connecting members to key organizations and services. While public health has a history of disseminating interventions in faith communities, little work has explored the use of crowdsourcing to tailor interventions to the unique culture of each church community. Following Community Based Participatory Research principles, we partnered with two predominantly Black churches, and report on a series of three participatory design sessions with nine participants. We developed a novel storyboarding method to explore how crowdsourcing could promote health in these faith-based communities. Our findings characterize existing supports within the church community, and how church social structures impact member access to these supports. We further identify motivations to engage with a church-situated health application, and how these motivations translate to crowdsourcing tasks. Finally, we discuss considerations for public health crowdsourcing tasks.
Completing a Crowdsourcing Task Instead of an Assignment; What do University Students Think?	Vassilis-Javed Khan, Konstantinos Papangelis, Panos Markopoulos	chi2020a	University educators actively seek realistic projects to include in their educational activities. However, finding an actually realistic project is not trivial. The rise of crowdsourcing platforms, in which a variety of tasks are offered in the form of an open call, might be an alternative source to help educators scale up project- based learning. But how do university students feel about executing crowdsourcing tasks instead of their typical assignments? In a study with 24 industrial design students we investigate students' attitudes on introducing crowdsourcing tasks as assignments. Based on our study we offer four suggestions to universities that consider integrating crowdsourcing tasks in their educational activities.
Who Is Changing Your Question on a Social Q&A Website?	Si Chen, Haocong Cheng, Yun Huang	chi2020a	Effective moderation of online communities is an important, but challenging, topic in HCI. In this paper, we study people's co-editing behavior on one of the most popular social Q&A websites in China, called Zhihu.com. We examined question logs to understand who/when/how a question is edited differently by multiple users; we also conducted semi-structured interviews with users who edited others' questions on Zhihu to understand their motivations and their perceptions of co-editing behavior, as well as their concerns and suggestions for future website designs for moderating such behavior. Our findings reveal that although co-editing questions is perceived as a positive and effective approach for improving questions' answerability and shaping norms in the online community, effective moderation mechanisms need to be designed to improve transparency and communication about co-editing behavior and to address possible tensions as a result of co-editing wars.
Urban Immersion: A Web-based Crowdsourcing Platform for Collecting Urban Space Perception Data.	Qianhui Liang, Meijie Wang, Takehiko Nagakura	chi2020a	We introduce Urban Immersion, a web-based platform for collecting crowdsourcing immersive perception data of urban space. Current research based on crowdsourcing data mainly utilize 2D representation tools, and it has limitations in the studies involving spatial features. Thus, in our design and implementation of the platform, which aims to help architects, urban researchers, or people in spatial management to understand their users' preferences, we incorporate webVR and 360-video techniques to display both realistic and abstract representation of the urban environment. We chose the city Shanghai for the first round of the experiment to test our crowdsourcing platform. We first did internal testing for the prototyping of the platform and then published it in social media and invited 771 people to participate in rating their perception preference. We did a rating analysis and visual mapping of the 5735 valid data we collected. We evaluated this round of crowdsourcing data collection on Urban Immersion platform. We checked the effectiveness of applying 3D representation techniques to crowdsourcing platforms and proposed how we could improve the platform in future work.
Designing a Crowdsourcing System for the Elderly: A Gamified Approach to Speech Collection.	Eunjin Seong, Seungjun Kim	chi2020a	Despite the need for representation of older adults in crowdsourced data, crowd work is generally not designed for older adults and participation by older adults is low. In this paper, we demonstrate a process for designing crowd work for older adults; identifying their needs, designing an approach to foster their participation, and verifying its effectiveness. We found when older people feel connected to others while doing crowd work, they are highly motivated. Furthermore, gamification is an effective tool for fostering their engagement when aligned with their needs and values, as opposed to the needs and values of younger participants. Lastly, we suggest important considerations and opportunities for designing crowd work approaches for senior citizens.
Crowdsourcing Virtual Reality Experiments using VRChat.	David Saffo, Caglar Yildirim, Sara Di Bartolomeo, Cody Dunne	chi2020a	Research involving Virtual Reality (VR) headsets is becoming more and more popular. However, scaling VR experiments is challenging as researchers are often limited to using one or a small number of headsets for in-lab studies. One general way to scale experiments is through crowdsourcing so as to have access to a large pool of diverse participants with relatively little expense of time and money. Unfortunately, there is no easy way to crowdsource VR experiments. We demonstrate that it is possible to implement and run crowdsourced VR experiments using a pre-existing massively multiplayer online VR social platform - VRChat. Our small (n=10) demonstration experiment required participants to navigate a maze in VR. Participants searched for two targets then returned to the exit while we captured completion time and position over time. While there are some limitations with using VRChat, overall we have demonstrated a promising approach for running crowdsourced VR experiments.
Project Sidewalk: A Web-based Crowdsourcing Tool for Collecting Sidewalk Accessibility Data At Scale.	Manaswi Saha, Michael Saugstad, Hanuma Teja Maddali, Aileen Zeng, Ryan Holland, Steven Bower, Aditya Dash, Sage Chen, Anthony Li, Kotaro Hara, Jon Froehlich	chi2019	We introduce Project Sidewalk, a new web-based tool that enables online crowdworkers to remotely label pedestrian-related accessibility problems by virtually walking through city streets in Google Street View. To train, engage, and sustain users, we apply basic game design principles such as interactive onboarding, mission-based tasks, and progress dashboards. In an 18-month deployment study, 797 online users contributed 205,385 labels and audited 2,941 miles of Washington DC streets. We compare behavioral and labeling quality differences between paid crowdworkers and volunteers, investigate the effects of label type, label severity, and majority vote on accuracy, and analyze common labeling errors. To complement these findings, we report on an interview study with three key stakeholder groups (N=14) soliciting reactions to our tool and methods. Our findings demonstrate the potential of virtually auditing urban accessibility and highlight tradeoffs between scalability and quality compared to traditional approaches.
HistoryTracker: Minimizing Human Interactions in Baseball Game Annotation.	Jorge Piazentin Ono, Arvi Gjoka, Justin Salamon, Carlos A. Dietrich, Cláudio T. Silva	chi2019	The sport data tracking systems available today are based on specialized hardware (high-definition cameras, speed radars, RFID) to detect and track targets on the field. While effective, implementing and maintaining these systems pose a number of challenges, including high cost and need for close human monitoring. On the other hand, the sports analytics community has been exploring human computation and crowdsourcing in order to produce tracking data that is trustworthy, cheaper and more accessible. However, state-of-the-art methods require a large number of users to perform the annotation, or put too much burden into a single user. We propose HistoryTracker, a methodology that facilitates the creation of tracking data for baseball games by warm-starting the annotation process using a vast collection of historical data. We show that HistoryTracker helps users to produce tracking data in a fast and reliable way.
Modeling Mobile Interface Tappability Using Crowdsourcing and Deep Learning.	Amanda Swearngin, Yang Li	chi2019	Tapping is an immensely important gesture in mobile touchscreen interfaces, yet people still frequently are required to learn which elements are tappable through trial and error. Predicting human behavior for this everyday gesture can help mobile app designers understand an important aspect of the usability of their apps without having to run a user study. In this paper, we present an approach for modeling tappability of mobile interfaces at scale. We conducted large-scale data collection of interface tappability over a rich set of mobile apps using crowdsourcing and computationally investigated a variety of signifiers that people use to distinguish tappable versus not-tappable elements. Based on the dataset, we developed and trained a deep neural network that predicts how likely a user will perceive an interface element as tappable versus not tappable. Using the trained tappability model, we developed TapShoe, a tool that automatically diagnoses mismatches between the tappability of each element as perceived by a human user---predicted by our model, and the intended or actual tappable state of the element specified by the developer or designer. Our model achieved reasonable accuracy: mean precision 90.2% and recall 87.0%, in matching human perception on identifying tappable UI elements. The tappability model and TapShoe were well received by designers via an informal evaluation with 7 professional interaction designers.
ReCall: Crowdsourcing on Basic Phones to Financially Sustain Voice Forums.	Aditya Vashistha, Abhinav Garg, Richard J. Anderson	chi2019	Although voice forums are widely used to enable marginalized communities to produce, consume, and share information, their financial sustainability is a key concern among HCI4D researchers and practitioners. We present ReCall, a crowdsourcing marketplace accessible via phone calls where low-income rural residents vocally transcribe audio files to gain free airtime to participate in voice forums as well as to earn money. We conducted a series of experimental and usability evaluations with 28 low-income people in rural India to examine the effect of phone types, channel types, and review modes on speech transcription performance. We then deployed ReCall for two weeks to 24 low-income rural residents who placed 5,879 phone calls, completed 29,000 micro tasks to yield transcriptions with 85% accuracy, and earned INR 20,500. Our mixed-methods analysis indicates that each minute of crowd work on ReCall gives users eight minutes of free airtime on another voice forum, and thus illustrates a way to address the financial sustainability of voice forums.
Talking about Chat at Work in the Global South: An Ethnographic Study of Chat Use in India and Kenya.	Moira McGregor, Nicola J. Bidwell, Vidya Sarangapani, Jonathan Appavoo, Jacki O'Neill	chi2019	In this paper, we examine how two chat apps fit into the communication ecosystem of six large distributed enterprises, in India and Kenya. From the perspective of management, these chat apps promised to foster greater communication and awareness between workers in the field, and between fieldworkers and the enterprises administration and management centres. Each organisation had multiple different types of chat groups, characterised by the types of content and interaction patterns they mediate, and the different organisational functions they fulfil. Examining the interplay between chat and existing local practices for coordination, collaboration and knowledge-sharing, we discuss how chat manifests in the distributed workplace and how it fits -- or otherwise -- alongside the rhythms of both local and remote work. We contribute to understandings of chat apps for workplace communication and provide insights for shaping their ongoing development.
Crowdsourcing Interface Feature Design with Bayesian Optimization.	John J. Dudley, Jason T. Jacques, Per Ola Kristensson	chi2019	Designing novel interfaces is challenging. Designers typically rely on experience or subjective judgment in the absence of analytical or objective means for selecting interface parameters. We demonstrate Bayesian optimization as an efficient tool for objective interface feature refinement. Specifically, we show that crowdsourcing paired with Bayesian optimization can rapidly and effectively assist interface design across diverse deployment environments. Experiment 1 evaluates the approach on a familiar 2D interface design problem: a map search and review use case. Adding a degree of complexity, Experiment 2 extends Experiment 1 by switching the deployment environment to mobile-based virtual reality. The approach is then demonstrated as a case study for a fundamentally new and unfamiliar interaction design problem: web-based augmented reality. Finally, we show how the model generated as an outcome of the refinement process can be used for user simulation and queried to deliver various design insights.
Privacy, Power, and Invisible Labor on Amazon Mechanical Turk.	Shruti Sannon, Dan Cosley	chi2019	Tasks on crowdsourcing platforms such as Amazon Mechanical Turk often request workers' personal information, raising privacy risks that may be exacerbated by requester-worker power dynamics. We interviewed 14 workers to understand how they navigate these risks. We found that Turkers' decisions to provide personal information during tasks were based on evaluations of the pay rate, the requester, the purpose, and the perceived sensitivity of the request. Participants also engaged in multiple privacy-protective behaviors, such as abandoning tasks or providing inaccurate data, though there were costs associated with these behaviors, such as wasted time and risk of rejection. Finally, their privacy concerns and practices evolved as they learned about both the platform and worker-designed tools and forums. These findings deepen our understanding of both privacy decision-making and invisible labor in paid crowdsourcing, and emphasize a general need to understand how privacy stances change over time.
Crowdsourcing Multi-label Audio Annotation Tasks with Citizen Scientists.	Mark Cartwright, Graham Dove, Ana Elisa Méndez Méndez, Juan Pablo Bello, Oded Nov	chi2019	Annotating rich audio data is an essential aspect of training and evaluating machine listening systems. We approach this task in the context of temporally-complex urban soundscapes, which require multiple labels to identify overlapping sound sources. Typically this work is crowdsourced, and previous studies have shown that workers can quickly label audio with binary annotation for single classes. However, this approach can be difficult to scale when multiple passes with different focus classes are required to annotate data with multiple labels. In citizen science, where tasks are often image-based, annotation efforts typically label multiple classes simultaneously in a single pass. This paper describes our data collection on the Zooniverse citizen science platform, comparing the efficiencies of different audio annotation strategies. We compared multiple-pass binary annotation, single-pass multi-label annotation, and a hybrid approach: hierarchical multi-pass multi-label annotation. We discuss our findings, which support using multi-label annotation, with reference to volunteer citizen scientists' motivations.
"Beyond ""One-Size-Fits-All"": Understanding the Diversity in How Software Newcomers Discover and Make Use of Help Resources."	Kimia Kiani, George Cui, Andrea Bunt, Joanna McGrenere, Parmit K. Chilana	chi2019	"For most modern feature-rich software, considerable external help and learning resources are available on the web (e.g., documentation, tutorials, videos, Q&A forums). But, how do users new to an application discover and make use of such resources? We conducted in-lab and diary studies with 26 software newcomers from a variety of different backgrounds who were all using Fusion 360, a 3D modeling application, for the first time. Our results illustrate newcomers' diverse needs, perceptions, and help-seeking behaviors. We found a number of distinctions in how technical and non-technical users approached help-seeking, including: when and how they initiated the help-seeking process, their struggles in recognizing relevant help, the degree to which they made coordinated use of the application and different resources, and in how they perceived the utility of different help formats. We discuss implications for moving beyond ""one-size-fits-all"" help resources towards more structured, personalized, and curated help and learning materials."
Exploring Crowdsourced Work in Low-Resource Settings.	Manu Chopra, Indrani Medhi-Thies, Joyojeet Pal, Colin Scott, William Thies, Vivek Seshadri	chi2019	While researchers have studied the benefits and hazards of crowdsourcing for diverse classes of workers, most work has focused on those having high familiarity with both computers and English. We explore whether paid crowdsourcing can be inclusive of individuals in rural India, who are relatively new to digital devices and literate mainly in local languages. We built an Android application to measure the accuracy with which participants can digitize handwritten Marathi/Hindi words. The tasks were based on the real-world need for digitizing handwritten Devanagari script documents. Results from a two-week, mixed-methods study show that participants achieved 96.7% accuracy in digitizing handwritten words on low-end smartphones. A crowdsourcing platform that employs these users performs comparably to a professional transcription firm. Participants showed overwhelming enthusiasm for completing tasks, so much so that we recommend imposing limits to prevent overuse of the application. We discuss the implications of these results for crowdsourcing in low-resource areas.
Mapping the Landscape of Creativity Support Tools in HCI.	Jonas Frich, Lindsay MacDonald Vermeulen, Christian Remy, Michael Mose Biskjaer, Peter Dalsgaard	chi2019	Creativity Support Tools (CSTs) play a fundamental role in the study of creativity in Human-Computer Interaction (HCI). Even so, there is no consensus definition of the term 'CST' in HCI, and in most studies, CSTs have been construed as one-off exploratory prototypes, typically built by the researchers themselves. This makes it difficult to clearly demarcate CST research, but also to compare findings across studies, which impedes advancement in digital creativity as a growing field of research. Based on a literature review of 143 papers from the ACM Digital Library (1999-2018), we contribute a first overview of the key characteristics of CSTs developed by the HCI community. Moreover, we propose a tentative definition of a CST to help strengthen knowledge sharing across CST studies. We end by discussing our study's implications for future HCI research on CSTs and digital creativity.
Crowdworker Economics in the Gig Economy.	Jason T. Jacques, Per Ola Kristensson	chi2019	The nature of work is changing. As labor increasingly trends to casual work in the emerging gig economy, understanding the broader economic context is crucial to effective engagement with a contingent workforce. Crowdsourcing represents an early manifestation of this fluid, laisser-faire, on-demand workforce. This work analyzes the results of four large-scale surveys of US-based Amazon Mechanical Turk workers recorded over a six-year period, providing comparable measures to national statistics. Our results show that despite unemployment far higher than national levels, crowdworkers are seeing positive shifts in employment status and household income. Our most recent surveys indicate a trend away from full-time-equivalent crowdwork, coupled with a reduction in estimated poverty levels to below national figures. These trends are indicative of an increasingly flexible workforce, able to maximize their opportunities in a rapidly changing national labor market, which may have material impacts on existing models of crowdworker behavior.
Rehumanized Crowdsourcing: A Labeling Framework Addressing Bias and Ethics in Machine Learning.	Natã Miccael Barbosa, Monchu Chen	chi2019	The increased use of machine learning in recent years led to large volumes of data being manually labeled via crowdsourcing microtasks completed by humans. This brought about dehumanization effects, namely, when task requesters overlook the humans behind the task, leading to issues of ethics (e.g., unfair payment) and amplification of human biases, which are transferred into training data and affect machine learning in the real world. We propose a framework that allocates microtasks considering human factors of workers such as demographics and compensation. We deployed our framework to a popular crowdsourcing platform and conducted experiments with 1,919 workers collecting 160,345 human judgments. By routing microtasks to workers based on demographics and appropriate pay, our framework mitigates biases in the contributor sample and increases the hourly pay given to contributors. We discuss potential extensions and how it can promote transparency in crowdsourcing.
Career Mentoring in Online Communities: Seeking and Receiving Advice from an Online Community.	Maria Tomprou, Laura Dabbish, Robert E. Kraut, Fannie Liu	chi2019	Although people frequently seek mentoring or advice for their career, most mentoring is performed in person. Little research has examined the nature and quality of career mentoring online. To address this gap, we study how people use online Q&A forums for career advice. We develop a taxonomy of career advice requests based on a qualitative analysis of posts in a career-related online forum, identifying three key types: best practices, career threats, and time-sensitive requests. Our quantitative analysis of responses shows that both requesters and external viewers value general information, encouragement, and guidance, but not role modeling. We found no relation between the type of requests and features of responses, nor differences in responses valued by requesters versus external viewers. We present design recommendations for supporting online career advice exchange.
PALS: Patching ALS through Crowdsourced Advice, Social Links & Public Awareness.	Richard Cave, Karina Kocemba, Svjetlana Dajic, Holloe Bostock, Alastair Cook	chi2019a	Amyotrophic Lateral Sclerosis (ALS) is a serious and poorly understood disease, impacting 50,000 people a year globally. Our research found that people with ALS express a lack of connection with other people with the disease, and that the general public lacks awareness about ALS. We also identified an engagement problem with the currently available resources to connect and support people with ALS. To address these issues, we introduce 'PALS' - an accessible crowdsourcing and connection quilt, first hung like a tapestry in the ALS clinic, then later used as an interactive public display. The quilt offers the opportunity to access crowdsourced information concerning individual experiences of ALS. Our work offers three primary contributions: 1) adding to limited HCI research concerning the ALS community by establishing the needs, 2) applying the 'PALS' quilt design solution to these needs, and 3) combining three modalities: crowdsourcing, tangible tapestry displays, and interactive waiting education in a unique way.
Machine-Crowd-Expert Model for Increasing User Engagement and Annotation Quality.	Ana Elisa Méndez Méndez, Mark Cartwright, Juan Pablo Bello	chi2019a	Crowdsourcing and active learning have been combined in the past with the goal of reducing annotation costs. However, two issues arise with using AL and crowdsourcing: quality of the labels and user engagement. In this work, we propose a novel machine -> crowd <- expert loop model where the forward connections of the loop aim to improve the quality of the labels and the backward connections aim to increase user engagement. In addition, we propose a research agenda for evaluating the model.
Crowdsourcing Perspectives on Public Policy from Stakeholders.	Hyunwoo Kim, Eun-Young Ko, Donghoon Han, Sung-Chul Lee, Simon T. Perrault, Jihee Kim, Juho Kim	chi2019a	Personal deliberation, the process through which people can form an informed opinion on social issues, serves an important role in helping citizens construct a rational argument in the public deliberation. However, existing information channels for public policies deliver only few stakeholders' voices, thus failing to provide a diverse knowledge base for personal deliberation. This paper presents an initial design of PolicyScape, an online system that supports personal deliberation on public policies by helping citizens explore diverse stakeholders and their perspectives on the policy's effect. Building on literature on crowdsourced policymaking and policy stakeholders, we present several design choices for crowdsourcing stakeholder perspectives. We introduce perspective-taking as an approach for personal deliberation by helping users consider stakeholder perspectives on policy issues. Our initial results suggest that PolicyScape could collect diverse sets of perspectives from the stakeholders of public policies, and help participants discover unexpected viewpoints of various stakeholder groups.
A Gaze-Based Exploratory Study on the Information Seeking Behavior of Developers on Stack Overflow.	Cole S. Peterson, Jonathan A. Saddler, Natalie M. Halavick, Bonita Sharif	chi2019a	Software developers use Stack Overflow on a daily basis to search for solutions to problems they encounter during bug fixing and feature enhancement. In prior work, studies have been done on mining Stack Overflow data such as for predicting unanswered questions or how and why people post. However, no work exists on how developers actually use, or more importantly, read the information presented to them on Stack Overflow. To better understand this behavior, we conduct an eye tracking study on how developers seek for information on Stack Overflow while tasked with creating human-readable summaries of methods and classes in large Java projects. Eye gaze data is collected on both the source code elements and Stack Overflow document elements at a fine token-level granularity using iTrace, our eye tracking infrastructure. We found that developers look at the text more often than the title in posts. Code snippets were the second most looked at element. Tags and votes are rarely looked at. When switching between Stack Overflow and the Eclipse Integrated Development Environment (IDE), developers often looked at method signatures and then switched to code and text elements on Stack Overflow. Such heuristics provide insight to automated code summarization tools as they decide what to give more weight to while generating summaries.
Estimating confidence in voices using crowdsourcing for alleviating tension with altered auditory feedback.	Kana Naruse, Shinnosuke Takamichi, Tomohiro Tanikawa, Shigeo Yoshida, Takuji Narumi, Michitaka Hirose	asianchi2019	People tend to face difficulties while speaking, owing to excessive tension. To relieve this tension, we propose a method to alter their voice such that it appears more confident and feed this voice back to the speaker in real time. As determining the appropriate parameters for voice processing is difficult, we gathered data on the perception of confidence in voices through crowdsourcing and constructed a model for estimating confidence scores from voice-processing parameters. An analysis of the model showed that although the coefficient of determination was not considerably large, the inflection of speech tended to affect the perception of confidence.
Indonesian hospital knowledge management technology characteristics.	Yohannes Kurniawan, Natalia Limantara, Fredy Jingga	asianchi2019	The critical steps of knowledge sharing process in hospital is to change tacit knowledge into explicit knowledge (externalization) and use that explicit knowledge to improve tacit knowledge (internalization). As we know technology in the medical field is growing rapidly. Therefore, hospitals will be successful if they consistently create new knowledge and disseminate it to all of stakeholders in their organizations and quickly adopt their latest technologies and services, especially in the medical field.
Predicting Human Performance in Vertical Menu Selection Using Deep Learning.	Yang Li, Samy Bengio, Gilles Bailly	chi2018	Predicting human performance in interaction tasks allows designers or developers to understand the expected performance of a target interface without actually testing it with real users. In this work, we present a deep neural net to model and predict human performance in performing a sequence of UI tasks. In particular, we focus on a dominant class of tasks, i.e., target selection from a vertical list or menu. We experimented with our deep neural net using a public dataset collected from a desktop laboratory environment and a dataset collected from hundreds of touchscreen smartphone users via crowdsourcing. Our model significantly outperformed previous methods on these datasets. Importantly, our method, as a deep model, can easily incorporate additional UI attributes such as visual appearance and content semantics without changing model architectures. By understanding about how a deep learning model learns from human behaviors, our approach can be seen as a vehicle to discover new patterns about human behaviors to advance analytical modeling.
BSpeak: An Accessible Voice-based Crowdsourcing Marketplace for Low-Income Blind People.	Aditya Vashistha, Pooja Sethi, Richard J. Anderson	chi2018	BSpeak is an accessible crowdsourcing marketplace that enables blind people in developing regions to earn money by transcribing audio files through speech. We examine accessibility and usability barriers that 15 first-time users, who are low-income and blind, experienced while completing transcription tasks on BSpeak and Mechanical Turk (MTurk). Our mixed-methods analysis revealed severe accessibility barriers in MTurk due to the absence of landmarks, unlabeled UI elements, and improper use of HTML headings. Compared to MTurk, participants found BSpeak significantly more accessible and usable, and completed tasks with higher accuracy in lesser time due to its voice-based implementation. In a two-week field deployment of BSpeak in India, 24 low-income blind users earned rupee 7,310 by completing over 16,000 transcription tasks to yield transcriptions with 87% accuracy. Through our analysis of BSpeak's strengths and weaknesses, we provide recommendations for designing crowdsourcing marketplaces for low-income blind people in resource-constrained settings.
Crowdsourcing Rural Network Maintenance and Repair via Network Messaging.	Esther Han Beol Jang, Mary Claire Barela, Matthew Johnson, Philip A. Martinez, Cedric Festin, Margaret T. Lynn, Josephine Dionisio, Kurtis Heimerl	chi2018	Repair and maintenance requirements limit the successful operation of rural infrastructure. Current best practices are centralized management, which requires travel from urban areas and is prohibitively expensive, or intensively training community members, which limits scaling. We explore an alternative model: crowdsourcing repair from the community. Leveraging a Community Cellular Network in the remote Philippines, we sent SMS to all active network subscribers (n = 63) requesting technical support. From the pool of physical respondents, we explored their ability to repair through mock failures and conducted semi-structured interviews about their experiences with repair. We learned that community members would be eager to practice repair if allowed, would network to recruit more expertise, and seemingly have the collective capacity to resolve some common failures. They are most successful when repairs map directly to their lived experiences. We suggest infrastructure design considerations that could make repairs more tractable and argue for an inclusive approach.
Hit-or-Wait: Coordinating Opportunistic Low-effort Contributions to Achieve Global Outcomes in On-the-go Crowdsourcing.	Yongsung Kim, Darren Gergle, Haoqi Zhang	chi2018	"We consider the challenge of motivating and coordinating large numbers of people to contribute to solving local, communal problems through their existing routines. In order to design such ""on-the-go crowdsourcing"" systems, there is a need for mechanisms that can effectively coordinate contributions to address problem solving needs in the physical world while leveraging people's existing mobility with minimal disruption. We thus introduce Hit-or-Wait, a general decision-theoretic mechanism that intelligently controls decisions over when to notify a person of a task, in ways that reason both about system needs across tasks and about a helper's changing patterns of mobility. Through simulations and a field study in the context of community-based lost-and-found, we demonstrate that using Hit-or-Wait enables a system to make efficient use of people's contributions with minimal disruptions to their routines without the need for explicit coordination. Interviews with field study participants further suggest that highlighting an individual's contribution to the global goal may help people value their contributions more."
Crowd-Guided Ensembles: How Can We Choreograph Crowd Workers for Video Segmentation?	Alexandre Kaspar, Genevieve Patterson, Changil Kim, Yagiz Aksoy, Wojciech Matusik, Mohamed A. Elgharib	chi2018	In this work, we propose two ensemble methods leveraging a crowd workforce to improve video annotation, with a focus on video object segmentation. Their shared principle is that while individual candidate results may likely be insufficient, they often complement each other so that they can be combined into something better than any of the individual results---the very spirit of collaborative working. For one, we extend a standard polygon-drawing interface to allow workers to annotate negative space, and combine the work of multiple workers instead of relying on a single best one as commonly done in crowdsourced image segmentation. For the other, we present a method to combine multiple automatic propagation algorithms with the help of the crowd. Such combination requires an understanding of where the algorithms fail, which we gather using a novel coarse scribble video annotation task. We evaluate our ensemble methods, discuss our design choices for them, and make our web-based crowdsourcing tools and results publicly available.
Inferring Loop Invariants through Gamification.	Dimitar Bounov, Anthony DeRossi, Massimiliano Menarini, William G. Griswold, Sorin Lerner	chi2018	In today's modern world, bugs in software systems incur significant costs. One promising approach to improve software quality is automated software verification. In this approach, an automated tool tries to prove the software correct once and for all. Although significant progress has been made in this direction, there are still many cases where automated tools fail. We focus specifically on one aspect of software verification that has been notoriously hard to automate: inferring loop invariants that are strong enough to enable verification. In this paper, we propose a solution to this problem through gamification and crowdsourcing. In particular, we present a puzzle game where players find loop invariants without being aware of it, and without requiring any expertise on software verification. We show through an experiment with Mechanical Turk users that players enjoy the game, and are able to solve verification tasks that automated state-of-the-art tools cannot.
CrowdLayout: Crowdsourced Design and Evaluation of Biological Network Visualizations.	Divit P. Singh, Lee Lisle, T. M. Murali, Kurt Luther	chi2018	Biologists often perform experiments whose results generate large quantities of data, such as interactions between molecules in a cell, that are best represented as networks (graphs). To visualize these networks and communicate them in publications, biologists must manually position the nodes and edges of each network to reflect their real-world physical structure. This process does not scale well, and graph layout algorithms lack the biological underpinnings to offer a viable alternative. In this paper, we present CrowdLayout, a crowdsourcing system that leverages human intelligence and creativity to design layouts of biological network visualizations. CrowdLayout provides design guidelines, abstractions, and editing tools to help novice workers perform like experts. We evaluated CrowdLayout in two experiments with paid crowd workers and real biological network data, finding that crowds could both create and evaluate meaningful, high-quality layouts. We also discuss implications for crowdsourced design and network visualizations in other domains.
Crowdsourcing Treatments for Low Back Pain.	Simo Johannes Hosio, Jaro Karppinen, Esa-Pekka Takala, Jani Takatalo, Jorge Gonçalves, Niels van Berkel, Shin'ichi Konomi, Vassilis Kostakos	chi2018	Low back pain (LBP) is a globally common condition with no silver bullet solutions. Further, the lack of therapeutic consensus causes challenges in choosing suitable solutions to try. In this work, we crowdsourced knowledge bases on LBP treatments. The knowledge bases were used to rank and offer best-matching LBP treatments to end users. We collected two knowledge bases: one from clinical professionals and one from non-professionals. Our quantitative analysis revealed that non-professional end users perceived the best treatments by both groups as equally good. However, the worst treatments by non-professionals were clearly seen as inferior to the lowest ranking treatments by professionals. Certain treatments by professionals were also perceived significantly differently by non-professionals and professionals themselves. Professionals found our system handy for self-reflection and for educating new patients, while non-professionals appreciated the reliable decision support that also respected the non-professional opinion.
Crowdsourcing vs Laboratory-Style Social Acceptability Studies?: Examining the Social Acceptability of Spatial User Interactions for Head-Worn Displays.	Fouad Shoie Alallah, Ali Neshati, Nima Sheibani, Yumiko Sakamoto, Andrea Bunt, Pourang Irani, Khalad Hasan	chi2018	The use of crowdsourcing platforms for data collection in HCI research is attractive in their ability to provide rapid access to large and diverse participant samples. As a result, several researchers have conducted studies investigating the similarities and differences between data collected through crowdsourcing and more traditional, laboratory-style data collection. We add to this body of research by examining the feasibility of conducting social acceptability studies via crowdsourcing. Social acceptability can be a key determinant for the early adoption of emerging technologies, and as such, we focus our investigation on social acceptability for Head-Worn Display (HWD) input modalities. Our results indicate that data collected via a crowdsourced experiment and a laboratory-style setting did not differ at a statistically significant level. These results provide initial support for crowdsourcing platforms as viable options for conducting social acceptability research.
Crowdsourcing Exercise Plans Aligned with Expert Guidelines and Everyday Constraints.	Elena Agapie, Bonnie Chinh, Laura R. Pina, Diana Oviedo, Molly C. Welsh, Gary Hsieh, Sean Munson	chi2018	Exercise plans help people implement behavior change. Crowd workers can help create exercise plans for clients, but their work may result in lower quality plans than produced by experts. We built CrowdFit, a tool that provides feedback about compliance with exercise guidelines and leverages strengths of crowdsourcing to create plans made by non-experts. We evaluated CrowdFit in a comparative study with 46 clients using exercise plans for two weeks. Clients received plans from crowd planners using CrowdFit, crowd planners without CrowdFit, or from expert planners. Compared to crowd planners not using CrowdFit, crowd planners using CrowdFit created plans that are more actionable and more aligned with exercise guidelines. Compared to experts, crowd planners created more actionable plans, and plans that are not significantly different with respect to tailoring, strength and aerobic principles. They struggled, however, to satisfy exercise requirements of amount of exercise. We discuss opportunities for designing technology supporting physical activity planning by non-experts.
Examining Wikipedia With a Broader Lens: Quantifying the Value of Wikipedia's Relationships with Other Large-Scale Online Communities.	Nicholas Vincent, Isaac L. Johnson, Brent J. Hecht	chi2018	The extensive Wikipedia literature has largely considered Wikipedia in isolation, outside of the context of its broader Internet ecosystem. Very recent research has demonstrated the significance of this limitation, identifying critical relationships between Google and Wikipedia that are highly relevant to many areas of Wikipedia-based research and practice. This paper extends this recent research beyond search engines to examine Wikipedia's relationships with large-scale online communities, Stack Overflow and Reddit in particular. We find evidence of consequential, albeit unidirectional relationships. Wikipedia provides substantial value to both communities, with Wikipedia content increasing visitation, engagement, and revenue, but we find little evidence that these websites contribute to Wikipedia in return. Overall, these findings highlight important connections between Wikipedia and its broader ecosystem that should be considered by researchers studying Wikipedia. Critically, our results also emphasize the key role that volunteer-created Wikipedia content plays in improving other websites, even contributing to revenue generation.
Identifying Speech Input Errors Through Audio-Only Interaction.	Jonggi Hong, Leah Findlater	chi2018	Speech has become an increasingly common means of text input, from smartphones and smartwatches to voice-based intelligent personal assistants. However, reviewing the recognized text to identify and correct errors is a challenge when no visual feedback is available. In this paper, we first quantify and describe the speech recognition errors that users are prone to miss, and investigate how to better support this error identification task by manipulating pauses between words, speech rate, and speech repetition. To achieve these goals, we conducted a series of four studies. Study 1, an in-lab study, showed that participants missed identifying over 50% of speech recognition errors when listening to audio output of the recognized text. Building on this result, Studies 2 to 4 were conducted using an online crowdsourcing platform and showed that adding a pause between words improves error identification compared to no pause, the ability to identify errors degrades with higher speech rates (300 WPM), and repeating the speech output does not improve error identification. We derive implications for the design of audio-only speech dictation.
Visualizing API Usage Examples at Scale.	Elena L. Glassman, Tianyi Zhang, Björn Hartmann, Miryung Kim	chi2018	Using existing APIs properly is a key challenge in programming, given that libraries and APIs are increasing in number and complexity. Programmers often search for online code examples in Q&A forums and read tutorials and blog posts to learn how to use a given API. However, there are often a massive number of related code examples and it is difficult for a user to understand the commonalities and variances among them, while being able to drill down to concrete details. We introduce an interactive visualization for exploring a large collection of code examples mined from open-source repositories at scale. This visualization summarizes hundreds of code examples in one synthetic code skeleton with statistical distributions for canonicalized statements and structures enclosing an API call. We implemented this interactive visualization for a set of Java APIs and found that, in a lab study, it helped users (1) answer significantly more API usage questions correctly and comprehensively and (2) explore how other programmers have used an unfamiliar API.
"""We Don't Do That Here"": How Collaborative Editing with Mentors Improves Engagement in Social Q&A Communities."	Denae Ford, Kristina Lustig, Jeremy Banks, Chris Parnin	chi2018	Online question-and-answer (Q&A) communities like Stack Overflow have norms that are not obvious to novice users. Novices create and post programming questions without feedback, and the community enforces site norms through public downvoting and commenting. This can leave novices discouraged from further participation. We deployed a month long, just-in-time mentorship program to Stack Overflow in which we redirected novices in the process of asking a question to an on-site Help Room. There, novices received feedback on their question drafts from experienced Stack Overflow mentors. We present examples and discussion of various question improvements including: question context, code formatting, and wording that adheres to on-site cultural norms. We find that mentored questions are substantially improved over non-mentored questions, with average scores increasing by 50%. We provide design implications that challenge how socio-technical communities onboard novices across domains.
Crowd-AI Systems for Non-Visual Information Access in the Real World.	Anhong Guo	chi2018a	The world is full of information, interfaces and environments that are inaccessible to blind people. When navigating indoors, blind people are often unaware of key visual information, such as posters, signs, and exit doors. When accessing specific interfaces, blind people cannot independently do so without at least first learning their layout and labeling them with sighted assistance. My work investigates interactive systems that integrates computer vision, on-demand crowdsourcing, and wearables to amplify the abilities of blind people, offering solutions for real-time environment and interface navigation. My work provides more options for blind people to access information and increases their freedom in navigating the world.
Investigating Crowdsourcing as a Method to Collect Emotion Labels for Images.	Olga Korovina, Fabio Casati, Radoslaw Nielek, Marcos Báez, Olga Berestneva	chi2018a	"Labeling images is essential towards enabling the search and organization of digital media. This is true for both ""factual"", objective tags such as time, place and people, as well as for subjective labels, such as the emotion a picture generates. Indeed, the ability to associate emotions to images is one of the key functionality most image analysis services today strive to provide. In this paper we study how emotion labels for images can be crowdsourced and uncover limitations of the approach commonly used to gather training data today, that of harvesting images and tags from social media."
How Creative is the Crowd in Describing Smart Home Scenarios?	Tahir Abbas, Vassilis-Javed Khan, Daniel Tetteroo, Panos Markopoulos	chi2018a	Internet of Things (IoT) is recently attracting vendors like Google, Homey, and Samsung that have already brought to the market a plethora of devices and services supporting smart home automation. However, recent studies have shown that end-users having little knowledge of the features and possibilities of IoT devices, face difficulties in conjuring up meaningful use scenarios that combine such devices. Therefore, they fail to anticipate useful configurations beyond those provided by vendors and hence missing out on the vast potential of the IoT. We present an on-going investigation that explores the potential of sourcing IoT-relevant scenarios from a popular microtask-crowdsourcing platform, and a preliminary evaluation of such scenarios with respect to their originality and practicality. This work paves the way for the automated leverage of crowdsourced user scenarios to support IoT end-users.
MyriadHub: Efficiently Scaling Personalized Email Conversations with Valet Crowdsourcing.	Nicolas Kokkalis, Chengdiao Fan, Johannes Roith, Michael S. Bernstein, Scott R. Klemmer	chi2017	Email has scaled our ability to communicate with large groups, but has not equivalently scaled our ability to listen and respond. For example, emailing many people for feedback requires either impersonal surveys or manual effort to hold many similar conversations. To scale personalized conversations, we introduce techniques that exploit similarities across conversations to recycle relevant parts of previous conversations. These techniques reduce the authoring burden, save senders' time, and maintain recipient engagement through personalized responses. We introduce MyriadHub, a mail client where users start conversations and then crowd workers extract underlying conversational patterns and rules to accelerate responses to future similar emails. In a within-subjects experiment comparing MyriadHub to existing mass email techniques, senders spent significantly less time planning events with MyriadHub. In a second experiment comparing MyriadHub to a standard email survey, MyriadHub doubled the recipients' response rate and tripled the number of words in their responses.
Crowdsourcing GO: Effect of Worker Situation on Mobile Crowdsourcing Performance.	Kazushi Ikeda, Keiichiro Hoashi	chi2017	"The increasing popularity of mobile crowdsourcing platforms has enabled crowd workers to accept jobs wherever/whenever they are, and also provides opportunity for task requesters to order time/location specific tasks to workers. Since workers on mobile platforms are working on the go, the situation of the workers is expected to influence their performance. However, the effects of mobile worker situations to task performance is an uninvestigated area. In this paper, our research question is, ""do worker situations affect task completion, price and quality on mobile crowdsourcing platforms?"" We draw on economics and psychology research to examine whether worker situations such as busyness, fatigue and presence of companions affect their performance. Our three-week between-subjects field experiment revealed that worker busyness caused 30.1% relative decrease of task completion rate. Mean accepted task price increased by 7.6% when workers are with companions. Worker fatigue caused 37.4% relative decrease of task quality."
Respeak: A Voice-based, Crowd-powered Speech Transcription System.	Aditya Vashistha, Pooja Sethi, Richard J. Anderson	chi2017	Speech transcription is an expensive service with high turnaround time for audio files containing languages spoken in developing countries and regional accents of well-represented languages. We present Respeak - a voice-based, crowd-powered system that capitalizes on the strengths of crowdsourcing and automatic speech recognition (instead of typing) to transcribe such audio files. We created Respeak and optimized its design through a series of cognitive experiments. We deployed it with 25 university students in India who completed 5464 micro-transcription tasks, transcribing 55 minutes of widely-varied audio content, and collectively earning USD 46 as mobile airtime. The Respeak engine aligned the transcript generated by five randomly selected users to transcribe Hindi and Indian English audio files with a word error rate (WER) of 8.6% and 15.2%, respectively. The cost of speech transcription was USD 0.83 per minute with a turnaround time of 39.8 hours, substantially less than industry standards. Using a mixed-methods analysis of cognitive experiments, system performance and qualitative interviews, we evaluate Respeak's design, user experience, strengths, and weaknesses. Our findings suggest that Respeak improves the quality of speech transcription while enhancing the earning potential of low-income populations in resource-constrained settings.
Scalable Annotation of Fine-Grained Categories Without Experts.	Timnit Gebru, Jonathan Krause, Jia Deng, Li Fei-Fei	chi2017	We present a crowdsourcing workflow to collect image annotations for visually similar synthetic categories without requiring experts. In animals, there is a direct link between taxonomy and visual similarity: e.g. a collie (type of dog) looks more similar to other collies (e.g. smooth collie) than a greyhound (another type of dog). However, in synthetic categories such as cars, objects with similar taxonomy can have very different appearance: e.g. a 2011 Ford F-150 Supercrew-HD looks the same as a 2011 Ford F-150 Supercrew-LL but very different from a 2011 Ford F-150 Supercrew-SVT. We introduce a graph based crowdsourcing algorithm to automatically group visually indistinguishable objects together. Using our workflow, we label 712,430 images by ~1,000 Amazon Mechanical Turk workers; resulting in the largest fine-grained visual dataset reported to date with 2,657 categories of cars annotated at 1/20th the cost of hiring experts.
Revolt: Collaborative Crowdsourcing for Labeling Machine Learning Datasets.	Joseph Chee Chang, Saleema Amershi, Ece Kamar	chi2017	Crowdsourcing provides a scalable and efficient way to construct labeled datasets for training machine learning systems. However, creating comprehensive label guidelines for crowdworkers is often prohibitive even for seemingly simple concepts. Incomplete or ambiguous label guidelines can then result in differing interpretations of concepts and inconsistent labels. Existing approaches for improving label quality, such as worker screening or detection of poor work, are ineffective for this problem and can lead to rejection of honest work and a missed opportunity to capture rich interpretations about data. We introduce Revolt, a collaborative approach that brings ideas from expert annotation workflows to crowd-based labeling. Revolt eliminates the burden of creating detailed label guidelines by harnessing crowd disagreements to identify ambiguous concepts and create rich structures (groups of semantically related items) for post-hoc label decisions. Experiments comparing Revolt to traditional crowdsourced labeling show that Revolt produces high quality labels without requiring label guidelines in turn for an increase in monetary cost. This up front cost, however, is mitigated by Revolt's ability to produce reusable structures that can accommodate a variety of label boundaries without requiring new data to be collected. Further comparisons of Revolt's collaborative and non-collaborative variants show that collaboration reaches higher label accuracy with lower monetary cost.
CrowdVerge: Predicting If People Will Agree on the Answer to a Visual Question.	Danna Gurari, Kristen Grauman	chi2017	Visual question answering systems empower users to ask any question about any image and receive a valid answer. However, existing systems do not yet account for the fact that a visual question can lead to a single answer or multiple different answers. While a crowd often agrees, disagreements do arise for many reasons including that visual questions are ambiguous, subjective, or difficult. We propose a model, CrowdVerge, for automatically predicting from a visual question whether a crowd would agree on one answer. We then propose how to exploit these predictions in a novel application to efficiently collect all valid answers to visual questions. Specifically, we solicit fewer human responses when answer agreement is expected and more human responses otherwise. Experiments on 121,811 visual questions asked by sighted and blind people show that, compared to existing crowdsourcing systems, our system captures the same answer diversity with typically 14-23% less crowd involvement.
Flash Organizations: Crowdsourcing Complex Work by Structuring Crowds As Organizations.	Melissa A. Valentine, Daniela Retelny, Alexandra To, Negar Rahmati, Tulsee Doshi, Michael S. Bernstein	chi2017	This paper introduces flash organizations: crowds structured like organizations to achieve complex and open-ended goals. Microtask workflows, the dominant crowdsourcing structures today, only enable goals that are so simple and modular that their path can be entirely pre-defined. We present a system that organizes crowd workers into computationally-represented structures inspired by those used in organizations - roles, teams, and hierarchies - which support emergent and adaptive coordination toward open-ended goals. Our system introduces two technical contributions: 1) encoding the crowd's division of labor into de-individualized roles, much as movie crews or disaster response teams use roles to support coordination between on-demand workers who have not worked together before; and 2) reconfiguring these structures through a model inspired by version control, enabling continuous adaptation of the work and the division of labor. We report a deployment in which flash organizations successfully carried out open-ended and complex goals previously out of reach for crowdsourcing, including product design, software development, and game production. This research demonstrates digitally networked organizations that flexibly assemble and reassemble themselves from a globally distributed online workforce to accomplish complex work.
ReTool: Interactive Microtask and Workflow Design through Demonstration.	Chen Chen, Xiaojun Meng, Shengdong Zhao, Morten Fjeld	chi2017	In addition to simple form filling, there is an increasing need for crowdsourcing workers to perform freeform interactions directly on content in microtask crowdsourcing (e.g. proofreading articles or specifying object boundary in an image). Such microtasks are often organized within well-designed workflows to optimize task quality and workload distribution. However, designing and implementing the interface and workflow for such microtasks is challenging because it typically requires programming knowledge and tedious manual effort. We present ReTool, a web-based tool for requesters to design and publish interactive microtasks and workflows by demonstrating the microtasks for text and image content. We evaluated ReTool against a task-design tool from a popular crowdsourcing platform and showed the advantages of ReTool over the existing approach.
The Role of Explanations in Casual Observational Learning about Nutrition.	Marissa Burgermaster, Krzysztof Z. Gajos, Patricia G. Davidson, Lena Mamykina	chi2017	The ubiquity of internet-based nutrition information sharing indicates an opportunity to use social computing platforms to promote nutrition literacy and healthy nutritional choices. We conducted a series of experiments with unpaid volunteers using an online Nutrition Knowledge Test. The test asked participants to examine pairs of photographed meals and identify meals higher in a specific macronutrient (e.g., carbohydrate). After each answer, participants received no feedback on the accuracy of their answers, viewed proportions of peers choosing each response, received correctness feedback from an expert dietitian with or without expert-generated explanations, or received correctness feedback with crowd-generated explanations. The results showed that neither viewing peer responses nor correctness feedback alone improved learning. However, correctness feedback with explanations (i.e., modeling) led to significant learning gains, with no significant difference between explanations generated by experts or peers. This suggests the importance of explanations in social computing-based casual learning about nutrition and the potential for scaling this approach via crowdsourcing.
Leveraging Complementary Contributions of Different Workers for Efficient Crowdsourcing of Video Captions.	Yun Huang, Yifeng Huang, Na Xue, Jeffrey P. Bigham	chi2017	Hearing-impaired people and non-native speakers rely on captions for access to video content, yet most videos remain uncaptioned or have machine-generated captions with high error rates. In this paper, we present the design, implementation and evaluation of BandCaption, a system that combines automatic speech recognition with input from crowd workers to provide a cost-efficient captioning solution for accessible online videos. We consider four stakeholder groups as our source of crowd workers: (i) individuals with hearing impairments, (ii) second-language speakers with low proficiency, (iii) second-language speakers with high proficiency, and (iv) native speakers. Each group has different abilities and incentives, which our workflow leverages. Our findings show that BandCaption enables crowd workers who have different needs and strengths to accomplish micro-tasks and make complementary contributions. Based on our results, we outline opportunities for future research and provide design suggestions to deliver cost-efficient captioning solutions.
Narratives in Crowdsourced Evaluation of Visualizations: A Double-Edged Sword?	Evanthia Dimara, Anastasia Bezerianos, Pierre Dragicevic	chi2017	We explore the effects of providing task context when evaluating visualization tools using crowdsourcing. We gave crowdsource workers i) abstract information visualization tasks without any context, ii) tasks where we added semantics to the dataset, and iii) tasks with two types of backstory narratives: an analytic narrative and a decision-making narrative. Contrary to our expectations, we did not find evidence that adding data semantics increases accuracy, and further found that our backstory narratives can even decrease accuracy. Adding dataset semantics can however increase attention and provide subjective benefits in terms of confidence, perceived easiness, task enjoyability and perceived usefulness of the visualization. Nevertheless, our backstory narratives did not appear to provide additional subjective benefits. These preliminary findings suggest that narratives may have complex and unanticipated effects, calling for more studies in this area.
Differences in Crowdsourced vs. Lab-based Mobile and Desktop Input Performance Data.	Leah Findlater, Joan Zhang, Jon E. Froehlich, Karyn Moffatt	chi2017	Research on the viability of using crowdsourcing for HCI performance experiments has concluded that online results are similar to those achieved in the lab---at least for desktop interactions. However, mobile devices, the most popular form of online access today, may be more problematic due to variability in the user's posture and in movement of the device. To assess this possibility, we conducted two experiments with 30 lab-based and 303 crowdsourced participants using basic mouse and touchscreen tasks. Our findings show that: (1) separately analyzing the crowd and lab data yields different study conclusions-touchscreen input was significantly less error prone than mouse input in the lab but more error prone online; (2) age-matched crowdsourced participants were significantly faster and less accurate than their lab-based counterparts, contrasting past work; (3) variability in mobile device movement and orientation increased as experimenter control decreased--a potential factor affecting the touchscreen error differences. This study cautions against assuming that crowdsourced data for performance experiments will directly reflect lab-based data, particularly for mobile devices.
The Onboarding Effect: Leveraging User Engagement and Retention in Crowdsourcing Platforms.	Marina Cascaes Cardoso	chi2017a	Onboarding is essential for any online service or platform interested in acquiring more users, members or customers. The beginning of a relationship between the user and a platform is the moment where motivations take part and the overall strategy? the flow, messages, interactions and UI design elements -- should be carefully planned, so the experience as a whole can support new users' engagement. Such an important strategy still lacks systematic investigation. Most of what is employed by the industry result from empirical testing that turns out to serve a particular product in a certain instance. This research aims to provide scientific evidence regarding the effect of each onboarding element on user engagement and experience, in addition to solid guidelines that support decision making for future designs.
EduBang: Envisioning the Next Generation Video Sharing.	Hyowon Lee, Gayathri Balasubramanian, Poon King Wang	chi2017a	This video envisages what our future application for video consumption would be like, based on a series of interviews with researchers from various technology fields. A novel video browsing interaction was designed as a result, and illustrates how far more sophisticated and fine-grained levels of crowdsourcing on the web and a combination of emerging computational technologies will allow searching and browsing in an online video repository such as YouTube, but with the meaningfully segmented chunks of videos as the main unit of retrieval, resulting in easy concatenation of chunks to create longer videos and share.
Ethical Encounters in HCI: Implications for Research in Sensitive Settings.	Jenny Waycott, Cosmin Munteanu, Hilary Davis, Anja Thieme, Stacy M. Branham, Wendy Moncur, Roisin McNaney, John Vines	chi2017a	This workshop builds on the success of prior workshops that brought together HCI researchers to share stories about ethical challenges faced when conducting research in sensitive settings. There is growing recognition that reflective and empathetic approaches are needed to conduct ethical research in settings involving people who might be considered vulnerable or marginalized. At our previous workshops, researchers discussed personal experiences and described the complex challenges they have faced in research as diverse as designing information systems for families of children in palliative care to analyzing social media posts about mental health. In this follow-up workshop we aim to extend opportunities for knowledge-sharing, build on the lessons learned, and generate a range of resources to help HCI researchers manage complex ethical issues when working in sensitive settings.
On the Genesis of an Assistive Technology Crowdsourcing Community.	Christopher Michael Homan, Jon I. Schull, Akshai Prabhu	chi2017a	The e-NABLE movement is a global confederation that designs, builds, and distributes free, 3D-printed, upper limb assistive devices to children born without fingers and hands. It has been called one of the most inspiring philanthropic efforts of the 21st century. We use social network analysis and natural language processing on the original e-NABLE Google+ community to understand the challenges and opportunities in organizing a rapidly growing real-world social entrepreneurship venture via social media. Our results provide important lessons and benchmarks for similar communities.
Crowdsourcing and Crowd Work.	Jeffrey P. Bigham, Chinmay Kulkarni, Walter S. Lasecki	chi2017a	In recent years, companies have been getting access to larger pools of workers, and the phenomenon of crowdsourcing has emerged as a new pattern of digitally mediated collaboration. In parallel, an ongoing digitalisation has been accelerating the division of labour through hyperspecialisation and giving rise to new forms of work, for example crowd work. This paper illustrates the differences between crowdsourcing as an alternative concept of organizing, and crowd work as a new form of digital gainful work. The variety of crowdsourcing applications on the one hand, and the different forms of crowd work on the other, will be introduced. In summary, more and more individuals decide to work online in the crowd, and those crowds consist of people of any social strata, age or location. Hence, with the rise of crowd work, several opportunities and risks for all of these participants can be observed and need to be addressed.
Policy Impacts on the HCI Research Community.	Jofish Kaye, Casey Fiesler, Neha Kumar, Bryan C. Semaan	chi2017a	Recent policy developments in both the USA and elsewhere have sparked significant concern within the HCI research community. Predominantly, increasing isolationism can have a serious impact on collaboration, community engagement and wellness, and knowledge sharing. Policy shifts also have the potential to affect freedom of scientific inquiry, the free flow of information, and funding for computing research. Moreover, what can SIGCHI as an organization do to best support the community under current conditions? In this panel discussion, designed to be heavily participatory, we will take on these issues and discuss what we can do to support our strong, diverse community of researchers.
A Community Rather Than A Union: Understanding Self-Organization Phenomenon on MTurk and How It Impacts Turkers and Requesters.	Xinyi Wang, Haiyi Zhu, Yangyun Li, Yu Cui, Joseph A. Konstan	chi2017a	"This paper aims to understand the self-organization phenomenon among the workers of Amazon Mechanical Turk (MTurk), a well-known crowdsourcing platform. Specifically, we explored 1) why MTurk workers self-organize into online communities (Turker communities), and 2) how the workers' self-organization impacts the requesters and 3) the workers. In the first study, we conducted a field experiment by advertising the same survey tasks on both MTurk and on Turker communities. In the second study, we interviewed two founders of the Turker communities. The results suggest that 1) workers' main motivation to participate in communities is to ""find good HITs"". 2) For requesters, there is no indication of differences in work quality between the tasks posted on MTurk and the ones advertised on Turker communities. 3) For workers, participation in Turker communities is associated with higher income, controlled for working hours. We also learned from the interviews about why founders built the communities and their future plans."
Exploring Coordination Models for Ad Hoc Programming Teams.	Sang Won Lee, Yan Chen, Noah Klugman, Sai R. Gouravajhala, Angela Chen, Walter S. Lasecki	chi2017a	Software development is a complex task with inherently interdependent sub-components. Prior work on crowdsourcing software engineering has addressed this problem by performing an a priori decomposition of the task into well-defined microtasks that individual crowd workers can complete independently. Alternatively, ad hoc teams of experts recruited from online crowds can remotely collaborate, avoiding the up-front cost to end users of task decomposition. However, these temporary ad hoc teams can lead to high coordination costs during the session itself. In this paper, we explore the types and causes of these coordination costs for transient software teams in existing collaborative programming tools: a version control system and a real-time shared editor. Based on our findings, we suggest design elements of shared programming environments that help teams effectively self-coordinate on their task.
Unsupervised Clickstream Clustering for User Behavior Analysis.	Gang Wang, Xinyi Zhang, Shiliang Tang, Haitao Zheng, Ben Y. Zhao	chi2016	"Online services are increasingly dependent on user participation. Whether it's online social networks or crowdsourcing services, understanding user behavior is important yet challenging. In this paper, we build an unsupervised system to capture dominating user behaviors from clickstream data (traces of users' click events), and visualize the detected behaviors in an intuitive manner. Our system identifies ""clusters"" of similar users by partitioning a similarity graph (nodes are users; edges are weighted by clickstream similarity). The partitioning process leverages iterative feature pruning to capture the natural hierarchy within user clusters and produce intuitive features for visualizing and understanding captured user behaviors. For evaluation, we present case studies on two large-scale clickstream traces (142 million events) from real social networks. Our system effectively identifies previously unknown behaviors, e.g., dormant users, hostile chatters. Also, our user study shows people can easily interpret identified behaviors using our visualization tool."
Crowd-Designed Motivation: Motivational Messages for Exercise Adherence Based on Behavior Change Theory.	Roelof Anne Jelle de Vries, Khiet P. Truong, Sigrid Kwint, Constance H. C. Drossaert, Vanessa Evers	chi2016	Developing motivational technology to support long-term behavior change is a challenge. A solution is to incorporate insights from behavior change theory and design technology to tailor to individual users. We carried out two studies to investigate whether the processes of change, from the Transtheoretical Model, can be effectively represented by motivational text messages. We crowdsourced peer-designed text messages and coded them into categories based on the processes of change. We evaluated whether people perceived messages tailored to their stage of change as motivating. We found that crowdsourcing is an effective method to design motivational messages. Our results indicate that different messages are perceived as motivating depending on the stage of behavior change a person is in. However, while motivational messages related to later stages of change were perceived as motivational for those stages, the motivational messages related to earlier stages of change were not. This indicates that a person's stage of change may not be the (only) key factor that determines behavior change. More individual factors need to be considered to design effective motivational technology.
"""Why would anybody do this?"": Understanding Older Adults' Motivations and Challenges in Crowd Work."	Robin Brewer, Meredith Ringel Morris, Anne Marie Piper	chi2016	Diversifying participation in crowd work can benefit the worker and requester. Increasing numbers of older adults are online, but little is known about their awareness of or how they engage in mainstream crowd work. Through an online survey with 505 seniors, we found that most have never heard of crowd work but would be motivated to complete tasks by earning money or working on interesting or stimulating tasks. We follow up results from the survey with interviews and observations of 14 older adults completing crowd work tasks. While our survey data suggests that financial incentives are encouraging, in-depth interviews reveal that a combination of personal and social incentives may be stronger drivers of participation, but only if older adults can overcome accessibility issues and understand the purpose of crowd work. This paper contributes insights into how crowdsourcing sites could better engage seniors and other users.
The Knowledge Accelerator: Big Picture Thinking in Small Pieces.	Nathan Hahn, Joseph Chang, Ji Eun Kim, Aniket Kittur	chi2016	Crowdsourcing offers a powerful new paradigm for online work. However, real world tasks are often interdependent, requiring a big picture view of the difference pieces involved. Existing crowdsourcing approaches that support such tasks -- ranging from Wikipedia to flash teams -- are bottlenecked by relying on a small number of individuals to maintain the big picture. In this paper, we explore the idea that a computational system can scaffold an emerging interdependent, big picture view entirely through the small contributions of individuals, each of whom sees only a part of the whole. To investigate the viability, strengths, and weaknesses of this approach we instantiate the idea in a prototype system for accomplishing distributed information synthesis and evaluate its output across a variety of topics. We also contribute a set of design patterns that may be informative for other systems aimed at supporting big picture thinking in small pieces.
"A Feminist HCI Approach to Designing Postpartum Technologies: ""When I first saw a breast pump I was wondering if it was a joke""."	Catherine D'Ignazio, Alexis Hope, Becky Michelson, Robyn Churchill, Ethan Zuckerman	chi2016	In recent years, the CHI community has begun to discuss how HCI research could improve the experience of motherhood. In this paper, we take up the challenge of designing for this complex life phase and present an analysis of data collected from a design process that included over 1,000 mother-submitted ideas to improve the breast pump, a technology that allows mothers around the world to collect and store their breast milk. In addition to presenting a range of ideas to improve this specific technology, we discuss environmental, legal, social, and emotional dimensions of the postpartum period that suggest opportunities for a range of additional supportive technologies. We close with insights linking our findings to ongoing discussions related to Feminist HCI theory, crowdsourcing, and participatory design.
Toward a Learning Science for Complex Crowdsourcing Tasks.	Shayan Doroudi, Ece Kamar, Emma Brunskill, Eric Horvitz	chi2016	We explore how crowdworkers can be trained to tackle complex crowdsourcing tasks. We are particularly interested in training novice workers to perform well on solving tasks in situations where the space of strategies is large and workers need to discover and try different strategies to be successful. In a first experiment, we perform a comparison of five different training strategies. For complex web search challenges, we show that providing expert examples is an effective form of training, surpassing other forms of training in nearly all measures of interest. However, such training relies on access to domain expertise, which may be expensive or lacking. Therefore, in a second experiment we study the feasibility of training workers in the absence of domain expertise. We show that having workers validate the work of their peer workers can be even more effective than having them review expert examples if we only present solutions filtered by a threshold length. The results suggest that crowdsourced solutions of peer workers may be harnessed in an automated training pipeline.
Learning From the Crowd: Observational Learning in Crowdsourcing Communities.	Lena Mamykina, Thomas N. Smyth, Jill P. Dimond, Krzysztof Z. Gajos	chi2016	Crowd work provides solutions to complex problems effectively, efficiently, and at low cost. Previous research showed that feedback, particularly correctness feedback can help crowd workers improve their performance; yet such feedback, particularly when generated by experts, is costly and difficult to scale. In our research we investigate approaches to facilitating continuous observational learning in crowdsourcing communities. In a study conducted with workers on Amazon Mechanical Turk, we asked workers to complete a set of tasks identifying nutritional composition of different meals. We examined workers' accuracy gains after being exposed to expert-generated feedback and to two types of peer-generated feedback: direct accuracy assessment with explanations of errors, and a comparison with solutions generated by other workers. The study further confirmed that expert-generated feedback is a powerful mechanism for facilitating learning and leads to significant gains in accuracy. However, the study also showed that comparing one's own solutions with a variety of solutions suggested by others and their comparative frequencies leads to significant gains in accuracy. This solution is particularly attractive because of its low cost, minimal impact on time and cost of job completion, and high potential for adoption by a variety of crowdsourcing platforms.
Atelier: Repurposing Expert Crowdsourcing Tasks as Micro-internships.	Ryo Suzuki, Niloufar Salehi, Michelle S. Lam, Juan C. Marroquin, Michael S. Bernstein	chi2016	Expert crowdsourcing marketplaces have untapped potential to empower workers' career and skill development. Currently, many workers cannot afford to invest the time and sacrifice the earnings required to learn a new skill, and a lack of experience makes it difficult to get job offers even if they do. In this paper, we seek to lower the threshold to skill development by repurposing existing tasks on the marketplace as mentored, paid, real-world work experiences, which we refer to as micro-internships. We instantiate this idea in Atelier, a micro-internship platform that connects crowd interns with crowd mentors. Atelier guides mentor-intern pairs to break down expert crowdsourcing tasks into milestones, review intermediate output, and problem-solve together. We conducted a field experiment comparing Atelier's mentorship model to a non-mentored alternative on a real-world programming crowdsourcing task, finding that Atelier helped interns maintain forward progress and absorb best practices.
How One Microtask Affects Another.	Edward Newell, Derek Ruths	chi2016	Microtask platforms are becoming commonplace tools for performing human research, producing gold-standard data, and annotating large datasets. These platforms connect requesters (researchers or companies) with large populations (crowds) of workers, who perform small tasks, typically taking less than five minutes each. A topic of ongoing research concerns the design of tasks that elicit high quality annotations. Here we identify a seemingly banal feature of nearly all crowdsourcing workflows that profoundly impacts workers' responses. Microtask assignments typically consist of a sequence of tasks sharing a common format (e.g., circle galaxies in an image). Using image-labeling, a canonical microtask format, we show that earlier tasks can have a strong influence on responses to later tasks, shifting the distribution of future responses by 30-50% (total variational distance). Specifically, prior tasks influence the content that workers focus on, as well as the richness and specialization of responses. We call this phenomenon intertask effects. We compare intertask effects to framing, effected by stating the requester's research interest, and find that intertask effects are on par or stronger. If uncontrolled, intertask effects could be a source of systematic bias, but our results suggest that, with appropriate task design, they might be leveraged to hone worker focus and acuity, helping to elicit reproducible, expert-level judgments. Intertask effects are a crucial aspect of human computation that should be considered in the design of any crowdsourced study.
Embracing Error to Enable Rapid Crowdsourcing.	Ranjay A. Krishna, Kenji Hata, Stephanie Chen, Joshua Kravitz, David A. Shamma, Li Fei-Fei, Michael S. Bernstein	chi2016	Microtask crowdsourcing has enabled dataset advances in social science and machine learning, but existing crowdsourcing schemes are too expensive to scale up with the expanding volume of data. To scale and widen the applicability of crowdsourcing, we present a technique that produces extremely rapid judgments for binary and categorical labels. Rather than punishing all errors, which causes workers to proceed slowly and deliberately, our technique speeds up workers' judgments to the point where errors are acceptable and even expected. We demonstrate that it is possible to rectify these errors by randomizing task order and modeling response latency. We evaluate our technique on a breadth of common labeling tasks such as image verification, word similarity, sentiment analysis and topic classification. Where prior work typically achieves a 0.25x to 1x speedup over fixed majority vote, our approach often achieves an order of magnitude (10x) speedup.
HapTurk: Crowdsourcing Affective Ratings of Vibrotactile Icons.	Oliver S. Schneider, Hasti Seifi, Salma Kashani, Matthew Chun, Karon E. MacLean	chi2016	Vibrotactile (VT) display is becoming a standard component of informative user experience, where notifications and feedback must convey information eyes-free. However, effective design is hindered by incomplete understanding of relevant perceptual qualities, together with the need for user feedback to be accessed in-situ. To access evaluation streamlining now common in visual design, we introduce proxy modalities as a way to crowdsource VT sensations by reliably communicating high-level features through a crowd-accessible channel. We investigate two proxy modalities to represent a high-fidelity tactor: a new VT visualization, and low-fidelity vibratory translations playable on commodity smartphones. We translated 10 high-fidelity vibrations into both modalities, and in two user studies found that both proxy modalities can communicate affective features, and are consistent when deployed remotely over Mechanical Turk. We analyze fit of features to modalities, and suggest future improvements.
Now Check Your Input: Brief Task Lockouts Encourage Checking, Longer Lockouts Encourage Task Switching.	Sandy J. J. Gould, Anna Louise Cox, Duncan P. Brumby, Alice Wickersham	chi2016	Data-entry is a common activity that is usually performed accurately. When errors do occur though, people are poor at spotting them even if they are told to check their input. We considered whether making people pause for a brief moment before confirming their input would make them more likely to check it. We ran a lab experiment to test this idea. We found that task lockouts encouraged checking. Longer lockout durations made checking more likely. We ran a second experiment on a crowdsourcing platform to find out whether lockouts would still be effective in a less controlled setting. We discovered that longer lockouts induced workers to switch to other activities. This made the lockouts less effective. To be useful in practice, the duration of lockouts needs to be carefully calibrated. If lockouts are too brief they will not encourage checking. If they are too long they will induce switching.
Curiosity Killed the Cat, but Makes Crowdwork Better.	Edith Law, Ming Yin, Joslin Goh, Kevin Chen, Michael A. Terry, Krzysztof Z. Gajos	chi2016	Crowdsourcing systems are designed to elicit help from humans to accomplish tasks that are still difficult for computers. How to motivate workers to stay longer and/or perform better in crowdsourcing systems is a critical question for designers. Previous work have explored different motivational frameworks, both extrinsic and intrinsic. In this work, we examine the potential for curiosity as a new type of intrinsic motivational driver to incentivize crowd workers. We design crowdsourcing task interfaces that explicitly incorporate mechanisms to induce curiosity and conduct a set of experiments on Amazon's Mechanical Turk. Our experiment results show that curiosity interventions improve worker retention without degrading performance, and the magnitude of the effects are influenced by both personal characteristics of the worker and the nature of the task.
Pay It Backward: Per-Task Payments on Crowdsourcing Platforms Reduce Productivity.	Kazushi Ikeda, Michael S. Bernstein	chi2016	Paid crowdsourcing marketplaces have gained popularity by using piecework, or payment for each microtask, to incentivize workers. This norm has remained relatively unchallenged. In this paper, we ask: is the pay-per-task method the right one? We draw on behavioral economic research to examine whether payment in bulk after every ten tasks, saving money via coupons instead of earning money, or material goods rather than money will increase the number of completed tasks. We perform a twenty-day, between-subjects field experiment (N=300) on a mobile crowdsourcing application and measure how often workers responded to a task notification to fill out a short survey under each incentive condition. Task completion rates increased when paying in bulk after ten tasks: doing so increased the odds of a response by 1.4x, translating into 8% more tasks through that single intervention. Payment with coupons instead of money produced a small negative effect on task completion rates. Material goods were the most robust to decreasing participation over time.
Investigating the Impact of 'Emphasis Frames' and Social Loafing on Player Motivation and Performance in a Crowdsourcing Game.	Geoff Kaufman, Mary Flanagan, Sukdith Punjasthitkul	chi2016	"With an increasing reliance on crowdsourcing games as data-gathering tools, it is imperative to understand how to motivate and sustain high levels of voluntary contribution. To this end, the present work directly compared the impact of various ""emphasis frames,"" highlighting distinct intrinsic motivational factors, used to describe an online game in which players provide descriptive metadata ""tags"" for digitized images. An initial study showed that, compared to frames emphasizing personal enjoyment or altruistic motivations, a frame emphasizing a ""growing community of players"" solicited significantly fewer contributions. A second study tested the hypothesis that this lower level of contribution resulted from social loafing (the tendency to exert less effort in collective tasks in which contributions are anonymous and pooled). Results revealed that, compared to a no-frame control condition, a frame emphasizing the preponderance of other players reduced contribution levels and game replay likelihood, whereas a frame emphasizing the scarcity of fellow players increased contribution and replay levels. Various strategies for counteracting social loafing in crowdsourcing contexts are discussed."
Speeching: Mobile Crowdsourced Speech Assessment to Support Self-Monitoring and Management for People with Parkinson's.	Roisin McNaney, Mohammad Othman, Dan Richardson, Paul Dunphy, Telmo Amaral, Nick Miller, Helen Stringer, Patrick Olivier, John Vines	chi2016	We present Speeching, a mobile application that uses crowdsourcing to support the self-monitoring and management of speech and voice issues for people with Parkinson's (PwP). The application allows participants to audio record short voice tasks, which are then rated and assessed by crowd workers. Speeching then feeds these results back to provide users with examples of how they were perceived by listeners unconnected to them (thus not used to their speech patterns). We conducted our study in two phases. First we assessed the feasibility of utilising the crowd to provide ratings of speech and voice that are comparable to those of experts. We then conducted a trial to evaluate how the provision of feedback, using Speeching, was valued by PwP. Our study highlights how applications like Speeching open up new opportunities for self-monitoring in digital health and wellbeing, and provide a means for those without regular access to clinical assessment services to practice and get meaningful feedback on their speech.
Extracting Heart Rate from Videos of Online Participants.	Thomas Muender, Matthew K. Miller, Max Valentin Birk, Regan L. Mandryk	chi2016	Crowdsourcing experiments online allows for low-cost data gathering with large participant pools; however, collecting data online does not give researchers access to certain metrics. For example, physiological measures such as heart rate (HR) can provide high-resolution data about the physical, emotional, and mental state of the participant. We investigate and characterize the feasibility of gathering HR from videos of online participants engaged in single user and social tasks. We show that room lighting, head motion, and network bandwidth influence measurement quality, but that instructing participants in good practices substantially improves measurement quality. Our work takes a step towards online physiological data collection.
Crowdnection: Connecting High-level Concepts with Historical Documents via Crowdsourcing.	Nai-Ching Wang	chi2016a	To form and test hypotheses and finally produce conclusions, people use existing schemas to search a pool of data for evidence. The quality of the search largely depends on the quality of connections between the schemas and the data. Making good connections between schemas and unprocessed data is challenging because it is time-consuming and may require expertise. Crowdsourcing provides a potential solution because with appropriate methods, humans are often more effective at synthesizing diverse information than automated techniques. This paper introduces Crowdnection, which leverages crowdsourcing methods to examine the effect of amount of context on performance in making connections between raw texts of historical textual documents and high-level concepts. The results suggest novices are able to help process information to provide meaningful insights, and indicate that there is an ideal amount of context facilitating the sensemaking process.
Team Dating: A Self-Organized Team Formation Strategy for Collaborative Crowdsourcing.	Ioanna Lykourentzou, Shannon Wang, Robert E. Kraut, Steven P. Dow	chi2016a	"Online crowds have the potential to do more complex work in teams, rather than as individuals. However, at such a large scale, team formation can be difficult to coordinate. (How) can we rely on the crowd itself to organize into effective teams? Our research explores a strategy for ""team dating"", a self-organized crowd team formation approach where workers try out and rate different candidate partners. In two online experiments, we find that team dating affects the way that people select partners and how they evaluate them. We use these results to draw useful conclusions for the future of team dating and its implications for collaborative crowdsourcing."
Exploring the Potential of Children in Crowdsourcing.	Stefan Manojlovic, Katerina Gavrilo, Jan de Wit, Vassilis-Javed Khan, Panos Markopoulos	chi2016a	Recently, companies and academia have turned to crowdsourcing to stimulate creativity and innovation. Although children's creative nature has been well documented in the design process in co-creation for new products and/or services, this has not yet extended to crowdsourcing. With this paper, we investigate -- through crowdsourcing -- the gap between children and crowdsourcing. To gather a diverse sample of participants we used CrowdFlower, a crowdsourcing platform, to generate, evaluate and rank ideas and concepts. Results show that 93% of parents and 80% of non-parents would involve children in crowdsourcing. The most valued concept of the crowd was the collaboration between parents and children, who are innovating for companies. This concept involves publishing companies requesting drawings from children for book illustrations.
Among the Machines: Human-Bot Interaction on Social Q&A Websites.	Alessandro Murgia, Daan Janssens, Serge Demeyer, Bogdan Vasilescu	chi2016a	With the rise of social media and advancements in AI technology, human-bot interaction will soon be commonplace. In this paper we explore human-bot interaction in STACK OVERFLOW, a question and answer website for developers. For this purpose, we built a bot emulating an ordinary user answering questions concerning the resolution of git error messages. In a first run this bot impersonated a human, while in a second run the same bot revealed its machine identity. Despite being functionally identical, the two bot variants elicited quite different reactions.
Connecting Online Work and Online Education at Scale.	Markus Krause, Margeret Hall, Joseph Jay Williams, Praveen K. Paritosh, John Prip, Simon Caton	chi2016a	"Education is one of the eight Millennium Development Goals (MDG) of the United Nations. Considerable interest has been displayed in online education at scale, a new arising concept to realize the MDG. Yet connecting online education to real jobs is still a challenge. This CHI workshop bridges this gap by bringing together groups and insights from related work at HCOMP, CSCW, and Learning at Scale. The workshop aims at providing opportunities for groups not yet in the focus of online education, exemplified by low SES and less educated students who have not have equal access to higher education, compared to typical students in MOOCs. The focus is on theoretical and empirical connections between online education and job opportunities which can reduce the financial gap, by providing students with an income during their studies. The workshop explores the technological analogue of the concept of ""apprenticeship"", long established in the European Union, and education research (Collins, Seely Brown, Newman, 1989). This allows students to do useful work as an apprentice during their studies. This workshop tackles such questions by bringing together participants from industry (e.g., platforms similar to Upwork, Amazon Mechanical Turk); education, psychology, and MOOCs (e.g., attendees of AERA, EDM, AIED, Learning at Scale); crowdsourcing and collaborative work (e.g., attendees of CHI, CSCW, NIPS, AAAI's HCOMP)."
Crowd Dynamics: Exploring Conflicts and Contradictions in Crowdsourcing.	Karin Hansson, Michael J. Muller, Tanja Aitamurto, Lilly Irani, Athanasios Mazarakis, Neha Gupta, Thomas Ludwig	chi2016a	Unfair reputation systems, slow payments, lack of transparency, and socio-spatial inequalities are only some of the many reasons for conflicts in crowdsourcing. The divisive logic of the system and the sharing processes in the peer-community create interesting dynamics and new foci on old conflicts. In this workshop we explore the reasons, processes, power relations, and dynamics of conflicts within crowdsourcing. We invite participants from a diversity of disciplines and perspectives to contribute with insights from different types of crowdsourcing, and thereby deepen our understanding of the relations in contexts such as crowd-work, crowdfunding, peer-production and citizen science. Furthermore, we examine strategies for accommodating differences in crowdsourcing environments.
Using smartphones in cities to crowdsource dangerous road sections and give effective in-car warnings.	Mark D. Dunlop, Marc Roper, Mark Elliot, Rebecca McCartan, Bruce McGregor	seachi2016	The widespread day-to-day carrying of powerful smartphones gives opportunities for crowd-sourcing information about the users' activities to gain insight into patterns of use of a large population in cities. Here we report the design and initial investigations into a crowdsourcing approach for sudden decelerations to identify dangerous road sections. Sudden brakes and near misses are much more common than police reportable accidents but under exploited and have the potential for more responsive reaction than waiting for accidents. We also discuss different multimodal feedback conditions to warn drivers approaching a dangerous zone. We believe this crowdsourcing approach gives cost and coverage benefits over infrastructural smart-city approaches but that users need incentivized for use.
Crowdsourcing: Tackling Challenges in the Engagement of Citizens with Smart City Initiatives.	Long Pham, Conor Linehan	seachi2016	The engagement and involvement of citizens with the design of Smart City (SC) initiatives help ensure a maximisation of benefit for all stakeholders. However, undertaking processes that facilitate citizen engagement often involves prohibitive challenges in cost, design and deployment mechanisms, particularly for small cities which have limited resources. We report on a project carried out in Cork City, a small city in Ireland, where a crowdsourcing-inspired method was used. Academics, local government, volunteers and civil organisations came together to collaboratively design and carry out a study to represent local interests around the deployment of smart city infrastructure. Our project demonstrates a new way of translating crowdsourcing for use in government problem-solving. It was three-times less in cost, creative in design, and flexible but collaborative in deployment, resulting in high volume of reliable data for project prioritisation and implementation.
Crowdsourced Exploration of Security Configurations.	Qatrunnada Ismail, Tousif Ahmed, Apu Kapadia, Michael K. Reiter	chi2015	Smartphone apps today request permission to access a multitude of sensitive resources, which users must accept completely during installation (e.g., on Android) or selectively configure after installation (e.g., on iOS, but also planned for Android). Everyday users, however, do not have the ability to make informed decisions about which permissions are essential for their usage. For enhanced privacy, we seek to leverage crowdsourcing to find minimal sets of permissions that will preserve the usability of the app for diverse users. We advocate an efficient 'lattice-based' crowd-management strategy to explore the space of permissions sets. We conducted a user study (N = 26) in which participants explored different permission sets for the popular Instagram app. This study validates our efficient crowd management strategy and shows that usability scores for diverse users can be predicted accurately, enabling suitable recommendations.
In-group Questions and Out-group Answers: Crowdsourcing Daily Living Advice for Individuals with Autism.	Hwajung Hong, Eric Gilbert, Gregory D. Abowd, Rosa I. Arriaga	chi2015	Difficulty in navigating daily life can lead to frustration and decrease independence for people with autism. While they turn to online autism communities for information and advice for coping with everyday challenges, these communities may present only a limited perspective because of their in-group nature. Obtaining support from out-group sources beyond the in-group community may prove valuable in dealing with challenging situations such as public anxiety and workplace conflicts. In this paper, we explore the value of supplementary out-group support from crowdsourced responders added to in-group support from a community of members. We find that out-group sources provide relatively rapid, concise responses with direct and structured information, socially appropriate coping strategies without compromising emotional value. Using an autism community as a motivating example, we conclude by providing design implications for combining in-group and out-group resources that may enhance the question-and-answer experience.
Mobile Gamification for Crowdsourcing Data Collection: Leveraging the Freemium Model.	Kristen K. Dergousoff, Regan L. Mandryk	chi2015	Classic ways of gathering data on human behaviour are time-consuming, costly and are subject to limited participant pools. Crowdsourcing offers a reduction in operating costs and access to a diverse and large participant pool; however issues arise concerning low worker pay and questions about data quality. Gamification provides a motivation to participate, but also requires the development of specialized, research-question specific games that can be costly to produce. Our solution combines gamification and crowdsourcing in a smartphone-based system that emulates the popular Freemium model of play to motivate voluntary participation through in-game rewards, using a robust framework to study multiple unrelated research questions within the same system. We deployed our game on the Android store and compared it to a gamified laboratory version and a non-gamified laboratory version, and found that players who used the in-game rewards were motivated to do experimental tasks. There was no difference between the systems for performance on a motor task; however, performance on the cognitive task was worse for the crowdsourced game. We discuss options for improving performance on tasks requiring attention.
Measuring Crowdsourcing Effort with Error-Time Curves.	Justin Cheng, Jaime Teevan, Michael S. Bernstein	chi2015	Crowdsourcing systems lack effective measures of the effort required to complete each task. Without knowing how much time workers need to execute a task well, requesters struggle to accurately structure and price their work. Objective measures of effort could better help workers identify tasks that are worth their time. We propose a data-driven effort metric, ETA (error-time area), that can be used to determine a task's fair price. It empirically models the relationship between time and error rate by manipulating the time that workers have to complete a task. ETA reports the area under the error-time curve as a continuous metric of worker effort. The curve's 10th percentile is also interpretable as the minimum time most workers require to complete the task without error, which can be used to price the task. We validate the ETA metric on ten common crowdsourcing tasks, including tagging, transcription, and search, and find that ETA closely tracks how workers would rank these tasks by effort. We also demonstrate how ETA allows requesters to rapidly iterate on task designs and measure whether the changes improve worker efficiency. Our findings can facilitate the process of designing, pricing, and allocating crowdsourcing tasks.
The Effects of Sequence and Delay on Crowd Work.	Walter S. Lasecki, Jeffrey M. Rzeszotarski, Adam Marcus, Jeffrey P. Bigham	chi2015	A common approach in crowdsourcing is to break large tasks into small microtasks so that they can be parallelized across many crowd workers and so that redundant work can be more easily compared for quality control. In practice, this can result in the microtasks being presented out of their natural order and often introduces delays between individual microtasks. In this paper, we demonstrate in a study of 338 crowd workers that non-sequential microtasks and the introduction of delays significantly decreases worker performance. We show that interruptions where a large delay occurs between two related tasks can cause up to a 102% slowdown in completion time, and interruptions where workers are asked to perform different tasks in sequence can slow down completion time by 57%. We conclude with a set of design guidelines to improve both worker performance and realized pay, and instructions for implementing these changes in existing interfaces for crowd work.
TurkBench: Rendering the Market for Turkers.	Benjamin V. Hanrahan, Jutta K. Willamowski, Saiganesh Swaminathan, David B. Martin	chi2015	Crowdsourcing is a relatively new model of labor where both the workers and work providers are experiencing its growing pains. A dominant platform that implements this model of labor is Amazon Mechanical Turk (AMT). While AMT has evolved over the years, the changes have focused mainly on work providers and have not addressed the problems workers face (e.g. dealing with market volatility and unpaid time searching for work). In this paper we present emph{TurkBench}, a tool meant to provide workers with personalized market visualization and session management. We discuss the design philosophy of the tool, briefly discuss four Turkers' reaction to a demo, and outline future work.
Understanding Malicious Behavior in Crowdsourcing Platforms: The Case of Online Surveys.	Ujwal Gadiraju, Ricardo Kawase, Stefan Dietze, Gianluca Demartini	chi2015	Crowdsourcing is increasingly being used as a means to tackle problems requiring human intelligence. With the ever-growing worker base that aims to complete microtasks on crowdsourcing platforms in exchange for financial gains, there is a need for stringent mechanisms to prevent exploitation of deployed tasks. Quality control mechanisms need to accommodate a diverse pool of workers, exhibiting a wide range of behavior. A pivotal step towards fraud-proof task design is understanding the behavioral patterns of microtask workers. In this paper, we analyze the prevalent malicious activity on crowdsourcing platforms and study the behavior exhibited by trustworthy and untrustworthy workers, particularly on crowdsourced surveys. Based on our analysis of the typical malicious activity, we define and identify different types of workers in the crowd, propose a method to measure malicious activity, and finally present guidelines for the efficient design of crowdsourced surveys.
Is This Thing On?: Crowdsourcing Privacy Indicators for Ubiquitous Sensing Platforms.	Serge Egelman, Raghudeep Kannavara, Richard Chow	chi2015	We are approaching an environment where ubiquitous computing devices will constantly accept input via audio and video channels: kiosks that determine demographic information of passersby, gesture controlled home entertainment systems and audio controlled wearable devices are just a few examples. To enforce the principle of least privilege, recent proposals have suggested technical approaches to limit third-party applications to receiving only the data they need, rather than entire audio or video streams. For users to make informed privacy decisions, applications will still need to communicate what data they are accessing and indicators will be needed to communicate this information. We performed several crowdsourcing experiments to examine how potential users might conceptualize and understand privacy indicators on ubiquitous sensing platforms.
Apparition: Crowdsourced User Interfaces that Come to Life as You Sketch Them.	Walter S. Lasecki, Juho Kim, Nick Rafter, Onkur Sen, Jeffrey P. Bigham, Michael S. Bernstein	chi2015	Prototyping allows designers to quickly iterate and gather feedback, but the time it takes to create even a Wizard-of-Oz prototype reduces the utility of the process. In this paper, we introduce crowdsourcing techniques and tools for prototyping interactive systems in the time it takes to describe the idea. Our Apparition system uses paid microtask crowds to make even hard-to-automate functions work immediately, allowing more fluid prototyping of interfaces that contain interactive elements and complex behaviors. As users sketch their interface and describe it aloud in natural language, crowd workers and sketch recognition algorithms translate the input into user interface elements, add animations, and provide Wizard-of-Oz functionality. We discuss how design teams can use our approach to reflect on prototypes or begin user studies within seconds, and how, over time, Apparition prototypes can become fully-implemented versions of the systems they simulate. Powering Apparition is the first self-coordinated, real-time crowdsourcing infrastructure. We anchor this infrastructure on a new, lightweight write-locking mechanism that workers can use to signal their intentions to each other.
Crowdsourcing Stereotypes: Linguistic Bias in Metadata Generated via GWAP.	Jahna Otterbacher	chi2015	Games with a Purpose (GWAP) is a popular approach for metadata creation, enabling institutions to collect descriptions of digital artifacts on a mass scale. Creating metadata is challenging not only because one must recognize the artifact; the description must then be encoded into natural language. Language behaviors are influenced by many social factors, particularly when we are asked to describe other people. We consider labels for images of people generated via the ESP Game. While ESP has been shown to produce relevant labels, critics claim they are obvious and stereotypical. Based on theories of linguistic biases, we examine whether there are systematic differences in the ways players describe images of men versus women. Our first analysis considers images of people generally, and reveals a tendency for women to be described with subjective adjectives. A second analysis compares images depicting men and women within each of six occupational roles. Images of women receive more labels related to appearance, whereas those depicting men receive more occupation-related labels. Our work exposes the presence of gender-based stereotypes through linguistic biases, illustrates the forms in which they manifest, and raises important implications for those who design systems or train algorithms using data produced via GWAP.
Exploring Subtle Foot Plantar-based Gestures with Sock-placed Pressure Sensors.	Koumei Fukahori, Daisuke Sakamoto, Takeo Igarashi	chi2015	We propose subtle foot-based gestures named foot plantar-based (FPB) gestures that are used with sock-placed pressure sensors. In this system, the user can control a computing device by changing his or her foot plantar distributions, e.g., pressing the floor with his/her toe. Because such foot movement is subtle, it is suitable for use especially in a public space such as a crowded train. In this study, we first conduct a guessability study to design a user-defined gesture set for interaction with a computing device. Then, we implement a gesture recognizer with a machine learning technique. To avoid unexpected gesture activations, we also collect foot plantar pressure patterns made during daily activities such as walking, as negative training data. Additionally, we evaluate the unobservability of FPB gestures by using crowdsourcing. Finally, we conclude with several applications to further illustrate the utility of FPB gestures.
Break It Down: A Comparison of Macro- and Microtasks.	Justin Cheng, Jaime Teevan, Shamsi T. Iqbal, Michael S. Bernstein	chi2015	A large, seemingly overwhelming task can sometimes be transformed into a set of smaller, more manageable microtasks that can each be accomplished independently. For example, it may be hard to subjectively rank a large set of photographs, but easy to sort them in spare moments by making many pairwise comparisons. In crowdsourcing systems, microtasking enables unskilled workers with limited commitment to work together to complete tasks they would not be able to do individually. We explore the costs and benefits of decomposing macrotasks into microtasks for three task categories: arithmetic, sorting, and transcription. We find that breaking these tasks into microtasks results in longer overall task completion times, but higher quality outcomes and a better experience that may be more resilient to interruptions. These results suggest that microtasks can help people complete high quality work in interruption-driven environments.
Designing for Citizen Data Analysis: A Cross-Sectional Case Study of a Multi-Domain Citizen Science Platform.	Ramine Tinati, Max Van Kleek, Elena Simperl, Markus Luczak-Rösch, Robert J. Simpson, Nigel Shadbolt	chi2015	Designing an effective and sustainable citizen science (CS)project requires consideration of a great number of factors. This makes the overall process unpredictable, even when a sound, user-centred design approach is followed by an experienced team of UX designers. Moreover, when such systems are deployed, the complexity of the resulting interactions challenges any attempt to generalisation from retrospective analysis. In this paper, we present a case study of the largest single platform of citizen driven data analysis projects to date, the Zooniverse. By eliciting, through structured reflection, experiences of core members of its design team, our grounded analysis yielded four sets of themes, focusing on Task Specificity, Community Development, Task Design and Public Relations and Engagement, supported by two-to-four specific design claims each. For each, we propose a set of design claims (DCs), drawing comparisons to the literature on crowdsourcing and online communities to contextualise our findings.
GAZE: Using Mobile Devices to Promote Discovery and Data Collection.	Zachary Allen	chi2015a	Developments in citizen science, community sensing, and crowdsourcing offer the opportunity of large scale data collection of the physical world because of the ubiquity of sensor-rich, mobile devices. Despite this opportunity, large-scale data collection about physical spaces is currently not widespread because of high-effort participation. In this paper, we explore the ability for people to contribute on the go. We developed Gaze, a system that will collect information about people's responses to physical spaces through low-effort feedback. To enable low-effort contributions for large scale data collection, we have developed a design pattern called Identify-Focus-Capture that identifies opportunities for users given current situational context, helps users to focus in on the opportunity, and captures useful data through simple actions or gestures. Through our pilot, users successfully helped collect 50+ data points about their environment, showing that useful data can be collected when the opportunity is low-effort.
Libero: On-the-go Crowdsourcing for Package Delivery.	Yongsung Kim	chi2015a	Throughout the world, millions of people walk, bike, and run the same routes at the same time, every day. This patterned, collective effort represents a potentially valuable yet underutilized resource for sensing, transporting goods, or completing small tasks that advance individual and societal goals. In this paper, we introduce a system called Libero, which utilizes people's existing routine for package delivery by incorporating just-in-time notifications in hopes of reducing task distance to an extreme (50 meters) and having a community support itself in doing simple tasks for one another. The results of preliminary studies show that just-in-time notifications helped promoting delivery, but other factors, such as reciprocity, community building, and social obligation were also important drivers for promoting participation.
CrowdFound: A Mobile Crowdsourcing System to Find Lost Items On-the-Go.	Emily Harburg, Yongsung Kim, Elizabeth Gerber, Haoqi Zhang	chi2015a	We present CrowdFound, a mobile crowdsourcing system to find lost items. CrowdFound allows users to input lost item descriptions on a map and then sends notifications to users passing near tagged areas. To assess the system's efficacy, we conducted interviews and user testing on CrowdFound. Our results show that users were able to find lost items when using a combination of the notification, map, and item description features. In addition, users were willing to deviate off path to look for lost items, particularly when exercising. Our findings also suggest socio-technical features to promote more effective on-the-go crowdsourced help on microtasks. This research builds our understanding of physical crowdsourcing as a tool for solving societal problems and suggests broader implications for utilizing mobile crowds.
Task Lockouts Induce Crowdworkers to Switch to Other Activities.	Sandy J. J. Gould, Anna Louise Cox, Duncan P. Brumby	chi2015a	Paid crowdsourcing has established itself as a useful way of getting work done. The availability of large, responsive pools of workers means that low quality work can often be treated as noise and dealt with through standard data processing techniques. This approach is not practical in all scenarios though, so efforts have been made to stop poor performance occurring by preventing satisficing behaviours that can compromise result quality. In this paper we test an intervention -- a task lockout -- designed to prevent satisficing behaviour in a simple data-entry task on Amazon Mechanical Turk. Our results show that workers are highly adaptable: when faced with the intervention they develop workaround strategies, allocating their time elsewhere during lockout periods. This suggests that more subtle techniques may be required to substantially influence worker behaviour.
Tag & Link: Supporting Regional and Relational Tagging in Images with Direct Annotation.	Hsing-Lin Tsai, Cheng-Hsien Han, En-Hsin Wu, Chi-Lan Yang, Hao-Chuan Wang	chi2015a	Image tagging is crucial to the use of digital photos and value-added applications. We present Tag & Link, a tagging interface featured direct annotations to collect fine-grained descriptions of image content that can be hard to obtain otherwise. Tag & Link provides a set of drawing tools, allowing users to directly specify regions of interest (ROIs), and the relations between different ROIs on an image. Through a crowdsourcing study, we show that Tag & Link enhances tagging productivity and helps collect annotations on small objects and behaviors embedded in images. We demonstrate the value of direct annotation on the elicitation of fine-grained human knowledge of image content.
The Price of the Priceless: Understanding Estimated Costs of Work in Friendsourcing.	Joey Chiao-Yin Hsiao, Mei-Hua Pan, Hao-Chuan Wang, Jane Yung-jen Hsu	chi2015a	Friendsourcing, or outsourcing tasks to one's online and offline friends, is increasingly common and versatile. As regular crowdsourcing, friendsourcing requesters needs to incentivize potential workers (i.e., friends) to actually engage and complete the requested tasks. However, it is unclear how to effectively motivate friendsourcing workers and what incentives, which may include both social and monetary ones, are considered feasible in friendsourcing, especially by taking social relations between requesters and workers as part of the calculation. In an exploratory study, we asked participants to report their estimations of feasible payment as a requester, and reward as a worker in friendsourcing. We compare the estimated costs of friendsourcing to regular crowdsourcing, and find that there exists a gap between requesters' and workers' expected costs. Individuals would like to pay more as a requester, and expect to receive less as a worker in friendsourcing. Consideration of social transaction and relationship maintenance is involved. We discuss the implications for designing friendsourcing systems.
ApplianceReader: A Wearable, Crowdsourced, Vision-based System to Make Appliances Accessible.	Anhong Guo, Xiang 'Anthony' Chen, Jeffrey P. Bigham	chi2015a	Visually impaired people can struggle to use everyday appliances with inaccessible control panels. To address this problem, we present ApplianceReader - a system that combines a wearable point-of-view camera with on-demand crowdsourcing and computer vision to make appliance interfaces accessible. ApplianceReader sends photos of appliance interfaces that it has not seen previously to the crowd, who work in parallel to quickly label and describe elements of the interface. Computer vision techniques then track the user's finger pointing at the controls and read out the labels previously provided by the crowd. This enables visually impaired users to interactively explore and use appliances without asking the crowd repetitively. ApplianceReader broadly demonstrates the potential of hybrid approaches that combine human and machine intelligence to effectively realize intelligent, interactive access technology today.
Gamifying Research: Strategies, Opportunities, Challenges, Ethics.	Sebastian Deterding, Alessandro Canossa, Casper Harteveld, Seth Cooper, Lennart E. Nacke, Jennifer R. Whitson	chi2015a	"From social sciences to biology and physics, gamified systems and games are increasingly being used as contexts and tools for research: as ""petri dishes"" for observing macro-social and economic dynamics; as sources of ""big"" and/or ecologically valid user behavior and health data; as crowdsourcing tools for research tasks; or as a means to motivate e.g. survey completion. However, this gamification of research comes with significant ethical ramifications. This workshop therefore explores opportunities, challenges, best practices, and ethical issues arising from different strategies of gamifying research."
Extracting references between text and charts via crowdsourcing.	Nicholas Kong, Marti A. Hearst, Maneesh Agrawala	chi2014	News articles, reports, blog posts and academic papers often include graphical charts that serve to visually reinforce arguments presented in the text. To help readers better understand the relation between the text and the chart, we present a crowdsourcing pipeline to extract the references between them. Specifically, we give crowd workers paragraph-chart pairs and ask them to select text phrases as well as the corresponding visual marks in the chart. We then apply automated clustering and merging techniques to unify the references generated by multiple workers into a single set. Comparing the crowdsourced references to a set of gold standard references using a distance measure based on the F1 score, we find that the average distance between the raw set of references produced by a single worker and the gold standard is 0.54 (out of a max of 1.0). When we apply clustering and merging techniques the average distance between the unified set of references and the gold standard reduces to 0.39; an improvement of 27%. We conclude with an interactive document viewing application that uses the extracted references; readers can select phrases in the text and the system highlights the related marks in the chart.
Effects of simultaneous and sequential work structures on distributed collaborative interdependent tasks.	Paul André, Robert E. Kraut, Aniket Kittur	chi2014	Distributed online groups have great potential for generating interdependent and complex products like encyclopedia articles or product design. However, coordinating multiple group members to work together effectively while minimizing process losses remains an open challenge. We conducted an experiment comparing the effectiveness of two coordination strategies (simultaneous vs. sequential work) on a complex creative task as the number of group members increased. Our results indicate that, contrary to prior work, a sequential work structure was more effective than a simultaneous work structure as the size of the group increased. A mediation analysis suggests that social processes such as territoriality partially accounts for these results. A follow up experiment giving workers specific roles mitigated the detrimental effects of the simultaneous work structure. These results have implications for small group theory and crowdsourcing research.
Searching for analogical ideas with crowds.	Lixiu Yu, Aniket Kittur, Robert E. Kraut	chi2014	Seeking solutions from one domain to solve problems in another is an effective process of innovation. This process of analogy searching is difficult for both humans and machines. In this paper, we present a novel approach for re-presenting a problem in terms of its abstract structure, and then allowing people to use this structural representation to find analogies. We propose a crowdsourcing process that helps people navigate a large dataset to find analogies. Through two experiments, we show the benefits of using abstract structural representations to search for ideas that are analogous to a source problem, and that these analogies result in better solutions than alternative approaches. This work provides a useful method for finding analogies, and can streamline innovation for both novices and professional designers.
Emergent, crowd-scale programming practice in the IDE.	Ethan Fast, Daniel Steffee, Lucy Wang, Joel R. Brandt, Michael S. Bernstein	chi2014	While emergent behaviors are uncodified across many domains such as programming and writing, interfaces need explicit rules to support users. We hypothesize that by codifying emergent programming behavior, software engineering interfaces can support a far broader set of developer needs. To explore this idea, we built Codex, a knowledge base that records common practice for the Ruby programming language by indexing over three million lines of popular code. Codex enables new data-driven interfaces for programming systems: statistical linting, identifying code that is unlikely to occur in practice and may constitute a bug; pattern annotation, automatically discovering common programming idioms and annotating them with metadata using expert crowdsourcing; and library generation, constructing a utility package that encapsulates and reflects emergent software practice. We evaluate these applications to find Codex captures a broad swatch of programming practice, statistical linting detects problematic code snippets, and pattern annotation discovers nontrivial idioms such as basic HTTP authentication and database migration templates. Our work suggests that operationalizing practice-driven knowledge in structured domains such as programming can enable a new class of user interfaces.
Towards crowd-based customer service: a mixed-initiative tool for managing Q&A sites.	Tiziano Piccardi, Gregorio Convertino, Massimo Zancanaro, Ji Wang, Cédric Archambeau	chi2014	In this paper, we propose a mixed-initiative approach to integrate a Q&A site based on a crowd of volunteers with a standard operator-based help desk, ensuring quality of customer service. Q&A sites have emerged as an efficient way to address questions in various domains by leveraging crowd knowledge. However, they lack sufficient reliability to be the sole basis of customer service applications. We built a proof-of-concept mixed-initiative tool that helps a crowd-manager to decide if a question will get a satisfactory and timely answer by the crowd or if it should be redirected to a dedicated operator. A user experiment found that our tool reduced the participants' cognitive load and improved their performance, in terms of their precision and recall. In particular, those with higher performance benefited more than those with lower performance.
Combining crowdsourcing and learning to improve engagement and performance.	Mira Dontcheva, Robert R. Morris, Joel R. Brandt, Elizabeth M. Gerber	chi2014	Crowdsourcing complex creative tasks remains difficult, in part because these tasks require skilled workers. Most crowdsourcing platforms do not help workers acquire the skills necessary to accomplish complex creative tasks. In this paper, we describe a platform that combines learning and crowdsourcing to benefit both the workers and the requesters. Workers gain new skills through interactive step-by-step tutorials and test their knowledge by improving real-world images submitted by requesters. In a series of three deployments spanning two years, we varied the design of our platform to enhance the learning experience and improve the quality of the crowd work. We tested our approach in the context of LevelUp for Photoshop, which teaches people how to do basic photograph improvement tasks using Adobe Photoshop. We found that by using our system workers gained new skills and produced high-quality edits for requested images, even if they had little prior experience editing images.
Twitch crowdsourcing: crowd contributions in short bursts of time.	Rajan Vaish, Keith Wyngarden, Jingshu Chen, Brandon Cheung, Michael S. Bernstein	chi2014	To lower the threshold to participation in crowdsourcing, we present twitch crowdsourcing: crowdsourcing via quick contributions that can be completed in one or two seconds. We introduce Twitch, a mobile phone application that asks users to make a micro-contribution each time they unlock their phone. Twitch takes advantage of the common habit of turning to the mobile phone in spare moments. Twitch crowdsourcing activities span goals such as authoring a census of local human activity, rating stock photos, and extracting structured data from Wikipedia pages. We report a field deployment of Twitch where 82 users made 11,240 crowdsourcing contributions as they used their phone in the course of everyday life. The median Twitch activity took just 1.6 seconds, incurring no statistically distinguishable costs to unlock speed or cognitive load compared to a standard slide-to-unlock interface.
Crowdsourcing the future: predictions made with a social network.	Clifton Forlines, Sarah Miller, Leslie Guelcher, Robert Bruzzi	chi2014	"Researchers have long known that aggregate estimations built from the collected opinions of a large group of people often outperform the estimations of individual experts. This phenomenon is generally described as the ""Wisdom of Crowds"". This approach has shown promise with respect to the task of accurately forecasting future events. Previous research has demonstrated the value of utilizing meta-forecasts (forecasts about what others in the group will predict) when aggregating group predictions. In this paper, we describe an extension to meta-forecasting and demonstrate the value of modeling the familiarity among a population's members (its social network) and applying this model to forecast aggregation. A pair of studies demonstrates the value of taking this model into account, and the described technique produces aggregate forecasts for future events that are significantly better than the standard Wisdom of Crowds approach as well as previous meta-forecasting techniques."
Cognitively inspired task design to improve user performance on crowdsourcing platforms.	Harini Alagarai Sampath, Rajeev Rajeshuni, Bipin Indurkhya	chi2014	Recent research in human computation has focused on improving the quality of work done by crowd workers on crowdsourcing platforms. Multiple approaches have been adopted like filtering crowd workers through qualification tasks, and aggregating responses from multiple crowd workers to obtain consensus. We investigate here how improving the presentation of the task itself by using cognitively inspired features affects the performance of crowd workers. We illustrate this with a case-study for the task of extracting text from scanned images. We generated six task-presentation designs by modifying two parameters - visual saliency of the target fields and working memory requirements - and conducted experiments on Amazon Mechanical Turk (AMT) and with an eye-tracker in the lab setting. Our results identify which task-design parameters (e.g. highlighting target fields) result in improved performance, and which ones do not (e.g. reducing the number of distractors). In conclusion, we claim that the use of cognitively inspired features for task design is a powerful technique for maximizing the performance of crowd workers.
Crowdsourcing step-by-step information extraction to enhance existing how-to videos.	Juho Kim, Phu Tran Nguyen, Sarah A. Weir, Philip J. Guo, Robert C. Miller, Krzysztof Z. Gajos	chi2014	Millions of learners today use how-to videos to master new skills in a variety of domains. But browsing such videos is often tedious and inefficient because video player interfaces are not optimized for the unique step-by-step structure of such videos. This research aims to improve the learning experience of existing how-to videos with step-by-step annotations. We first performed a formative study to verify that annotations are actually useful to learners. We created ToolScape, an interactive video player that displays step descriptions and intermediate result thumbnails in the video timeline. Learners in our study performed better and gained more self-efficacy using ToolScape versus a traditional video player. To add the needed step annotations to existing how-to videos at scale, we introduce a novel crowdsourcing workflow. It extracts step-by-step structure from an existing video, including step times, descriptions, and before and after images. We introduce the Find-Verify-Expand design pattern for temporal and visual annotation, which applies clustering, text processing, and visual analysis algorithms to merge crowd output. The workflow does not rely on domain-specific customization, works on top of existing videos, and recruits untrained crowd workers. We evaluated the workflow with Mechanical Turk, using 75 cooking, makeup, and Photoshop videos on YouTube. Results show that our workflow can extract steps with a quality comparable to that of trained annotators across all three domains with 77% precision and 81% recall.
BeFaced: a casual game to crowdsource facial expressions in the wild.	Chek Tien Tan, Hemanta Sapkota, Daniel Rosser	chi2014a	Creating good quality image databases for affective computing systems is key to most computer vision research, but is unfortunately costly and time-consuming. This paper describes BeFaced, a tile matching casual tablet game that enables massive crowdsourcing of facial expressions to advance facial expression analysis. BeFaced uses state-of-the-art facial expression tracking technology with dynamic difficulty adjustment to keep the player engaged and hence obtain a large and varied face dataset. CHI attendees will be able to experience a novel game interface that uses the iPad's front camera to track and capture facial expressions as the primary player input, and also investigate how the game design in general enables massive crowdsourcing in an extensible manner.
Building castles in quicksand: blueprint for a crowdsourced study.	Arne Renkema-Padmos, Melanie Volkamer, Karen Renaud	chi2014a	Finding participants for experiments has always been a challenge. As technology advanced, running experiments online became a viable way to carry out research that did not require anything more than a personal computer. The natural next step in this progression emerged as crowdsourcing became an option. We report on our experience of joining this new wave of practice, and the difficulties and challenges we encountered when crowdsourcing a study. This led us to re-evaluate the validity of crowdsourced research. We report our findings, and conclude with guidelines for crowdsourced experiments.
Design methods for the future that is now: have disruptive technologies disrupted our design methodologies?	Karen Holtzblatt, Ilpo Koskinen, Janaki Kumar, David B. Rondeau, John Zimmerman	chi2014a	"Responsive Design. Mobile First. Agile Development. Lean UX. Crowdsourcing. Machine Learning. In a world that has been disrupted by new technologies and approaches, what does it mean to do ""front-end"" or ""user-centered design""? Are we relying on methods that are too rooted in the past? What techniques are succeeding and what changes do we need to be making? Do we need different data collected, different styles of prototyping, different design principles, and ways of structuring products or apps when building across multiple platforms? How will any changes in UCD techniques be received by current professionals as well as product managers and developers? This panel explores whether user research and design methods as we know them need to be radically overhauled, or some even eliminated."
Crowdfunding: an emerging field of research.	Elizabeth M. Gerber, Michael J. Muller, Rick Wash, Lilly Irani, Amanda M. Williams, Elizabeth F. Churchill	chi2014a	Crowdfunding, the request of resources through social media, has generated much discussion in the popular press; however, there have been few systematic empirical studies of this growing phenomenon. We bring together the leading HCI researchers in crowdfunding and crowdsourcing to discuss this potentially transformative socio-technical innovation that may advance (or harm) human capabilities to innovate and collaborate. We will discuss current empirical research on crowdfunding and the future of research in this field from diverse perspectives including computer science, social science, communications, and design, using both qualitative and quantitative research methods. To make real progress towards realizing future research, we will lead a discussion with the audience of new research agendas in crowdfunding.
Playful science: deriving computer games from complex systems.	Reuben Kirkham, Jesse M. Blum, Michael A. Brown	chi2014a	We explore the possibility of converting computational models of real-world phenomena into computer games. Fusing the fields of computer games and complexity science enables us to not only directly educate the public about science, but also perform valuable scientific research through crowdsourcing whilst introducing genuinely innovative gaming experiences. We highlight the natural overlap between these concerns, before offering our vision as to how to take this forwards as a cohesive research agenda.
MOODs: building massive open online diaries for researchers, teachers and contributors.	Sandy J. J. Gould, Dominic Furniss, Charlene Ianthe Jennett, Sarah Wiseman, Ioanna Iacovides, Anna Louise Cox	chi2014a	Internet-based research conducted in partnership with paid crowdworkers and volunteer citizen scientists is an increasingly common method for collecting data from large, diverse populations. We wanted to leverage web-based citizen science to gain insights into phenomena that are part of people's everyday lives. To do this, we developed the concept of a Massive Open Online Diary (MOOD). A MOOD is a tool for capturing, storing and presenting short updates from multiple contributors on a particular topic. These updates are aggregated into public corpora that can be viewed, analysed and shared. MOODs offer a novel method for crowdsourcing diary-like data in a way that provides value for researchers, teachers and contributors. MOODs also come with unique community-building and ethical challenges. We describe the benefits and challenges of MOODs in relation to Errordiary.org, a MOOD we created to aid our exploration of human error.
Selfsourcing personal tasks.	Jaime Teevan, Daniel J. Liebling, Walter S. Lasecki	chi2014a	Large tasks can be overwhelming. For example, many people have thousands of digital photographs that languish in unorganized archives because it is difficult and time consuming to gather them into meaningful collections. Such tasks are hard to start because they seem to require long uninterrupted periods of effort to make meaningful progress. We propose the idea of selfsourcing as a way to help people to perform large personal information tasks by breaking them into manageable microtasks. Using ideas from crowdsourcing and task management, selfsourcing can help people take advantage of existing gaps in time and recover quickly from interruptions. We present several achievable selfsourcing scenarios and explore how they can facilitate information work in interruption-driven environments.
Who's the boss?: requester transparency and motivation in a microtask marketplace.	Jennifer Marlow, Laura A. Dabbish	chi2014a	Workers on crowdsourcing platforms such as Amazon's Mechanical Turk often receive little to no information about who is requesting the task they are asked to perform. This can lead to psychological distance and reduced work quality as a result. In this study we varied the amount and type of information provided about the requester of a task on Mechanical Turk (no information, text only, or text and picture) to see its effect on the amount of work completed. We found that providing information about the work requester led to increased effort, with further differences in effects depending on the location of the task participants.
Form digitization in BPO: from outsourcing to crowdsourcing?	Jacki O'Neill, Shourya Roy, Antonietta Grasso, David B. Martin	chi2013	This paper describes an ethnographic study of an outsourced business process - the digitization of healthcare forms. The aim of the study was to understand how the work is currently organized, with an eye to uncovering the research challenges which need to be addressed if that work is to be crowdsourced. The findings are organised under four emergent themes: Workplace Ecology, Data Entry Skills and Knowledge, Achieving Targets and Collaborative Working. For each theme a description of how the work is undertaken in the outsourcer's Indian office locations is given, followed by the implications for crowdsourcing that work. This research is a first step in understanding how crowdsourcing might be applied to BPO activities. The paper examines features specific to form digitization - extreme distribution and form decomposition - and lightly touches on the crowdsourcing of BPO work more generally.
Crowdsourcing performance evaluations of user interfaces.	Steven Komarov, Katharina Reinecke, Krzysztof Z. Gajos	chi2013	Online labor markets, such as Amazon's Mechanical Turk (MTurk), provide an attractive platform for conducting human subjects experiments because the relative ease of recruitment, low cost, and a diverse pool of potential participants enable larger-scale experimentation and faster experimental revision cycle compared to lab-based settings. However, because the experimenter gives up the direct control over the participants' environments and behavior, concerns about the quality of the data collected in online settings are pervasive. In this paper, we investigate the feasibility of conducting online performance evaluations of user interfaces with anonymous, unsupervised, paid participants recruited via MTurk. We implemented three performance experiments to re-evaluate three previously well-studied user interface designs. We conducted each experiment both in lab and online with participants recruited via MTurk. The analysis of our results did not yield any evidence of significant or substantial differences in the data collected in the two settings: All statistically significant differences detected in lab were also present on MTurk and the effect sizes were similar. In addition, there were no significant differences between the two settings in the raw task completion times, error rates, consistency, or the rates of utilization of the novel interaction mechanisms introduced in the experiments. These results suggest that MTurk may be a productive setting for conducting performance evaluations of user interfaces providing a complementary approach to existing methodologies.
A pilot study of using crowds in the classroom.	Steven Dow, Elizabeth Gerber, Audris Wong	chi2013	Industry relies on higher education to prepare students for careers in innovation. Fulfilling this obligation is especially difficult in classroom settings, which often lack authentic interaction with the outside world. Online crowdsourcing has the potential to change this. Our research explores if and how online crowds can support student learning in the classroom. We explore how scalable, diverse, immediate (and often ambiguous and conflicting) input from online crowds affects student learning and motivation for project-based innovation work. In a pilot study with three classrooms, we explore interactions with the crowd at four key stages of the innovation process: needfinding, ideating, testing, and pitching. Students reported that online crowds helped them quickly and inexpensively identify needs and uncover issues with early-stage prototypes, although they favored face-to-face interactions for more contextual feed-back. We share early evidence and discuss implications for creating a socio-technical infrastructure to more effectively use crowdsourcing in education.
Using crowdsourcing to support pro-environmental community activism.	Elaine Massung, David Coyle, Kirsten F. Cater, Marc Jay, Chris Preist	chi2013	Community activist groups typically rely on core groups of highly motivated members. In this paper we consider how crowdsourcing strategies can be used to supplement the activities of pro-environmental community activists, thus increasing the scalability of their campaigns. We focus on mobile data collection applications and strategies that can be used to engage casual participants in pro-environmental data collection. We report the results of a study that used both quantitative and qualitative methods to investigate the impact of different motivational factors and strategies, including both intrinsic and extrinsic motivators. The study compared and provides empirical evidence for the effectiveness of two extrinsic motivation strategies, pointification - a subset of gamification - and financial incentives. Prior environmental interest is also assessed as an intrinsic motivation factor. In contrast to previous HCI research on pro-environmental technology, much of which has focused on individual behavior change, this paper offers new insights and recommendations on the design of systems that target groups and communities.
Don't hide in the crowd!: increasing social transparency between peer workers improves crowdsourcing outcomes.	Shih-Wen Huang, Wai-Tat Fu	chi2013	This paper studied how social transparency and different peer-dependent reward schemes (i.e., individual, teamwork, and competition) affect the outcomes of crowdsourcing. The results showed that when social transparency was increased by asking otherwise anonymous workers to share their demographic information (e.g., name, nationality) to the paired worker, they performed significantly better. A more detailed analysis showed that in a teamwork reward scheme, in which the reward of the paired workers depended only on the collective outcomes, increasing social transparency could offset effects of social loafing by making them more accountable to their teammates. In a competition reward scheme, in which workers competed against each other and the reward depended on how much they outperformed their opponent, increasing social transparency could augment effects of social facilitation by providing more incentives for them to outperform their opponent. The results suggested that a careful combination of methods that increase social transparency and different reward schemes can significantly improve crowdsourcing outcomes.
Combining crowdsourcing and google street view to identify street-level accessibility problems.	Kotaro Hara, Vicki Le, Jon Froehlich	chi2013	Poorly maintained sidewalks, missing curb ramps, and other obstacles pose considerable accessibility challenges; however, there are currently few, if any, mechanisms to determine accessible areas of a city a priori. In this paper, we investigate the feasibility of using untrained crowd workers from Amazon Mechanical Turk (turkers) to find, label, and assess sidewalk accessibility problems in Google Street View imagery. We report on two studies: Study 1 examines the feasibility of this labeling task with six dedicated labelers including three wheelchair users; Study 2 investigates the comparative performance of turkers. In all, we collected 13,379 labels and 19,189 verification labels from a total of 402 turkers. We show that turkers are capable of determining the presence of an accessibility problem with 81% accuracy. With simple quality control methods, this number increases to 93%. Our work demonstrates a promising new, highly scalable method for acquiring knowledge about sidewalk accessibility.
Labor dynamics in a mobile micro-task market.	Mohamed Musthag, Deepak Ganesan	chi2013	The ubiquity of smartphones has led to the emergence of mobile crowdsourcing markets, where smartphone users participate to perform tasks in the physical world. Mobile crowdsourcing markets are uniquely different from their online counterparts in that they require spatial mobility, and are therefore impacted by geographic factors and constraints that are not present in the online case. Despite the emergence and importance of such mobile marketplaces, little to none is known about the labor dynamics and mobility patterns of agents. This paper provides an in-depth exploration of labor dynamics in mobile task markets based on a year-long dataset from a leading mobile crowdsourcing platform. We find that a small core group of workers (< 10%) account for a disproportionately large proportion of activity (> 80%) generated in the market. We find that these super agents are more efficient than other agents across several dimensions: a) they are willing to move longer distances to perform tasks, yet they amortize travel across more tasks, b) they work and search for tasks more efficiently, c) they have higher data quality in terms of accepted submissions, and d) they improve in almost all of these efficiency measures over time. We find that super agent efficiency stems from two simple optimizations --- they are 3x more likely than other agents to chain tasks and they pick fewer lower priced tasks than other agents. We compare mobile and online micro-task markets, and discuss differences in demographics, data quality, and time of use, as well as similarities in super agent behavior. We conclude with a discussion of how a mobile micro-task market might leverage some of our results to improve performance.
Cascade: crowdsourcing taxonomy creation.	Lydia B. Chilton, Greg Little, Darren Edge, Daniel S. Weld, James A. Landay	chi2013	Taxonomies are a useful and ubiquitous way of organizing information. However, creating organizational hierarchies is difficult because the process requires a global understanding of the objects to be categorized. Usually one is created by an individual or a small group of people working together for hours or even days. Unfortunately, this centralized approach does not work well for the large, quickly changing datasets found on the web. Cascade is an automated workflow that allows crowd workers to spend as little at 20 seconds each while collectively making a taxonomy. We evaluate Cascade and show that on three datasets its quality is 80-90% of that of experts. Cascade has a competitive cost to expert information architects, despite taking six times more human labor. Fortunately, this labor can be parallelized such that Cascade will run in as fast as four minutes instead of hours or days.
Warping time for more effective real-time crowdsourcing.	Walter S. Lasecki, Christopher D. Miller, Jeffrey P. Bigham	chi2013	"In this paper, we introduce the idea of ""warping time"" to improve crowd performance on the difficult task of captioning speech in real-time. Prior work has shown that the crowd can collectively caption speech in real-time by merging the partial results of multiple workers. Because non-expert workers cannot keep up with natural speaking rates, the task is frustrating and prone to errors as workers buffer what they hear to type later. The TimeWarp approach automatically increases and decreases the speed of speech playback systematically across individual workers who caption only the periods played at reduced speed. Studies with 139 remote crowd workers and 24 local participants show that this approach improves median coverage (14.8%), precision (11.2%), and per-word latency (19.1%). Warping time may also help crowds outperform individuals on other difficult real-time performance tasks."
CrowdUtility: know the crowd that works for you.	Koustuv Dasgupta, Vaibhav Rajan, Saraschandra Karanam, Kovendhan Ponnavaikko, Chithralekha Balamurugan, Nischal M. Piratla	chi2013a	Crowdsourcing platforms aim to leverage the collective intelligence of a largely distributed Internet workforce to solve a wide range of tasks. Crowd workers (unlike in a typical organization), exhibit varying work patterns, expertise, and performance - with little or no control that can be imposed on them. Requesters (e.g. enterprises) also exhibit diverse requirements in terms of the size, complexity and timings of the tasks, as well as SLAs (performance expectations). Clearly, the heterogeneity makes the choice of a platform suited for a given task difficult for the user. This paper highlights this problem and proposes CrowdUtility - a first-of-a-kind statistical machine learning approach, which models the dynamic behavioral characteristics of crowdsourcing platforms and uses them to recommend the best platform for the enterprise task(s). Initial results from real-world experiments suggest that the proposed system provides an attractive solution to this erstwhile unsolved problem
Generating annotations for how-to videos using crowdsourcing.	Phu Tran Nguyen, Juho Kim, Robert C. Miller	chi2013a	How-to videos can be valuable for learning, but searching for and following along with them can be difficult. Having labeled events such as the tools used in how-to videos could improve video indexing, searching, and browsing. We introduce a crowdsourcing annotation tool for Photoshop how-to videos with a three-stage method that consists of: (1) gathering timestamps of important events, (2) labeling each event, and (3) capturing how each event affects the task of the tutorial. Our ultimate goal is to generalize our method to be applied to other domains of how-to videos. We evaluate our annotation tool with Amazon Mechanical Turk workers to investigate the accuracy, costs, and feasibility of our three-stage method for annotating large numbers of video tutorials. Improvements can be made for stages 1 and 3, but stage 2 produces accurate labels over 90% of the time using majority voting. We have observed that changes in the instructions and interfaces of each task can improve the accuracy of the results significantly.
Mixsourcing: a remix framework as a form of crowdsourcing.	Sarah Hallacher, Jenny Rodenhouse, Andrés Monroy-Hernández	chi2013a	In this paper, we introduce the concept of mixsourcing as a modality of crowdsourcing focused on using remixing as a framework to get people to perform creative tasks. We explore this idea through the design of a system that helped us identify the promises and challenges of this peer-production modality.
Crowdsourcing as a method for indexing digital media.	Seyong Ha, Dongwhan Kim, Joonhwan Lee	chi2013a	As people spend more time online, watching YouTube or playing games, a number of research studies arose in ways to make use of the time and energy from the crowd in doing such activities. In this paper, we have explored the possibility of converting the collective resources from the crowd in making useful information back to people. We collected posts from the online forums about soap operas on the air, and extracted instances when the name of characters in the play has been mentioned. These crowdsourced indexes become good search keywords to find the scenes where the characters mentioned in the posts appear.
Exploring augmented reality for user-generated hyperlocal news content.	Heli Väätäjä, Mari J. Ahvenainen, Markus S. Jaakola, Thomas Olsson	chi2013a	To support and enrich crowdsourcing, sharing and consuming of hyperlocal news content created by the readers we created four scenarios that utilize augmented reality (AR). We used the scenarios to gain an initial understanding of the feasibility and utility of AR in this context by interviewing five contributors of hyperlocal news content to explore their perceptions and further ideas on the scenarios. Findings indicate that AR is an interesting and acceptable solution for the content creators: AR can be applied in hyperlocal news 1) in the crowdsourcing processes to support crowdworkers' activity and its planning and 2) for sharing and consuming location-based user-generated content. In implementation of AR solutions, attention needs to be paid to the interestingness of tasks and created content, ensuring the quality of the content as well as how interaction with the AR application and access to the information and content are implemented.
Do games attract or sustain engagement in citizen science?: a study of volunteer motivations.	Ioanna Iacovides, Charlene Jennett, Cassandra Cornish-Trestrail, Anna Louise Cox	chi2013a	Increasingly, games are being incorporated into online citizen science (CS) projects as a way of crowdsourcing data; yet the influence of gamification on volunteer motivations and engagement in CS projects is still unknown. In an interview study with 8 CS volunteers (4 from Foldit, 4 from Eyewire), we found that game elements and communication tools are not necessary for attracting new volunteers to a project; however they may help to sustain engagement over time, by allowing volunteers to participate in a range of social interactions and through enabling meaningful recognition of achievements.
CredibleWeb: a platform for web credibility evaluation.	Zhicong Huang, Alexandra Olteanu, Karl Aberer	chi2013a	The web content is the main source of information for many users. However, due to the open nature of today's web anyone can produce and publish content, which, as a result, is not always reliable. As such, mechanisms to evaluate the web content credibility are needed. In this paper, we describe CredibleWeb, a prototype crowdsourcing platform for web content evaluation with a two-fold goal: (1) to build a social enhanced and large scale dataset of credibility labeled web pages that enables the evaluation of different strategies for web credibility prediction, and (2) to investigate how various design elements are useful in engaging users to actively evaluate web pages credibility. We outline the challenges related with the design of a crowdsourcing platform for web credibility evaluation and describe our initial efforts.
Maater: crowdsourcing to improve online journalism.	Raymond Liaw, Ari Zilnik, Mark Baldwin, Stephanie Butler	chi2013a	A system that acts as a tool to correct inaccuracies and biases in online news articles is needed to alleviate the flow of misinformation perpetuated by the fast paced nature of the Internet. We propose Maater, which counteracts these issues by leveraging crowdsourced corrections and fact checking to help other readers engaged with a particular article better understand it. The system incorporates user-generated in-line commentary and corrections, which are vetted by other readers through a ranking system. Highly ranked comments gain more social value and are prominently displayed. This provides corrections with greater prominence than they are given by news outlets.
Xpress: crowdsourcing native speakers to learn colloquial expressions in a second language.	Yung-Ju Chang, Lezhong Li, Shih-Hsuan Chou, Min-Chih Liu, Surong Ruan	chi2013a	"Many second language (SL) learners want to speak fluently to native speakers. However, formal language education and existing tools are insufficient for learning language for use in daily life. We propose Xpress, a mobile Q&A-based system that crowdsources native speakers to provide everyday expressions to SL learners. To enable native speakers to understand SL learners' questions, Xpress allows SL learners to compose ""context-embedded"" questions with the help of multimedia. In addition, Xpress allows SL learners to explore expressions broadly and search for topic-specific expressions. Finally, Xpress provides several facilities to help effective learning of expressions. The results of our study confirm the above design idea and show Xpress' potential to help SL learners effectively learn colloquial expressions."
Multiverse: crowd algorithms on existing interfaces.	Kyle I. Murray	chi2013a	Crowd-powered systems implement crowd algorithms to improve crowd work through techniques like redundancy, iteration, and task decomposition. Existing approaches require substantial programming to package tasks for the crowd and apply crowd algorithms. We introduce Multiverse, a system that allows crowd algorithms to be applied to existing interfaces, reducing one-off programming effort and potentially allowing end users to directly employ crowdsourcing on the interfaces they care about. Multiverse encapsulates existing applications into cloneable virtual machines (VMs) that crowd workers control remotely. Because task state is captured in the VM, multiple workers can operate simultaneously on separate instances. We demonstrate the utility of this approach by implementing three existing crowd algorithms: (i) branch-and-vote, (ii) find-fix-verify, and (iii) partition-map-reduce. To implement these we introduce new crowd programming patterns: crowd merge and crowd annotate.
Mixsourcing: turn this into that.	Sarah Hallacher, Jenny Rodenhouse, Andrés Monroy-Hernández	chi2013a	We introduce the concept of mixsourcing as a modality of crowdsourcing focused on using remixing as a framework to get people to perform creative tasks. We explore this idea through the design of a web based system called Turn This into That which combines the structure of task-driven crowdsourcing systems with the free-form creativity of remixing; encouraging individual interpretation, multiple mediums, and connecting existing sources and remixers.
Art mapping in Paris.	Laura Carletti, Dominic Price, Rebecca Sinker, Gabriella Giannachi, Derek McAuley, John Stack, Kirstie Beaver, Jennifer Mundy	chi2013a	In this work, we describe a proposed technology demonstrator for Art Maps, a collaborative research project exploring the relation between artworks and the location that they depict, through the support of a cloud-based crowdsourcing platform with web and mobile interfaces. The Art Maps demonstration entails two types of hands-on experiences for the conference attendees: an in-CHI-experience and an optional bespoke outdoor activity to experience Paris through Art Maps.
Geographic human-computer interaction.	Brent J. Hecht, Johannes Schöning, Muki Haklay, Licia Capra, Afra J. Mashhadi, Loren G. Terveen, Mei-Po Kwan	chi2013a	Geography is playing an increasingly important role in areas of HCI ranging from social computing to natural user interfaces. At the same time, research in geography has focused more and more on technology-mediated interaction with spatiotemporal phenomena. Despite the growing popularity of this geographic human-computer interaction (GeoHCI) in both fields, there have been few opportunities for GeoHCI knowledge sharing, knowledge creation or community building in either discipline, let alone between them. The goal of this workshop is thus two-fold. First, we will seek to sum up the state of GeoHCI knowledge and address GeoHCI core issues by inviting prominent researchers in the space to share and discuss the most important high-level findings from their work. Second, through our interdisciplinary organizing committee, we will recruit participants from both fields, with the goal of laying the groundwork for a community that works across intra- and interdisciplinary boundaries.
Human computation tasks with global constraints.	Haoqi Zhang, Edith Law, Rob Miller, Krzysztof Gajos, David C. Parkes, Eric Horvitz	chi2012	An important class of tasks that are underexplored in current human computation systems are complex tasks with global constraints. One example of such a task is itinerary planning, where solutions consist of a sequence of activities that meet requirements specified by the requester. In this paper, we focus on the crowdsourcing of such plans as a case study of constraint-based human computation tasks and introduce a collaborative planning system called Mobi that illustrates a novel crowdware paradigm. Mobi presents a single interface that enables crowd participants to view the current solution context and make appropriate contributions based on current needs. We conduct experiments that explain how Mobi enables a crowd to effectively and collaboratively resolve global constraints, and discuss how the design principles behind Mobi can more generally facilitate a crowd to tackle problems involving global constraints.
Strategies for crowdsourcing social data analysis.	Wesley Willett, Jeffrey Heer, Maneesh Agrawala	chi2012	"Web-based social data analysis tools that rely on public discussion to produce hypotheses or explanations of the patterns and trends in data, rarely yield high-quality results in practice. Crowdsourcing offers an alternative approach in which an analyst pays workers to generate such explanations. Yet, asking workers with varying skills, backgrounds and motivations to simply ""Explain why a chart is interesting"" can result in irrelevant, unclear or speculative explanations of variable quality. To address these problems, we contribute seven strategies for improving the quality and diversity of worker-generated explanations. Our experiments show that using (S1) feature-oriented prompts, providing (S2) good examples, and including (S3) reference gathering, (S4) chart reading, and (S5) annotation subtasks increases the quality of responses by 28% for US workers and 196% for non-US workers. Feature-oriented prompts improve explanation quality by 69% to 236% depending on the prompt. We also show that (S6) pre-annotating charts can focus workers' attention on relevant details, and demonstrate that (S7) generating explanations iteratively increases explanation diversity without increasing worker attrition. We used our techniques to generate 910 explanations for 16 datasets, and found that 63% were of high quality. These results demonstrate that paid crowd workers can reliably generate diverse, high-quality explanations that support the analysis of specific datasets."
Direct answers for search queries in the long tail.	Michael S. Bernstein, Jaime Teevan, Susan T. Dumais, Daniel J. Liebling, Eric Horvitz	chi2012	Web search engines now offer more than ranked results. Queries on topics like weather, definitions, and movies may return inline results called answers that can resolve a searcher's information need without any additional interaction. Despite the usefulness of answers, they are limited to popular needs because each answer type is manually authored. To extend the reach of answers to thousands of new information needs, we introduce Tail Answers: a large collection of direct answers that are unpopular individually, but together address a large proportion of search traffic. These answers cover long-tail needs such as the average body temperature for a dog, substitutes for molasses, and the keyboard shortcut for a right-click. We introduce a combination of search log mining and paid crowdsourcing techniques to create Tail Answers. A user study with 361 participants suggests that Tail Answers significantly improved users' subjective ratings of search quality and their ability to solve needs without clicking through to a result. Our findings suggest that search engines can be extended to directly respond to a large new class of queries.
CommunitySourcing: engaging local crowds to perform expert work via physical kiosks.	Kurtis Heimerl, Brian Gawalt, Kuang Chen, Tapan S. Parikh, Björn Hartmann	chi2012	Online labor markets, such as Amazon's Mechanical Turk, have been used to crowdsource simple, short tasks like image labeling and transcription. However, expert knowledge is often lacking in such markets, making it impossible to complete certain classes of tasks. In this work we introduce an alternative mechanism for crowdsourcing tasks that require specialized knowledge or skill: communitysourcing --- the use of physical kiosks to elicit work from specific populations. We investigate the potential of communitysourcing by designing, implementing and evaluating Umati: the communitysourcing vending machine. Umati allows users to earn credits by performing tasks using a touchscreen attached to the machine. Physical rewards (in this case, snacks) are dispensed through traditional vending mechanics. We evaluated whether communitysourcing can accomplish expert work by using Umati to grade Computer Science exams. We placed Umati in a university Computer Science building, targeting students with grading tasks for snacks. Over one week, 328 unique users (302 of whom were students) completed 7771 tasks (7240 by students). 80% of users had never participated in a crowdsourcing market before. We found that Umati was able to grade exams with 2% higher accuracy (at the same price) or at 33% lower cost (at equivalent accuracy) than traditional single-expert grading. Mechanical Turk workers had no success grading the same exams. These results indicate that communitysourcing can successfully elicit high-quality expert work from specific communities.
mClerk: enabling mobile crowdsourcing in developing regions.	Aakar Gupta, William Thies, Edward Cutrell, Ravin Balakrishnan	chi2012	Global crowdsourcing platforms could offer new employment opportunities to low-income workers in developing countries. However, the impact to date has been limited because poor communities usually lack access to computers and the Internet. This paper presents mClerk, a new platform for mobile crowdsourcing in developing regions. mClerk sends and receives tasks via SMS, making it accessible to anyone with a low-end mobile phone. However, mClerk is not limited to text: it leverages a little-known protocol to send small images via ordinary SMS, enabling novel distribution of graphical tasks. Via a 5-week deployment in semi-urban India, we demonstrate that mClerk is effective for digitizing local-language documents. Usage of mClerk spread virally from 10 users to 239 users, who digitized over 25,000 words during the study. We discuss the social ecosystem surrounding this usage, and evaluate the potential of mobile crowdsourcing to both deliver and derive value from users in developing regions.
Taming wild behavior: the input observer for text entry and mouse pointing measures from everyday computer use.	Abigail Evans, Jacob O. Wobbrock	chi2012	"We present the Input Observer, a tool that can run quietly in the background of users' computers and measure their text entry and mouse pointing performance from everyday use. In lab studies, participants are presented with prescribed tasks, enabling easy identification of speeds and errors. In everyday use, no such prescriptions exist. We devised novel algorithms to segment text entry and mouse pointing input streams into ""trials"". We are the first to measure errors for unprescribed text entry and mouse pointing. To measure errors, we utilize web search engines, adaptive offline dictionaries, an Automation API, and crowdsourcing. Capturing errors allows us to employ Crossman's (1957) speed-accuracy normalization when calculating Fitts' law throughputs. To validate the Input Observer, we compared its measures from 12 participants over a week of computer use to the same participants' results from a lab study. Overall, in the lab and field, average text entry speeds were 74.47 WPM and 80.59 WPM, respectively. Average uncorrected error rates were near zero, at 0.12% and 0.28%. For mouse pointing, average movement times were 971 ms and 870 ms. Average pointing error rates were 4.42% and 4.66%. Average throughputs were 3.48 bits/s and 3.45 bits/s. Device makers, researchers, and assistive technology specialists may benefit from measures of everyday use."
Understanding mobile Q&A usage: an exploratory study.	Uichin Lee, Hyanghong Kang, Eunhee Yi, Mun Yi, Jussi Kantola	chi2012	Recently questioning and answering (Q&A) communities that facilitate knowledge sharing among people have been introduced to the mobile environments such as Naver Mobile Q&A and ChaCha. These mobile Q&A services are very different from traditional Q&A sites in that questions/answers are short in length and are exchanged via mobile devices (e.g., SMS or mobile Internet). While traditional Q&A sites have been well investigated, so far little is known about the mobile Q&A usage. To understand mobile Q&A usage, we analyzed 2.4 million question/answer pairs spanning a 14 month period from Naver Mobile Q&A and performed a complementary survey study of 555 active mobile Q&A users. We find that mobile Q&A is deeply wired into users' everyday life activities - its usage is largely dependent on users' spatial, temporal, and social contexts; the key factors of mobile Q&A usage are accessibility/convenience of mobile Q&A, promptness of receiving answers, and users' satisficing behavior of information seeking (i.e., minimizing efforts and settling with good enough information). We also observe that users tend to seek more factual information attributed to everyday life activities than they do on traditional Q&A sites and that they exhibit unique interaction patterns such as repeating and refining questions as coping strategies in seeking information needs. Our main findings reported in the paper have significant implications on the design of mobile Q&A systems.
Massively distributed authorship of academic papers.	Bill Tomlinson, Joel Ross, Paul André, Eric P. S. Baumer, Donald J. Patterson, Joseph Corneli, Martin Mahaux, Syavash Nobarany, Marco Lazzari, Birgit Penzenstadler, Andrew Torrance, David Callele, Gary M. Olson, M. Six Silberman, Marcus Ständer, Fabio Romancini Palamedi, Albert Ali Salah, Eric Morrill, Xavier Franch, Florian Mueller, Joseph Kaye, Rebecca W. Black, Marisa Leavitt Cohn, Patrick C. Shih, Johanna Brewer, Nitesh Goyal, Pirjo Näkki, Jeff Huang, Nilufar Baghaei, Craig Saper	chi2012a	Wiki-like or crowdsourcing models of collaboration can provide a number of benefits to academic work. These techniques may engage expertise from different disciplines, and potentially increase productivity. This paper presents a model of massively distributed collaborative authorship of academic papers. This model, developed by a collective of thirty authors, identifies key tools and techniques that would be necessary or useful to the writing process. The process of collaboratively writing this paper was used to discover, negotiate, and document issues in massively authored scholarship. Our work provides the first extensive discussion of the experiential aspects of large-scale collaborative research.
User-driven collaborative intelligence: social networks as crowdsourcing ecosystems.	Zann Gill	chi2012a	"Vernor Vinge proposed, """"In network and interface research there is something as profound (and potentially wild) as Artificial Intelligence."""" How, in this 2012 Centenary of Alan Turing, can we explore that wild CHI opportunity to create futures of intelligence? User experience data can co-evolve synergies across computer data processing and human capacity for pattern recognition, developing collaborative intelligence applications that engage distributed creativity, processing crowd-sourced analytics to plan and track projects, so that data gathered, bottom-up, can improve decision-making."
Crowdsourcing an emotional wardrobe.	Lucy Hughes, Douglas Atkinson, Nadia Berthouze, Sharon Baurley	chi2012a	Selecting clothing online requires decision-making about sensorial experiences, but online environments provide only limited sensorial information. Inferences are therefore made on the basis of product pictures and their textual description. This is often unreliable as it is either based on the designer's understanding of the product or deprived of perceptual content due to the difficulty of expressing such experiences. Using a purpose built website that combines and cross references multi-modal descriptive media, this study aims at investigating the possibility of using crowdsourcing mechanisms and multi-modal language to engage consumers in providing enriched descriptions of their tactile experiences of garments.
Motion chain: a webcam game for crowdsourcing gesture collection.	Ian Spiro	chi2012a	This paper describes the development and preliminary design of a game with a purpose that attempts to build a corpus of useful and original videos of human motion. This content is intended for use in applications of machine learning and computer vision. The game, Motion Chain, encourages users to respond to text and video prompts by recording videos with a web camera. The game seeks to entertain not through an explicit achievement or point system but through the fun of performance and the discovery inherent in observing other players. This paper describes two specific forms of the game, Chains and Charades, and proposes future possibilities. The paper describes the phases of game design as well as implementation details then discusses an approach for evaluating the game's effectiveness.
Socialproof: using crowdsourcing for correcting errors to improve speech based dictation experiences.	Shaojian Zhu	chi2012a	Though various Automatic Speech Recognition (ASR) based text correction techniques have been proposed, it is still difficult to correct dictation errors using speech based commands. Inspired by the successful use of crowdsourcing to solve computation tasks, we propose SocialProof, a crowdsourcing powered ASR dictation enhancement, to provide a powerful and accurate but fairly cheap ASR dictation system. SocialProof begins with the output produced by ASR engines and enhances this output using the power of crowd intelligence via MTurk service. Our system splits one ASR dictation scenario into several smaller tasks, allowing multiple people to work on different pieces of the task at the same time. Data merging strategies are used to combine multiple responses from MTurk workers to provide improved results. An evaluation of SocialProof strongly supports the effectiveness of this approach.
A crowdsourcing quality control model for tasks distributed in parallel.	Shaojian Zhu, Shaun K. Kane, Jinjuan Feng, Andrew Sears	chi2012a	Quality control for crowdsourcing systems has been identified as a significant challenge [2]. We propose a data-driven model for quality control in the context of crowdsourcing systems with the goal of assessing the quality of each individual contribution for parallel distributed tasks (allowing multiple people working on a same task). The model is initiated with a data training process providing a rough estimate for several quality-related performance measures (e.g. time spent on a task). The initial estimates are combined with observations of results produced by workers to estimate the quality for each individual contribution. We conduct a study to evaluate the model in the context of improving speech recognition-based text correction using MTurk services. Results indicate that the model accurately predicts quality for more than 92% of the non-negative (useful) contributions and 96% of the negative (useless) ones.
Using real-time feedback to improve visual question answering.	Yu Zhong, Phyo Thiha, Grant He, Walter S. Lasecki, Jeffrey P. Bigham	chi2012a	Technology holds great promise for improving the everyday lives of people with disabilities; however, automated systems are prone to errors and cannot handle many real-world tasks. VizWiz, a system for answering visual questions for blind users, has shown that crowdsourcing can be used for assistive technology in such domains. Our work extends the VizWiz model by enabling users to interact with the crowd via a real-time feedback loop. We introduce Legion:View, a system that enables such a real-time feedback loop for visual questions between users and crowd workers. Legion:View sends audio questions and streaming video to the crowd, and forwards feedback about the position and orientation of the camera and answers to questions back to users.
Self-correcting crowds.	Walter S. Lasecki, Jeffrey P. Bigham	chi2012a	Much of the current work in crowdsourcing is focused on increasing the quality of responses. Quality issues are most often due to a small subset of low quality workers. The ability to distinguish between high and low quality workers would allow a wide range of error correction to be performed for such tasks. However, differentiating between these types is difficult when no measure of individual success is available. We propose it is possible to use higher quality workers to compensate for lower quality ones, without explicitly identifying them, by allowing them to observe and react to the input of the collective. In this paper, we present initial work on eliciting this behavior and discuss how it may be possible to leverage self-correction in the crowd for better performance on continuous real-time tasks.
CrowdCamp: rapidly iterating ideas related to collective intelligence & crowdsourcing.	Paul André, Michael S. Bernstein, Mira Dontcheva, Elizabeth Gerber, Aniket Kittur, Rob Miller	chi2012a	The field of collective intelligence - encompassing aspects of crowdsourcing, human computation, and social computing - is having tremendous impact on our lives, and the fields are rapidly growing. We propose a hands-on event that takes the main benefits of a workshop - provocative discussion and community building - and allows time to focus on developing ideas into actual outputs: experiment designs, in-depth thoughts on wicked problems, paper or coded prototypes. We will bring together researchers to discuss future visions and make tangible headway on those visions, as well as seeding collaboration. The outputs from brainstorming, discussion, and building will persist after the workshop for attendees and the community to view, and will be written up.
Guess who?: enriching the social graph through a crowdsourcing game.	Ido Guy, Adam Perer, Tal Daniel, Ohad Greenshpan, Itai Turbahn	chi2011	Despite the tremendous popularity of social network sites both on the web and within enterprises, the relationship information they contain may be often incomplete or outdated. We suggest a novel crowdsourcing approach that uses a game to help enrich and expand the social network topology. The game prompts players to provide the names of people who have a relationship with individuals they know. The game was deployed for a one-month period within a large global organization. We provide an analysis of the data collected through this deployment, in comparison with the data from the organization's social network site. Our results indicate that the game rapidly collects large volumes of valid information that can be used to enrich and reinforce an existing social network site's data. We point out other aspects and benefits of using a crowdsourcing game to harvest social network information.
Cooks or cobblers?: crowd creativity through combination.	Lixiu Yu, Jeffrey V. Nickerson	chi2011	A sketch combination system is introduced and tested: a crowd of 1047 participated in an iterative process of design, evaluation and combination. Specifically, participants in a crowdsourcing marketplace sketched chairs for children. One crowd created a first generation of chairs, and then successive crowds created new generations by combining the chairs made by previous crowds. Other participants evaluated the chairs. The crowd judged the chairs from the third generation more creative than those from the first generation. An analysis of the design evolution shows that participants inherited and modified presented features, and also added new features. These findings suggest that crowd based design processes may be effective, and point the way toward computer-human interactions that might further encourage crowd creativity.
Human computation: a survey and taxonomy of a growing field.	Alexander J. Quinn, Benjamin B. Bederson	chi2011	"The rapid growth of human computation within research and industry has produced many novel ideas aimed at organizing web users to do great things. However, the growth is not adequately supported by a framework with which to understand each new system in the context of the old. We classify human computation systems to help identify parallels between different systems and reveal ""holes"" in the existing work as opportunities for new research. Since human computation is often confused with ""crowdsourcing"" and other terms, we explore the position of human computation with respect to these related topics."
Utility of human-computer interactions: toward a science of preference measurement.	Michael Toomim, Travis Kriplean, Claus Pörtner, James A. Landay	chi2011	The success of a computer system depends upon a user choosing it, but the field of Human-Computer Interaction has little ability to predict this user choice. We present a new method that measures user choice, and quantifies it as a measure of utility. Our method has two core features. First, it introduces an economic definition of utility, one that we can operationalize through economic experiments. Second, we employ a novel method of crowdsourcing that enables the collection of thousands of economic judgments from real users.
Redesign as an act of violence: disrupted interaction patterns and the fragmenting of a social Q&A community.	Rich Gazan	chi2011	The worst-case scenario for the redesign of an established online community is a subsequent mass migration of its core members to other sites. Using data from transaction logs, content analysis and participant observation, this paper presents a descriptive analysis of the fragmentation of a social question answering (Q&A) community in the immediate aftermath of a fundamental redesign, where site-based communication mechanisms no longer functioned. The ways in which the community and its diaspora reacted, reconnected and resettled on other sites provides empirical data to support recent research on the life cycle of online communities. The results suggest that many of the same processes that help social Q&A sites generate content and motivate participation can work to dismantle an established community if communications between members are even temporarily disrupted. Modeling a redesign as an attack on a community can help future designers anticipate alternative paths of communication and information flows.
Design lessons from the fastest q&a site in the west.	Lena Mamykina, Bella Manoim, Manas Mittal, George Hripcsak, Björn Hartmann	chi2011	This paper analyzes a Question & Answer site for programmers, Stack Overflow, that dramatically improves on the utility and performance of Q&A systems for technical domains. Over 92% of Stack Overflow questions about expert topics are answered - in a median time of 11 minutes. Using a mixed methods approach that combines statistical data analysis with user interviews, we seek to understand this success. We argue that it is not primarily due to an a priori superior technical design, but also to the high visibility and daily involvement of the design team within the community they serve. This model of continued community leadership presents challenges to both CSCW systems research as well as to attempts to apply the Stack Overflow model to other specialized knowledge domains.
Data collection by the people, for the people.	Christine Robson, Sean Kandel, Jeffrey Heer, Jeffrey S. Pierce	chi2011a	Data Collection by the People, for the People is a CHI 2011 workshop to explore data from the crowd, bringing together mobile crowdsourcing & participatory urbanism researchers with data analysis and visualization researchers. The workshop is two-day event beginning with day of field work in the city of Vancouver, trying out mobile crowdsourcing applications and data analysis tools. Participants are encouraged to contribute applications and tools which they wish to share. Our goal is to provoke discussion and brainstorming, enabling both data collection researchers and data manipulationanalysis researchers to benefit from mutually learned lessons about crowdsourced data.
Crowdsourcing and human computation: systems, studies and platforms.	Michael S. Bernstein, Ed H. Chi, Lydia B. Chilton, Björn Hartmann, Aniket Kittur, Robert C. Miller	chi2011a	Crowdsourcing and human computation are transforming human-computer interaction, and CHI has led the way. The seminal publication in human computation was initially published in CHI in 2004 [1], and the first paper investigating Mechanical Turk as a user study platform has amassed over one hundred citations in two years [5]. However, we are just beginning to stake out a coherent research agenda for the field. This workshop will bring together researchers in the young field of crowdsourcing and human computation and produce three artifacts: a research agenda for the field, a vision for ideal crowdsourcing platforms, and a group-edited bibliography. These resources will be publically disseminated on the web and evolved and maintained by the community.
Geographic human-computer interaction.	Brent J. Hecht, Johannes Schöning, Thomas Erickson, Reid Priedhorsky	chi2011a	Geography is playing an increasingly important role in areas of HCI ranging from social computing to natural user interfaces. At the same time, research in geography has focused more and more on technology-mediated interaction with spatiotemporal phenomena. Despite the growing popularity of this geographic human-computer interaction (GeoHCI) in both fields, there have been few opportunities for GeoHCI knowledge sharing, knowledge creation or community building in either discipline, let alone between them. The goal of this workshop is thus two-fold. First, we will seek to sum up the state of GeoHCI knowledge and address GeoHCI core issues by inviting prominent researchers in the space to share and discuss the most important high-level findings from their work. Second, through our interdisciplinary organizing committee, we will recruit participants from both fields, with the goal of laying the groundwork for a community that works across intra- and interdisciplinary boundaries.
A crowdsourcing model for receiving design critique.	Anbang Xu, Brian P. Bailey	chi2011a	"Designers in many domains are increasingly turning to online communities to receive critiques of early design ideas. However, members of these communities may not contribute an effective critique due to limited skills, motivation, or time, and therefore many critiques may not go beyond ""I (don't) like it"". We propose a new approach for designers to receive online critique. Our approach is novel because it adopts a theoretical framework for effective critique and implements the framework on a popular crowdsourcing platform. Preliminary results show that our approach allows designers to acquire quality critiques in a timely manner that compare favorably with critiques produced from a well-known online community."
Crowdsourcing suggestions to programming problems for dynamic web development languages.	Dhawal Mujumdar, Manuel Kallenbach, Brandon Liu, Björn Hartmann	chi2011a	Developers increasingly consult online examples and message boards to find solutions to common programming tasks. On the web, finding solutions to debugging problems is harder than searching for working code. Prior research introduced a social recommender system, HelpMeOut, that crowdsources debugging suggestions by presenting fixes to errors that peers have applied in the past. However, HelpMeOut only worked for statically typed, compiled programming languages like Java. We investigate how suggestions can be provided for dynamic, interpreted web development languages. Our primary insight is to instrument test-driven development to collect examples of bug fixes. We present Crowd::Debug, a tool for Ruby programmers that realizes these benefits.
Video summarization via crowdsourcing.	Shao-Yu Wu, Ruck Thawonmas, Kuan-Ta Chen	chi2011a	Although video summarization has been studied extensively, existing schemes are neither lightweight nor generalizable to all types of video content. To generate accurate abstractions of all types of video, we propose a framework called Click2SMRY, which leverages the wisdom of the crowd to generate video summaries with a low workload for workers. The framework is lightweight because workers only need to click a dedicated key when they feel that the video being played is reaching a highlight. One unique feature of the framework is that it can generate different abstraction levels of video summaries according to viewers' preferences in real time. The results of experiments conducted to evaluate the framework demonstrate that it can generate satisfactory summaries for different types of video clips.
CrowdForge: crowdsourcing complex work.	Aniket Kittur, Boris Smus, Robert E. Kraut	chi2011a	Micro-task markets such as Amazon's Mechanical Turk represent a new paradigm for accomplishing work, in which employers can tap into a large population of workers around the globe to accomplish tasks in a fraction of the time and money of more traditional methods. However, such markets typically support only simple, independent tasks, such as labeling an image or judging the relevance of a search result. Here we present a general purpose framework for micro-task markets that provides a scaffolding for more complex human computation tasks which require coordination among many individuals, such as writing an article.
ViewSer: a tool for large-scale remote studies of web search result examination.	Dmitry Lagun, Eugene Agichtein	chi2011a	Web search behavior studies, including eye-tracking studies of search result examination, have resulted in numerous insights to improve search result quality and presentation. Yet, these studies have been severely restricted in scale, due to the expense and effort required. We propose a novel methodology for crowdsourcing web search behavior studies - specifically focusing on performing large-scale studies of result examination behavior. We present a viewport-based examination interface (ViewSer), which enables remotely tracking searcher examination behavior, without requiring eye tracking equipment. We show that ViewSer induces similar viewing and clickthrough behavior, compared to in-lab users monitored with eye tracking, in a study with over 100 remote participants. ViewSer is a first step towards large-scale behavioral evaluation of web search, which would help improve web search result presentation, result ranking, and ultimately improve the web search experience overall.
Turkomatic: automatic recursive task and workflow design for mechanical turk.	Anand Pramod Kulkarni, Matthew Can, Björn Hartmann	chi2011a	Completing complex tasks on crowdsourcing platforms like Mechanical Turk currently requires significant up-front investment into task decomposition and workflow design. We present a new method for automating task and workflow design for high-level, complex tasks. Unlike previous approaches, our strategy is recursive, recruiting workers from the crowd to help plan out how problems can be solved most effectively. Our initial experiments suggest that this strategy can successfully create workflows to solve tasks considered difficult from an AI perspective, although it is highly sensitive to the design choices made by workers.
SAMM: driving asistance system for the senior citizen.	Víctor M. González, Roberto Lapuente Romo, Luis Eduardo Pérez Estrada	chi2011a	Within a frame of possible ubiquitous urban applications this research explores ways to support the mobility of senior citizens in vehicles by finding efficient routes to reach their destination and managing their time. Our solution is called SAMM and operates following a context-aware and crowdsourcing model where data used by the system to optimize routes is both, taken from user's agenda and received from other users in real time. This allows older adults to optimize their trips by doing several things without leaving a main route, and to be aware of traffic jams long before other media report it. It is expected that a ubiquitous urban application like SAMM will be of great benefit for elderly people by increasing trip efficiency and avoiding the problems associated with spending excessive time sitting in a vehicle as well as encouraging them to take an active and independent role in society.
Crowdsourcing graphical perception: using mechanical turk to assess visualization design.	Jeffrey Heer, Michael Bostock	chi2010	Understanding perception is critical to effective visualization design. With its low cost and scalability, crowdsourcing presents an attractive option for evaluating the large design space of visualizations; however, it first requires validation. In this paper, we assess the viability of Amazon's Mechanical Turk as a platform for graphical perception experiments. We replicate previous studies of spatial encoding and luminance contrast and compare our results. We also conduct new experiments on rectangular area perception (as in treemaps or cartograms) and on chart size and gridline spacing. Our results demonstrate that crowdsourced perception experiments are viable and contribute new insights for visualization design. Lastly, we report cost and performance data from our experiments and distill recommendations for the design of crowdsourced studies.
Managing nomadic knowledge: a case study of the European social forum.	Saqib Saeed, Volkmar Pipek, Markus Rohde, Volker Wulf	chi2010	In this paper we portray a specific type of knowledge which we term 'nomadic knowledge'. It is required periodically by different actors and travels along foreseeable paths between groups or communities of actors. This type of knowledge lets us question generally held assumptions about the way knowledge is enacted. We illustrate our point with an ethnographical field study analyzing the European Social Forum (ESF), a network of political activist organizations. In this network different actors organize a periodic (biannual) event in which some 13,000 activists participated in 2008. We investigate how knowledge about organizing and managing the ESF is transferred between two events respectively, the actors and communities involved. Our study highlights the specific challenges in sharing nomadic knowledge and the consequences of deficiencies on the organizing process. The paper contributes to a better understanding of knowledge sharing practices and opens new directions for technical support.
Who are the crowdworkers?: shifting demographics in mechanical turk.	Joel Ross, Lilly Irani, M. Six Silberman, Andrew Zaldivar, Bill Tomlinson	chi2010a	Amazon Mechanical Turk (MTurk) is a crowdsourcing system in which tasks are distributed to a population of thousands of anonymous workers for completion. This system is increasingly popular with researchers and developers. Here we extend previous studies of the demographics and usage behaviors of MTurk workers. We describe how the worker population has changed over time, shifting from a primarily moderate-income, U.S.-based workforce towards an increasingly international group with a significant population of young, well-educated Indian workers. This change in population points to how workers may treat Turking as a full-time job, which they rely on to make ends meet.
Facts or friends?: distinguishing informational and conversational questions in social Q&A sites.	F. Maxwell Harper, Daniel Moy, Joseph A. Konstan	chi2009	"Tens of thousands of questions are asked and answered every day on social question and answer (Q&A) Web sites such as Yahoo Answers. While these sites generate an enormous volume of searchable data, the problem of determining which questions and answers are archival quality has grown. One major component of this problem is the prevalence of conversational questions, identified both by Q&A sites and academic literature as questions that are intended simply to start discussion. For example, a conversational question such as ""do you believe in evolution?"" might successfully engage users in discussion, but probably will not yield a useful web page for users searching for information about evolution. Using data from three popular Q&A sites, we confirm that humans can reliably distinguish between these conversational questions and other informational questions, and present evidence that conversational questions typically have much lower potential archival value than informational questions. Further, we explore the use of machine learning techniques to automatically classify questions as conversational or informational, learning in the process about categorical, linguistic, and social differences between different question types. Our algorithms approach human performance, attaining 89.7% classification accuracy in our experiments."
Stress outsourced: a haptic social network via crowdsourcing.	Keywon Chung, Carnaven Chiu, Xiao Xiao, Pei-Yu (Peggy) Chi	chi2009a	Stress OutSourced (SOS) is a peer-to-peer network that allows anonymous users to send each other therapeutic massages to relieve stress. By applying the emerging concept of crowdsourcing to haptic therapy, SOS brings physical and affective dimensions to our already networked lifestyle while preserving the privacy of its members. This paper first describes the system, its three unique design choices regarding privacy model, combining mobility and scalability, and affective communication for an impersonal crowd, and contrasts them with other efforts in their respective areas. Finally, this paper describes future work and opportunities in the area of haptic social networks.
Crowdsourcing user studies with Mechanical Turk.	Aniket Kittur, Ed H. Chi, Bongwon Suh	chi2008	User studies are important for many aspects of the design process and involve techniques ranging from informal surveys to rigorous laboratory studies. However, the costs involved in engaging users often requires practitioners to trade off between sample size, time requirements, and monetary costs. Micro-task markets, such as Amazon's Mechanical Turk, offer a potential paradigm for engaging a large number of users for low time and monetary costs. Here we investigate the utility of a micro-task market for collecting user measurements, and discuss design considerations for developing remote micro user evaluation tasks. Although micro-task markets have great potential for rapidly collecting user measurements at low costs, we found that special care is needed in formulating tasks in order to harness the capabilities of the approach.
Predictors of answer quality in online Q&A sites.	F. Maxwell Harper, Daphne R. Raban, Sheizaf Rafaeli, Joseph A. Konstan	chi2008	Question and answer (Q&A) sites such as Yahoo! Answers are places where users ask questions and others answer them. In this paper, we investigate predictors of answer quality through a comparative, controlled field study of responses provided across several online Q&A sites. Along with several quantitative results concerning the effects of factors such as question topic and rhetorical strategy, we present two high-level messages. First, you get what you pay for in Q&A sites. Answer quality was typically higher in Google Answers (a fee-based site) than in the free sites we studied, and paying more money for an answer led to better outcomes. Second, we find that a Q&A site's community of users contributes to its success. Yahoo! Answers, a Q&A site where anybody can answer questions, outperformed sites that depend on specific individuals to answer questions, such as library reference services.
Portalis: using competitive online interactions to support aid initiatives for the homeless.	Cheng-Lun Li, Ayse G. Buyuktur, David K. Hutchful, Natasha B. Sant, Satyendra Kumar Nainwal	chi2008a	We designed a web-based system with game-like properties that utilizes crowdsourcing to facilitate the beneficial transfer-of-knowledge to case managers (CMs) working with the homeless. This has two significant impacts: First, Portalis allows CMs to make informed decisions in managing client cases. Second, it enables individuals who would like to volunteer their services but are limited by time constraints to contribute.
Capturing, sharing, and using local place information.	Pamela J. Ludford, Reid Priedhorsky, Ken Reily, Loren G. Terveen	chi2007	With new technology, people can share information about everyday places they go; the resulting data helps others find and evaluate places. Recent applications like Dodgeball and Sharescape repurpose everyday place information: users create local place data for personal use, and the systems display it for public use. We explore both the opportunities -- new local knowledge, and concerns -- privacy risks, raised by this implicit information sharing. We conduct two empirical studies: subjects create place data when using PlaceMail, a location-based reminder system, and elect whether to share it on Sharescape, a community map-building system. We contribute by: (1) showing location-based reminders yield new local knowledge about a variety of places, (2) identifying heuristics people use when deciding what place-related information to share (and their prevalence), (3) detailing how these decision heuristics can inform local knowledge sharing system design, and (4) identifying new uses of shared place information, notably opportunistic errand planning.
Towards computer-supported face-to-face knowledge sharing.	Sho Iwasaki, Yasufumi Hirakawa, Harumi Mase, Eiji Tokunaga, Tatsuo Nakajima	chi2006a	Although a lot of systems provide co-located collaboration support, few existing technologies provide support for fluid knowledge sharing. To fluidly share our knowledge in co-located environments, each person's digital experience should be merged and presented on a collaborative display device such as a face-to-face tabletop display. For capturing concrete requirements for such face-to-face fluid knowledge sharing, we built a prototype system that presents merged multi-users' Web browsing histories on a tabletop display. We experimented with our prototype in an exhibition and collected over 100 filled questionnaires and informal observations.
Knowledge sharing, maintenance, and use in online support communities.	Derek L. Hansen	chi2006a	Widespread adoption of collaborative authoring tools (such as Wikis) by online communities has fostered new ways of storing, sharing, maintaining, and using community knowledge. My dissertation research examines the effect and potential use of these shared knowledge repositories within online technical and medical support communities using short-term ethnography (including content analysis and interviews), surveys, and quantitative analysis of behavior traces. I characterize the key technological mechanisms, and the processes and social norms at play. I then use this knowledge to propose best practices and novel social and technical designs.
