Providing Real-time Assistance for Repairing Runtime Exceptions using Stack Overflow Posts.	Sonal Mahajan, Mukul R. Prasad	icst2022	Runtime Exceptions (REs) are an important class of bugs that occur frequently during code development. Traditional Automatic Program Repair (APR) tools are of limited use in this “in-development” use case, since they require a test-suite to be available as a patching oracle. Thus, developers typically tend to manually resolve their in-development REs, often by referring to technical forums, such as Stack Overflow (SO). To automate this manual process we extend our previous work, MaesTro, to provide real-time assistance to developers for repairing Java REs by recommending a relevant patch-suggesting SO post and synthesizing a repair patch from this post to fix the RE in the developer's code. Maestro exploits a library of Runtime Exception Patterns (REPs) semi-automatically mined from SO posts, through a relatively inexpensive, one-time, incremental process. An REP is an abstracted sequence of statements that triggers a given RE. REPs are used to index SO posts, retrieve a post most relevant to the RE instance exhibited by a developer's code and then mediate the process of extracting a concrete repair from the SO post, abstracting out post-specific details, and concretizing the repair to the developer's buggy code. We evaluate MaesTro on a published RE benchmark comprised of 78 instances. Maestro is able to generate a correct repair patch at the top position in 27% of the cases, within the top-3 in 40% of the cases and overall return a useful artifact in 81% of the cases. Further, the use of REPs proves instrumental to all aspects of Maestro's performance, from ranking and searching of SO posts to synthesizing patches from a given post. In particular, 45% of correct patches generated by MaesTro could not be produced by a baseline technique not using REPs, even when provided with Maestro's SO-post ranking. Maestro is also fast, needing around 1 second, on average, to generate its output. Overall, these results indicate that Maestro can provide effective real-time assistance to developers in repairing REs.
AI-based Test Automation: A Grey Literature Analysis.	Filippo Ricca, Alessandro Marchetto, Andrea Stocco	icst2021w	This paper provides the results of a survey of the grey literature concerning the use of artificial intelligence to improve test automation practices. We surveyed more than 1,200 sources of grey literature (e.g., blogs, white-papers, user manuals, StackOverflow posts) looking for highlights by professionals on how AI is adopted to aid the development and evolution of test code. Ultimately, we filtered 136 relevant documents from which we extracted a taxonomy of problems that AI aims to tackle, along with a taxonomy of AI-enabled solutions to such problems. Manual code development and automated test generation are the most cited problem and solution, respectively. The paper concludes by distilling the six most prevalent tools on the market, along with think-aloud reflections about the current and future status of artificial intelligence for test automation.
Towards a Gamified Equivalent Mutants Detection Platform.	Thomas Laurent, Laura Guillot, Motomichi Toyama, Ross Smith, Dan Bean, Anthony Ventresque	icstw2017	This poster presents a gamified system for equivalent mutants detection. This system can be used as a standalone tool for developers and testing teams alike - but we plan to use this system on a crowdsourcing platform to evaluate the various parameters involved in the detection of equivalent mutants, such as, expertise (coding and testing), familiarity with the code base, complexity of the code and tests, measured likelihood of equivalent mutants.
Guided Test Generation for Finding Worst-Case Stack Usage in Embedded Systems.	Tingting Yu, Myra B. Cohen	icst2015	Embedded systems are challenging to program correctly, because they use an interrupt programming paradigm and run in resource constrained environments. This leads to a class of faults for which we need customized verification techniques. One such class of faults, stack overflows, are caused when the combination of active methods and interrupt invocations on the stack grows too large, and these can lead to data loss and other significant device failures. Developers need to estimate the worst-case stack usage (WCSU) during system design, but determining the actual maximum value is known to be a hard problem. The state of the art for calculating WCSU uses static analysis, however this has a tendency to over approximate the potential stack which can lead to wasted resources. Dynamic techniques such as random testing often under approximate the WCSU. In this paper, we present SIMSTACK, a framework that utilizes a combination of static analysis and a genetic algorithm to search for WCSUs. We perform an empirical study to evaluate the effectiveness of SIMSTACK and show that SIMSTACK is competitive with the WCSU values obtained by static analysis, and improves significantly over a random algorithm. When we use only the genetic algorithm, SIMSTACK performs almost as well as the guided technique, but takes significantly longer to converge on the maximum WCSUs.
Crowdsourcing GUI Tests.	Eelco Dolstra, Raynor Vliegendhart, Johan A. Pouwelse	icst2013	Graphical user interfaces are difficult to test: automated tests are hard to create and maintain, while manual tests are time-consuming, expensive and hard to integrate in a continuous testing process. In this paper, we show that it is possible to crowdsource GUI tests, that is, to outsource them to individuals drawn from a large pool of workers on the Internet, by instantiating virtual machines (VMs) running the system under test and letting testers access the VMs through their web browsers. This enables semi-automated continuous testing of GUIs and usability experiments with large numbers of participants at low cost. Several large experiments on the Amazon Mechanical Turk demonstrate that our approach is technically feasible and sufficiently reliable.
CrowdOracles: Can the Crowd Solve the Oracle Problem?	Fabrizio Pastore, Leonardo Mariani, Gordon Fraser	icst2013	Despite the recent advances in test generation, fully automatic software testing remains a dream: Ultimately, any generated test input depends on a test oracle that determines correctness, and, except for generic properties such as “the program shall not crash”, such oracles require human input in one form or another. CrowdSourcing is a recently popular technique to automate computations that cannot be performed by machines, but only by humans. A problem is split into small chunks, that are then solved by a crowd of users on the Internet. In this paper we investigate whether it is possible to exploit CrowdSourcing to solve the oracle problem: We produce tasks asking users to evaluate CrowdOracles - assertions that reflect the current behavior of the program. If the crowd determines that an assertion does not match the behavior described in the code documentation, then a bug has been found. Our experiments demonstrate that CrowdOracles are a viable solution to automate the oracle problem, yet taming the crowd to get useful results is a difficult task.
Challenges and Solutions in Test Staff Relocations within a Software Consultancy Company.	Daniel Larsson, Håkan Bertilsson, Robert Feldt	icst2008	Test staff in modern software consultancy companies often has to work in multiple projects that differ not only technically, but also from organizational, management and social aspects. The ease and speed with which staff can adapt to new projects and environments is crucial for the success and profitability of the consultancy company. This paper investigates how management in a Swedish software company can facilitate test staff relocation practices. Consultants in the testing department were interviewed to elicit the differences between testing projects they are involved in and their views on the challenges of and learning needed when relocating between projects. Based on this we present an approach to better support such staff relocations. The approach is based on a knowledge sharing process and the introduction of specific templates to capture testing experience. Initial, static validation in the associated company shows that the approach has merit and should be further evaluated.
